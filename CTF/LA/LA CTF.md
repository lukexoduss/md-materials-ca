# Syllabus

## 1. Log File Fundamentals

- Common log file locations
- Log file formats (plaintext, JSON, XML, binary)
- Timestamp formats and time zones
- Log rotation and compression
- Log file permissions and ownership
- System vs application logs

## 2. Linux System Logs

- /var/log/syslog and /var/log/messages
- /var/log/auth.log and /var/log/secure
- /var/log/kern.log
- /var/log/dmesg
- /var/log/boot.log
- systemd journal (journalctl)

## 3. Authentication & Access Logs

- SSH login logs
- Failed authentication attempts
- sudo command logs
- PAM (Pluggable Authentication Modules) logs
- wtmp, utmp, btmp analysis
- lastlog examination
- Login session tracking

## 4. Web Server Logs

- Apache access.log format
- Apache error.log analysis
- Nginx access and error logs
- Combined log format (CLF)
- Custom log formats
- Virtual host logs
- SSL/TLS logs

## 5. Application Logs

- PHP error logs
- Python application logs
- Node.js logs
- Java application logs
- Database query logs
- Custom application logging

## 6. Network Service Logs

- FTP server logs (vsftpd, proftpd)
- SMTP mail server logs
- DNS server logs (BIND, dnsmasq)
- DHCP server logs
- Proxy server logs (Squid)
- VPN logs (OpenVPN, WireGuard)

## 7. Firewall & Security Logs

- iptables/nftables logs
- UFW (Uncomplicated Firewall) logs
- fail2ban logs
- SELinux audit logs
- AppArmor logs
- IDS/IPS logs (Snort, Suricata)

## 8. Database Logs

- MySQL/MariaDB logs
- PostgreSQL logs
- MongoDB logs
- Redis logs
- Query logs
- Slow query logs
- Error logs

## 9. Windows Event Logs

- Security event logs
- System event logs
- Application event logs
- Event ID interpretation
- PowerShell logging
- Windows Defender logs

## 10. Command-Line Tools

- grep, egrep, fgrep
- awk and sed
- cut, sort, uniq
- head, tail, less
- wc (word count)
- find and locate
- strings

## 11. Advanced Parsing Tools

- jq (JSON processor)
- xmllint (XML parser)
- csvkit
- awk advanced patterns
- Python one-liners
- Perl one-liners

## 12. Log Aggregation Tools

- Logstash basics
- Filebeat
- rsyslog configuration
- syslog-ng
- Fluentd
- Graylog

## 13. Timeline Analysis

- Event sequencing
- Time correlation
- Time zone conversion
- Chronological reconstruction
- Gap analysis
- Timestamp normalization

## 14. Pattern Recognition

- Regex patterns
- Anomaly detection
- Frequency analysis
- Baseline establishment
- Outlier identification
- Signature matching

## 15. Attack Pattern Identification

- Brute force attempts
- SQL injection indicators
- XSS attack patterns
- Command injection traces
- Path traversal attempts
- File inclusion patterns
- SSRF indicators

## 16. Network Traffic Logs

- tcpdump log analysis
- Wireshark capture files
- NetFlow data
- Zeek (Bro) logs
- Connection logs
- Protocol analysis

## 17. Incident Response Logs

- Process execution logs
- File access logs
- Registry modifications (Windows)
- Memory dump analysis logs
- Persistence mechanisms
- Lateral movement traces

## 18. Container & Virtualization Logs

- Docker container logs
- Kubernetes logs
- LXC/LXD logs
- VMware logs
- VirtualBox logs
- Container runtime logs

## 19. Cloud Service Logs

- AWS CloudTrail
- Azure Activity Logs
- Google Cloud Audit Logs
- S3 access logs
- Lambda function logs
- API Gateway logs

## 20. Log Encoding & Obfuscation

- Base64 encoded data
- URL encoding
- Unicode encoding
- Hex encoding
- Compression artifacts
- Encrypted log sections

## 21. Log Injection Detection

- Log poisoning
- Log forging
- CRLF injection
- Newline injection
- Log tampering indicators
- Integrity verification

## 22. Statistical Analysis

- Log volume analysis
- Event frequency counting
- Distribution analysis
- Correlation coefficients
- Moving averages
- Percentile calculations

## 23. Visualization Techniques

- gnuplot usage
- matplotlib for log data
- Elastic Stack visualization
- Grafana basics
- Timeline charts
- Heat maps

## 24. Scripting & Automation

- Bash scripting for log analysis
- Python log parsing libraries
- Regular expression building
- Loop constructs for batch processing
- Pipeline chaining
- Output formatting

## 25. Kali Linux Specific Tools

- logwatch
- swatch
- MultiTail
- ccze (colorized log viewer)
- lnav (log navigator)
- angle-grinder (agrind)

## 26. Forensic Log Analysis

- Log artifact preservation
- Chain of custody
- Hash verification
- Write-blocker usage
- Evidence extraction
- Deleted log recovery

## 27. CTF-Specific Techniques

- Flag format recognition
- Hidden data in logs
- Steganography in logs
- Code obfuscation
- Puzzle solving from logs
- Multi-stage challenges

## 28. Log File Formats

- Syslog format (RFC 3164, RFC 5424)
- JSON logs
- CSV logs
- Binary logs
- Windows EVT/EVTX
- Proprietary formats

## 29. Data Extraction

- IP address extraction
- Email extraction
- URL extraction
- Username enumeration
- Credential harvesting
- Hash extraction

## 30. Correlation Analysis

- Multi-source correlation
- User activity tracking
- Session reconstruction
- Attack chain mapping
- Pivot point identification
- Cross-reference techniques

## 31. Performance & Optimization

- Large file handling
- Memory-efficient parsing
- Parallel processing
- Indexed searching
- Caching strategies
- Stream processing

## 32. Output & Reporting

- Summary generation
- Report formatting
- Evidence presentation
- Key finding extraction
- IOC (Indicator of Compromise) lists
- Timeline documentation

## 33. Log Analysis Frameworks

- SIEM concepts
- ELK Stack (Elasticsearch, Logstash, Kibana)
- Splunk basics
- Graylog
- OSSEC
- Wazuh

## 34. Cryptographic Artifacts in Logs

- SSL/TLS handshake logs
- Certificate information
- Cipher suite identification
- Encryption protocol logs
- Key exchange logs
- Hash algorithm usage

## 35. Malware Activity Indicators

- C2 communication patterns
- Backdoor connections
- Persistence mechanisms
- File system modifications
- Registry changes (Windows)
- Scheduled task logs

## 36. Web Application Attack Logs

- OWASP Top 10 indicators
- API abuse patterns
- Rate limiting bypass
- Authentication bypass attempts
- Session hijacking traces
- CSRF attack patterns

## 37. Data Exfiltration Detection

- Large data transfers
- Unusual protocols
- Off-hours activity
- Compression indicators
- Encryption before transfer
- Staging directories

## 38. Troubleshooting & Debugging

- Incomplete logs
- Corrupted log files
- Missing timestamps
- Encoding issues
- Truncated entries
- Circular log buffers

---

**Note**: [Inference] This syllabus is structured for CTF log analysis challenges and assumes availability of standard Kali Linux tools. Specific tool capabilities and log formats may vary by challenge.

---

# Log File Fundamentals

## Common Log File Locations

### Linux/Unix Systems

**System-wide logs:**

- `/var/log/` - Primary log directory for most Linux distributions
- `/var/log/syslog` or `/var/log/messages` - General system activity (Debian/Ubuntu vs RHEL/CentOS)
- `/var/log/auth.log` or `/var/log/secure` - Authentication attempts, sudo usage, SSH sessions
- `/var/log/kern.log` - Kernel messages and hardware events
- `/var/log/dmesg` - Boot-time kernel ring buffer messages
- `/var/log/boot.log` - System boot sequence information

**Service-specific logs:**

- `/var/log/apache2/` or `/var/log/httpd/` - Apache web server logs
    - `access.log` - HTTP requests
    - `error.log` - Server errors and warnings
- `/var/log/nginx/` - Nginx web server logs
- `/var/log/mysql/` or `/var/log/mariadb/` - Database server logs
- `/var/log/postgresql/` - PostgreSQL database logs
- `/var/log/vsftpd.log` - FTP server activity
- `/var/log/mail.log` - Mail server (Postfix, Sendmail, etc.)
- `/var/log/cron` - Scheduled task execution

**Application logs:**

- `/var/log/apt/` - Package management (Debian/Ubuntu)
- `/var/log/yum.log` - Package management (RHEL/CentOS)
- `/var/log/fail2ban.log` - Intrusion prevention service
- `/var/log/ufw.log` - Uncomplicated Firewall logs
- `/var/log/audit/audit.log` - SELinux/audit daemon events

**User-specific:**

- `~/.bash_history` - Bash command history per user
- `~/.zsh_history` - Zsh command history
- `~/.mysql_history` - MySQL client command history
- `~/.python_history` - Python interpreter history

### Windows Systems

**Event Logs (viewed via Event Viewer or `wevtutil`):**

- `C:\Windows\System32\winevt\Logs\`
    - `Security.evtx` - Authentication, privilege use, object access
    - `System.evtx` - System component events, drivers, services
    - `Application.evtx` - Application-specific events
    - `Setup.evtx` - Installation and update events
    - `Forwarded Events.evtx` - Events from remote systems

**Common application logs:**

- `C:\inetpub\logs\LogFiles\` - IIS web server logs
- `C:\Windows\NTDS\` - Active Directory database and logs
- `C:\Program Files\` - Application-specific subdirectories
- `C:\ProgramData\` - Hidden directory with application data/logs

**PowerShell logs:**

- PowerShell operational logs in Event Viewer under `Applications and Services Logs > Microsoft > Windows > PowerShell`
- Transcript logs (if enabled) in user-specified locations

**Forensic artifacts:**

- `C:\Windows\Prefetch\` - Application execution artifacts
- `C:\Windows\Tasks\` or `C:\Windows\System32\Tasks\` - Scheduled tasks
- `C:\$Recycle.Bin\` - Deleted file metadata

### Network Devices

**Common paths (varies by vendor):**

- Cisco IOS: `flash:` or `nvram:` (use `show logging`)
- Juniper: `/var/log/` (accessed via CLI or shell)
- Syslog servers: Centralized logging location (configurable)

### Container/Cloud Environments

- Docker: `docker logs <container_id>` or `/var/lib/docker/containers/<container_id>/*-json.log`
- Kubernetes: `kubectl logs <pod_name>` or node-specific paths
- Cloud providers: AWS CloudWatch, Azure Monitor, GCP Cloud Logging (API/console access)

---

## Log File Formats

### Plaintext Logs

**Standard text format** - Most common in Linux/Unix environments.

**Common delimiters:**

- Space-separated: Traditional syslog format
- Tab-separated: Some application logs
- Custom separators: Pipe (`|`), comma (in non-JSON contexts)

**Example (syslog format):**

```
Oct 28 14:23:45 webserver sshd[1234]: Failed password for invalid user admin from 192.168.1.100 port 52341 ssh2
```

**Structure:** `timestamp hostname service[pid]: message`

**Key characteristics:**

- Human-readable
- Easy to grep/parse with basic tools
- No strict schema enforcement
- Variable field counts depending on message type

**Common parsing tools:**

- `grep`, `awk`, `sed` for pattern matching
- `cut` for field extraction
- `tail -f` for real-time monitoring

### JSON Format

**Structured data format** - Increasingly common in modern applications and centralized logging.

**Example:**

```json
{
  "timestamp": "2025-10-28T14:23:45.123Z",
  "level": "ERROR",
  "source_ip": "192.168.1.100",
  "username": "admin",
  "event": "authentication_failure",
  "service": "sshd",
  "pid": 1234
}
```

**Key characteristics:**

- Structured key-value pairs
- Consistent schema (when enforced)
- Easily parsed programmatically
- Supports nested objects and arrays
- Larger file sizes than plaintext

**Parsing tools:**

- `jq` - Command-line JSON processor (essential for CTF)
    
    ```bash
    cat app.log | jq '.source_ip' | sort | uniq -ccat app.log | jq 'select(.level=="ERROR")'
    ```
    
- `python -m json.tool` - Built-in JSON formatter
- Python `json` module for scripting

### XML Format

**Markup-based format** - Common in Windows Event Logs and some enterprise applications.

**Example:**

```xml
<Event>
  <System>
    <EventID>4625</EventID>
    <TimeCreated SystemTime="2025-10-28T14:23:45.123Z"/>
    <Computer>WORKSTATION01</Computer>
  </System>
  <EventData>
    <Data Name="TargetUserName">admin</Data>
    <Data Name="IpAddress">192.168.1.100</Data>
  </EventData>
</Event>
```

**Key characteristics:**

- Hierarchical structure with tags
- Schema validation possible (XSD)
- Verbose compared to JSON
- Self-documenting with tag names
- Windows Event Logs stored as binary but exported as XML

**Parsing tools:**

- `xmllint` - XML parsing and validation
    
    ```bash
    xmllint --xpath '//EventID/text()' event.xml
    ```
    
- `xmlstarlet` - XML transformation and querying
    
    ```bash
    xmlstarlet sel -t -v "//Data[@Name='TargetUserName']" event.xml
    ```
    
- Python `xml.etree.ElementTree` or `lxml` modules

### Binary Formats

**Non-human-readable formats** - Require specific tools to access.

**Common binary log types:**

1. **Windows Event Logs (.evtx)**
    
    - Binary XML format
    - Indexed for performance
    - **Tools:**
        
        ```bash
        # On Windowswevtutil qe Security /f:textGet-WinEvent -LogName Security# On Linux (parsing exported .evtx)python-evtx dump Security.evtxevtx_dump.py Security.evtx
        ```
        
2. **systemd Journal (binary)**
    
    - `/var/log/journal/` or `/run/log/journal/`
    - **Tool:**
        
        ```bash
        journalctl                    # View all logsjournalctl -u sshd.service   # Specific servicejournalctl --since "1 hour ago"journalctl -o json           # Output as JSON
        ```
        
3. **Database logs**
    
    - Some databases use binary formats for transaction logs
    - Require database-specific tools (e.g., MySQL binlog utilities)
4. **Network packet captures (not traditional logs but relevant)**
    
    - `.pcap`, `.pcapng` files
    - **Tools:** `tcpdump`, `tshark`, `Wireshark`

**CTF considerations:**

- Binary logs may hide clues in raw hex data
- Check file headers: `xxd -l 256 logfile` or `hexdump -C logfile | head`
- Magic bytes identify format: `file logfile`

### Compressed/Archived Logs

**Rotation and archival** - Logs are often compressed to save space.

**Common formats:**

- `.gz` - gzip compression: `zcat logfile.gz` or `gunzip -c`
- `.bz2` - bzip2 compression: `bzcat logfile.bz2`
- `.xz` - xz compression: `xzcat logfile.xz`
- `.tar.gz` / `.tgz` - tar archive with gzip: `tar -xzf archive.tar.gz`

**Rotation naming conventions:**

- `logfile.1`, `logfile.2.gz` - incremental rotation
- `logfile-20251028.gz` - date-based rotation

**Direct searching without extraction:**

```bash
zgrep "pattern" logfile.gz
zcat logfile.gz | grep "pattern"
```

---

## Timestamp Formats and Time Zones

### Common Timestamp Formats

**1. Unix Epoch (Seconds since 1970-01-01 00:00:00 UTC)**

```
1730125425
```

- **Conversion:**
    
    ```bash
    date -d @1730125425# Output: Sun Oct 28 14:23:45 UTC 2025
    ```
    
- **Common in:** APIs, databases, system internals

**2. Unix Epoch (Milliseconds)**

```
1730125425123
```

- **Conversion:**
    
    ```bash
    date -d @$(echo 1730125425123 | awk '{print $1/1000}')
    ```
    

**3. ISO 8601 / RFC 3339**

```
2025-10-28T14:23:45Z
2025-10-28T14:23:45.123Z
2025-10-28T14:23:45+08:00
```

- **Format:** `YYYY-MM-DDTHH:MM:SS[.mmm][TZ]`
- **Time zone indicators:**
    - `Z` = UTC (Zulu time)
    - `+HH:MM` or `-HH:MM` = offset from UTC
- **Common in:** JSON logs, modern applications, APIs

**4. RFC 2822 (Email/HTTP date format)**

```
Sun, 28 Oct 2025 14:23:45 +0000
```

- **Common in:** Email headers, HTTP headers

**5. Syslog format (RFC 3164)**

```
Oct 28 14:23:45
```

- **No year** - assumes current year
- **No time zone** - assumes local system time
- **Critical for CTF:** Must determine the system's time zone configuration

**6. Syslog format (RFC 5424)**

```
2025-10-28T14:23:45.123456+00:00
```

- Includes year, fractional seconds, time zone

**7. Apache/Nginx Common Log Format**

```
[28/Oct/2025:14:23:45 +0000]
```

**8. Windows Event Log**

```
2025-10-28T14:23:45.1234567Z
```

- High precision (100-nanosecond intervals)

**9. Custom/Application-specific**

```
28-10-2025 14:23:45
10/28/2025 2:23:45 PM
```

- Varies widely; may require regex patterns for parsing

### Time Zone Handling

**UTC (Coordinated Universal Time)**

- Reference time zone (offset +00:00)
- Most system logs should use UTC for consistency
- **Verification:**
    
    ```bash
    timedatectl                 # Check system timezonecat /etc/timezone           # Debian/Ubuntuls -l /etc/localtime        # Symlink to zoneinfo
    ```
    

**Time zone database locations:**

- `/usr/share/zoneinfo/` - TZ database files

**Converting between time zones:**

```bash
# Convert UTC to specific timezone
TZ='America/New_York' date -d '2025-10-28 14:23:45 UTC'

# Convert specific timezone to UTC
date -u -d '2025-10-28 14:23:45 EST'
```

**Common CTF pitfalls:**

1. **Logs in different time zones** - Correlating events requires normalization
2. **Missing time zone indicators** - Syslog format doesn't include TZ
3. **Daylight Saving Time (DST)** - Changes UTC offset twice yearly
4. **Log rotation around midnight** - May split events across files in different days

**Time synchronization:**

- NTP (Network Time Protocol) should keep systems synchronized
- Check for time drift: `ntpq -p` or `timedatectl show-timesync`
- Desynchronized clocks complicate log correlation in distributed systems

### Parsing Timestamps in CTF Scenarios

**Using `date` for conversion:**

```bash
# Parse various formats to Unix epoch
date -d "Oct 28 14:23:45" +%s
date -d "2025-10-28T14:23:45Z" +%s

# Parse from epoch to human-readable
date -d @1730125425 "+%Y-%m-%d %H:%M:%S"
```

**Using `awk` for timestamp extraction:**

```bash
# Extract timestamp from syslog
awk '{print $1, $2, $3}' /var/log/syslog

# Convert to sortable format
awk '{print $1, $2, $3}' log | while read ts; do date -d "$ts" "+%s"; done
```

**Python for complex parsing:**

```python
from datetime import datetime
import pytz

# Parse ISO 8601
dt = datetime.fromisoformat("2025-10-28T14:23:45+00:00")

# Convert to Unix epoch
epoch = dt.timestamp()

# Parse custom format
dt = datetime.strptime("28/Oct/2025:14:23:45", "%d/%b/%Y:%H:%M:%S")
```

**Time range filtering:**

```bash
# journalctl
journalctl --since "2025-10-28 14:00:00" --until "2025-10-28 15:00:00"

# Using awk with epoch comparison
awk -v start=$(date -d "2025-10-28 14:00" +%s) -v end=$(date -d "2025-10-28 15:00" +%s) \
  '{ts=mktime($0); if(ts>=start && ts<=end) print}' log
```

### Critical CTF Considerations

1. **Always normalize timestamps to UTC** when correlating multiple log sources
2. **Check for clock skew** between systems - may indicate compromise or misconfiguration
3. **Fractional seconds matter** for precise event ordering in high-frequency systems
4. **Year rollover** - Syslog format may create ambiguity if logs span multiple years
5. **Binary timestamp formats** (Windows Event Logs) store as 64-bit integers requiring conversion

**Recommended approach for CTF:**

- Extract all timestamps to Unix epoch format for uniform comparison
- Create a timeline of all events sorted by normalized timestamp
- Flag any time anomalies (backwards time jumps, large gaps, future timestamps)

---

## Log Rotation and Compression

Log rotation prevents disk exhaustion by archiving and compressing old log entries. Understanding rotation mechanisms is critical for CTF scenarios where you need to access historical data or identify when evidence was rotated out.

### logrotate Configuration

Primary configuration file: `/etc/logrotate.conf` Service-specific configs: `/etc/logrotate.d/`

**Key directives:**

```bash
# View main configuration
cat /etc/logrotate.conf

# Check service-specific rotation rules
ls -la /etc/logrotate.d/
cat /etc/logrotate.d/rsyslog
```

**Common rotation parameters:**

- `daily/weekly/monthly` - rotation frequency
- `rotate [n]` - number of rotations to keep
- `compress` - gzip old logs (creates .gz files)
- `delaycompress` - compress on second rotation cycle
- `missingok` - don't error if log is missing
- `notifempty` - don't rotate empty logs
- `create [mode] [owner] [group]` - permissions for new log
- `postrotate/endscript` - commands after rotation

**Manual rotation trigger:**

```bash
# Force rotation for all configs
logrotate -f /etc/logrotate.conf

# Test rotation without executing
logrotate -d /etc/logrotate.conf

# Force rotation for specific service
logrotate -f /etc/logrotate.d/rsyslog

# Check rotation status
cat /var/lib/logrotate/status
```

### Compression Analysis

Rotated logs typically use gzip compression (`.gz` extension). In CTF scenarios, you must decompress to analyze:

```bash
# View compressed log without extracting
zcat /var/log/auth.log.2.gz
zgrep "Failed password" /var/log/auth.log.*.gz

# Search across all compressed auth logs
zgrep -h "session opened" /var/log/auth.log.*.gz | wc -l

# Extract compressed log
gunzip /var/log/syslog.1.gz

# Extract to stdout (preserve original)
gunzip -c /var/log/syslog.1.gz > syslog.1

# Decompress multiple logs
gunzip /var/log/*.gz
```

**Alternative compression formats:**

```bash
# bzip2 compression (.bz2)
bzcat /var/log/messages.1.bz2
bunzip2 /var/log/messages.1.bz2

# xz compression (.xz)
xzcat /var/log/kern.log.1.xz
unxz /var/log/kern.log.1.xz
```

### Rotation Timing Analysis

Determine when logs were rotated to establish timeline gaps:

```bash
# View rotation timestamps
ls -lht /var/log/auth.log*
stat /var/log/auth.log.1

# Check logrotate execution history
grep logrotate /var/log/syslog

# Identify last rotation time
ls -l --time-style=full-iso /var/log/*.gz
```

## Log File Permissions and Ownership

Incorrect permissions can indicate tampering or misconfigurations exploitable in privilege escalation scenarios.

### Standard Permission Patterns

**Critical system logs:**

```bash
# View permissions for key logs
ls -la /var/log/auth.log      # 640 root:adm (typical)
ls -la /var/log/syslog        # 640 root:adm
ls -la /var/log/kern.log      # 640 root:adm
ls -la /var/log/messages      # 640 root:adm

# Check Apache/Nginx logs
ls -la /var/log/apache2/      # 640 root:adm or 644 root:root
ls -la /var/log/nginx/        # 640 root:adm

# Application logs (varies)
ls -la /var/log/mysql/        # Often 640 mysql:adm
```

**Expected ownership patterns:**

- System logs: `root:adm` or `root:root`
- Service logs: `[service_user]:[adm|service_group]`
- Application logs: Application user with appropriate group

### Permission Enumeration Commands

```bash
# Recursively list all log permissions
find /var/log -type f -exec ls -la {} \; 2>/dev/null

# Find world-readable logs (potential info disclosure)
find /var/log -type f -perm -004 2>/dev/null

# Find world-writable logs (tampering risk)
find /var/log -type f -perm -002 2>/dev/null

# Find logs owned by non-root users
find /var/log -type f ! -user root -ls 2>/dev/null

# Check for SUID/SGID on log files (unusual)
find /var/log -type f \( -perm -4000 -o -perm -2000 \) 2>/dev/null

# Find logs readable by current user
find /var/log -type f -readable 2>/dev/null
```

### Group Membership Analysis

The `adm` group traditionally provides read access to logs:

```bash
# Check current user's groups
id
groups

# List all users in adm group
getent group adm

# Check if specific user can read logs
sudo -u [username] cat /var/log/auth.log 2>&1

# Enumerate group memberships for privilege escalation
for user in $(cut -d: -f1 /etc/passwd); do 
    groups $user | grep -q adm && echo "$user is in adm group"
done
```

### Permission Modification Detection

```bash
# Check for recent permission changes
find /var/log -type f -mmin -60 -ls   # Modified in last 60 min

# Compare against known-good baseline
stat /var/log/auth.log
getfacl /var/log/auth.log

# Check for extended attributes (rare on logs)
lsattr /var/log/auth.log

# Review audit logs for permission changes
ausearch -m CHMOD -ts recent 2>/dev/null
```

## System vs Application Logs

Understanding log categories helps prioritize analysis during time-constrained CTF scenarios.

### System Logs (Linux)

Located in `/var/log/`, managed by `rsyslog` or `systemd-journald`:

**Primary system logs:**

```bash
# Authentication events (SSH, sudo, login attempts)
/var/log/auth.log          # Debian/Ubuntu
/var/log/secure            # RHEL/CentOS

# General system messages
/var/log/syslog            # Debian/Ubuntu  
/var/log/messages          # RHEL/CentOS

# Kernel ring buffer
/var/log/kern.log
dmesg | less

# Boot messages
/var/log/boot.log

# Cron job execution
/var/log/cron
```

**Quick access commands:**

```bash
# View auth events in real-time
tail -f /var/log/auth.log

# Filter SSH connections
grep sshd /var/log/auth.log

# Last 50 sudo commands
grep sudo /var/log/auth.log | tail -50

# System errors only
grep -i error /var/log/syslog
```

### systemd Journal

Modern systems use `journalctl` for centralized logging:

```bash
# View all logs
journalctl

# Logs since boot
journalctl -b

# Logs from previous boot
journalctl -b -1

# Follow logs (real-time)
journalctl -f

# Filter by service
journalctl -u ssh.service
journalctl -u apache2.service

# Filter by priority
journalctl -p err           # Errors only
journalctl -p warning       # Warnings and above

# Time range filtering
journalctl --since "2024-01-01 00:00:00"
journalctl --since "1 hour ago"
journalctl --since today

# Combine filters
journalctl -u ssh.service --since "10 minutes ago" -p err

# Export to file for analysis
journalctl -u ssh.service > ssh_logs.txt
```

**Journal persistence check:**

```bash
# Verify journal storage location
ls -la /var/log/journal/

# Check journal size
journalctl --disk-usage

# View journal configuration
cat /etc/systemd/journald.conf
```

### Application Logs

Application-specific logs often contain exploitation artifacts:

**Web servers:**

```bash
# Apache
/var/log/apache2/access.log     # HTTP requests
/var/log/apache2/error.log      # Server errors
/var/log/apache2/other_vhosts_access.log

# Nginx  
/var/log/nginx/access.log
/var/log/nginx/error.log

# Quick analysis
tail -1000 /var/log/apache2/access.log | grep -E "\.php|\.jsp|\.asp"
```

**Databases:**

```bash
# MySQL/MariaDB
/var/log/mysql/error.log
/var/log/mysql/mysql.log
/var/log/mysql/slow-query.log

# PostgreSQL
/var/log/postgresql/postgresql-[version]-main.log

# Check for SQL injection attempts (basic)
grep -iE "union|select.*from|insert.*into" /var/log/mysql/mysql.log
```

**Mail servers:**

```bash
# Postfix/Sendmail
/var/log/mail.log
/var/log/mail.err
/var/log/mail.warn
```

**FTP:**

```bash
# vsftpd
/var/log/vsftpd.log

# ProFTPD
/var/log/proftpd/proftpd.log
```

### Application vs System Log Differentiation

**System logs characteristics:**

- Managed by syslog daemon or journald
- Standard facilities (auth, kern, daemon, user)
- Centralized location (`/var/log/`)
- Root or adm group ownership
- Follow syslog format conventions

**Application logs characteristics:**

- Application-specific format
- Often in application subdirectories (`/var/log/[app]/`)
- May use custom rotation mechanisms
- Application user ownership
- Format varies by software (Apache combined format, JSON, custom)

```bash
# Identify log type by format
head -5 /var/log/syslog         # Syslog format: timestamp hostname process[pid]: message
head -5 /var/log/apache2/access.log  # Apache combined format

# Find application-specific log directories
find /var/log -type d -maxdepth 1 | grep -v "^/var/log$"

# Identify active log writers
lsof +D /var/log | grep -v "DIR"
```

### CTF-Specific Log Priority

**High-value targets for CTF scenarios:**

1. `/var/log/auth.log` - privilege escalation, lateral movement
2. Web server access logs - injection payloads, enumeration
3. Application error logs - verbose errors exposing paths/configs
4. Command history (not traditional logs): `~/.bash_history`, `~/.zsh_history`
5. `/var/log/syslog` - service starts/stops, system events

**Critical but often overlooked:**

```bash
# Package manager logs (software changes)
/var/log/apt/history.log        # Debian/Ubuntu
/var/log/dpkg.log
/var/log/yum.log                # RHEL/CentOS

# User activity logs
/var/log/wtmp                   # Login records (binary)
/var/log/btmp                   # Failed logins (binary)
/var/log/lastlog                # Last login per user (binary)

# Parse binary logs
last -f /var/log/wtmp
lastb -f /var/log/btmp
lastlog
```

---

# Linux System Logs

## /var/log/syslog and /var/log/messages

### Overview

`/var/log/syslog` (Debian/Ubuntu-based) and `/var/log/messages` (RHEL/CentOS-based) serve as centralized system activity logs capturing daemon messages, system events, and general operational data. In CTF scenarios, these logs reveal service startups, network configuration changes, cron job executions, and system-level anomalies that may indicate exploitation attempts or misconfigurations.

### File Structure

Both files follow standard syslog format:

```
timestamp hostname process[PID]: message
```

Example:

```
Oct 28 14:23:45 ctf-box systemd[1]: Started Session 5 of user ctfplayer.
```

### Key Analysis Techniques

**Real-time Monitoring:**

```bash
tail -f /var/log/syslog
```

**Filtering by Time Range:**

```bash
grep "Oct 28 14:" /var/log/syslog
sed -n '/Oct 28 14:00/,/Oct 28 15:00/p' /var/log/syslog
```

**Service-Specific Extraction:**

```bash
grep "sshd" /var/log/syslog
grep -E "apache2|nginx|httpd" /var/log/syslog
```

**Pattern Detection for Exploitation Indicators:**

```bash
# Failed service starts (potential exploitation aftermath)
grep -i "failed" /var/log/syslog

# Privilege escalation artifacts
grep -E "sudo|su\[" /var/log/syslog

# Network service restarts (possible config manipulation)
grep -i "restart" /var/log/syslog | grep -E "network|ssh|http"
```

### CTF-Specific Analysis Patterns

**Identify Service Dependencies:**

```bash
# Services that started around a specific timestamp
awk '/Oct 28 14:2[0-9]/ && /Started|Stopped/' /var/log/syslog
```

**Detect Scheduled Task Activity:**

```bash
grep -i "cron" /var/log/syslog
grep "CRON" /var/log/syslog | awk '{print $6,$9}' | sort | uniq -c
```

**Extract Error Messages:**

```bash
grep -i "error\|fail\|critical" /var/log/syslog | less
```

**Correlation with System Boot:**

```bash
# Find boot sequence messages
grep "kernel:" /var/log/syslog | head -n 50

# Identify services started post-boot
awk '/systemd\[1\]: Started/ {print $0}' /var/log/syslog
```

### Advanced Filtering with awk

**Extract Specific Fields:**

```bash
# Show only timestamp, hostname, and message
awk '{print $1, $2, $3, $5, $6, $7}' /var/log/syslog

# Isolate messages from specific hour
awk '$3 ~ /^14:/' /var/log/syslog
```

**Process-Based Analysis:**

```bash
# Count messages per process
awk '{print $5}' /var/log/syslog | sort | uniq -c | sort -rn | head -20
```

### Integration with Other Tools

**Combine with journalctl (systemd systems):**

```bash
# Cross-reference syslog with journal
journalctl --since "14:00" --until "15:00" -u sshd.service

# Compare syslog entries with systemd journal
diff <(grep "sshd" /var/log/syslog) <(journalctl -u sshd -o short-precise --no-pager)
```

**Log Rotation Awareness:**

```bash
# Check rotated logs
zgrep "pattern" /var/log/syslog.*.gz

# Search across current and rotated logs
for log in /var/log/syslog*; do
    if [[ $log == *.gz ]]; then
        zgrep "searchterm" "$log"
    else
        grep "searchterm" "$log"
    fi
done
```

---

## /var/log/auth.log and /var/log/secure

### Overview

`/var/log/auth.log` (Debian/Ubuntu) and `/var/log/secure` (RHEL/CentOS) record authentication events, including SSH logins, sudo usage, user authentication attempts, and PAM (Pluggable Authentication Modules) activity. These are critical for identifying privilege escalation, brute-force attempts, and unauthorized access in CTF challenges.

### Authentication Event Categories

**SSH Authentication:**

```bash
# Successful SSH logins
grep "Accepted" /var/log/auth.log

# Failed SSH attempts
grep "Failed password" /var/log/auth.log

# SSH key authentication
grep "Accepted publickey" /var/log/auth.log
```

**Sudo Activity:**

```bash
# All sudo commands executed
grep "sudo:" /var/log/auth.log

# Specific user sudo history
grep "sudo:.*USER=root" /var/log/auth.log

# Failed sudo attempts
grep "sudo:.*incorrect password" /var/log/auth.log
```

**User Authentication (su, login, etc.):**

```bash
# Direct root login attempts
grep "su\[" /var/log/auth.log | grep "root"

# Session opened/closed
grep "session opened" /var/log/auth.log
```

### Brute-Force Detection

**Identify Multiple Failed Attempts:**

```bash
# Count failed password attempts by IP
grep "Failed password" /var/log/auth.log | awk '{print $(NF-3)}' | sort | uniq -c | sort -rn

# Extract usernames targeted in attacks
grep "Failed password" /var/log/auth.log | awk '{print $9}' | sort | uniq -c | sort -rn

# Timeline of failed attempts for specific user
grep "Failed password for ctfuser" /var/log/auth.log | awk '{print $1, $2, $3}'
```

**Successful Login After Multiple Failures:**

```bash
# Potential successful brute-force
IP="192.168.1.100"
grep "$IP" /var/log/auth.log | grep -E "Failed|Accepted"
```

### Privilege Escalation Detection

**Sudo Command Analysis:**

```bash
# Extract full command with user context
grep "COMMAND=" /var/log/auth.log | awk -F'COMMAND=' '{print $2}'

# Commands executed as root
grep "COMMAND=" /var/log/auth.log | grep "USER=root"

# Suspicious binary executions
grep "COMMAND=" /var/log/auth.log | grep -E "/tmp|/dev/shm|wget|curl|nc|bash -i"
```

**User Switching Activity:**

```bash
# su attempts with context
grep "su:" /var/log/auth.log | grep -v "session"

# Successful su to root
grep "su:.*session opened for user root" /var/log/auth.log
```

### SSH Key Forensics

**Extract Key Fingerprints:**

```bash
grep "Accepted publickey" /var/log/auth.log | awk '{print $9, $11}'

# Identify key-based logins by user
grep "Accepted publickey" /var/log/auth.log | awk '{print $1, $2, $3, $9}' | sort
```

**Detect Key Addition Events:**

```bash
# This requires correlation with syslog/messages
grep -E "authorized_keys|ssh-keygen" /var/log/syslog
```

### Session Tracking

**Map User Sessions:**

```bash
# Extract session IDs
grep "pam_unix.*session" /var/log/auth.log | awk '{print $3, $11, $13, $15}'

# Session duration analysis [Inference - requires manual calculation]
grep "session opened for user ctfuser" /var/log/auth.log
grep "session closed for user ctfuser" /var/log/auth.log
```

**TTY Analysis:**

```bash
# Identify physical console logins vs remote
grep "session opened" /var/log/auth.log | grep -E "tty[0-9]|pts"
```

### PAM Module Activity

**Authentication Module Failures:**

```bash
grep "pam_" /var/log/auth.log | grep -i "fail\|error\|denied"

# Specific PAM module analysis
grep "pam_unix" /var/log/auth.log
grep "pam_sss" /var/log/auth.log  # SSSD for LDAP/AD
```

### CTF-Specific Patterns

**Backdoor User Detection:**

```bash
# Recently created users (requires cross-referencing /etc/passwd)
grep "useradd\|adduser" /var/log/auth.log

# Users with UID 0 (root equivalent)
awk -F: '$3 == 0 {print $1}' /etc/passwd
```

**Credential Stuffing Indicators:**

```bash
# Multiple usernames from same IP
grep "Failed password" /var/log/auth.log | awk '{print $(NF-3), $9}' | sort | uniq
```

**Anomalous Authentication Times:**

```bash
# Logins during unusual hours (example: 2-5 AM)
grep "Accepted" /var/log/auth.log | awk '$3 ~ /^0[2-5]:/'
```

### Advanced Analysis with awk and sed

**Extract Failed Login Statistics:**

```bash
awk '/Failed password/ {users[$9]++} END {for (u in users) print users[u], u}' /var/log/auth.log | sort -rn
```

**Timeline Reconstruction:**

```bash
# Format: Time | Event | User | IP
grep -E "Accepted|Failed" /var/log/auth.log | awk '{print $1" "$2" "$3, $0}' | sort
```

---

## /var/log/kern.log

### Overview

`/var/log/kern.log` contains kernel-level messages including hardware events, driver loading/unloading, firewall (netfilter/iptables) logs, USB device activity, and kernel panics. In CTF contexts, this log reveals kernel exploits, rootkit loading, network filtering events, and hardware-based attacks.

### Kernel Message Categories

**Hardware Events:**

```bash
# USB device connections
grep -i "usb" /var/log/kern.log

# Disk events
grep -E "sd[a-z]|nvme" /var/log/kern.log

# Network interface changes
grep -E "eth[0-9]|wlan[0-9]|link" /var/log/kern.log
```

**Module Loading:**

```bash
# Loaded kernel modules
grep "module" /var/log/kern.log | grep -i "load"

# Specific module tracking
grep "module_name" /var/log/kern.log

# Module removal
grep "module" /var/log/kern.log | grep -i "unload\|remov"
```

### Firewall and Network Filtering

**iptables/netfilter Logs:**

```bash
# Blocked connections (requires logging rules configured)
grep "UFW BLOCK" /var/log/kern.log
grep "iptables:" /var/log/kern.log

# Extract blocked IPs
grep "UFW BLOCK" /var/log/kern.log | awk '{for(i=1;i<=NF;i++) if($i~/SRC=/) print $i}' | cut -d'=' -f2 | sort | uniq -c | sort -rn

# Blocked ports
grep "UFW BLOCK" /var/log/kern.log | awk '{for(i=1;i<=NF;i++) if($i~/DPT=/) print $i}' | cut -d'=' -f2 | sort | uniq -c | sort -rn
```

**Connection Tracking:**

```bash
# NAT/masquerading events
grep -i "nat\|masq" /var/log/kern.log

# Connection state messages
grep "conntrack" /var/log/kern.log
```

### Exploit Detection Indicators

**Segmentation Faults (Potential Exploits):**

```bash
# Crashed processes
grep "segfault" /var/log/kern.log

# Extract crashing binary and address
grep "segfault" /var/log/kern.log | awk '{print $NF}' | sort | uniq -c | sort -rn
```

**Out of Memory Conditions:**

```bash
# OOM killer activity
grep -i "out of memory" /var/log/kern.log
grep "oom-killer" /var/log/kern.log

# Processes killed by OOM
grep "Killed process" /var/log/kern.log
```

**Kernel Panic Analysis:**

```bash
# Panic messages
grep -i "panic\|oops" /var/log/kern.log

# Extract panic context [Inference - manual analysis needed]
sed -n '/panic/,+20p' /var/log/kern.log
```

### Rootkit Detection

**Suspicious Module Activity:**

```bash
# Modules loaded from unusual paths
grep "insmod\|modprobe" /var/log/kern.log | grep -v "/lib/modules"

# Hidden module detection (requires correlation with lsmod)
lsmod | awk '{print $1}' > /tmp/loaded_modules.txt
grep "module.*loaded" /var/log/kern.log | awk '{print $(NF-1)}' | sort | uniq > /tmp/kern_modules.txt
diff /tmp/loaded_modules.txt /tmp/kern_modules.txt
```

**Kernel Address Space Warnings:**

```bash
# KASLR/SMEP/SMAP violations
grep -E "KASLR|SMEP|SMAP" /var/log/kern.log
```

### USB Device Forensics

**Device Connection Timeline:**

```bash
# USB insertion events
grep "New USB device" /var/log/kern.log

# Extract device identifiers
grep "idVendor\|idProduct" /var/log/kern.log

# Mass storage device mounting
grep "USB Mass Storage" /var/log/kern.log
```

**HID Device Analysis:**

```bash
# Keyboard/mouse connections (potential USB Rubber Ducky)
grep "USB HID" /var/log/kern.log
grep -E "keyboard|mouse" /var/log/kern.log
```

### Network Interface Events

**Interface State Changes:**

```bash
# Link up/down events
grep "link" /var/log/kern.log | grep -E "up|down"

# Promiscuous mode (potential sniffing)
grep "promiscuous" /var/log/kern.log
```

**Driver Loading:**

```bash
# Network driver initialization
grep -E "e1000|rtl8139|ath9k" /var/log/kern.log
```

### Disk and Filesystem Events

**Mount/Unmount Operations:**

```bash
grep -E "mount|umount" /var/log/kern.log

# Filesystem errors
grep -i "ext4\|xfs\|btrfs" /var/log/kern.log | grep -i "error"
```

**SMART Errors:**

```bash
# Disk health warnings
grep -i "smart\|ata.*error" /var/log/kern.log
```

### Advanced Filtering Techniques

**Time-Correlated Event Extraction:**

```bash
# Events within 5 minutes of a specific timestamp
TIMESTAMP="Oct 28 14:30"
grep "$TIMESTAMP" /var/log/kern.log
awk -v start="14:25" -v end="14:35" '$3 >= start && $3 <= end' /var/log/kern.log
```

**Kernel Ring Buffer Cross-Reference:**

```bash
# Compare kern.log with current kernel messages
dmesg -T > /tmp/dmesg_current.txt
diff <(tail -100 /var/log/kern.log) /tmp/dmesg_current.txt
```

**Rate-Based Anomaly Detection:**

```bash
# Messages per minute (high rate = potential DoS/exploit)
awk '{print $1, $2, substr($3,1,5)}' /var/log/kern.log | uniq -c | sort -rn | head -20
```

### CTF Log Correlation Strategy

When analyzing these three log sources together:

```bash
# Cross-reference authentication with kernel events
AUTH_TIME=$(grep "Accepted.*sshd" /var/log/auth.log | tail -1 | awk '{print $1, $2, $3}')
grep "$AUTH_TIME" /var/log/kern.log

# Find system events during suspicious sudo commands
SUDO_TIME=$(grep "COMMAND=/suspicious/path" /var/log/auth.log | awk '{print $1, $2, $3}')
grep "$SUDO_TIME" /var/log/syslog
grep "$SUDO_TIME" /var/log/kern.log

# Timeline reconstruction across all logs
cat <(awk '{print $1, $2, $3, "AUTH:", $0}' /var/log/auth.log) \
    <(awk '{print $1, $2, $3, "SYSLOG:", $0}' /var/log/syslog) \
    <(awk '{print $1, $2, $3, "KERN:", $0}' /var/log/kern.log) | sort
```

**Important Note:** [Inference] Log correlation techniques assume synchronized system clocks. Clock skew may require adjustment. [Unverified] Advanced rootkits may manipulate log entries; always correlate with other forensic artifacts like filesystem timestamps and process memory.

---

## /var/log/dmesg

The kernel ring buffer log capturing hardware initialization, driver loading, and kernel-level events from boot and runtime.

### Direct File Access

```bash
# Read entire dmesg log
cat /var/log/dmesg

# View with pagination
less /var/log/dmesg

# Search for specific hardware
grep -i "usb" /var/log/dmesg
```

### Live Kernel Buffer Access

```bash
# Display current kernel ring buffer
dmesg

# Human-readable timestamps
dmesg -T

# Follow new kernel messages in real-time
dmesg -w

# Show only error and warning messages
dmesg -l err,warn

# Display with facility and level information
dmesg -x

# Clear the ring buffer (requires root)
dmesg -c
```

### CTF-Relevant Filtering

```bash
# Hardware detection (USB devices inserted)
dmesg | grep -i usb

# Network interface changes
dmesg | grep eth0

# Disk/storage events
dmesg | grep -i "sd[a-z]"

# Specific time range (last 100 lines)
dmesg | tail -100

# Parse for suspicious kernel module loading
dmesg | grep "module"
```

### Forensic Analysis Patterns

```bash
# Detect hardware keystroke loggers or USB devices
dmesg | grep -E "USB|HID|input"

# Identify storage device connections (potential data exfiltration)
dmesg -T | grep -E "sd[a-z]|Attached SCSI"

# Look for out-of-memory kills (compromise indicators)
dmesg | grep -i "killed process"

# Check for segmentation faults from exploits
dmesg | grep "segfault"
```

### Output Format Options

```bash
# JSON output (useful for scripting)
dmesg -J

# Raw binary format
dmesg --raw

# Specific facility (kern, user, mail, daemon, etc.)
dmesg -f daemon
```

**Note**: The ring buffer has limited size (typically 256KB-1MB). Old messages are overwritten. The `/var/log/dmesg` file captures boot-time buffer only and is not updated during runtime.

---

## /var/log/boot.log

System service startup log capturing init system messages, service failures, and boot sequence events.

### File Location Variations

```bash
# Standard Red Hat/CentOS/Fedora location
/var/log/boot.log

# Debian/Ubuntu often use
/var/log/boot

# Check which exists
ls -lh /var/log/boot*
```

### Basic Analysis

```bash
# View complete boot log
cat /var/log/boot.log

# Check most recent boot
less /var/log/boot.log

# Find failed services
grep -i "fail" /var/log/boot.log

# Search for specific service
grep "sshd" /var/log/boot.log
```

### Service Status Extraction

```bash
# Extract all OK/FAILED statuses
grep -E "\[.*OK.*\]|\[.*FAILED.*\]" /var/log/boot.log

# List only failed services
grep "FAILED" /var/log/boot.log | awk '{print $NF}'

# Count failures
grep -c "FAILED" /var/log/boot.log
```

### Multi-Boot Analysis

```bash
# View previous boot logs (if logrotate enabled)
cat /var/log/boot.log.1
zcat /var/log/boot.log.2.gz

# Compare current vs previous boot
diff /var/log/boot.log /var/log/boot.log.1
```

### CTF Compromise Indicators

```bash
# Check for suspicious service failures
grep -i -E "fail|error|denied" /var/log/boot.log

# Identify unexpected service starts
grep -i "start" /var/log/boot.log

# Look for persistence mechanisms at boot
grep -E "rc\.local|init\.d" /var/log/boot.log
```

**Note**: Not all distributions populate boot.log extensively. Systemd-based systems primarily use journalctl for boot logging.

---

## systemd journal (journalctl)

The systemd journal provides centralized logging for all system services, kernel messages, and user sessions with structured, indexed binary logs.

### Basic journalctl Commands

```bash
# View all logs (paginated)
journalctl

# Show most recent entries
journalctl -n 50

# Follow live log stream
journalctl -f

# Reverse order (newest first)
journalctl -r

# Show from current boot only
journalctl -b

# Show from previous boot
journalctl -b -1
```

### Time-Based Filtering

```bash
# Since specific timestamp
journalctl --since "2025-10-28 14:30:00"

# Last hour
journalctl --since "1 hour ago"

# Specific time range
journalctl --since "2025-10-28 00:00:00" --until "2025-10-28 23:59:59"

# Yesterday's logs
journalctl --since yesterday --until today

# Last 30 minutes
journalctl --since "30 min ago"
```

### Unit/Service Filtering

```bash
# Logs for specific service
journalctl -u sshd

# Multiple services
journalctl -u sshd -u apache2

# Kernel messages only
journalctl -k

# Follow specific service
journalctl -u nginx -f

# Show service status with last log entries
journalctl -u sshd -n 20 --no-pager
```

### Priority Level Filtering

```bash
# Show only errors and above
journalctl -p err

# Warning level and above
journalctl -p warning

# Critical messages
journalctl -p crit

# Priority levels: emerg(0), alert(1), crit(2), err(3), warning(4), notice(5), info(6), debug(7)
journalctl -p 3
```

### Output Format Options

```bash
# JSON output
journalctl -o json

# JSON-pretty (readable)
journalctl -o json-pretty

# Short format (traditional syslog)
journalctl -o short

# Verbose (all fields)
journalctl -o verbose

# Export binary format
journalctl -o export

# Cat (no metadata)
journalctl -o cat
```

### User and Process Filtering

```bash
# Logs from specific user
journalctl _UID=1000

# Logs from specific process
journalctl _PID=1234

# Logs from executable path
journalctl /usr/bin/sshd

# Combine filters
journalctl _UID=1000 _COMM=bash
```

### Disk Usage and Maintenance

```bash
# Show journal disk usage
journalctl --disk-usage

# Verify journal file integrity
journalctl --verify

# Rotate journal files
journalctl --rotate

# Vacuum by size
journalctl --vacuum-size=500M

# Vacuum by time
journalctl --vacuum-time=7d

# Remove old archived journals
journalctl --vacuum-files=5
```

### Boot Analysis

```bash
# List all boots
journalctl --list-boots

# View specific boot by number
journalctl -b 0     # current
journalctl -b -1    # previous
journalctl -b -2    # two boots ago

# Boot with time range
journalctl -b --since "10 min ago"
```

### Advanced Filtering

```bash
# Grep with context
journalctl -u sshd | grep "Failed password"

# Field-based filtering
journalctl _SYSTEMD_UNIT=sshd.service _PID=1234

# Combine multiple criteria
journalctl -u sshd -p err --since today

# Case-insensitive grep
journalctl -u apache2 | grep -i error

# Show only entries with specific field
journalctl -o verbose | grep "FIELD_NAME="
```

### CTF Exploitation Scenarios

```bash
# Detect SSH brute force attempts
journalctl -u sshd | grep "Failed password"

# Identify privilege escalation
journalctl | grep -i "sudo\|su:"

# Find authentication failures
journalctl _COMM=su | grep "FAILED"

# Detect service crashes (potential exploits)
journalctl -p err | grep -E "segfault|core dump"

# Track user session activity
journalctl _SYSTEMD_UNIT=systemd-logind.service

# Find UID 0 (root) activity
journalctl _UID=0 --since today

# Identify network service authentication
journalctl -u ssh* -u ftp* -u telnet* | grep -i "auth"

# Detect suspicious systemd unit creation
journalctl | grep -E "Started.*\.service" --since "1 hour ago"
```

### Persistent Journal Configuration

```bash
# Location of journal files
ls -lh /var/log/journal/
ls -lh /run/log/journal/  # volatile

# Enable persistent storage
sudo mkdir -p /var/log/journal
sudo systemd-tmpfiles --create --prefix /var/log/journal
sudo systemctl restart systemd-journald

# Configure retention in /etc/systemd/journald.conf
# SystemMaxUse=500M
# MaxRetentionSec=7day
```

### Export and Backup

```bash
# Export to file
journalctl > journal_dump.txt

# Export specific timeframe
journalctl --since "2025-10-28" --until "2025-10-29" > oct28_logs.txt

# Binary export
journalctl -o export > journal.export

# Compressed backup
journalctl | gzip > journal_backup.gz
```

### Real-Time Monitoring Patterns

```bash
# Monitor authentication attempts
journalctl -f | grep -E "sshd|login|su|sudo"

# Watch for errors across all services
journalctl -f -p err

# Track specific user activity
journalctl -f _UID=1000

# Monitor kernel messages
journalctl -f -k

# Multiple services with color
journalctl -f -u sshd -u apache2 --output=short-precise
```

### Journal Field Reference

Common fields for filtering:

- `_PID`: Process ID
- `_UID`: User ID
- `_GID`: Group ID
- `_COMM`: Command name
- `_EXE`: Executable path
- `_SYSTEMD_UNIT`: Systemd unit name
- `_HOSTNAME`: Hostname
- `_TRANSPORT`: Log transport mechanism
- `MESSAGE`: Log message content
- `PRIORITY`: Syslog priority level

---

## Key Differences Between Log Sources

|Feature|dmesg|boot.log|journalctl|
|---|---|---|---|
|**Scope**|Kernel only|Init/boot services|All system logs|
|**Persistence**|Ring buffer (volatile)|File-based|Binary journal|
|**Boot Coverage**|Hardware/kernel init|Service startup|Complete boot process|
|**Runtime Updates**|Yes (live kernel)|No (boot only)|Yes (continuous)|
|**Structured Data**|No|No|Yes (indexed fields)|
|**Timestamp Accuracy**|Relative/absolute|Service-dependent|Microsecond precision|

**Related Topics**: Application-specific logs (/var/log/apache2, /var/log/nginx), authentication logs (/var/log/auth.log, /var/log/secure), syslog configuration (/etc/rsyslog.conf), log rotation (logrotate), centralized logging (syslog-ng, rsyslog forwarding)

---

# Authentication & Access Logs

Authentication and access logs represent the first line of defense in identifying unauthorized access attempts, privilege escalation, and lateral movement in compromised systems. In CTF environments, these logs frequently contain critical evidence of attack paths, credential usage patterns, and backdoor establishment.

## SSH Login Logs

SSH authentication events are recorded across multiple log files depending on your Linux distribution. On Debian-based systems (including Kali), the primary location is `/var/log/auth.log`, while Red Hat-based systems use `/var/log/secure`.

### Primary Log Locations

```bash
# Debian/Ubuntu/Kali
tail -f /var/log/auth.log

# RHEL/CentOS/Fedora
tail -f /var/log/secure

# Generic systemd approach (works across distributions)
journalctl -u ssh -f
journalctl -u sshd -f
```

### Identifying Successful SSH Logins

```bash
# View all successful SSH logins
grep "Accepted" /var/log/auth.log

# Successful password authentications
grep "Accepted password" /var/log/auth.log

# Successful public key authentications
grep "Accepted publickey" /var/log/auth.log

# Extract successful logins with timestamp, user, and source IP
grep "Accepted" /var/log/auth.log | awk '{print $1, $2, $3, $9, $11}'

# Successful logins from specific IP
grep "Accepted" /var/log/auth.log | grep "192.168.1.100"
```

### SSH Session Tracking

```bash
# Find session opened events (indicates shell access granted)
grep "session opened" /var/log/auth.log

# Find session closed events (logout tracking)
grep "session closed" /var/log/auth.log

# Match sessions to specific users
grep "session opened for user" /var/log/auth.log | grep "username"

# Calculate session duration (requires correlation)
grep -E "(session opened|session closed)" /var/log/auth.log | grep "username"
```

### SSH Connection Analysis

```bash
# View all SSH connection attempts
grep "sshd" /var/log/auth.log

# Identify disconnection reasons
grep "Disconnected" /var/log/auth.log

# Find port scanning activity (rapid connection attempts)
grep "Did not receive identification string" /var/log/auth.log

# Identify key exchange and protocol issues
grep "Unable to negotiate" /var/log/auth.log
```

### Advanced SSH Log Parsing

```bash
# Extract unique source IPs that successfully authenticated
grep "Accepted" /var/log/auth.log | awk '{print $11}' | sort -u

# Count successful logins per user
grep "Accepted" /var/log/auth.log | awk '{print $9}' | sort | uniq -c | sort -rn

# Timeline of SSH activity for forensic analysis
grep "sshd" /var/log/auth.log | awk '{print $1, $2, $3}' | uniq -c

# Find authentication method changes (potential attacker adaptation)
grep "Accepted" /var/log/auth.log | awk '{print $1, $2, $6, $9}' | sort
```

## Failed Authentication Attempts

Failed authentication attempts indicate brute-force attacks, credential stuffing, misconfigured services, or reconnaissance activity. High volumes of failures from single sources suggest automated attacks.

### Basic Failed Authentication Queries

```bash
# All failed password attempts
grep "Failed password" /var/log/auth.log

# Failed attempts with usernames and source IPs
grep "Failed password" /var/log/auth.log | awk '{print $1, $2, $9, $11}'

# Count failures per source IP
grep "Failed password" /var/log/auth.log | awk '{print $11}' | sort | uniq -c | sort -rn

# Failed attempts for specific user (identify targeted accounts)
grep "Failed password for" /var/log/auth.log | grep "root"
```

### Invalid User Detection

```bash
# Attempts with non-existent usernames (reconnaissance)
grep "Invalid user" /var/log/auth.log

# Extract invalid usernames being tested
grep "Invalid user" /var/log/auth.log | awk '{print $8}' | sort | uniq -c | sort -rn

# Correlate invalid users with source IPs
grep "Invalid user" /var/log/auth.log | awk '{print $8, $10}' | sort
```

### Public Key Authentication Failures

```bash
# Failed public key authentications
grep "Failed publickey" /var/log/auth.log

# Identify users attempting key-based auth without valid keys
grep "Connection closed by authenticating user" /var/log/auth.log
```

### Brute Force Detection Patterns

```bash
# Rapid authentication attempts (potential brute force)
grep "Failed password" /var/log/auth.log | awk '{print $1, $2, $11}' | uniq -c | awk '$1 > 5'

# Multiple failures followed by success (compromised credential indicator)
grep "Failed password\|Accepted password" /var/log/auth.log | grep "192.168.1.50"

# Break-in attempts (system-flagged potential compromises)
grep "POSSIBLE BREAK-IN ATTEMPT" /var/log/auth.log
```

### Authentication Failure Analysis Tools

```bash
# Using fail2ban logs to identify banned IPs
cat /var/log/fail2ban.log | grep "Ban"

# Check fail2ban status for SSH jail
fail2ban-client status sshd

# Parse auth.log with custom threshold
awk '/Failed password/ {print $11}' /var/log/auth.log | sort | uniq -c | awk '$1 > 10 {print $2, $1}'
```

## sudo Command Logs

The `sudo` logging mechanism tracks privilege escalation attempts and successful elevated command execution. This is critical for identifying unauthorized privilege escalation, insider threats, and post-exploitation activity.

### Basic sudo Log Analysis

```bash
# All sudo commands executed
grep "sudo" /var/log/auth.log

# Successful sudo commands with full context
grep "COMMAND=" /var/log/auth.log

# Extract user, command, and timestamp
grep "COMMAND=" /var/log/auth.log | awk '{print $1, $2, $3, $6, $15}'

# sudo commands by specific user
grep "COMMAND=" /var/log/auth.log | grep "USER=username"
```

### Failed sudo Attempts

```bash
# Authentication failures during sudo usage
grep "sudo.*authentication failure" /var/log/auth.log

# Incorrect password attempts
grep "sudo.*pam_unix.*authentication failure" /var/log/auth.log

# Users attempting unauthorized sudo access
grep "NOT in sudoers" /var/log/auth.log

# Extract users violating sudo policy
grep "NOT in sudoers" /var/log/auth.log | awk '{print $6}' | sort | uniq
```

### Privilege Escalation Indicators

```bash
# sudo to root shell (high-risk activity)
grep "COMMAND=/bin/bash\|COMMAND=/bin/sh" /var/log/auth.log

# Password changes via sudo
grep "COMMAND=/usr/bin/passwd" /var/log/auth.log

# User additions (potential backdoor creation)
grep "COMMAND=/usr/sbin/useradd\|COMMAND=/usr/sbin/adduser" /var/log/auth.log

# Sudoers file modifications
grep "COMMAND=.*visudo\|COMMAND=.*sudoers" /var/log/auth.log
```

### Detailed sudo Command Extraction

```bash
# Full command line arguments
grep "COMMAND=" /var/log/auth.log | sed 's/.*COMMAND=/COMMAND=/'

# Commands executed as specific target users
grep "sudo.*USER=root" /var/log/auth.log

# Track sudo session activity
grep "sudo.*session opened\|session closed" /var/log/auth.log

# Identify TTY used for sudo (terminal tracking)
grep "COMMAND=" /var/log/auth.log | grep -oP "TTY=[^ ]+" | sort | uniq -c
```

### Advanced sudo Analysis

```bash
# Correlate sudo failures with successes (learning attempts)
grep "username" /var/log/auth.log | grep -E "(sudo.*authentication failure|COMMAND=)"

# Unusual sudo command patterns
grep "COMMAND=" /var/log/auth.log | awk -F'COMMAND=' '{print $2}' | sort | uniq -c | sort -rn

# Nighttime sudo activity (potential malicious timing)
grep "COMMAND=" /var/log/auth.log | awk '$2 ~ /^(0[0-5]|2[0-3]):/'

# sudo without password (NOPASSWD configured - security concern)
grep "sudo.*NO_PASSWD" /var/log/auth.log
```

## PAM (Pluggable Authentication Modules) Logs

PAM provides the underlying authentication framework for Linux systems. PAM logs capture authentication decisions, module execution, and security policy enforcement across all authentication mechanisms.

### PAM Authentication Events

```bash
# All PAM authentication attempts
grep "pam_unix" /var/log/auth.log

# PAM session management events
grep "pam_unix(.*:session)" /var/log/auth.log

# PAM authentication module decisions
grep "pam_unix(.*:auth)" /var/log/auth.log

# Account validation events
grep "pam_unix(.*:account)" /var/log/auth.log
```

### Service-Specific PAM Analysis

```bash
# SSH PAM authentication
grep "pam_unix(sshd:auth)" /var/log/auth.log

# Login service PAM events
grep "pam_unix(login:auth)" /var/log/auth.log

# sudo PAM authentication
grep "pam_unix(sudo:auth)" /var/log/auth.log

# CRON job PAM sessions
grep "pam_unix(cron:session)" /var/log/auth.log
```

### PAM Authentication Failures

```bash
# All authentication failures through PAM
grep "pam_unix.*authentication failure" /var/log/auth.log

# Extract failed authentication details (user, service, source)
grep "pam_unix.*authentication failure" /var/log/auth.log | grep -oP "(logname=[^ ]+|uid=[^ ]+|ruser=[^ ]+|rhost=[^ ]+)"

# Account locked/expired through PAM
grep "pam_unix.*account.*expired" /var/log/auth.log

# Access denied by PAM policy
grep "pam_access.*denied" /var/log/auth.log
```

### PAM Security Module Analysis

```bash
# pam_tally2 lockouts (deprecated but still present in some systems)
grep "pam_tally2" /var/log/auth.log

# pam_faillock events (modern account lockout mechanism)
grep "pam_faillock" /var/log/auth.log

# Check faillock status for specific user
faillock --user username

# pam_limits violations (resource limit enforcement)
grep "pam_limits" /var/log/auth.log

# pam_deny explicit rejections
grep "pam_deny" /var/log/auth.log
```

### PAM Session Tracking

```bash
# Session opened events (user login established)
grep "pam_unix.*session opened" /var/log/auth.log

# Session closed events (user logout)
grep "pam_unix.*session closed" /var/log/auth.log

# Correlate sessions by UID
grep "pam_unix.*session" /var/log/auth.log | grep "uid=1000"

# Non-standard session creations (potential security issue)
grep "pam_systemd.*session" /var/log/auth.log
```

### Advanced PAM Forensics

```bash
# Multiple PAM module failures (authentication stack issues)
grep "pam_" /var/log/auth.log | grep -v "pam_unix" | grep "failure\|error\|denied"

# PAM module execution order analysis
grep "pam_" /var/log/auth.log | awk '{print $6}' | sort | uniq -c

# Identify custom PAM configurations in use
grep "pam_exec\|pam_script" /var/log/auth.log

# SELinux PAM denials
grep "pam_selinux" /var/log/auth.log | grep "denied"
```

### Cross-Log PAM Correlation

```bash
# Correlate PAM failures across auth.log and syslog
grep "pam_unix.*authentication failure" /var/log/auth.log /var/log/syslog

# Systemd journal PAM entries
journalctl -t sudo -t sshd | grep "pam_"

# PAM configuration validation
grep "pam_" /var/log/auth.log | grep -i "error\|unknown"
```

### Practical CTF Authentication Log Scenarios

```bash
# Scenario 1: Find compromised account
# Look for: Multiple failures followed by success
grep "Failed password\|Accepted password" /var/log/auth.log | grep "username" | tail -20

# Scenario 2: Identify brute force source
# Extract top attacking IPs
grep "Failed password" /var/log/auth.log | awk '{print $11}' | sort | uniq -c | sort -rn | head -10

# Scenario 3: Track privilege escalation timeline
# Correlate sudo commands with authentication events
grep -E "(Accepted|COMMAND=)" /var/log/auth.log | grep "username"

# Scenario 4: Detect backdoor user creation
grep -E "(useradd|adduser|COMMAND=.*user)" /var/log/auth.log

# Scenario 5: Find suspicious administrative activity
grep "COMMAND=" /var/log/auth.log | grep -E "(visudo|sudoers|shadow|passwd)"
```

### Log Rotation Considerations

```bash
# Check rotated logs (may contain historical evidence)
zgrep "Failed password" /var/log/auth.log.*.gz

# Search across all rotations
for file in /var/log/auth.log*; do 
    echo "=== $file ===" 
    zgrep -h "Accepted password" "$file" 2>/dev/null || grep -h "Accepted password" "$file" 2>/dev/null
done

# Verify log rotation configuration
cat /etc/logrotate.d/rsyslog
```

**Important Related Topics**: To develop comprehensive log analysis skills for CTF scenarios, you should also study: **Application & Service Logs** (covering web server, database, and custom service logging), **System Event Logs** (kernel messages, boot sequences, hardware events), and **Network Activity Logs** (firewall rules, connection tracking, DNS queries). These complement authentication analysis by providing context around service exploitation and lateral movement.

---

## wtmp, utmp, btmp Analysis

### Overview

These three binary log files form the core authentication tracking system on Linux:

- **wtmp** (`/var/log/wtmp`): Historical record of all login/logout events, system boots/shutdowns
- **utmp** (`/var/run/utmp`): Current login sessions (who is logged in right now)
- **btmp** (`/var/log/btmp`): Failed login attempts

All three use the same binary structure (utmp format) but serve different purposes. They are NOT plaintext and require specific tools for analysis.

### wtmp Analysis

**Primary Tool: last**

```bash
# Basic wtmp reading - shows all login sessions
last

# Show last N entries
last -n 20

# Show logins for specific user
last username

# Show system reboot history
last reboot

# Show system shutdown events
last -x shutdown

# Full timestamps (no truncation)
last -F

# Show login source IP/hostname
last -a

# Read from specific wtmp file (useful for analyzing old/rotated logs)
last -f /var/log/wtmp.1

# Filter by time range
last -s 2024-10-01 -t 2024-10-28

# Show logins from specific terminal/TTY
last tty1
```

**Alternative Tool: utmpdump**

```bash
# Dump wtmp to human-readable format
utmpdump /var/log/wtmp

# Convert to text for grep/awk processing
utmpdump /var/log/wtmp | grep "username"

# Restore from text dump (rarely needed)
utmpdump -r < wtmp.txt > /var/log/wtmp
```

**Manual Parsing with Python:**

```bash
python3 << 'EOF'
import struct
import time

# utmp structure format varies by architecture
# Standard x86_64 format
UTMP_SIZE = 384

with open('/var/log/wtmp', 'rb') as f:
    while True:
        data = f.read(UTMP_SIZE)
        if not data or len(data) < UTMP_SIZE:
            break
        
        # Basic parsing (simplified - real structure is complex)
        ut_type = struct.unpack('h', data[0:2])[0]
        ut_user = data[4:36].split(b'\x00')[0].decode('utf-8', errors='ignore')
        ut_line = data[36:68].split(b'\x00')[0].decode('utf-8', errors='ignore')
        ut_time = struct.unpack('i', data[68:72])[0]
        
        if ut_type in [6, 7, 8]:  # USER_PROCESS, DEAD_PROCESS, BOOT_TIME
            print(f"{time.ctime(ut_time)} | {ut_user:12} | {ut_line}")
EOF
```

**Key wtmp Record Types:**

- Type 0: EMPTY (unused entry)
- Type 1: RUN_LVL (system runlevel change)
- Type 2: BOOT_TIME (system boot)
- Type 5: INIT_PROCESS (getty processes)
- Type 6: LOGIN_PROCESS (login prompt)
- Type 7: USER_PROCESS (actual user login)
- Type 8: DEAD_PROCESS (logout/session end)

### utmp Analysis

**Primary Tool: who/w**

```bash
# Show currently logged in users
who

# Detailed information with idle time
w

# Show all information
who -a

# Show boot time
who -b

# Show current runlevel
who -r

# Show login processes
who -l

# Machine-readable output
who -H

# Using utmpdump for current sessions
utmpdump /var/run/utmp
```

**Location:** `/var/run/utmp` (tmpfs, lost on reboot)

**CTF Context:** Useful for detecting concurrent attacker sessions or identifying persistence mechanisms that maintain login sessions.

### btmp Analysis

**Primary Tool: lastb**

```bash
# Show all failed login attempts (requires root)
sudo lastb

# Last N failed attempts
sudo lastb -n 50

# Failed attempts for specific user
sudo lastb username

# Full timestamps
sudo lastb -F

# Show source IPs
sudo lastb -a

# Analyze old btmp files
sudo lastb -f /var/log/btmp.1

# Count failed attempts per user
sudo lastb | awk '{print $1}' | sort | uniq -c | sort -nr

# Identify brute force attempts (many failures from same IP)
sudo lastb | awk '{print $NF}' | sort | uniq -c | sort -nr
```

**Location:** `/var/log/btmp` (requires root access)

**Brute Force Detection Pattern:**

```bash
#!/bin/bash
# Detect potential SSH brute force attacks

echo "Top 10 IPs with failed login attempts:"
sudo lastb | awk '{print $NF}' | grep -E '^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$' | sort | uniq -c | sort -nr | head -10

echo -e "\nTop targeted usernames:"
sudo lastb | awk '{print $1}' | sort | uniq -c | sort -nr | head -10

echo -e "\nFailed attempts timeline (last 24h):"
sudo lastb -F | grep "$(date +%Y)" | head -20
```

### Log Rotation Handling

```bash
# Check for rotated logs
ls -lh /var/log/wtmp* /var/log/btmp*

# Analyze across rotations
for log in /var/log/wtmp*; do
    echo "=== $log ==="
    last -f "$log" | head -10
done

# Combine multiple btmp files for brute force analysis
sudo cat /var/log/btmp* | sudo lastb -f /dev/stdin
```

## lastlog Examination

### Overview

**lastlog** tracks the last login time for EVERY user account on the system, including service accounts. Unlike wtmp/utmp, this is a sparse file indexed by UID.

**Location:** `/var/log/lastlog`

### Basic Analysis

```bash
# Show last login for all users
lastlog

# Specific user
lastlog -u username

# By UID
lastlog -u 1000

# Users who never logged in
lastlog | grep "Never"

# Recent logins (within 7 days)
lastlog -t 7

# Format with full timestamps
lastlog -C

# Limit to specific UID range
lastlog -u 1000-2000
```

### Forensic Applications

**Identify Compromised Service Accounts:**

```bash
# Service accounts (UID < 1000) that have recent logins
lastlog -u 0-999 | grep -v "Never" | grep -v "Username"
```

**[Inference]** If service accounts show recent login activity, this may indicate:

- Privilege escalation exploitation
- Direct credential compromise
- Lateral movement using service credentials

**Detect Dormant Account Usage:**

```bash
# Find accounts that never logged in but exist
awk -F: '{print $1}' /etc/passwd | while read user; do
    lastlog -u "$user" 2>/dev/null | grep -q "Never" && echo "$user: Never logged in"
done
```

### lastlog File Structure

The lastlog file is a **sparse file** where each record is 292 bytes, indexed by UID:

```bash
# Check file size (appears large but is sparse)
ls -lh /var/log/lastlog
du -h /var/log/lastlog  # Shows actual disk usage

# Manual parsing (rarely needed)
python3 << 'EOF'
import struct
import pwd

LASTLOG_SIZE = 292

with open('/var/log/lastlog', 'rb') as f:
    for uid in range(0, 2000):
        f.seek(uid * LASTLOG_SIZE)
        data = f.read(LASTLOG_SIZE)
        
        if len(data) < LASTLOG_SIZE:
            break
            
        ll_time = struct.unpack('i', data[0:4])[0]
        ll_line = data[4:36].split(b'\x00')[0].decode('utf-8', errors='ignore')
        ll_host = data[36:292].split(b'\x00')[0].decode('utf-8', errors='ignore')
        
        if ll_time > 0:
            try:
                username = pwd.getpwuid(uid).pw_name
                import time
                print(f"UID {uid} ({username}): {time.ctime(ll_time)} from {ll_host} on {ll_line}")
            except KeyError:
                pass
EOF
```

## Login Session Tracking

### Active Session Monitoring

**Real-time Tracking:**

```bash
# Current sessions with details
w

# More detailed user information
who -a -H

# Process-level session tracking
ps aux | grep -E "(sshd|login|bash|su)"

# TTY ownership (who owns which terminal)
ls -l /dev/pts/*

# Active SSH sessions specifically
ss -tnp | grep :22 | grep ESTAB

# Show user's processes and terminals
ps -ef --forest | grep -A 5 "sshd:"
```

**Session Relationship Tracking:**

```bash
# Parent-child process relationships for login sessions
pstree -p -u -a

# Find session leader processes
ps -eo pid,ppid,sess,cmd | grep -E "(bash|ssh|login)"

# Identify tmux/screen sessions
ps aux | grep -E "(tmux|screen)"
tmux ls 2>/dev/null
screen -ls 2>/dev/null
```

### Historical Session Analysis

**Session Duration Calculation:**

```bash
# Calculate average session duration for user
last username | awk '/^username/ {
    if ($NF ~ /[0-9]/) {
        # Extract duration (assumes format like (00:05))
        match($0, /\(([0-9:]+)\)/, dur);
        print dur[1];
    }
}'

# Sessions longer than expected (potential persistence)
last | grep -E '\([0-9]{2}:[0-9]{2}\)' | grep -v "(00:"
```

**Timeline Reconstruction:**

```bash
# Build complete authentication timeline
{
    echo "=== Successful Logins ==="
    last -F | head -50
    
    echo -e "\n=== Failed Attempts ==="
    sudo lastb -F | head -20
    
    echo -e "\n=== Last Login Per User ==="
    lastlog -C
} | tee auth_timeline.txt
```

### Detecting Anomalous Sessions

**Identify Suspicious Patterns:**

```bash
#!/bin/bash
# Session anomaly detection script

echo "=== Checking for Unusual Login Times ==="
# Logins during off-hours (example: 2 AM - 6 AM)
last -F | awk '{print $0}' | grep -E '0[2-6]:[0-9]{2}:[0-9]{2}'

echo -e "\n=== Multiple Concurrent Sessions (Same User) ==="
last | head -50 | awk '{print $1}' | sort | uniq -c | sort -nr | head

echo -e "\n=== Logins from Unusual IPs ==="
# Shows all unique source IPs
last -a | awk '{print $NF}' | grep -E '^[0-9]+\.' | sort -u

echo -e "\n=== Sessions Without Logout ==="
# Still logged in sessions might be suspicious
last | grep "still logged in"

echo -e "\n=== Root SSH Logins (if disabled by policy) ==="
last root | grep -v "reboot"

echo -e "\n=== Service Account Interactive Logins ==="
for user in www-data mysql postgres nginx; do
    if last "$user" 2>/dev/null | grep -q "$user"; then
        echo "WARNING: $user has login history!"
    fi
done
```

### Session Correlation with Other Logs

**Combine with System Logs:**

```bash
# Match auth events with system logs by timestamp
# Extract timestamp from last output and query journalctl

last -F | head -20 | while read line; do
    timestamp=$(echo "$line" | awk '{print $4, $5, $6, $7}')
    user=$(echo "$line" | awk '{print $1}')
    
    echo "=== Events for $user at $timestamp ==="
    journalctl --since "$timestamp" --until "1 hour" | grep -E "(ssh|login|session|$user)" | head -5
done
```

### Log Tampering Detection

**[Inference]** Attackers may attempt to clear authentication logs. Detection methods:

```bash
# Check for gaps in wtmp timeline
last | awk '{print $5, $6, $7}' | uniq -c

# Verify log file integrity
stat /var/log/wtmp /var/log/btmp /var/log/lastlog

# Check for immutable flags (protection against deletion)
lsattr /var/log/wtmp /var/log/btmp /var/log/lastlog

# Compare actual login activity with log entries
# If users report being logged in but no wtmp record exists, logs may be tampered
```

**Log Backup Strategy (CTF Defense):**

```bash
# Copy logs to secure location immediately
mkdir -p /root/log_backups
cp -p /var/log/wtmp /var/log/btmp /var/log/lastlog /root/log_backups/

# Create hash for integrity verification
md5sum /var/log/wtmp /var/log/btmp > /root/log_hashes.txt
```

### Tool Integration

**Using aureport (from auditd):**

```bash
# If auditd is enabled, provides detailed authentication reports
aureport --auth
aureport --login
aureport -l --summary
aureport -u --summary  # User activity summary
```

**Using journalctl for Session Tracking:**

```bash
# SSH authentication events
journalctl -u ssh.service | grep -E "(Accepted|Failed)"

# PAM authentication events
journalctl -t sshd | grep pam

# User session events
journalctl _UID=1000  # Track specific user by UID
```

## Critical CTF Considerations

1. **Temporal Analysis**: Cross-reference authentication timestamps with command history (`~/.bash_history`), file modifications, and network connections
2. **Privilege Escalation Tracking**: Look for transitions from regular user to root in session logs
3. **Persistence Detection**: Check for sessions that remain active across system reboots (may indicate backdoors)
4. **Lateral Movement**: Correlate logins from internal IPs after initial compromise
5. **Log Clearing**: Empty or suspiciously recent wtmp/btmp files indicate anti-forensics

**Essential commands for quick triage:**

```bash
last -10                    # Last 10 login sessions
sudo lastb -10              # Last 10 failed attempts
lastlog | grep -v Never     # All accounts that logged in
who -a                      # Current session details
w                           # Active users with commands
```

---

# Web Server Logs

## Apache access.log Format

Apache access logs record every HTTP request made to the server. The default format is the **Common Log Format (CLF)**, though many systems use the **Combined Log Format**.

### Common Log Format (CLF) Structure

```
127.0.0.1 - frank [10/Oct/2025:13:55:36 -0700] "GET /apache_pb.gif HTTP/1.0" 200 2326
```

**Field breakdown:**

- `127.0.0.1` - Client IP address
- `-` - RFC 1413 identity (usually unused, shown as `-`)
- `frank` - HTTP authenticated username (or `-` if none)
- `[10/Oct/2025:13:55:36 -0700]` - Timestamp with timezone
- `"GET /apache_pb.gif HTTP/1.0"` - Request line (method, URI, protocol)
- `200` - HTTP status code
- `2326` - Response size in bytes (or `-` if none)

### Location and Configuration

**Default log locations:**

```bash
# Debian/Ubuntu
/var/log/apache2/access.log
/var/log/apache2/other_vhosts_access.log

# RHEL/CentOS/Fedora
/var/log/httpd/access_log
```

**Configuration directive:**

```apache
CustomLog /var/log/apache2/access.log common
```

### CTF Analysis Commands

**Basic log inspection:**

```bash
# View last 50 entries
tail -n 50 /var/log/apache2/access.log

# Follow log in real-time
tail -f /var/log/apache2/access.log

# Search for specific IP
grep "192.168.1.100" /var/log/apache2/access.log

# Count requests per IP
awk '{print $1}' /var/log/apache2/access.log | sort | uniq -c | sort -rn

# Extract all requested URIs
awk '{print $7}' /var/log/apache2/access.log | sort | uniq -c | sort -rn
```

**Detecting suspicious activity:**

```bash
# Find failed requests (4xx/5xx status codes)
awk '$9 ~ /^[45]/ {print}' /var/log/apache2/access.log

# Identify potential SQL injection attempts
grep -E "(union|select|insert|update|delete|drop|--)" /var/log/apache2/access.log -i

# Detect directory traversal attempts
grep -E "\.\.|%2e%2e|%252e" /var/log/apache2/access.log -i

# Find POST requests (potential data exfiltration/uploads)
grep "\"POST" /var/log/apache2/access.log

# Identify user-agent anomalies
awk -F'"' '{print $6}' /var/log/apache2/access.log | sort | uniq -c | sort -rn
```

**Time-based analysis:**

```bash
# Extract requests within specific timeframe
awk -F'[' '{print $2}' /var/log/apache2/access.log | grep "10/Oct/2025:13"

# Count requests per hour
awk -F'[: []' '{print $2":"$3}' /var/log/apache2/access.log | sort | uniq -c

# Find requests outside business hours
awk -F'[: []' '$3 ~ /^(0[0-6]|2[0-3])$/ {print}' /var/log/apache2/access.log
```

## Apache error.log Analysis

Error logs capture diagnostic information, including server errors, client errors, and security-related events.

### Error Log Format

```
[Fri Oct 10 13:55:36.123456 2025] [core:error] [pid 35708:tid 140737328441984] [client 192.168.1.100:52312] File does not exist: /var/www/html/favicon.ico
```

**Field breakdown:**

- `[Fri Oct 10 13:55:36.123456 2025]` - Timestamp with microseconds
- `[core:error]` - Module name and log level
- `[pid 35708:tid 140737328441984]` - Process and thread IDs
- `[client 192.168.1.100:52312]` - Client IP and port
- Message content

### Log Levels

From highest to lowest severity:

- `emerg` - System is unusable
- `alert` - Immediate action required
- `crit` - Critical conditions
- `error` - Error conditions
- `warn` - Warning conditions
- `notice` - Normal but significant
- `info` - Informational
- `debug` - Debug-level messages

### Location and Configuration

**Default locations:**

```bash
# Debian/Ubuntu
/var/log/apache2/error.log

# RHEL/CentOS/Fedora
/var/log/httpd/error_log
```

**Configuration:**

```apache
ErrorLog /var/log/apache2/error.log
LogLevel warn
```

### CTF Analysis Commands

**Identifying security issues:**

```bash
# View critical errors only
grep "\[error\]" /var/log/apache2/error.log

# Find PHP execution errors (potential webshell activity)
grep "PHP" /var/log/apache2/error.log

# Detect permission denied errors (privilege escalation attempts)
grep "Permission denied" /var/log/apache2/error.log

# Find segmentation faults (potential exploitation)
grep -i "segfault" /var/log/apache2/error.log

# Identify file not found errors (reconnaissance activity)
grep "File does not exist" /var/log/apache2/error.log | awk '{print $NF}' | sort | uniq -c | sort -rn
```

**Module-specific issues:**

```bash
# SSL/TLS errors
grep "\[ssl:" /var/log/apache2/error.log

# ModSecurity alerts
grep "ModSecurity" /var/log/apache2/error.log

# Authentication failures
grep -E "(auth|password)" /var/log/apache2/error.log -i
```

**Correlation with access logs:**

```bash
# Extract error timestamps and find matching access entries
grep "192.168.1.100" /var/log/apache2/error.log | \
awk '{print $2,$3}' | while read date time; do
    grep "$date.*$time" /var/log/apache2/access.log
done
```

## Nginx Access and Error Logs

Nginx uses similar logging concepts but with different default formats and locations.

### Nginx Access Log Format

**Default combined format:**

```
192.168.1.100 - - [10/Oct/2025:13:55:36 +0000] "GET /index.html HTTP/1.1" 200 612 "http://example.com/" "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
```

**Field breakdown:**

- `192.168.1.100` - Client IP
- `-` - Remote user (RFC 1413)
- `-` - Authenticated user
- `[10/Oct/2025:13:55:36 +0000]` - Timestamp
- `"GET /index.html HTTP/1.1"` - Request line
- `200` - Status code
- `612` - Bytes sent
- `"http://example.com/"` - HTTP Referer
- `"Mozilla/5.0..."` - User-Agent

### Nginx Log Locations

**Default paths:**

```bash
# Access logs
/var/log/nginx/access.log

# Error logs
/var/log/nginx/error.log

# Per-site logs (common configuration)
/var/log/nginx/example.com.access.log
/var/log/nginx/example.com.error.log
```

### Configuration

**nginx.conf log format definition:**

```nginx
log_format combined '$remote_addr - $remote_user [$time_local] '
                    '"$request" $status $body_bytes_sent '
                    '"$http_referer" "$http_user_agent"';

access_log /var/log/nginx/access.log combined;
error_log /var/log/nginx/error.log warn;
```

**Custom format for enhanced CTF analysis:**

```nginx
log_format detailed '$remote_addr - $remote_user [$time_local] '
                    '"$request" $status $body_bytes_sent '
                    '"$http_referer" "$http_user_agent" '
                    'rt=$request_time uct="$upstream_connect_time" '
                    'uht="$upstream_header_time" urt="$upstream_response_time"';
```

### CTF Analysis Commands for Nginx

**Basic parsing:**

```bash
# Extract client IPs
awk '{print $1}' /var/log/nginx/access.log | sort | uniq -c | sort -rn | head -20

# Find requests by status code
awk '$9 == 404 {print $7}' /var/log/nginx/access.log | sort | uniq -c | sort -rn

# Extract User-Agents
awk -F'"' '{print $6}' /var/log/nginx/access.log | sort | uniq -c | sort -rn

# Find largest responses
awk '{print $10, $7}' /var/log/nginx/access.log | sort -rn | head -20
```

**Attack pattern detection:**

```bash
# Identify scanning activity (multiple 404s from same IP)
awk '$9 == 404 {print $1}' /var/log/nginx/access.log | sort | uniq -c | sort -rn | awk '$1 > 50'

# Find potential shell upload attempts
grep -E "\.php|\.jsp|\.asp" /var/log/nginx/access.log | grep "POST"

# Detect XXE/SSRF attempts
grep -E "(file://|gopher://|dict://|ftp://)" /var/log/nginx/access.log

# Identify credential stuffing
awk '$9 == 401 || $9 == 403 {print $1}' /var/log/nginx/access.log | sort | uniq -c | sort -rn
```

### Nginx Error Log Analysis

**Error log format:**

```
2025/10/10 13:55:36 [error] 1234#1234: *1 open() "/var/www/html/favicon.ico" failed (2: No such file or directory), client: 192.168.1.100, server: example.com, request: "GET /favicon.ico HTTP/1.1", host: "example.com"
```

**Analysis commands:**

```bash
# Find all errors by level
grep "\[error\]" /var/log/nginx/error.log

# Critical errors only
grep "\[crit\]" /var/log/nginx/error.log

# Identify client-side issues
grep "client:" /var/log/nginx/error.log | awk -F'client: ' '{print $2}' | awk '{print $1}' | sort | uniq -c | sort -rn

# Find upstream failures (backend server issues)
grep "upstream" /var/log/nginx/error.log

# SSL/TLS errors
grep -i "ssl" /var/log/nginx/error.log
```

## Combined Log Format (CLF)

The **Combined Log Format** extends the Common Log Format by adding Referer and User-Agent fields.

### Format Specification

```
%h %l %u %t "%r" %>s %b "%{Referer}i" "%{User-agent}i"
```

**Apache directive:**

```apache
LogFormat "%h %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-agent}i\"" combined
CustomLog /var/log/apache2/access.log combined
```

**Nginx equivalent:**

```nginx
log_format combined '$remote_addr - $remote_user [$time_local] '
                    '"$request" $status $body_bytes_sent '
                    '"$http_referer" "$http_user_agent"';
```

### Variable Definitions

- `%h` / `$remote_addr` - Remote host IP
- `%l` / `-` - Remote logname (RFC 1413, typically unused)
- `%u` / `$remote_user` - Remote user (HTTP auth)
- `%t` / `$time_local` - Time of request
- `%r` / `$request` - Request line
- `%>s` / `$status` - Final status code
- `%b` / `$body_bytes_sent` - Response size
- `%{Referer}i` / `$http_referer` - Referer header
- `%{User-agent}i` / `$http_user_agent` - User-Agent header

### CTF-Specific Combined Format Analysis

**Referer-based detection:**

```bash
# Find requests without referer (potential direct access/bots)
awk -F'"' '$4 == "-" {print}' /var/log/apache2/access.log

# Identify external referers (potential hotlinking/injection)
awk -F'"' '$4 !~ /example\.com/ && $4 != "-" {print $4}' /var/log/apache2/access.log | sort | uniq -c

# Find XSS attempts in referer
grep -E "(<script|javascript:|onerror=|onload=)" /var/log/apache2/access.log
```

**User-Agent analysis:**

```bash
# Detect common attack tools
grep -E "(sqlmap|nikto|nmap|masscan|nessus|burp|metasploit)" /var/log/apache2/access.log -i

# Find empty or suspicious User-Agents
awk -F'"' '$6 == "" || $6 == "-" {print}' /var/log/apache2/access.log

# Identify custom/unusual User-Agents
awk -F'"' '{print $6}' /var/log/apache2/access.log | grep -v "Mozilla" | sort | uniq -c | sort -rn
```

### Log Parsing with GoAccess

GoAccess provides real-time web log analysis:

```bash
# Install GoAccess
apt-get install goaccess

# Apache combined format analysis (real-time)
goaccess /var/log/apache2/access.log --log-format=COMBINED

# Generate HTML report
goaccess /var/log/apache2/access.log -o report.html --log-format=COMBINED

# Real-time dashboard
goaccess /var/log/apache2/access.log -o report.html --log-format=COMBINED --real-time-html

# Nginx analysis
goaccess /var/log/nginx/access.log --log-format=COMBINED
```

### Advanced Parsing with AWK Scripts

**Extract all failed login attempts with context:**

```bash
awk '$9 == 401 || $9 == 403 {
    printf "IP: %s | Time: %s | Request: %s | User-Agent: %s\n", 
    $1, $4, $7, $12
}' /var/log/apache2/access.log
```

**Calculate bandwidth per IP:**

```bash
awk '{ip[$1]+=$10} END {for (i in ip) printf "%-15s %10d bytes\n", i, ip[i]}' \
/var/log/apache2/access.log | sort -k2 -rn
```

**Identify potential DDoS sources:**

```bash
awk '{print $1}' /var/log/nginx/access.log | \
sort | uniq -c | sort -rn | \
awk '$1 > 1000 {printf "%s requests from %s\n", $1, $2}'
```

### Log Analysis Tools Summary

**Command-line tools:**

- `grep` - Pattern matching
- `awk` - Field extraction and processing
- `sed` - Stream editing
- `cut` - Column extraction
- `sort` / `uniq` - Sorting and deduplication
- `wc` - Counting
- `tail` / `head` - File beginning/end viewing

**Specialized tools:**

- `goaccess` - Real-time web log analyzer
- `logwatch` - Automated log analysis
- `awstats` - Web analytics
- `webalizer` - Web statistics
- `logstalgia` - Visual log analysis
- `apache-scalp` - Attack pattern detection

---

## Custom Log Formats

Web servers allow administrators to define custom logging formats beyond the default configurations. Understanding these formats is critical for CTF log analysis as they may contain hidden indicators or deliberately obfuscated information.

### Apache Custom Log Formats

Apache uses the `LogFormat` directive to define custom formats. The syntax is:

```apache
LogFormat "format_string" nickname
CustomLog /path/to/logfile nickname
```

**Common Format Tokens:**

- `%h` - Remote hostname/IP
- `%l` - Remote logname (usually `-`)
- `%u` - Remote user (authenticated)
- `%t` - Time (format: `[day/month/year:hour:minute:second zone]`)
- `%r` - First line of request
- `%>s` - Final status code
- `%b` - Size of response in bytes (excluding headers)
- `%{Referer}i` - Referer header
- `%{User-Agent}i` - User-Agent string
- `%D` - Time taken to serve request (microseconds)
- `%T` - Time taken to serve request (seconds)
- `%I` - Bytes received (including headers)
- `%O` - Bytes sent (including headers)
- `%X` - Connection status when response completed
- `%{COOKIE_NAME}C` - Contents of specific cookie
- `%{HEADER_NAME}i` - Contents of specific request header

**Analyzing Custom Formats in CTF:**

```bash
# Identify the log format from Apache config
grep -r "LogFormat" /etc/apache2/
grep -r "CustomLog" /etc/apache2/

# Common locations on Kali/Debian systems
cat /etc/apache2/apache2.conf
cat /etc/apache2/sites-available/*.conf
```

**Parsing Custom Formats:**

```bash
# Extract the format string from config
grep "LogFormat" /etc/apache2/apache2.conf | grep "combined"

# Use awk with field positions matching your custom format
# Example: Custom format with IP, timestamp, request, status, bytes
awk '{print $1, $4, $7, $9, $10}' /var/log/apache2/access.log

# For complex formats, use Python
python3 << 'EOF'
import re
log_format = r'(?P<ip>\S+) \S+ \S+ \[(?P<time>.*?)\] "(?P<request>.*?)" (?P<status>\d+) (?P<bytes>\S+)'
with open('/var/log/apache2/access.log') as f:
    for line in f:
        match = re.match(log_format, line)
        if match:
            print(match.groupdict())
EOF
```

### Nginx Custom Log Formats

Nginx uses the `log_format` directive:

```nginx
log_format custom_format '$remote_addr - $remote_user [$time_local] "$request" '
                         '$status $body_bytes_sent "$http_referer" '
                         '"$http_user_agent" "$http_x_forwarded_for"';
access_log /var/log/nginx/access.log custom_format;
```

**Key Nginx Variables:**

- `$remote_addr` - Client IP
- `$remote_user` - HTTP authenticated user
- `$time_local` - Local timestamp
- `$request` - Full request line
- `$status` - Response status
- `$body_bytes_sent` - Bytes sent (body only)
- `$http_referer` - Referer header
- `$http_user_agent` - User-Agent
- `$http_x_forwarded_for` - X-Forwarded-For header (proxy chain)
- `$request_time` - Request processing time (seconds, milliseconds precision)
- `$upstream_response_time` - Backend response time
- `$ssl_protocol` - SSL/TLS protocol version
- `$ssl_cipher` - Cipher suite used

**CTF Analysis Commands:**

```bash
# Find Nginx log format
grep -r "log_format" /etc/nginx/
cat /etc/nginx/nginx.conf

# Parse with awk (adjust field numbers)
awk '{print $1, $7, $9}' /var/log/nginx/access.log

# Extract JSON-formatted logs (if custom format uses JSON)
cat /var/log/nginx/access.log | jq '.request, .status, .ip'
```

### IIS Custom Log Formats

IIS logs can use W3C Extended format with custom fields selected via GUI or configuration.

**Common W3C Fields:**

- `date`, `time` - Timestamp components
- `s-ip` - Server IP
- `cs-method` - HTTP method
- `cs-uri-stem` - URI path
- `cs-uri-query` - Query string
- `s-port` - Server port
- `cs-username` - Authenticated username
- `c-ip` - Client IP
- `cs(User-Agent)` - User-Agent
- `cs(Referer)` - Referer
- `sc-status` - HTTP status
- `sc-substatus` - Sub-status code (IIS-specific)
- `sc-win32-status` - Windows error code
- `time-taken` - Request duration (milliseconds)

**Analysis on Kali (IIS logs obtained during CTF):**

```bash
# IIS logs are space-delimited with header lines starting with #
grep -v "^#" iis.log | awk '{print $9, $5, $8}' # c-ip, cs-method, sc-status

# Extract headers to understand field positions
grep "^#Fields:" iis.log
```

### CTF-Specific Custom Format Indicators

In CTF challenges, custom formats may include:

1. **Base64-encoded fields** - Custom tokens that contain encoded flags
2. **Timing covert channels** - Microsecond-level timing fields hiding data
3. **Custom headers** - Non-standard headers logged via `%{X-Custom-Header}i`
4. **Cookie extraction** - Specific cookie values containing session data
5. **Binary data** - Unusual characters or encoding in logged fields

**Detection techniques:**

```bash
# Find unusual characters (potential encoding)
cat access.log | tr -cd '\11\12\15\40-\176' | less # Show only printable ASCII

# Extract all unique custom headers logged
grep -oP '"\K[^"]+(?=")' access.log | grep "^X-" | sort -u

# Identify base64 patterns in logs
grep -oE '[A-Za-z0-9+/]{20,}={0,2}' access.log

# Find timing anomalies
awk '{print $NF}' access.log | sort -n | uniq -c
```

## Virtual Host Logs

Virtual hosts (vhosts) allow multiple domains/sites to run on a single web server. Each vhost can have separate log files, creating multiple log sources to analyze in CTF scenarios.

### Apache Virtual Host Configuration

**Configuration structure:**

```apache
<VirtualHost *:80>
    ServerName example.com
    ServerAlias www.example.com
    DocumentRoot /var/www/example
    
    ErrorLog ${APACHE_LOG_DIR}/example_error.log
    CustomLog ${APACHE_LOG_DIR}/example_access.log combined
</VirtualHost>

<VirtualHost *:80>
    ServerName admin.example.com
    DocumentRoot /var/www/admin
    
    ErrorLog ${APACHE_LOG_DIR}/admin_error.log
    CustomLog ${APACHE_LOG_DIR}/admin_access.log combined
</VirtualHost>
```

**Locating vhost configurations:**

```bash
# Debian/Kali Apache
ls -la /etc/apache2/sites-available/
ls -la /etc/apache2/sites-enabled/

# View all enabled vhosts
apache2ctl -S

# Extract ServerName directives
grep -r "ServerName" /etc/apache2/sites-enabled/

# Find all CustomLog directives
grep -r "CustomLog" /etc/apache2/sites-enabled/
```

### Nginx Virtual Host Configuration

```nginx
server {
    listen 80;
    server_name example.com www.example.com;
    root /var/www/example;
    
    access_log /var/log/nginx/example_access.log;
    error_log /var/log/nginx/example_error.log;
}

server {
    listen 80;
    server_name admin.example.com;
    root /var/www/admin;
    
    access_log /var/log/nginx/admin_access.log;
    error_log /var/log/nginx/admin_error.log;
}
```

**Locating Nginx vhosts:**

```bash
# Nginx configuration locations
ls -la /etc/nginx/sites-available/
ls -la /etc/nginx/sites-enabled/

# Show all server blocks
nginx -T | grep -A 10 "server {"

# Extract server_name directives
grep -r "server_name" /etc/nginx/sites-enabled/

# Find all access_log directives
grep -r "access_log" /etc/nginx/sites-enabled/
```

### CTF Analysis of Virtual Host Logs

**Identifying all vhost logs:**

```bash
# Find all log files in standard directories
find /var/log/apache2/ -name "*access*" -o -name "*error*"
find /var/log/nginx/ -name "*access*" -o -name "*error*"

# List by modification time (find recently active vhosts)
ls -lt /var/log/apache2/
ls -lt /var/log/nginx/

# Count log entries per vhost
for log in /var/log/apache2/*access*.log; do
    echo "$log: $(wc -l < $log) entries"
done
```

**Correlating activity across vhosts:**

```bash
# Merge multiple vhost logs by timestamp
# Apache format: [day/month/year:hour:minute:second zone]
sort -t'[' -k2 /var/log/apache2/example_access.log /var/log/apache2/admin_access.log

# For Nginx (ISO timestamp)
sort -k4 /var/log/nginx/example_access.log /var/log/nginx/admin_access.log

# Find common IPs across different vhosts
grep -h -oE '([0-9]{1,3}\.){3}[0-9]{1,3}' /var/log/apache2/example_access.log | sort -u > example_ips.txt
grep -h -oE '([0-9]{1,3}\.){3}[0-9]{1,3}' /var/log/apache2/admin_access.log | sort -u > admin_ips.txt
comm -12 example_ips.txt admin_ips.txt
```

**Name-based vs IP-based vhosts:**

Name-based vhosts rely on the `Host` header, which can be spoofed or missing:

```bash
# Find requests without proper Host header
grep -v "Host:" /var/log/apache2/access.log

# Identify default vhost usage (requests that didn't match any vhost)
# Check logs in default or first-listed vhost config
```

### Hidden or Development Vhosts

CTF scenarios often include undiscovered vhosts that contain flags or vulnerabilities.

**Discovery techniques:**

```bash
# Extract ServerName/ServerAlias from configs
grep -rh "ServerName\|ServerAlias" /etc/apache2/ | awk '{print $2}' | sort -u

# Check /etc/hosts for mapped domains
cat /etc/hosts | grep -v "^#" | grep -v "localhost"

# Analyze Host headers in logs to find undocumented vhosts
awk '{print $1}' /var/log/nginx/access.log | grep -oP 'Host: \K[^\s]+' | sort -u

# DNS enumeration (if applicable)
gobuster dns -d example.com -w /usr/share/wordlists/subdomains.txt
```

**Testing discovered vhosts:**

```bash
# Direct access with curl
curl -H "Host: admin.example.com" http://target-ip/

# Modify /etc/hosts for browser access
echo "192.168.1.100 admin.example.com" >> /etc/hosts
```

### Vhost-Specific Security Misconfigurations

Different vhosts may have different security postures:

```bash
# Compare authentication requirements
grep -r "AuthType" /etc/apache2/sites-enabled/
grep -r "auth_basic" /etc/nginx/sites-enabled/

# Check directory listing permissions
grep -r "Options.*Indexes" /etc/apache2/sites-enabled/
grep -r "autoindex" /etc/nginx/sites-enabled/

# Identify PHP execution differences
grep -r "php_admin_flag" /etc/apache2/sites-enabled/
```

## SSL/TLS Logs

SSL/TLS logs contain information about encrypted connections, certificate validation, cipher negotiations, and protocol versions. These logs are critical for identifying man-in-the-middle attacks, weak cryptography, and certificate-based authentication in CTF challenges.

### Apache SSL/TLS Logging

Apache's `mod_ssl` module provides SSL-specific logging variables.

**Enable SSL logging:**

```apache
<VirtualHost *:443>
    ServerName secure.example.com
    SSLEngine on
    SSLCertificateFile /etc/ssl/certs/server.crt
    SSLCertificateKeyFile /etc/ssl/private/server.key
    
    # Custom SSL log format
    LogFormat "%h %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-Agent}i\" SSL:%{SSL_PROTOCOL}x Cipher:%{SSL_CIPHER}x" ssl_combined
    CustomLog ${APACHE_LOG_DIR}/ssl_access.log ssl_combined
    ErrorLog ${APACHE_LOG_DIR}/ssl_error.log
    
    # Detailed SSL logging
    LogLevel ssl:debug
</VirtualHost>
```

**Key SSL variables for logging:**

- `%{SSL_PROTOCOL}x` - SSL/TLS version (SSLv3, TLSv1, TLSv1.1, TLSv1.2, TLSv1.3)
- `%{SSL_CIPHER}x` - Cipher suite negotiated
- `%{SSL_CIPHER_EXPORT}x` - Whether export-grade cipher
- `%{SSL_CIPHER_USEKEYSIZE}x` - Key size in use
- `%{SSL_CIPHER_ALGKEYSIZE}x` - Advertised key size
- `%{SSL_CLIENT_S_DN}x` - Client certificate subject DN
- `%{SSL_CLIENT_I_DN}x` - Client certificate issuer DN
- `%{SSL_CLIENT_VERIFY}x` - Client cert verification result
- `%{SSL_SESSION_ID}x` - SSL session ID
- `%{SSL_SESSION_RESUMED}x` - Whether session resumed
- `%{SSL_TLS_SNI}x` - Server Name Indication (SNI) value

**Analysis commands:**

```bash
# Find SSL-specific log entries
grep "SSL:" /var/log/apache2/ssl_access.log

# Extract cipher suites in use
grep -oP 'Cipher:\K[^\s]+' /var/log/apache2/ssl_access.log | sort | uniq -c

# Find weak ciphers (export, DES, RC4, MD5)
grep -E 'Cipher:.*(EXPORT|DES|RC4|MD5)' /var/log/apache2/ssl_access.log

# Identify TLS versions
grep -oP 'SSL:\K[^\s]+' /var/log/apache2/ssl_access.log | sort | uniq -c

# Find deprecated protocols (SSLv3, TLSv1.0, TLSv1.1)
grep -E 'SSL:(SSLv3|TLSv1\.[01])' /var/log/apache2/ssl_access.log

# Check SSL error log for handshake failures
grep -i "ssl" /var/log/apache2/ssl_error.log
grep -i "handshake" /var/log/apache2/ssl_error.log
```

### Nginx SSL/TLS Logging

Nginx provides similar SSL variables for custom logging.

**Configuration:**

```nginx
log_format ssl_combined '$remote_addr - $remote_user [$time_local] "$request" '
                        '$status $body_bytes_sent "$http_referer" '
                        '"$http_user_agent" '
                        'SSL:$ssl_protocol Cipher:$ssl_cipher '
                        'Session:$ssl_session_id Reused:$ssl_session_reused';

server {
    listen 443 ssl;
    server_name secure.example.com;
    
    ssl_certificate /etc/ssl/certs/server.crt;
    ssl_certificate_key /etc/ssl/private/server.key;
    
    access_log /var/log/nginx/ssl_access.log ssl_combined;
    error_log /var/log/nginx/ssl_error.log info;
}
```

**Key Nginx SSL variables:**

- `$ssl_protocol` - TLS version
- `$ssl_cipher` - Cipher suite
- `$ssl_ciphers` - Client-offered ciphers
- `$ssl_curves` - Supported curves
- `$ssl_session_id` - Session ID
- `$ssl_session_reused` - `r` if reused, `.` if not
- `$ssl_server_name` - SNI hostname
- `$ssl_client_cert` - Client certificate (PEM)
- `$ssl_client_s_dn` - Client cert subject
- `$ssl_client_i_dn` - Client cert issuer
- `$ssl_client_verify` - Verification result (SUCCESS, FAILED, NONE)

**Analysis commands:**

```bash
# Extract SSL protocols
awk '{print $12}' /var/log/nginx/ssl_access.log | sort | uniq -c

# Find cipher suites
awk '{print $14}' /var/log/nginx/ssl_access.log | sort | uniq -c

# Identify session resumption rate
grep -o "Reused:[r\.]" /var/log/nginx/ssl_access.log | sort | uniq -c

# Check SSL errors
grep -i "ssl" /var/log/nginx/ssl_error.log
```

### Client Certificate Authentication Logs

When client certificate authentication is enabled, logs contain certificate details.

**Apache configuration:**

```apache
SSLVerifyClient require
SSLVerifyDepth 2
SSLCACertificateFile /etc/ssl/certs/ca-bundle.crt

LogFormat "%h %u \"%{SSL_CLIENT_S_DN}x\" %t \"%r\" %>s %b" clientcert
CustomLog ${APACHE_LOG_DIR}/clientcert_access.log clientcert
```

**Analysis:**

```bash
# Extract client certificate subjects
grep -oP 'CN=[^,]+' /var/log/apache2/clientcert_access.log | sort | uniq -c

# Find failed client cert validations
grep "SSL_CLIENT_VERIFY:FAILED" /var/log/apache2/ssl_error.log

# Identify specific users by certificate CN
grep "CN=john.doe" /var/log/apache2/clientcert_access.log
```

### SNI (Server Name Indication) Analysis

SNI allows multiple SSL certificates on a single IP. The requested hostname is visible in plaintext during the TLS handshake.

```bash
# Extract SNI values from Apache logs
grep -oP 'SSL_TLS_SNI:\K[^\s]+' /var/log/apache2/ssl_access.log | sort | uniq -c

# From Nginx logs
grep -oP '\$ssl_server_name:\K[^\s]+' /var/log/nginx/ssl_access.log | sort | uniq -c

# Network-level SNI extraction (if pcap available)
tshark -r capture.pcap -Y "ssl.handshake.extensions_server_name" -T fields -e ssl.handshake.extensions_server_name | sort | uniq -c
```

### TLS Handshake Failure Analysis

Handshake failures indicate mismatched configurations, attacks, or reconnaissance.

```bash
# Apache SSL error log patterns
grep "SSL handshake failed" /var/log/apache2/ssl_error.log
grep "certificate verify failed" /var/log/apache2/ssl_error.log
grep "no shared cipher" /var/log/apache2/ssl_error.log
grep "unsupported protocol" /var/log/apache2/ssl_error.log

# Nginx error log patterns
grep "SSL_do_handshake() failed" /var/log/nginx/ssl_error.log
grep "peer closed connection" /var/log/nginx/ssl_error.log
grep "certificate verification failed" /var/log/nginx/ssl_error.log

# Count failures by type
grep "SSL" /var/log/apache2/ssl_error.log | awk '{print $NF}' | sort | uniq -c
```

### Perfect Forward Secrecy (PFS) Detection

PFS ensures session keys aren't compromised if the server's private key is compromised later.

```bash
# Identify non-PFS ciphers (typically RSA key exchange)
grep -E 'Cipher:.*-RSA-' /var/log/apache2/ssl_access.log

# Find PFS ciphers (ECDHE, DHE)
grep -E 'Cipher:.*(ECDHE|DHE)' /var/log/apache2/ssl_access.log

# Calculate PFS usage percentage
total=$(wc -l < /var/log/apache2/ssl_access.log)
pfs=$(grep -cE 'Cipher:.*(ECDHE|DHE)' /var/log/apache2/ssl_access.log)
echo "scale=2; $pfs * 100 / $total" | bc
```

### CTF-Specific SSL/TLS Log Analysis

**Common CTF scenarios:**

1. **Weak cipher exploitation** - Finding deprecated ciphers that can be downgraded
2. **Certificate CN/SAN analysis** - Hidden domains in certificate alternate names
3. **Session ID correlation** - Tracking users across multiple connections
4. **Timing attacks** - SSL handshake timing variations hiding covert channels
5. **Client cert flags** - Flags embedded in certificate fields

**Targeted commands:**

```bash
# Find uncommon cipher suites (potential CTF flags)
grep -oP 'Cipher:\K[^\s]+' ssl_access.log | sort | uniq -c | sort -n

# Extract all certificate DNs for hidden data
grep -oP 'SSL_CLIENT_S_DN:\K[^"]+' ssl_access.log

# Analyze SSL session IDs for patterns
grep -oP 'Session:\K[A-F0-9]+' ssl_access.log | while read sid; do
    echo "$sid" | xxd -r -p 2>/dev/null || echo "$sid"
done

# Check for SSL renegotiation patterns
grep -i "renegotiat" /var/log/apache2/ssl_error.log

# Identify BEAST/CRIME/BREACH vulnerable configurations
grep -E "Cipher:.*(CBC|DEFLATE)" ssl_access.log
```

### Tools for SSL/TLS Log Analysis

```bash
# Parse logs with ssldump (if you have packet captures)
ssldump -r capture.pcap -d | grep -A 5 "ServerHello"

# Analyze SSL configuration (not logs, but useful for context)
sslscan target.example.com
nmap --script ssl-enum-ciphers -p 443 target.example.com

# Extract certificates from logs or live connections
openssl s_client -connect target.example.com:443 -showcerts

# Decode base64-encoded certificates in logs
grep "BEGIN CERTIFICATE" logfile | base64 -d | openssl x509 -text -noout
```

---

**Important related subtopics for comprehensive CTF log analysis:**

- **Log correlation techniques** - Combining multiple log sources (web, system, application) for complete attack chain analysis
- **Time synchronization issues** - Handling logs with clock skew or timezone mismatches
- **Log injection and evasion techniques** - Identifying attacker attempts to poison or hide in logs

---

# Application Logs

Application logs record runtime events, errors, and diagnostic information from web applications and services. In CTF scenarios, these logs often contain sensitive information leakage, authentication attempts, error states revealing misconfigurations, or indicators of vulnerable code paths.

## PHP Error Logs

PHP error logs capture runtime errors, warnings, notices, and debug information from PHP applications. These logs are critical for identifying code vulnerabilities, information disclosure, and application logic flaws.

### Log Location Paths

**Linux/Unix Systems:**

- `/var/log/apache2/error.log` (Debian/Ubuntu with Apache)
- `/var/log/httpd/error_log` (RHEL/CentOS with Apache)
- `/var/log/nginx/error.log` (Nginx)
- `/var/log/php-fpm/error.log` (PHP-FPM)
- `/var/log/php/error.log` (standalone PHP)
- Custom locations defined in `php.ini` via `error_log` directive

**Windows Systems:**

- `C:\xampp\apache\logs\error.log` (XAMPP)
- `C:\wamp64\logs\php_error.log` (WAMP)
- `C:\inetpub\logs\LogFiles\` (IIS with PHP)

### Configuration Files

Check `php.ini` for logging configuration:

```bash
# Locate php.ini
php --ini

# Check current error reporting settings
php -i | grep -E "error_log|error_reporting|display_errors"
```

Key directives:

- `error_reporting = E_ALL` - Controls which errors are reported
- `display_errors = On/Off` - Whether errors appear in output
- `log_errors = On` - Enables error logging
- `error_log = /path/to/file` - Log destination

### Log Analysis Commands

**Basic log inspection:**

```bash
# View recent errors
tail -f /var/log/apache2/error.log

# Search for specific error types
grep -i "fatal error" /var/log/apache2/error.log
grep -i "parse error" /var/log/apache2/error.log
grep -i "warning" /var/log/apache2/error.log

# Find SQL errors (potential SQLi indicators)
grep -iE "mysql|mysqli|pdo|sql syntax" /var/log/apache2/error.log

# Search for file inclusion errors
grep -iE "include|require|fopen|file_get_contents" /var/log/apache2/error.log

# Find path disclosure
grep -iE "/var/www|/home|C:\\" /var/log/apache2/error.log
```

**Time-based filtering:**

```bash
# Errors from specific date
grep "28-Oct-2025" /var/log/apache2/error.log

# Last hour of errors (requires timestamp parsing)
awk -v d="$(date --date='1 hour ago' '+%d/%b/%Y:%H')" '$0 ~ d' /var/log/apache2/error.log
```

**Pattern extraction:**

```bash
# Extract all file paths from errors
grep -oE '/[a-zA-Z0-9_/.-]+\.php' /var/log/apache2/error.log | sort -u

# Extract IP addresses from error contexts
grep -oE '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}' /var/log/apache2/error.log | sort -u

# Find stack traces
grep -A 20 "Stack trace:" /var/log/apache2/error.log
```

### CTF-Specific Exploitation Patterns

**Information Disclosure:**

```bash
# Database credentials in error messages
grep -iE "access denied.*using password|connection failed.*mysql" /var/log/apache2/error.log

# Absolute path disclosure
grep -oE '/var/www/[a-zA-Z0-9_/.-]+' /var/log/apache2/error.log | sort -u

# Configuration file paths
grep -iE "config\.php|settings\.php|\.env" /var/log/apache2/error.log
```

**Vulnerable Function Usage:**

```bash
# Dangerous functions that may indicate vulnerabilities
grep -iE "eval\(|exec\(|system\(|passthru\(|shell_exec\(|assert\(" /var/log/apache2/error.log

# Unserialize errors (potential object injection)
grep -i "unserialize" /var/log/apache2/error.log

# File operation errors
grep -iE "file_get_contents|fopen|readfile|include|require" /var/log/apache2/error.log
```

**Authentication Bypass Indicators:**

```bash
# Session errors
grep -iE "session_start|session_destroy|session error" /var/log/apache2/error.log

# Authentication failures revealing logic
grep -iE "login|auth|password|credential" /var/log/apache2/error.log
```

### Log Format Understanding

Standard PHP error log format:

```
[Day Mon DD HH:MM:SS.mmmmmm YYYY] [php7:error] [pid XXXXX] [client IP:PORT] PHP Fatal error: Message in /path/to/file.php on line XX
```

Components to analyze:

- **Timestamp**: Correlate with attack timing
- **Error level**: `Fatal error`, `Warning`, `Notice`, `Parse error`
- **Client IP**: Identify attacker source
- **File path**: Vulnerable script location
- **Line number**: Exact vulnerability location
- **Error message**: Nature of the problem

### Automated Analysis Tools

**phpLogReader (custom script example):**

```bash
#!/bin/bash
# Extract critical PHP errors
LOGFILE="/var/log/apache2/error.log"

echo "[+] Fatal Errors:"
grep "PHP Fatal error" "$LOGFILE" | tail -20

echo -e "\n[+] Path Disclosures:"
grep -oE '/var/www/[^ ]+' "$LOGFILE" | sort -u | head -20

echo -e "\n[+] Database Errors:"
grep -iE "mysql|mysqli|pdo" "$LOGFILE" | tail -10
```

**Using awk for structured parsing:**

```bash
# Extract errors by severity
awk -F'[][]' '/PHP Fatal error/ {print $6}' /var/log/apache2/error.log

# Count errors by file
awk '{for(i=1;i<=NF;i++) if($i~/\.php/) print $i}' /var/log/apache2/error.log | sort | uniq -c | sort -rn
```

### Common CTF Scenarios

**Scenario 1: SQL Injection Detection**

```bash
# Look for SQL syntax errors revealing query structure
grep -i "sql syntax" /var/log/apache2/error.log
# Output example: "You have an error in your SQL syntax... near 'admin' LIMIT 1'"
```

**Scenario 2: LFI/RFI Vulnerability**

```bash
# Failed include attempts
grep -iE "failed to open stream|No such file" /var/log/apache2/error.log | grep -E "include|require"
```

**Scenario 3: Credential Discovery**

```bash
# Database connection errors exposing credentials
grep -i "access denied for user" /var/log/apache2/error.log
# May reveal: "Access denied for user 'webapp'@'localhost' (using password: YES)"
```

## Python Application Logs

Python applications use various logging frameworks, most commonly the built-in `logging` module. Log format, location, and verbosity depend heavily on application configuration.

### Log Location Patterns

**Framework-specific locations:**

- **Flask**: Often configured in app initialization, default to stderr/stdout
- **Django**: `settings.py` defines logging, commonly in project root or `/var/log/django/`
- **FastAPI**: Uvicorn/Gunicorn logs, typically stdout or `/var/log/uvicorn/`
- **General Python**: Custom locations via `logging.basicConfig(filename=...)`

**Common paths:**

```bash
/var/log/python/application.log
/var/log/uwsgi/app.log
/var/log/gunicorn/error.log
/home/user/app/logs/app.log
./app.log (application directory)
```

### Python Logging Module Levels

```
CRITICAL (50) - Serious errors, program may crash
ERROR (40) - Serious problems, function failed
WARNING (30) - Indication something unexpected happened
INFO (20) - Confirmation things are working
DEBUG (10) - Detailed diagnostic information
```

### Log Analysis Commands

**Basic inspection:**

```bash
# View application logs
tail -f /var/log/python/application.log

# Filter by log level
grep "ERROR" /var/log/python/application.log
grep "CRITICAL" /var/log/python/application.log
grep "WARNING" /var/log/python/application.log

# Search for exceptions
grep -A 10 "Traceback (most recent call last):" /var/log/python/application.log

# Find specific exception types
grep -iE "ValueError|TypeError|AttributeError|KeyError" /var/log/python/application.log
```

**Framework-specific searches:**

**Django:**

```bash
# Django debug messages
grep "DEBUG" /var/log/django/debug.log

# Database query errors
grep -i "DatabaseError\|OperationalError" /var/log/django/application.log

# Authentication failures
grep -i "authentication\|login failed" /var/log/django/application.log

# Template errors
grep "TemplateDoesNotExist\|TemplateSyntaxError" /var/log/django/application.log
```

**Flask:**

```bash
# Flask errors (often in werkzeug logs)
grep -i "werkzeug" /var/log/flask/application.log

# Route errors
grep -i "404\|405 Method Not Allowed" /var/log/flask/application.log

# Application exceptions
grep -B 5 -A 10 "Exception on" /var/log/flask/application.log
```

### Traceback Analysis

Python tracebacks reveal execution flow and code structure:

```bash
# Extract complete tracebacks
awk '/Traceback \(most recent call last\):/{flag=1} flag; /^[^ ]/ && flag{print "---"; flag=0}' /var/log/python/application.log

# Find file paths in tracebacks
grep -oE 'File "[^"]+", line [0-9]+' /var/log/python/application.log | sort -u

# Extract function names from tracebacks
grep -oE 'in [a-zA-Z_][a-zA-Z0-9_]+' /var/log/python/application.log | sort -u
```

### CTF-Specific Exploitation Patterns

**Deserialization Vulnerabilities:**

```bash
# Pickle-related errors
grep -i "pickle\|unpickle" /var/log/python/application.log

# YAML deserialization (PyYAML)
grep -i "yaml.load" /var/log/python/application.log
```

**Command Injection Indicators:**

```bash
# Subprocess/shell errors
grep -iE "subprocess|os\.system|os\.popen|shell=True" /var/log/python/application.log

# Command not found errors
grep -i "command not found\|No such file or directory" /var/log/python/application.log
```

**Path Traversal:**

```bash
# File operation errors
grep -iE "FileNotFoundError|PermissionError" /var/log/python/application.log

# Suspicious path patterns
grep -oE "(\.\./|\.\.\\\\)+" /var/log/python/application.log
```

**SQL Injection (SQLAlchemy, Django ORM):**

```bash
# SQL syntax errors
grep -i "syntax error\|OperationalError" /var/log/python/application.log

# Raw SQL queries in errors
grep -i "SELECT\|INSERT\|UPDATE\|DELETE" /var/log/python/application.log | grep -i error
```

**SSTI (Server-Side Template Injection):**

```bash
# Template errors revealing injection
grep -iE "TemplateSyntaxError|UndefinedError|SecurityError" /var/log/python/application.log

# Jinja2 specific
grep -i "jinja2" /var/log/python/application.log
```

### Information Disclosure Patterns

```bash
# API keys and secrets in error messages
grep -iE "api[_-]?key|secret|token|password" /var/log/python/application.log

# Environment variables in tracebacks
grep -oE "[A-Z_]+=[^ ]+" /var/log/python/application.log

# Database connection strings
grep -iE "postgresql://|mysql://|mongodb://" /var/log/python/application.log

# Internal IP addresses and ports
grep -oE "([0-9]{1,3}\.){3}[0-9]{1,3}:[0-9]+" /var/log/python/application.log
```

### Structured Log Analysis (JSON Logs)

Many modern Python applications use JSON logging:

```bash
# Pretty print JSON logs
cat /var/log/python/application.log | jq '.'

# Filter by log level
cat /var/log/python/application.log | jq 'select(.level == "ERROR")'

# Extract specific fields
cat /var/log/python/application.log | jq -r '.message'

# Find logs with exceptions
cat /var/log/python/application.log | jq 'select(.exc_info != null)'

# Time-based filtering
cat /var/log/python/application.log | jq 'select(.timestamp > "2025-10-28T10:00:00")'
```

### Automated Analysis Script

```bash
#!/bin/bash
# Python log analyzer for CTF

LOGFILE="${1:-/var/log/python/application.log}"

echo "[+] Exception Summary:"
grep -c "Traceback" "$LOGFILE"

echo -e "\n[+] Recent Errors:"
grep "ERROR\|CRITICAL" "$LOGFILE" | tail -10

echo -e "\n[+] Unique File Paths:"
grep -oE 'File "[^"]+\.py"' "$LOGFILE" | sort -u | head -20

echo -e "\n[+] Potential Vulnerabilities:"
grep -iE "pickle|yaml\.load|eval\(|exec\(|system\(" "$LOGFILE" | wc -l
echo "  - Deserialization/Code Exec hits found"

echo -e "\n[+] Authentication Events:"
grep -iE "login|authentication|unauthorized" "$LOGFILE" | tail -5
```

## Node.js Logs

Node.js applications typically log to stdout/stderr or use logging libraries like Winston, Bunyan, or Pino. Log structure varies significantly based on the framework and logging configuration.

### Log Location Patterns

**Common locations:**

- **Console output redirected**: `/var/log/nodejs/application.log`
- **PM2 managed**: `~/.pm2/logs/app-error.log` and `~/.pm2/logs/app-out.log`
- **Forever**: `/var/log/forever/application.log`
- **Systemd service**: `journalctl -u service-name`
- **Docker containers**: `docker logs container-name`
- **Custom Winston/Bunyan**: Application-defined paths

**Framework-specific:**

- **Express.js**: Often stdout/stderr, Morgan for HTTP logs
- **Next.js**: `.next/` directory or custom locations
- **Nest.js**: Configured in main.ts, often `/var/log/nestjs/`

### Accessing Logs by Deployment Method

**PM2:**

```bash
# View real-time logs
pm2 logs app-name

# View error logs only
pm2 logs app-name --err

# View specific lines
pm2 logs app-name --lines 100

# Log file locations
ls ~/.pm2/logs/
cat ~/.pm2/logs/app-error-0.log
```

**Systemd:**

```bash
# View service logs
journalctl -u nodejs-app.service

# Follow logs in real-time
journalctl -u nodejs-app.service -f

# Show last 100 lines
journalctl -u nodejs-app.service -n 100

# Filter by time
journalctl -u nodejs-app.service --since "2025-10-28 10:00:00"
```

**Docker:**

```bash
# View container logs
docker logs container-name

# Follow logs
docker logs -f container-name

# Last 100 lines
docker logs --tail 100 container-name

# With timestamps
docker logs -t container-name
```

### Log Analysis Commands

**Basic inspection:**

```bash
# View recent logs
tail -f /var/log/nodejs/application.log

# Search for errors
grep -i "error" /var/log/nodejs/application.log
grep -i "exception" /var/log/nodejs/application.log
grep -i "unhandled" /var/log/nodejs/application.log

# Find stack traces
grep -A 15 "Error:" /var/log/nodejs/application.log

# Specific error types
grep -E "TypeError|ReferenceError|SyntaxError|RangeError" /var/log/nodejs/application.log
```

**JSON log parsing (Winston/Bunyan/Pino):**

```bash
# Pretty print JSON logs
cat /var/log/nodejs/application.log | jq '.'

# Filter by level
cat /var/log/nodejs/application.log | jq 'select(.level == "error")'

# Extract messages
cat /var/log/nodejs/application.log | jq -r '.message'

# Find errors with stack traces
cat /var/log/nodejs/application.log | jq 'select(.stack != null)'

# Bunyan CLI tool
bunyan /var/log/nodejs/application.log

# Filter Bunyan logs
bunyan /var/log/nodejs/application.log -l error
```

### CTF-Specific Exploitation Patterns

**Prototype Pollution:**

```bash
# Look for prototype-related errors
grep -i "prototype\|__proto__\|constructor" /var/log/nodejs/application.log

# Object property errors
grep -i "Cannot read property\|Cannot set property" /var/log/nodejs/application.log
```

**Command Injection:**

```bash
# Child process errors
grep -iE "child_process|exec\(|spawn\(|execSync" /var/log/nodejs/application.log

# Shell command errors
grep -i "sh: \|bash: \|command not found" /var/log/nodejs/application.log

# ENOENT errors (file not found - may indicate command injection attempts)
grep "ENOENT" /var/log/nodejs/application.log
```

**NoSQL Injection (MongoDB):**

```bash
# MongoDB errors
grep -iE "mongodb|mongoose" /var/log/nodejs/application.log

# Query errors
grep -i "MongoError\|CastError" /var/log/nodejs/application.log

# Operator injection indicators
grep -E "\$where|\$gt|\$ne|\$regex" /var/log/nodejs/application.log
```

**Path Traversal:**

```bash
# File system errors
grep -i "EACCES\|EPERM\|ENOENT" /var/log/nodejs/application.log

# Suspicious path patterns
grep -oE "(\.\./|\.\.\\\\)+" /var/log/nodejs/application.log

# Filesystem function errors
grep -iE "readFile|writeFile|access|stat" /var/log/nodejs/application.log
```

**Deserialization/Code Injection:**

```bash
# Unsafe eval usage
grep -i "eval\(|Function(" /var/log/nodejs/application.log

# Serialization errors
grep -i "JSON.parse\|serialize\|deserialize" /var/log/nodejs/application.log

# Template injection (EJS, Pug, etc.)
grep -iE "ejs|pug|handlebars|template" /var/log/nodejs/application.log
```

**JWT and Authentication Issues:**

```bash
# JWT errors
grep -i "jwt\|jsonwebtoken\|token" /var/log/nodejs/application.log

# Authentication failures
grep -iE "authentication|unauthorized|401|403" /var/log/nodejs/application.log

# Session errors
grep -i "session" /var/log/nodejs/application.log
```

### Express.js Specific Analysis

**HTTP request errors:**

```bash
# Method not allowed
grep "405\|Method Not Allowed" /var/log/nodejs/application.log

# Not found routes
grep "404\|Cannot GET\|Cannot POST" /var/log/nodejs/application.log

# Server errors
grep "500\|Internal Server Error" /var/log/nodejs/application.log

# Morgan logs (HTTP request logs)
grep -E "GET|POST|PUT|DELETE|PATCH" /var/log/nodejs/application.log
```

**Middleware errors:**

```bash
# Body parser errors
grep -i "body-parser\|payload too large" /var/log/nodejs/application.log

# CORS errors
grep -i "cors\|cross-origin" /var/log/nodejs/application.log

# Rate limiting hits
grep -i "rate limit\|too many requests" /var/log/nodejs/application.log
```

### Information Disclosure Patterns

```bash
# Environment variables
grep -oE "[A-Z_]+=\S+" /var/log/nodejs/application.log

# API keys and secrets
grep -iE "api[_-]?key|secret|token|password|credential" /var/log/nodejs/application.log

# Database connection strings
grep -iE "mongodb://|postgres://|mysql://|redis://" /var/log/nodejs/application.log

# Internal paths
grep -oE "/home/[^ ]+|/var/[^ ]+|C:\\\\[^ ]+" /var/log/nodejs/application.log

# Port and IP disclosure
grep -oE "([0-9]{1,3}\.){3}[0-9]{1,3}:[0-9]+" /var/log/nodejs/application.log
```

### Unhandled Exceptions and Promise Rejections

```bash
# Unhandled promise rejections
grep -i "UnhandledPromiseRejectionWarning\|unhandledRejection" /var/log/nodejs/application.log

# Uncaught exceptions
grep -i "uncaughtException" /var/log/nodejs/application.log

# Extract full exception details
awk '/UnhandledPromiseRejectionWarning/,/^$/' /var/log/nodejs/application.log
```

### Performance and Resource Issues

```bash
# Memory leaks or out of memory
grep -i "heap\|memory\|FATAL ERROR" /var/log/nodejs/application.log

# Event loop lag
grep -i "event loop\|timeout\|ETIMEDOUT" /var/log/nodejs/application.log

# Connection pool exhaustion
grep -i "ECONNREFUSED\|ETIMEDOUT\|pool" /var/log/nodejs/application.log
```

### Automated Analysis Script

```bash
#!/bin/bash
# Node.js log analyzer for CTF

LOGFILE="${1:-/var/log/nodejs/application.log}"

echo "[+] Error Summary:"
echo "  Total errors: $(grep -ci error "$LOGFILE")"
echo "  Unhandled rejections: $(grep -c "UnhandledPromiseRejectionWarning" "$LOGFILE")"
echo "  Uncaught exceptions: $(grep -c "uncaughtException" "$LOGFILE")"

echo -e "\n[+] Recent Critical Errors:"
grep -i "error\|exception" "$LOGFILE" | tail -10

echo -e "\n[+] Potential Command Injection:"
grep -iE "child_process|exec|spawn" "$LOGFILE" | wc -l
echo "  child_process usage detected"

echo -e "\n[+] Path Traversal Indicators:"
grep -E "EACCES|ENOENT" "$LOGFILE" | wc -l
echo "  file access errors found"

echo -e "\n[+] Authentication Issues:"
grep -iE "401|403|unauthorized|authentication failed" "$LOGFILE" | tail -5

echo -e "\n[+] Disclosed Information:"
grep -oE "([0-9]{1,3}\.){3}[0-9]{1,3}" "$LOGFILE" | sort -u | head -10
echo "  (Internal IP addresses)"
```

### Log Correlation Across Application Types

When analyzing multiple application logs simultaneously:

```bash
# Correlate by timestamp
grep "2025-10-28 14:30" /var/log/*/application.log

# Find related errors across PHP, Python, Node
for log in /var/log/{apache2/error.log,python/app.log,nodejs/app.log}; do
  echo "=== $log ==="
  grep -i "database error" "$log" | tail -3
done

# Extract IP addresses from all logs
grep -hroE '([0-9]{1,3}\.){3}[0-9]{1,3}' /var/log/ | sort | uniq -c | sort -rn
```

---

**Important subtopics for deeper CTF log analysis:**

- Web server access logs (Apache, Nginx) - reveals request patterns, attack vectors, and response codes
- Authentication logs (auth.log, secure) - tracks login attempts, privilege escalation, and account compromises
- Database logs (MySQL, PostgreSQL, MongoDB) - exposes query patterns, slow queries, and injection attempts
- System logs (syslog, dmesg, kernel logs) - contains kernel-level events, service starts/stops, and security events

---

## Java Application Logs

Java applications use various logging frameworks including Log4j, Logback, SLF4J, and java.util.logging (JUL). Log format, location, and verbosity depend on framework configuration and application server deployment.

### Log Location Patterns

**Application Server Locations:**

- **Tomcat**: `/var/log/tomcat9/catalina.out`, `/opt/tomcat/logs/`
- **JBoss/WildFly**: `/var/log/jboss/server.log`, `$JBOSS_HOME/standalone/log/`
- **WebLogic**: `$DOMAIN_HOME/servers/AdminServer/logs/`
- **WebSphere**: `/opt/IBM/WebSphere/AppServer/profiles/*/logs/`
- **Spring Boot**: Application directory or configured path, often `./logs/application.log`
- **Standalone JAR**: Working directory or specified in configuration

**Framework-specific configurations:**

- **Log4j**: `log4j.properties` or `log4j2.xml` defines `log4j.appender.file.File`
- **Logback**: `logback.xml` defines `<file>` elements
- **JUL**: `logging.properties` specifies handlers

**Common paths:**

```bash
/var/log/java/application.log
/opt/app/logs/application.log
/home/user/app/logs/app.log
~/logs/application.log
./logs/spring.log
```

### Java Logging Levels

Standard hierarchy (most frameworks):

```
FATAL/SEVERE - Critical errors causing application failure
ERROR - Serious errors, operation failed
WARN/WARNING - Unexpected situations, potential issues
INFO - Informational messages, normal operations
DEBUG - Detailed diagnostic information
TRACE - Very detailed diagnostic information
```

### Log Analysis Commands

**Basic inspection:**

```bash
# View real-time logs
tail -f /var/log/tomcat9/catalina.out

# Filter by log level
grep "ERROR" /var/log/java/application.log
grep "FATAL" /var/log/java/application.log
grep "Exception" /var/log/java/application.log

# Find stack traces
grep -A 30 "Exception:" /var/log/java/application.log

# Specific exception types
grep -E "NullPointerException|SQLException|IOException|ClassCastException" /var/log/java/application.log
```

**Exception analysis:**

```bash
# Extract complete stack traces
awk '/Exception:|Error:/{flag=1} flag{print; if(/^[[:space:]]*at/) next; else flag=0}' /var/log/java/application.log

# Find exception causes
grep -B 2 "Caused by:" /var/log/java/application.log

# Count exception types
grep -oE "[a-zA-Z.]+Exception|[a-zA-Z.]+Error" /var/log/java/application.log | sort | uniq -c | sort -rn

# Extract class names from stack traces
grep -oE "at [a-zA-Z0-9.$_]+\(" /var/log/java/application.log | cut -d'(' -f1 | cut -d' ' -f2 | sort -u
```

**Tomcat-specific analysis:**

```bash
# Servlet errors
grep "servlet" /var/log/tomcat9/catalina.out

# JSP compilation errors
grep "org.apache.jasper" /var/log/tomcat9/catalina.out

# Context initialization errors
grep "Context initialization" /var/log/tomcat9/catalina.out

# Memory issues
grep -i "OutOfMemoryError\|heap\|memory" /var/log/tomcat9/catalina.out
```

**Spring Framework logs:**

```bash
# Spring context errors
grep "org.springframework" /var/log/java/application.log

# Bean creation failures
grep "BeanCreationException\|BeanInitializationException" /var/log/java/application.log

# Database connection issues
grep "DataSource\|JdbcTemplate\|HikariPool" /var/log/java/application.log

# Security/authentication
grep "org.springframework.security" /var/log/java/application.log
```

### CTF-Specific Exploitation Patterns

**Deserialization Vulnerabilities:**

```bash
# Java deserialization errors (potential RCE)
grep -i "ObjectInputStream\|readObject\|deserialize" /var/log/java/application.log

# Commons Collections exploitation indicators
grep "org.apache.commons.collections" /var/log/java/application.log

# Serialization-related exceptions
grep "InvalidClassException\|StreamCorruptedException\|NotSerializableException" /var/log/java/application.log

# Specific gadget chain indicators
grep -E "InvokerTransformer|ChainedTransformer|TemplatesImpl" /var/log/java/application.log
```

**SQL Injection (JDBC):**

```bash
# SQL syntax errors
grep -i "SQLException\|SQL syntax\|SQLSyntaxErrorException" /var/log/java/application.log

# Database error messages
grep -E "ORA-[0-9]+|MySQL|PostgreSQL|MSSQL" /var/log/java/application.log

# PreparedStatement errors
grep "PreparedStatement\|Statement" /var/log/java/application.log

# Exposed SQL queries
grep -iE "SELECT|INSERT|UPDATE|DELETE" /var/log/java/application.log | grep -i error
```

**XML External Entity (XXE):**

```bash
# XML parsing errors
grep -i "SAXParseException\|XMLStreamException\|ParserConfigurationException" /var/log/java/application.log

# External entity indicators
grep -i "DOCTYPE\|ENTITY" /var/log/java/application.log

# Specific parsers
grep -E "DocumentBuilder|SAXParser|XMLReader|Unmarshaller" /var/log/java/application.log
```

**Server-Side Request Forgery (SSRF):**

```bash
# URL connection errors
grep -i "URLConnection\|HttpURLConnection\|URL\|URI" /var/log/java/application.log

# Connection refused/timeout (internal scanning attempts)
grep -i "ConnectException\|SocketTimeoutException\|UnknownHostException" /var/log/java/application.log

# Internal IP access attempts
grep -oE "10\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}|192\.168\.[0-9]{1,3}\.[0-9]{1,3}|172\.(1[6-9]|2[0-9]|3[0-1])\.[0-9]{1,3}\.[0-9]{1,3}" /var/log/java/application.log
```

**Path Traversal:**

```bash
# File operation errors
grep -i "FileNotFoundException\|IOException\|AccessDeniedException" /var/log/java/application.log

# Path traversal patterns
grep -E "\.\./|\.\.\\\\|%2e%2e" /var/log/java/application.log

# File operation methods
grep -E "FileInputStream|FileOutputStream|File\(|Files\." /var/log/java/application.log
```

**Expression Language (EL) Injection:**

```bash
# EL evaluation errors
grep -i "ELException\|ExpressionFactory" /var/log/java/application.log

# JSP EL errors
grep "javax.el" /var/log/java/application.log

# OGNL injection (Struts)
grep -i "ognl\|struts" /var/log/java/application.log
```

**Log4j/Log4Shell (CVE-2021-44228):**

```bash
# JNDI lookup attempts
grep -i "jndi:ldap\|jndi:rmi\|jndi:dns" /var/log/java/application.log

# Log4j error messages
grep "log4j" /var/log/java/application.log

# LDAP connection attempts
grep -i "ldap://\|ldaps://" /var/log/java/application.log

# Suspicious patterns in logs
grep -E "\$\{jndi:|\$\{.*:/" /var/log/java/application.log
```

### Information Disclosure Patterns

```bash
# Database credentials
grep -iE "jdbc:|username|password|connection string" /var/log/java/application.log

# API keys and secrets
grep -iE "api[_-]?key|secret|token|bearer" /var/log/java/application.log

# File system paths
grep -oE "/[a-zA-Z0-9/_.-]+\.java|/[a-zA-Z0-9/_.-]+\.class" /var/log/java/application.log | sort -u

# Environment variables
grep -oE "[A-Z_]+=[^ ]+" /var/log/java/application.log

# Internal hostnames and ports
grep -oE "[a-z0-9.-]+:[0-9]+" /var/log/java/application.log | sort -u

# Class and package structure
grep -oE "at [a-z.]+\.[A-Z][a-zA-Z0-9.]+\(" /var/log/java/application.log | sed 's/at //' | sed 's/\..*//' | sort -u
```

### Spring Boot Specific Analysis

**Actuator endpoint exposure:**

```bash
# Actuator access
grep "/actuator" /var/log/java/application.log

# Health check endpoints
grep -E "/health|/info|/metrics|/env" /var/log/java/application.log

# Management endpoints
grep "management.endpoints" /var/log/java/application.log
```

**Configuration issues:**

```bash
# Property resolution errors
grep "PropertySourcesPlaceholderConfigurer\|@Value" /var/log/java/application.log

# Profile activation
grep -i "active profile" /var/log/java/application.log

# Autowiring failures
grep "NoSuchBeanDefinitionException\|UnsatisfiedDependencyException" /var/log/java/application.log
```

### Memory and Performance Issues

```bash
# Out of memory errors
grep -i "OutOfMemoryError\|Java heap space\|PermGen\|Metaspace" /var/log/java/application.log

# Garbage collection logs
grep "GC\|Full GC" /var/log/java/application.log

# Thread dumps
grep "Full thread dump" /var/log/java/application.log

# Deadlocks
grep -i "deadlock" /var/log/java/application.log
```

### Automated Analysis Script

```bash
#!/bin/bash
# Java application log analyzer for CTF

LOGFILE="${1:-/var/log/java/application.log}"

echo "[+] Exception Summary:"
echo "  Total exceptions: $(grep -c "Exception" "$LOGFILE")"
echo "  NullPointerException: $(grep -c "NullPointerException" "$LOGFILE")"
echo "  SQLException: $(grep -c "SQLException" "$LOGFILE")"

echo -e "\n[+] Top 10 Exception Types:"
grep -oE "[a-zA-Z.]+Exception" "$LOGFILE" | sort | uniq -c | sort -rn | head -10

echo -e "\n[+] Deserialization Indicators:"
DESER_COUNT=$(grep -ic "readObject\|deserialize\|ObjectInputStream" "$LOGFILE")
echo "  Potential deserialization usage: $DESER_COUNT occurrences"

echo -e "\n[+] XXE Vulnerability Indicators:"
XXE_COUNT=$(grep -ic "SAXParseException\|ENTITY\|DOCTYPE" "$LOGFILE")
echo "  XML parsing errors: $XXE_COUNT occurrences"

echo -e "\n[+] SSRF Indicators:"
SSRF_COUNT=$(grep -ic "UnknownHostException\|ConnectException" "$LOGFILE")
echo "  Connection errors: $SSRF_COUNT occurrences"

echo -e "\n[+] Log4Shell Indicators:"
grep -i "jndi:" "$LOGFILE" | head -5

echo -e "\n[+] Disclosed File Paths:"
grep -oE "/[a-zA-Z0-9/_.-]+\.java" "$LOGFILE" | sort -u | head -10

echo -e "\n[+] Database Connection Strings:"
grep -i "jdbc:" "$LOGFILE" | head -3

echo -e "\n[+] Recent Critical Errors:"
grep -E "ERROR|FATAL" "$LOGFILE" | tail -10
```

### Log4j Configuration Analysis

**Locate Log4j configuration:**

```bash
# Find Log4j config files
find / -name "log4j.properties" 2>/dev/null
find / -name "log4j2.xml" 2>/dev/null
find / -name "log4j.xml" 2>/dev/null

# Check classpath locations
find /opt -name "log4j*.jar" 2>/dev/null
find /usr/share -name "log4j*.jar" 2>/dev/null

# Inside running JAR
jar tf application.jar | grep log4j
```

**Analyze configuration:**

```bash
# View Log4j configuration
cat /path/to/log4j.properties

# Find log file locations
grep "log4j.appender.*File" /path/to/log4j.properties

# Check logging levels
grep "log4j.rootLogger" /path/to/log4j.properties
grep "log4j.logger" /path/to/log4j.properties
```

### Java Web Application Frameworks

**Struts2 exploitation indicators:**

```bash
# OGNL injection attempts
grep -i "ognl\|struts2" /var/log/java/application.log

# Struts2 exceptions
grep "com.opensymphony.xwork2\|org.apache.struts2" /var/log/java/application.log

# Action errors
grep "ActionSupport\|ActionContext" /var/log/java/application.log
```

**JSF (JavaServer Faces):**

```bash
# JSF lifecycle errors
grep "javax.faces" /var/log/java/application.log

# ViewState manipulation
grep -i "viewstate\|ViewExpiredException" /var/log/java/application.log
```

## Database Query Logs

Database query logs record executed queries, errors, slow queries, and connection events. These logs are critical for identifying SQL injection, privilege escalation, and data exfiltration attempts in CTF scenarios.

### MySQL/MariaDB Query Logs

**Log Types:**

- **General Query Log**: All client connections and queries
- **Error Log**: Server startup/shutdown, errors, warnings
- **Slow Query Log**: Queries exceeding `long_query_time`
- **Binary Log**: Data modification events (replication/recovery)

**Default locations (Linux):**

```bash
/var/log/mysql/error.log
/var/log/mysql/mysql.log
/var/log/mysql/mysql-slow.log
/var/lib/mysql/hostname.err
/var/lib/mysql/hostname.log
```

**Enable query logging:**

```sql
-- Check current settings
SHOW VARIABLES LIKE 'general_log%';
SHOW VARIABLES LIKE 'slow_query_log%';

-- Enable general query log
SET GLOBAL general_log = 'ON';
SET GLOBAL general_log_file = '/var/log/mysql/query.log';

-- Enable slow query log
SET GLOBAL slow_query_log = 'ON';
SET GLOBAL long_query_time = 2;
SET GLOBAL slow_query_log_file = '/var/log/mysql/slow.log';
```

**Configuration file settings (`/etc/mysql/my.cnf`):**

```ini
[mysqld]
general_log = 1
general_log_file = /var/log/mysql/query.log
slow_query_log = 1
slow_query_log_file = /var/log/mysql/slow.log
long_query_time = 2
log_queries_not_using_indexes = 1
```

**Query log analysis:**

```bash
# View recent queries
tail -f /var/log/mysql/query.log

# Find queries from specific IP/user
grep "Connect.*192.168.1.100" /var/log/mysql/query.log
grep "user@host: admin" /var/log/mysql/query.log

# Search for specific query patterns
grep -i "SELECT" /var/log/mysql/query.log
grep -i "INSERT\|UPDATE\|DELETE" /var/log/mysql/query.log
grep -i "DROP\|ALTER\|CREATE" /var/log/mysql/query.log

# Find authentication attempts
grep "Connect\|Quit" /var/log/mysql/query.log

# Access denied errors
grep "Access denied" /var/log/mysql/error.log
```

**SQL Injection detection patterns:**

```bash
# Union-based injection
grep -i "UNION.*SELECT" /var/log/mysql/query.log

# Boolean-based blind injection
grep -E "AND.*=|OR.*=|AND.*1=1|OR.*1=1" /var/log/mysql/query.log

# Time-based blind injection
grep -i "SLEEP\|BENCHMARK\|WAIT FOR DELAY" /var/log/mysql/query.log

# Error-based injection
grep -i "EXTRACTVALUE\|UPDATEXML\|EXP\(" /var/log/mysql/query.log

# Stacked queries
grep ";" /var/log/mysql/query.log | grep -v "^--"

# Information schema queries
grep -i "information_schema" /var/log/mysql/query.log

# Comment-based injection
grep -E "/\*|--|\#" /var/log/mysql/query.log
```

**Data exfiltration indicators:**

```bash
# Large result sets
grep -i "SELECT.*FROM" /var/log/mysql/query.log | grep -v "LIMIT"

# INTO OUTFILE usage
grep -i "INTO OUTFILE\|INTO DUMPFILE" /var/log/mysql/query.log

# LOAD_FILE attempts
grep -i "LOAD_FILE" /var/log/mysql/query.log

# Bulk data access
grep -i "SELECT \*" /var/log/mysql/query.log
```

**Privilege escalation indicators:**

```bash
# User manipulation
grep -i "CREATE USER\|DROP USER\|GRANT\|REVOKE" /var/log/mysql/query.log

# Password changes
grep -i "SET PASSWORD\|ALTER USER.*PASSWORD" /var/log/mysql/query.log

# File privilege operations
grep -i "FILE_PRIV\|SUPER_PRIV" /var/log/mysql/query.log
```

### PostgreSQL Query Logs

**Log location:**

```bash
/var/log/postgresql/postgresql-15-main.log
/var/lib/pgsql/data/log/postgresql-*.log
```

**Enable logging (`/etc/postgresql/*/main/postgresql.conf`):**

```ini
logging_collector = on
log_directory = 'log'
log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'
log_statement = 'all'           # none, ddl, mod, all
log_duration = on
log_connections = on
log_disconnections = on
log_hostname = on
log_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '
```

**Query log analysis:**

```bash
# View recent logs
tail -f /var/log/postgresql/postgresql-15-main.log

# Find specific user queries
grep "user=admin" /var/log/postgresql/postgresql-15-main.log

# Connection attempts
grep "connection authorized\|connection received" /var/log/postgresql/postgresql-15-main.log

# Failed authentication
grep "authentication failed\|password authentication failed" /var/log/postgresql/postgresql-15-main.log

# Query execution times
grep "duration:" /var/log/postgresql/postgresql-15-main.log | awk '{print $NF}' | sort -n
```

**SQL Injection patterns:**

```bash
# Union-based
grep -i "UNION.*SELECT" /var/log/postgresql/postgresql-15-main.log

# Boolean-based
grep -E "AND.*=.*|OR.*=.*" /var/log/postgresql/postgresql-15-main.log

# Time-based (pg_sleep)
grep -i "pg_sleep" /var/log/postgresql/postgresql-15-main.log

# Error-based
grep -i "ERROR:.*syntax error" /var/log/postgresql/postgresql-15-main.log

# Stacked queries
grep ";.*SELECT\|;.*UPDATE\|;.*DELETE" /var/log/postgresql/postgresql-15-main.log

# Information schema enumeration
grep -i "information_schema\|pg_catalog" /var/log/postgresql/postgresql-15-main.log
```

**PostgreSQL-specific exploitation:**

```bash
# COPY command (file read/write)
grep -i "COPY.*FROM\|COPY.*TO" /var/log/postgresql/postgresql-15-main.log

# Large objects (lo_import/lo_export)
grep -i "lo_import\|lo_export\|lo_create" /var/log/postgresql/postgresql-15-main.log

# Function execution
grep -i "CREATE FUNCTION\|CREATE LANGUAGE" /var/log/postgresql/postgresql-15-main.log

# Extension usage
grep -i "CREATE EXTENSION\|pg_execute_server_program" /var/log/postgresql/postgresql-15-main.log
```

### MongoDB Query Logs

**Log location:**

```bash
/var/log/mongodb/mongod.log
/var/log/mongodb/mongodb.log
```

**Enable profiling and logging:**

```javascript
// Connect to MongoDB
mongosh

// Enable profiling (0=off, 1=slow, 2=all)
db.setProfilingLevel(2);

// Check profiling status
db.getProfilingStatus();

// View profiled queries
db.system.profile.find().pretty();
```

**Configuration (`/etc/mongod.conf`):**

```yaml
systemLog:
  destination: file
  path: /var/log/mongodb/mongod.log
  logAppend: true
  verbosity: 1

operationProfiling:
  mode: all
```

**Query log analysis:**

```bash
# View recent logs
tail -f /var/log/mongodb/mongod.log

# Find slow queries
grep -i "Slow query" /var/log/mongodb/mongod.log

# Connection events
grep -i "connection.*accepted\|connection.*ended" /var/log/mongodb/mongod.log

# Authentication failures
grep -i "authentication failed\|SCRAM-SHA" /var/log/mongodb/mongod.log

# Command execution
grep '"command"' /var/log/mongodb/mongod.log
```

**NoSQL Injection patterns:**

```bash
# JavaScript injection in $where
grep '\$where' /var/log/mongodb/mongod.log

# Operator injection
grep -E '\$ne|\$gt|\$lt|\$gte|\$lte|\$regex|\$in' /var/log/mongodb/mongod.log

# eval() usage
grep 'eval(' /var/log/mongodb/mongod.log

# mapReduce injection
grep 'mapReduce' /var/log/mongodb/mongod.log
```

**Extract queries from profiler:**

```javascript
// View all profiled operations
db.system.profile.find({}).pretty();

// Find slow queries
db.system.profile.find({ millis: { $gt: 100 } }).pretty();

// Find queries by collection
db.system.profile.find({ ns: "database.collection" }).pretty();

// Find queries by user
db.system.profile.find({ user: "admin@admin" }).pretty();

// Extract command patterns
db.system.profile.find().forEach(function(doc) {
    printjson(doc.command);
});
```

### MSSQL Query Logs

**Log location:**

```
C:\Program Files\Microsoft SQL Server\MSSQL15.MSSQLSERVER\MSSQL\Log\ERRORLOG
```

**Enable auditing (T-SQL):**

```sql
-- Create server audit
USE master;
CREATE SERVER AUDIT CTF_Audit
TO FILE (FILEPATH = 'C:\SQLAudit\');

-- Create audit specification
CREATE SERVER AUDIT SPECIFICATION CTF_Audit_Spec
FOR SERVER AUDIT CTF_Audit
ADD (SUCCESSFUL_LOGIN_GROUP),
ADD (FAILED_LOGIN_GROUP),
ADD (BATCH_COMPLETED_GROUP);

-- Enable audit
ALTER SERVER AUDIT CTF_Audit WITH (STATE = ON);
ALTER SERVER AUDIT SPECIFICATION CTF_Audit_Spec WITH (STATE = ON);
```

**Query error log:**

```sql
-- Read error log
EXEC xp_readerrorlog;

-- Search for specific text
EXEC xp_readerrorlog 0, 1, 'Login failed';

-- Search in archive logs
EXEC xp_readerrorlog 1, 1, 'error';
```

**Command-line analysis (PowerShell):**

```powershell
# View error log
Get-Content "C:\Program Files\Microsoft SQL Server\MSSQL15.MSSQLSERVER\MSSQL\Log\ERRORLOG"

# Find login failures
Select-String -Path "C:\Program Files\Microsoft SQL Server\MSSQL15.MSSQLSERVER\MSSQL\Log\ERRORLOG" -Pattern "Login failed"

# Search for specific patterns
Select-String -Path "C:\Program Files\Microsoft SQL Server\MSSQL15.MSSQLSERVER\MSSQL\Log\ERRORLOG" -Pattern "xp_cmdshell|sp_configure"
```

**SQL Injection patterns:**

```powershell
# xp_cmdshell usage
Select-String -Path "ERRORLOG" -Pattern "xp_cmdshell"

# Dynamic SQL
Select-String -Path "ERRORLOG" -Pattern "EXEC.*sp_executesql"

# UNION injection
Select-String -Path "ERRORLOG" -Pattern "UNION.*SELECT"

# Error-based injection
Select-String -Path "ERRORLOG" -Pattern "conversion failed|cast.*error"
```

### SQLite Query Logs

[Inference] SQLite does not have built-in query logging. Query activity must be logged at the application level or through wrapper libraries. Standard SQLite databases write to disk but do not maintain separate query logs like client-server databases.

**Application-level logging approaches:**

- Use Python's `sqlite3` module with custom logging
- Implement query logging in application code
- Use SQLite extensions or triggers for audit trails

### Generic Database Log Analysis

**Automated multi-database log analyzer:**

```bash
#!/bin/bash
# Database log analyzer for CTF

echo "=== Database Log Analysis ==="

# MySQL
if [ -f /var/log/mysql/query.log ]; then
    echo -e "\n[+] MySQL Query Log Analysis:"
    echo "  Total queries: $(wc -l < /var/log/mysql/query.log)"
    echo "  UNION attacks: $(grep -ci "UNION.*SELECT" /var/log/mysql/query.log)"
    echo "  SLEEP usage: $(grep -ci "SLEEP\|BENCHMARK" /var/log/mysql/query.log)"
    echo "  File operations: $(grep -ci "INTO OUTFILE\|LOAD_FILE" /var/log/mysql/query.log)"
fi

# PostgreSQL
if [ -f /var/log/postgresql/postgresql-15-main.log ]; then
    echo -e "\n[+] PostgreSQL Log Analysis:"
    echo "  Total queries: $(grep -c "LOG:.*statement:" /var/log/postgresql/postgresql-15-main.log)"
    echo "  Failed auth: $(grep -c "authentication failed" /var/log/postgresql/postgresql-15-main.log)"
    echo "  COPY commands: $(grep -ci "COPY.*FROM\|COPY.*TO" /var/log/postgresql/postgresql-15-main.log)"
fi

# MongoDB
if [ -f /var/log/mongodb/mongod.log ]; then
    echo -e "\n[+] MongoDB Log Analysis:"
    echo "  Total connections: $(grep -c "connection.*accepted" /var/log/mongodb/mongod.log)"
    echo "  Operator injection: $(grep -cE '\$where|\$ne|\$gt' /var/log/mongodb/mongod.log)"
fi
```

**SQL Injection signature detection:**

```bash
#!/bin/bash
# SQL injection pattern detector

LOGFILE="$1"

echo "[+] SQL Injection Pattern Detection"

patterns=(
    "UNION.*SELECT:Union-based"
    "AND.*1=1|OR.*1=1:Boolean-based"
    "SLEEP\(|BENCHMARK\(:Time-based"
    "EXTRACTVALUE|UPDATEXML:Error-based"
    "INTO OUTFILE|LOAD_FILE:File operations"
    "information_schema:Schema enumeration"
    ";.*SELECT|;.*INSERT:Stacked queries"
    "--.*$|\#.*$|/\*:Comment injection"
)

for pattern in "${patterns[@]}"; do
    IFS=':' read -r regex desc <<< "$pattern"
    count=$(grep -ciE "$regex" "$LOGFILE" 2>/dev/null || echo "0")
    if [ "$count" -gt 0 ]; then
        echo "  [!] $desc: $count occurrences"
    fi
done
```

## Custom Application Logging

Custom application logging refers to application-specific log implementations beyond standard frameworks. These logs often contain business logic events, custom error handling, and application-specific security events valuable in CTF scenarios.

### Custom Log Formats

**Common custom formats:**

- **CSV**: Comma-separated values for structured data
- **JSON**: Structured logging with nested objects
- **Key-Value**: Simple key=value pairs
- **Fixed-width**: Column-based format
- **Syslog**: RFC 5424 or RFC 3164 format
- **Custom delimited**: Tab, pipe, or custom separators

**Examples of custom formats:**

```
# CSV format
timestamp,level,user,action,result,message
2025-10-28 14:30:15,INFO,admin,login,success,"User logged in from 192.168.1.100"

# JSON format
{"timestamp":"2025-10-28T14:30:15Z","level":"INFO","user":"admin","action":"login","ip":"192.168.1.100"}

# Key-value format
time=2025-10-28T14:30:15 level=INFO user=admin action=login ip=192.168.1.100

# Custom delimited
2025-10-28 14:30:15|INFO|admin|login|192.168.1.100|success
```

### Identifying Custom Log Locations

**Common discovery methods:**

```bash
# Search for log files by name patterns
find /var/log -type f -name "*.log" 2>/dev/null
find /opt -type f -name "*.log" 2>/dev/null
find /home -type f -name "*.log" 2>/dev/null

# Find recently modified log files
find /var/log -type f -mmin -60 2>/dev/null

# Search for files containing log-like content
grep -r "ERROR\|WARNING\|INFO" /var/log 2>/dev/null | cut -d: -f1 | sort -u

# Look for custom log directories
ls -la /var/log/*/
ls -la /opt/*/logs/

# Check running processes for open log files
lsof | grep -i "\.log"

# Process-specific log file discovery
lsof -p $(pgrep -f "application_name") | grep -E "\.log|\.txt"
```

**Configuration file analysis:**

```bash
# Find application config files
find / -name "config.ini" -o -name "settings.conf" -o -name ".env" 2>/dev/null

# Search configs for log paths
grep -r "log.*path\|log.*file\|log.*dir" /etc/ 2>/dev/null
grep -r "LOG_FILE\|LOG_PATH" /opt/ 2>/dev/null

# Check environment variables
env | grep -i log
```

### Analyzing CSV Format Logs

**Basic analysis:**

```bash
# View log structure
head -1 /var/log/custom/application.csv
head -10 /var/log/custom/application.csv

# Count total entries
wc -l /var/log/custom/application.csv

# Extract specific columns (column 4 = action)
awk -F',' '{print $4}' /var/log/custom/application.csv | sort -u

# Filter by specific field value
awk -F',' '$4 == "login"' /var/log/custom/application.csv

# Find entries with specific level
grep ",ERROR," /var/log/custom/application.csv grep ",CRITICAL," /var/log/custom/application.csv
````

**Advanced CSV analysis:**
```bash
# Count actions by type
awk -F',' '{print $4}' /var/log/custom/application.csv | sort | uniq -c | sort -rn

# Find failed operations
awk -F',' '$5 == "failed" || $5 == "error"' /var/log/custom/application.csv

# Extract entries for specific user
awk -F',' '$3 == "admin"' /var/log/custom/application.csv

# Time-based filtering (assuming timestamp in column 1)
awk -F',' '$1 >= "2025-10-28 14:00:00" && $1 <= "2025-10-28 15:00:00"' /var/log/custom/application.csv

# Find anomalies (e.g., multiple failed logins)
awk -F',' '$4 == "login" && $5 == "failed" {print $3}' /var/log/custom/application.csv | sort | uniq -c | sort -rn
````

**Using csvkit for advanced analysis:**

```bash
# Install csvkit (if available)
pip install csvkit

# View CSV structure
csvstat /var/log/custom/application.csv

# Query CSV like SQL
csvsql --query "SELECT user, COUNT(*) FROM application WHERE action='login' GROUP BY user" /var/log/custom/application.csv

# Convert to JSON
csvjson /var/log/custom/application.csv

# Filter rows
csvgrep -c action -m "login" /var/log/custom/application.csv
```

### Analyzing JSON Format Logs

**Basic JSON log analysis:**

```bash
# Pretty print JSON logs
cat /var/log/custom/application.json | jq '.'

# Extract all log levels
cat /var/log/custom/application.json | jq -r '.level'

# Filter by log level
cat /var/log/custom/application.json | jq 'select(.level == "ERROR")'

# Extract specific fields
cat /var/log/custom/application.json | jq -r '.timestamp, .user, .action'

# Count entries by action
cat /var/log/custom/application.json | jq -r '.action' | sort | uniq -c | sort -rn
```

**Advanced JSON analysis:**

```bash
# Filter by multiple conditions
cat /var/log/custom/application.json | jq 'select(.level == "ERROR" and .user == "admin")'

# Extract nested fields
cat /var/log/custom/application.json | jq -r '.metadata.ip_address'

# Find entries within time range
cat /var/log/custom/application.json | jq 'select(.timestamp >= "2025-10-28T14:00:00" and .timestamp <= "2025-10-28T15:00:00")'

# Group and count
cat /var/log/custom/application.json | jq -r '.action' | sort | uniq -c

# Search for specific patterns in messages
cat /var/log/custom/application.json | jq 'select(.message | test("password|credential"; "i"))'

# Extract failed operations with details
cat /var/log/custom/application.json | jq 'select(.result == "failed") | {time: .timestamp, user: .user, action: .action, reason: .message}'
```

**JSON log aggregation script:**

```bash
#!/bin/bash
# JSON log analyzer

LOGFILE="$1"

echo "[+] JSON Log Analysis"

echo -e "\n[+] Log Level Distribution:"
jq -r '.level' "$LOGFILE" | sort | uniq -c | sort -rn

echo -e "\n[+] Top 10 Users:"
jq -r '.user // "anonymous"' "$LOGFILE" | sort | uniq -c | sort -rn | head -10

echo -e "\n[+] Top 10 Actions:"
jq -r '.action' "$LOGFILE" | sort | uniq -c | sort -rn | head -10

echo -e "\n[+] Failed Operations:"
jq -r 'select(.result == "failed" or .status == "error") | "\(.timestamp) - \(.user) - \(.action) - \(.message)"' "$LOGFILE" | head -20

echo -e "\n[+] Unique IP Addresses:"
jq -r '.ip // .ip_address // .client_ip' "$LOGFILE" 2>/dev/null | sort -u | wc -l
```

### Analyzing Key-Value Format Logs

**Basic key-value analysis:**

```bash
# View log samples
head -20 /var/log/custom/application.log

# Extract specific key values
grep -oE "user=[^ ]+" /var/log/custom/application.log | cut -d= -f2 | sort -u

# Find entries with specific key-value
grep "action=login" /var/log/custom/application.log

# Extract multiple keys
awk '{for(i=1;i<=NF;i++) if($i~/^(time|user|action)=/) print $i}' /var/log/custom/application.log
```

**Convert key-value to structured format:**

```bash
#!/bin/bash
# Convert key-value logs to CSV

INPUT="$1"
OUTPUT="${2:-converted.csv}"

# Extract unique keys
keys=$(head -100 "$INPUT" | grep -oE "[a-zA-Z_]+=" | sed 's/=$//' | sort -u | tr '\n' ',' | sed 's/,$//')

echo "$keys" > "$OUTPUT"

# Parse and convert each line
while IFS= read -r line; do
    IFS=',' read -ra KEY_ARRAY <<< "$keys"
    row=""
    for key in "${KEY_ARRAY[@]}"; do
        value=$(echo "$line" | grep -oE "$key=[^ ]+" | cut -d= -f2)
        row="$row,$value"
    done
    echo "${row#,}" >> "$OUTPUT"
done < "$INPUT"

echo "[+] Converted to $OUTPUT"
```

**Advanced key-value parsing:**

```bash
# Extract all key-value pairs into associative array (bash 4+)
parse_kv() {
    local line="$1"
    declare -A kv_pairs
    
    for pair in $line; do
        if [[ $pair =~ ^([^=]+)=(.+)$ ]]; then
            kv_pairs["${BASH_REMATCH[1]}"]="${BASH_REMATCH[2]}"
        fi
    done
    
    # Access values
    echo "User: ${kv_pairs[user]}, Action: ${kv_pairs[action]}"
}

# Usage
while IFS= read -r line; do
    parse_kv "$line"
done < /var/log/custom/application.log
```

### Custom Log Pattern Analysis

**Common CTF patterns in custom logs:**

```bash
# Authentication events
grep -iE "login|logout|auth|signin|signout" /var/log/custom/application.log

# Failed authentication attempts
grep -iE "failed.*login|auth.*failed|invalid.*password|access.*denied" /var/log/custom/application.log

# Privilege escalation indicators
grep -iE "privilege|permission|role.*change|admin|sudo|su\s" /var/log/custom/application.log

# File access patterns
grep -iE "file.*read|file.*write|download|upload|access.*file" /var/log/custom/application.log

# Database operations
grep -iE "query|database|sql|insert|update|delete|select" /var/log/custom/application.log

# API calls
grep -iE "api.*call|endpoint|request.*to|response.*from" /var/log/custom/application.log

# Suspicious activity
grep -iE "suspicious|anomaly|blocked|rejected|violation" /var/log/custom/application.log

# Error conditions
grep -iE "error|exception|fatal|critical|failure" /var/log/custom/application.log
```

**User behavior analysis:**

```bash
#!/bin/bash
# User activity analyzer for custom logs

LOGFILE="$1"
USER="${2:-all}"

if [ "$USER" = "all" ]; then
    echo "[+] Top 20 Active Users:"
    grep -oE "user[=:][^ ,]+" "$LOGFILE" | cut -d= -f2 | cut -d: -f2 | sort | uniq -c | sort -rn | head -20
else
    echo "[+] Activity for user: $USER"
    grep -E "user[=:]$USER" "$LOGFILE" | wc -l
    echo "  Total actions"
    
    echo -e "\n[+] Actions breakdown:"
    grep -E "user[=:]$USER" "$LOGFILE" | grep -oE "action[=:][^ ,]+" | cut -d= -f2 | cut -d: -f2 | sort | uniq -c | sort -rn
    
    echo -e "\n[+] Failed operations:"
    grep -E "user[=:]$USER" "$LOGFILE" | grep -iE "fail|error|denied" | head -10
    
    echo -e "\n[+] Recent activity:"
    grep -E "user[=:]$USER" "$LOGFILE" | tail -20
fi
```

### Application Security Event Logs

**Common security events in custom logs:**

**Input validation failures:**

```bash
# SQL injection attempts
grep -iE "sql.*injection|union.*select|'; drop|admin'--" /var/log/custom/security.log

# XSS attempts
grep -iE "<script|javascript:|onerror=|onload=" /var/log/custom/security.log

# Path traversal
grep -E "\.\./|\.\.\\\\|%2e%2e" /var/log/custom/security.log

# Command injection
grep -iE ";.*ls|;.*cat|;.*wget|\|.*curl|`.*id`" /var/log/custom/security.log
```

**Rate limiting and abuse:**

```bash
# Identify rate-limited IPs
grep -i "rate.*limit\|too many requests\|429" /var/log/custom/application.log | grep -oE "([0-9]{1,3}\.){3}[0-9]{1,3}" | sort | uniq -c | sort -rn

# Brute force attempts
grep -i "login.*failed\|auth.*failed" /var/log/custom/security.log | grep -oE "user[=:][^ ,]+" | cut -d= -f2 | cut -d: -f2 | sort | uniq -c | sort -rn | head -20

# Multiple failures from same IP
grep -i "failed" /var/log/custom/security.log | grep -oE "([0-9]{1,3}\.){3}[0-9]{1,3}" | sort | uniq -c | sort -rn | head -20
```

**Session and token analysis:**

```bash
# Session hijacking indicators
grep -iE "session.*invalid|session.*expired|token.*invalid" /var/log/custom/security.log

# Multiple sessions from different IPs
awk '/session/ {print $0}' /var/log/custom/security.log | grep -oE "session[=:][^ ,]+" | cut -d= -f2 | cut -d: -f2 | sort | uniq -c | sort -rn

# Token usage patterns
grep -oE "token[=:][^ ,]+" /var/log/custom/application.log | cut -d= -f2 | cut -d: -f2 | sort | uniq -c | sort -rn
```

### Automated Custom Log Analysis Framework

**Generic log analyzer:**

```bash
#!/bin/bash
# Universal custom log analyzer for CTF

LOGFILE="$1"

if [ ! -f "$LOGFILE" ]; then
    echo "Usage: $0 <logfile>"
    exit 1
fi

echo "=== Custom Log Analysis ==="
echo "File: $LOGFILE"
echo "Size: $(du -h "$LOGFILE" | cut -f1)"
echo "Lines: $(wc -l < "$LOGFILE")"

# Detect log format
echo -e "\n[+] Detecting Log Format..."
if head -1 "$LOGFILE" | grep -q "^{"; then
    FORMAT="JSON"
elif head -1 "$LOGFILE" | grep -q ","; then
    FORMAT="CSV"
elif head -1 "$LOGFILE" | grep -qE "[a-zA-Z_]+=[^ ]+"; then
    FORMAT="KEY-VALUE"
else
    FORMAT="UNKNOWN"
fi
echo "  Detected format: $FORMAT"

# Sample entries
echo -e "\n[+] Sample Log Entries:"
head -5 "$LOGFILE"

# Error analysis
echo -e "\n[+] Error/Failure Analysis:"
ERROR_COUNT=$(grep -ciE "error|fail|exception|critical" "$LOGFILE")
echo "  Total errors/failures: $ERROR_COUNT"

if [ $ERROR_COUNT -gt 0 ]; then
    echo -e "\n  Recent errors:"
    grep -iE "error|fail|exception|critical" "$LOGFILE" | tail -10
fi

# Security event detection
echo -e "\n[+] Security Event Detection:"
AUTH_FAIL=$(grep -ciE "auth.*fail|login.*fail|access.*denied" "$LOGFILE")
echo "  Authentication failures: $AUTH_FAIL"

INJECTION=$(grep -ciE "union.*select|<script|\.\.\/|; drop" "$LOGFILE")
echo "  Potential injection attempts: $INJECTION"

# IP address analysis
echo -e "\n[+] IP Address Analysis:"
IPS=$(grep -oE "([0-9]{1,3}\.){3}[0-9]{1,3}" "$LOGFILE" | sort -u | wc -l)
echo "  Unique IP addresses: $IPS"

if [ $IPS -gt 0 ]; then
    echo -e "\n  Top 10 IPs:"
    grep -oE "([0-9]{1,3}\.){3}[0-9]{1,3}" "$LOGFILE" | sort | uniq -c | sort -rn | head -10
fi

# Time-based analysis
echo -e "\n[+] Time-Based Analysis:"
if grep -qE "[0-9]{4}-[0-9]{2}-[0-9]{2}" "$LOGFILE"; then
    echo "  First entry: $(grep -oE "[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}" "$LOGFILE" | head -1)"
    echo "  Last entry: $(grep -oE "[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}" "$LOGFILE" | tail -1)"
fi

# User analysis (if user field exists)
echo -e "\n[+] User Analysis:"
if grep -qE "user[=:]" "$LOGFILE"; then
    echo "  Top 10 users:"
    grep -oE "user[=:][^ ,]+" "$LOGFILE" | cut -d= -f2 | cut -d: -f2 | sort | uniq -c | sort -rn | head -10
fi

# Action/Event analysis
echo -e "\n[+] Action/Event Analysis:"
if grep -qE "action[=:]|event[=:]" "$LOGFILE"; then
    echo "  Top 10 actions/events:"
    grep -oE "(action|event)[=:][^ ,]+" "$LOGFILE" | cut -d= -f2 | cut -d: -f2 | sort | uniq -c | sort -rn | head -10
fi

# Anomaly detection
echo -e "\n[+] Potential Anomalies:"

# Unusual user agents
UA_COUNT=$(grep -ciE "user.*agent|useragent" "$LOGFILE")
if [ $UA_COUNT -gt 0 ]; then
    echo "  Unusual user agents detected: $UA_COUNT"
    grep -iE "bot|scanner|nikto|sqlmap|burp|curl|wget|python" "$LOGFILE" | head -5
fi

# Large payloads
LARGE_COUNT=$(grep -E ".{500,}" "$LOGFILE" | wc -l)
echo "  Entries with large payloads (>500 chars): $LARGE_COUNT"

# Rapid requests
echo -e "\n[+] Rapid Request Detection:"
if grep -qE "[0-9]{2}:[0-9]{2}:[0-9]{2}" "$LOGFILE"; then
    echo "  Analyzing request frequency..."
    grep -oE "[0-9]{2}:[0-9]{2}:[0-9]{2}" "$LOGFILE" | cut -d: -f1,2 | sort | uniq -c | sort -rn | head -5
fi

echo -e "\n=== Analysis Complete ==="
```

### Custom Log Correlation

**Cross-log correlation script:**

```bash
#!/bin/bash
# Correlate events across multiple custom logs

APP_LOG="$1"
SECURITY_LOG="$2"
TIMEFRAME="${3:-10}" # minutes

echo "[+] Cross-Log Correlation Analysis"

# Extract suspicious IPs from security log
echo -e "\n[+] Suspicious IPs from security log:"
SUSPICIOUS_IPS=$(grep -iE "blocked|rejected|failed" "$SECURITY_LOG" | grep -oE "([0-9]{1,3}\.){3}[0-9]{1,3}" | sort -u)

echo "$SUSPICIOUS_IPS" | head -10

# Find those IPs in application log
echo -e "\n[+] Suspicious IP activity in application log:"
for ip in $SUSPICIOUS_IPS; do
    count=$(grep -c "$ip" "$APP_LOG")
    if [ $count -gt 0 ]; then
        echo "  $ip: $count entries"
        grep "$ip" "$APP_LOG" | head -3
        echo "  ---"
    fi
done

# Time-based correlation
echo -e "\n[+] Time-Correlated Events:"
# Extract timestamps from security events
grep -iE "error|fail|blocked" "$SECURITY_LOG" | grep -oE "[0-9]{2}:[0-9]{2}:[0-9]{2}" | sort -u | while read timestamp; do
    hour=$(echo $timestamp | cut -d: -f1)
    minute=$(echo $timestamp | cut -d: -f2)
    
    # Find related events in app log
    app_events=$(grep "$hour:$minute" "$APP_LOG" | wc -l)
    if [ $app_events -gt 10 ]; then
        echo "  High activity at $timestamp: $app_events events"
    fi
done
```

### Custom Log Visualization Data Extraction

**Extract data for visualization:**

```bash
#!/bin/bash
# Extract time-series data for visualization

LOGFILE="$1"
OUTPUT="${2:-timeseries.csv}"

echo "timestamp,count,level" > "$OUTPUT"

# Extract timestamp and level, then count
grep -oE "[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}.*" "$LOGFILE" | while read line; do
    timestamp=$(echo "$line" | grep -oE "[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}")
    level=$(echo "$line" | grep -oE "ERROR|WARN|INFO|DEBUG" | head -1)
    echo "$timestamp,1,${level:-UNKNOWN}"
done | awk -F, '{count[$1","$3]++} END {for (key in count) print key","count[key]}' | sort >> "$OUTPUT"

echo "[+] Time-series data exported to $OUTPUT"
```

**Extract user activity for analysis:**

```bash
#!/bin/bash
# Extract user activity matrix

LOGFILE="$1"
OUTPUT="${2:-user_activity.csv}"

echo "user,action,count" > "$OUTPUT"

# Extract user and action pairs
grep -oE "(user|username)[=:][^ ,]+" "$LOGFILE" | cut -d= -f2 | cut -d: -f2 > /tmp/users.txt
grep -oE "action[=:][^ ,]+" "$LOGFILE" | cut -d= -f2 | cut -d: -f2 > /tmp/actions.txt

paste -d, /tmp/users.txt /tmp/actions.txt | sort | uniq -c | awk '{print $2","$1}' >> "$OUTPUT"

rm /tmp/users.txt /tmp/actions.txt

echo "[+] User activity matrix exported to $OUTPUT"
```

### Binary and Encoded Log Analysis

**Handling base64-encoded logs:**

```bash
# Decode base64 log entries
grep -oE "[A-Za-z0-9+/]{20,}={0,2}" /var/log/custom/encoded.log | while read encoded; do
    decoded=$(echo "$encoded" | base64 -d 2>/dev/null)
    if [ $? -eq 0 ]; then
        echo "Encoded: $encoded"
        echo "Decoded: $decoded"
        echo "---"
    fi
done

# Search for specific patterns in encoded logs
for line in $(grep -oE "[A-Za-z0-9+/]{20,}={0,2}" /var/log/custom/encoded.log); do
    decoded=$(echo "$line" | base64 -d 2>/dev/null)
    if echo "$decoded" | grep -qi "password\|secret\|key"; then
        echo "Sensitive data found: $decoded"
    fi
done
```

**Hex-encoded log analysis:**

```bash
# Decode hex logs
grep -oE "([0-9a-fA-F]{2} ){10,}" /var/log/custom/hex.log | while read hex; do
    echo "$hex" | xxd -r -p
    echo ""
done

# Search for specific strings in hex logs
echo -n "password" | xxd -p  # Get hex representation
# Then search for that pattern: grep "70617373776f7264" /var/log/custom/hex.log
```

### Log Tampering Detection

**Detect log manipulation:**

```bash
#!/bin/bash
# Detect potential log tampering

LOGFILE="$1"

echo "[+] Log Integrity Analysis"

# Check for time anomalies
echo -e "\n[+] Timestamp Anomalies:"
timestamps=$(grep -oE "[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}" "$LOGFILE" | sort)
prev_ts=""
for ts in $timestamps; do
    if [ -n "$prev_ts" ]; then
        # [Inference] Simple timestamp comparison - may need adjustment based on actual timestamp format
        if [[ "$ts" < "$prev_ts" ]]; then
            echo "  [!] Out-of-order timestamp detected: $ts (after $prev_ts)"
        fi
    fi
    prev_ts="$ts"
done

# Check for gaps in sequential IDs
echo -e "\n[+] Checking for Missing Entries:"
if grep -qE "id[=:]|#[0-9]+" "$LOGFILE"; then
    ids=$(grep -oE "(id[=:]|#)[0-9]+" "$LOGFILE" | grep -oE "[0-9]+" | sort -n)
    prev_id=0
    for id in $ids; do
        if [ $((id - prev_id)) -gt 1 ] && [ $prev_id -ne 0 ]; then
            echo "  [!] Gap detected: missing IDs between $prev_id and $id"
        fi
        prev_id=$id
    done
fi

# Check for suspicious patterns
echo -e "\n[+] Suspicious Patterns:"
DEL_COUNT=$(grep -ciE "delete|remove|clear|truncate" "$LOGFILE")
if [ $DEL_COUNT -gt 0 ]; then
    echo "  [!] Delete/clear operations found: $DEL_COUNT"
    grep -iE "delete|remove|clear|truncate" "$LOGFILE" | head -5
fi

# Check file modification time vs. log timestamps
echo -e "\n[+] File Modification Check:"
FILE_MOD=$(stat -c %y "$LOGFILE" 2>/dev/null || stat -f "%Sm" "$LOGFILE")
LAST_LOG=$(grep -oE "[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}" "$LOGFILE" | tail -1)
echo "  File modified: $FILE_MOD"
echo "  Last log entry: $LAST_LOG"
```

---

**Important related topics for comprehensive CTF log analysis:**

- Log aggregation and SIEM tools (Splunk, ELK Stack, Graylog) - centralized log management and advanced correlation
- Windows Event Logs (Security, Application, System) - Windows-specific log analysis for CTF scenarios
- Network device logs (firewall, IDS/IPS, router) - network-level attack detection and traffic analysis
- Container and orchestration logs (Docker, Kubernetes) - modern application deployment log analysis

---

# Network Service Logs

Network service logs are critical artifacts in CTF challenges and real-world incident response scenarios. These logs capture authentication attempts, file transfers, mail transactions, and DNS queries that often reveal attack patterns, data exfiltration, or misconfigurations. Understanding their structure, location, and analysis techniques is essential for both defensive blue team operations and offensive reconnaissance.

## FTP Server Logs (vsftpd, proftpd)

### vsftpd (Very Secure FTP Daemon)

**Default Log Locations:**

- `/var/log/vsftpd.log` - Main transaction log (if enabled)
- `/var/log/xferlog` - Transfer log (wu-ftpd format)
- Syslog integration: Check `/var/log/syslog` or `/var/log/messages`

**Configuration Files:**

- `/etc/vsftpd.conf` - Primary configuration
- `/etc/vsftpd/` - Additional configuration directory

**Key Configuration Directives for Logging:**

```bash
xferlog_enable=YES          # Enable transfer logging
xferlog_std_format=YES      # Use standard xferlog format
log_ftp_protocol=YES        # Log all FTP commands/responses
vsftpd_log_file=/var/log/vsftpd.log  # Custom log location
dual_log_enable=YES         # Enable both vsftpd and xferlog formats
```

**Log Format Analysis:**

Standard xferlog format (when `xferlog_std_format=YES`):

```
current-time transfer-time remote-host file-size filename transfer-type special-action-flag direction access-mode username service-name authentication-method authenticated-user-id completion-status
```

Example entry:

```
Mon Oct 28 14:23:45 2025 2 192.168.1.100 1024000 /home/ftp/secret.txt b _ o r ftpuser ftp 0 * c
```

Field breakdown:

- **Mon Oct 28 14:23:45 2025**: Timestamp
- **2**: Transfer time in seconds
- **192.168.1.100**: Remote IP address
- **1024000**: File size in bytes
- **/home/ftp/secret.txt**: File path
- **b**: Transfer type (a=ASCII, b=binary)
- **_**: Special action flag (C=compressed, U=uncompressed, T=tar, _=none)
- **o**: Direction (o=outgoing, i=incoming)
- **r**: Access mode (a=anonymous, g=guest, r=real user)
- **ftpuser**: Username
- **ftp**: Service name
- **0**: Authentication method (0=none, 1=RFC931)
- *****: Authenticated user ID
- **c**: Completion status (c=complete, i=incomplete)

vsftpd detailed logging format (when `log_ftp_protocol=YES`):

```
Mon Oct 28 14:23:45 2025 [pid 12345] [ftpuser] OK LOGIN: Client "192.168.1.100"
Mon Oct 28 14:23:46 2025 [pid 12345] [ftpuser] OK DOWNLOAD: Client "192.168.1.100", "/home/ftp/secret.txt", 1024000 bytes
```

**Analysis Commands:**

Extract failed login attempts:

```bash
grep "FAIL LOGIN" /var/log/vsftpd.log
grep "authentication failed" /var/log/vsftpd.log
```

Identify successful logins:

```bash
grep "OK LOGIN" /var/log/vsftpd.log | awk '{print $8, $11}'
```

List downloaded files:

```bash
grep "OK DOWNLOAD" /var/log/vsftpd.log | awk -F'"' '{print $4}'
```

Find file uploads:

```bash
grep "OK UPLOAD" /var/log/vsftpd.log
awk '$9 == "i"' /var/log/xferlog  # Using xferlog format
```

Analyze transfer patterns by user:

```bash
awk '{print $11}' /var/log/xferlog | sort | uniq -c | sort -rn
```

Identify large file transfers:

```bash
awk '$5 > 10000000' /var/log/xferlog  # Files larger than 10MB
```

Brute-force detection (multiple failed logins from same IP):

```bash
grep "FAIL LOGIN" /var/log/vsftpd.log | awk '{print $8}' | sort | uniq -c | sort -rn | head -20
```

Anonymous FTP access tracking:

```bash
awk '$10 == "a"' /var/log/xferlog
grep "ANONYMOUS" /var/log/vsftpd.log
```

### proftpd

**Default Log Locations:**

- `/var/log/proftpd/proftpd.log` - Main log
- `/var/log/proftpd/xferlog` - Transfer log
- Configured via `/etc/proftpd/proftpd.conf`

**Configuration Directives:**

```apache
SystemLog /var/log/proftpd/proftpd.log
TransferLog /var/log/proftpd/xferlog
ExtendedLog /var/log/proftpd/extended.log ALL
LogFormat custom "%h %l %u %t \"%r\" %s %b"
```

**Log Format:**

Standard proftpd log entry:

```
192.168.1.100 - ftpuser [28/Oct/2025:14:23:45 +0000] "RETR /uploads/data.zip" 226 1048576
```

Extended log format fields:

- `%a` - Remote IP address
- `%A` - Anonymous username
- `%b` - Bytes sent
- `%f` - Filename
- `%h` - Remote hostname
- `%m` - FTP command/method
- `%s` - Response code
- `%t` - Timestamp
- `%u` - Local username
- `%{protocol}` - Protocol used (FTP/FTPS)

**Analysis Commands:**

Monitor authentication failures:

```bash
grep "Login failed" /var/log/proftpd/proftpd.log
grep "no such user" /var/log/proftpd/proftpd.log
```

Track RETR (download) operations:

```bash
grep "RETR" /var/log/proftpd/xferlog
awk '/RETR/ {print $7, $1}' /var/log/proftpd/proftpd.log
```

Track STOR (upload) operations:

```bash
grep "STOR" /var/log/proftpd/xferlog
```

Identify privilege escalation attempts:

```bash
grep "denied" /var/log/proftpd/proftpd.log
grep "SITE" /var/log/proftpd/proftpd.log  # Administrative commands
```

Parse by response code:

```bash
awk '{print $9}' /var/log/proftpd/proftpd.log | sort | uniq -c | sort -rn
```

Common FTP response codes:

- **220** - Service ready
- **226** - Transfer complete
- **331** - Username OK, password required
- **421** - Service unavailable
- **425** - Cannot open data connection
- **426** - Connection closed, transfer aborted
- **530** - Login incorrect
- **550** - File unavailable (permission denied)

### CTF-Specific Analysis Techniques

**Data Exfiltration Detection:**

Identify suspicious outbound transfers:

```bash
# Large files uploaded/downloaded
awk '$5 > 5000000 && $9 == "o"' /var/log/xferlog

# Multiple file transfers in short timeframe
awk '{print $1, $2, $3, $8}' /var/log/xferlog | sort | uniq -c

# Unusual file extensions
grep -E "\.(rar|7z|zip|sql|db)$" /var/log/xferlog
```

**Timeline Analysis:**

Create transfer timeline:

```bash
cat /var/log/xferlog | awk '{print $1, $2, $3, $4, $8, $10, $11}' | column -t
```

**User Behavior Profiling:**

```bash
# Sessions per user
grep "OK LOGIN" /var/log/vsftpd.log | awk '{print $6}' | sort | uniq -c

# Average transfer size per user
awk '{users[$11]+=$5; count[$11]++} END {for (u in users) print u, users[u]/count[u]}' /var/log/xferlog
```

**Log Parsing with Python:**

```python
import re
from collections import defaultdict
from datetime import datetime

def parse_xferlog(logfile):
    """Parse standard xferlog format"""
    transfers = []
    with open(logfile, 'r') as f:
        for line in f:
            parts = line.split()
            if len(parts) >= 14:
                transfer = {
                    'timestamp': ' '.join(parts[0:4]),
                    'duration': parts[4],
                    'remote_ip': parts[5],
                    'filesize': int(parts[6]),
                    'filename': parts[7],
                    'transfer_type': parts[8],
                    'direction': parts[10],
                    'access_mode': parts[11],
                    'username': parts[12],
                    'completion': parts[16]
                }
                transfers.append(transfer)
    return transfers

def detect_anomalies(transfers):
    """Simple anomaly detection"""
    user_stats = defaultdict(lambda: {'count': 0, 'total_bytes': 0})
    
    for t in transfers:
        user = t['username']
        user_stats[user]['count'] += 1
        user_stats[user]['total_bytes'] += t['filesize']
    
    for user, stats in user_stats.items():
        avg_size = stats['total_bytes'] / stats['count']
        print(f"{user}: {stats['count']} transfers, avg {avg_size:.0f} bytes")

# Usage
transfers = parse_xferlog('/var/log/xferlog')
detect_anomalies(transfers)
```

## SMTP Mail Server Logs

SMTP logs track email transmission, authentication, relay attempts, and spam/phishing indicators. In CTF scenarios, mail logs often contain credentials, phishing campaigns, or command-and-control communications.

**Common SMTP Servers:**

- Postfix
- Sendmail
- Exim
- qmail

### Postfix

**Default Log Location:**

- `/var/log/mail.log` (Debian/Ubuntu)
- `/var/log/maillog` (RedHat/CentOS)
- Integrated with syslog facility `mail`

**Configuration:**

- `/etc/postfix/main.cf` - Main configuration
- `/etc/postfix/master.cf` - Service definitions

**Log Format:**

Postfix uses multi-line logging where a single transaction generates multiple log entries linked by queue ID:

```
Oct 28 14:23:45 mailserver postfix/smtpd[12345]: connect from unknown[192.168.1.100]
Oct 28 14:23:46 mailserver postfix/smtpd[12345]: 4A2B31000F2: client=unknown[192.168.1.100], sasl_method=PLAIN, sasl_username=user@example.com
Oct 28 14:23:47 mailserver postfix/cleanup[12346]: 4A2B31000F2: message-id=<abc123@example.com>
Oct 28 14:23:48 mailserver postfix/qmgr[12347]: 4A2B31000F2: from=<sender@example.com>, size=2048, nrcpt=1 (queue active)
Oct 28 14:23:49 mailserver postfix/smtp[12348]: 4A2B31000F2: to=<recipient@target.com>, relay=mx.target.com[10.0.0.1]:25, delay=2.1, delays=0.5/0.1/1.0/0.5, dsn=2.0.0, status=sent (250 2.0.0 Ok: queued)
Oct 28 14:23:50 mailserver postfix/qmgr[12347]: 4A2B31000F2: removed
```

**Key Log Components:**

- **Queue ID**: `4A2B31000F2` - Unique identifier for tracking message through system
- **Process**: `smtpd`, `cleanup`, `qmgr`, `smtp`, `local`, `bounce`
- **PID**: Process ID in brackets
- **Status codes**:
    - `2.x.x` - Success
    - `4.x.x` - Temporary failure
    - `5.x.x` - Permanent failure

**Analysis Commands:**

Track specific message by queue ID:

```bash
grep "4A2B31000F2" /var/log/mail.log
```

Extract all sender addresses:

```bash
grep "from=<" /var/log/mail.log | grep -oP 'from=<\K[^>]+'
```

Extract all recipient addresses:

```bash
grep "to=<" /var/log/mail.log | grep -oP 'to=<\K[^>]+'
```

Identify authentication attempts:

```bash
grep "sasl_method" /var/log/mail.log
```

Failed authentication (brute-force detection):

```bash
grep "authentication failed" /var/log/mail.log
grep "warning.*SASL" /var/log/mail.log | awk '{print $6}' | sort | uniq -c | sort -rn
```

Rejected connections:

```bash
grep "reject:" /var/log/mail.log
```

Identify relay attempts:

```bash
grep "Relay access denied" /var/log/mail.log
```

Messages by status:

```bash
grep "status=sent" /var/log/mail.log | wc -l
grep "status=bounced" /var/log/mail.log | wc -l
grep "status=deferred" /var/log/mail.log | wc -l
```

Top senders by volume:

```bash
grep "from=<" /var/log/mail.log | grep -oP 'from=<\K[^>]+' | sort | uniq -c | sort -rn | head -20
```

Large messages:

```bash
grep "size=" /var/log/mail.log | awk '{for(i=1;i<=NF;i++) if($i~/^size=/) print $i}' | sort -t= -k2 -n | tail -20
```

Connection sources:

```bash
grep "connect from" /var/log/mail.log | awk '{print $NF}' | sort | uniq -c | sort -rn
```

**Spam/Malware Indicators:**

Identify suspicious patterns:

```bash
# Multiple recipients (spam indicator)
grep "nrcpt=" /var/log/mail.log | grep -oP 'nrcpt=\K[0-9]+' | sort -n | tail -20

# High bounce rates
grep "status=bounced" /var/log/mail.log | grep -oP 'from=<\K[^>]+' | sort | uniq -c | sort -rn

# Dictionary attacks
grep "Recipient address rejected: User unknown" /var/log/mail.log | awk '{print $(NF-1)}' | sort | uniq -c | sort -rn
```

### Sendmail

**Default Log Location:**

- `/var/log/maillog` (most systems)
- Uses syslog facility `mail.info`

**Log Format:**

```
Oct 28 14:23:45 mailserver sendmail[12345]: 2A3B4C5D6E7F: from=<sender@example.com>, size=2048, class=0, nrcpts=1, msgid=<abc123@example.com>, proto=ESMTP, daemon=MTA, relay=client.example.com [192.168.1.100]
Oct 28 14:23:46 mailserver sendmail[12346]: 2A3B4C5D6E7F: to=<recipient@target.com>, delay=00:00:01, xdelay=00:00:00, mailer=esmtp, pri=32048, relay=mx.target.com. [10.0.0.1], dsn=2.0.0, stat=Sent
```

**Analysis Commands:**

Extract queue IDs and track messages:

```bash
grep "from=<" /var/log/maillog | awk '{print $6}' | cut -d: -f1 | sort -u
```

Failed deliveries:

```bash
grep "stat=Deferred" /var/log/maillog
grep "stat=Bounced" /var/log/maillog
```

Rejected recipients:

```bash
grep "reject" /var/log/maillog
```

### Exim

**Default Log Locations:**

- `/var/log/exim4/mainlog` (main log)
- `/var/log/exim4/rejectlog` (rejected messages)
- `/var/log/exim4/paniclog` (errors)

**Log Format:**

```
2025-10-28 14:23:45 1tABcD-0000Ex-0F <= sender@example.com H=client.example.com [192.168.1.100]:54321 P=esmtps X=TLS1.3:ECDHE_RSA_AES_256_GCM_SHA384:256 CV=no S=2048 id=abc123@example.com
2025-10-28 14:23:46 1tABcD-0000Ex-0F => recipient@target.com R=dnslookup T=remote_smtp H=mx.target.com [10.0.0.1] X=TLS1.2:RSA_AES_256_CBC_SHA1:256 CV=yes C="250 OK"
2025-10-28 14:23:47 1tABcD-0000Ex-0F Completed
```

**Key Symbols:**

- `<=` - Message arrival
- `=>` - Normal delivery
- `->` - Additional delivery (multiple recipients)
- `>>` - Cutthrough delivery
- `*>` - Delivery suppressed by `-N`
- `**` - Delivery failed (permanent error)
- `==` - Delivery deferred (temporary error)

**Analysis Commands:**

Count messages by status:

```bash
grep "Completed" /var/log/exim4/mainlog | wc -l
grep "==" /var/log/exim4/mainlog | wc -l  # Deferred
grep "\*\*" /var/log/exim4/mainlog | wc -l  # Failed
```

Extract authentication info:

```bash
grep "authenticator" /var/log/exim4/mainlog
```

Frozen messages:

```bash
grep "frozen" /var/log/exim4/mainlog
```

### CTF-Specific SMTP Analysis

**Email Header Extraction:**

If logs contain full headers (with verbose logging):

```bash
# Extract message-id headers
grep "message-id=" /var/log/mail.log | grep -oP 'message-id=<\K[^>]+'

# Extract X-Originating-IP
grep "X-Originating-IP" /var/log/mail.log
```

**Phishing Campaign Detection:**

```bash
# Identify bulk sending patterns
awk '/from=</ {print $0}' /var/log/mail.log | \
  awk '{for(i=1;i<=NF;i++) if($i~/^from=</) print $i}' | \
  sort | uniq -c | awk '$1 > 100'

# Common phishing subject patterns (if logged)
grep -i "urgent\|verify\|suspended\|confirm.*account" /var/log/mail.log
```

**Timeline Reconstruction:**

```bash
# Create timeline with all activities for an email
QUEUE_ID="4A2B31000F2"
grep "$QUEUE_ID" /var/log/mail.log | \
  awk '{print $1, $2, $3, $0}' | \
  sort -k1M -k2n -k3
```

**Python Script for Advanced Analysis:**

```python
import re
from datetime import datetime
from collections import defaultdict

def parse_postfix_log(logfile):
    """Parse Postfix mail.log and reconstruct email transactions"""
    messages = defaultdict(dict)
    
    with open(logfile, 'r') as f:
        for line in f:
            # Extract queue ID
            queue_match = re.search(r'postfix/[^[]+\[\d+\]: ([A-F0-9]+):', line)
            if not queue_match:
                continue
            
            queue_id = queue_match.group(1)
            
            # Extract sender
            from_match = re.search(r'from=<([^>]*)>', line)
            if from_match:
                messages[queue_id]['from'] = from_match.group(1)
            
            # Extract recipient
            to_match = re.search(r'to=<([^>]*)>', line)
            if to_match:
                if 'to' not in messages[queue_id]:
                    messages[queue_id]['to'] = []
                messages[queue_id]['to'].append(to_match.group(1))
            
            # Extract status
            status_match = re.search(r'status=(\w+)', line)
            if status_match:
                messages[queue_id]['status'] = status_match.group(1)
            
            # Extract size
            size_match = re.search(r'size=(\d+)', line)
            if size_match:
                messages[queue_id]['size'] = int(size_match.group(1))
            
            # Extract client IP
            client_match = re.search(r'client=.*\[([0-9.]+)\]', line)
            if client_match:
                messages[queue_id]['client_ip'] = client_match.group(1)
            
            # Extract SASL username
            sasl_match = re.search(r'sasl_username=([^\s,]+)', line)
            if sasl_match:
                messages[queue_id]['auth_user'] = sasl_match.group(1)
    
    return messages

def find_suspicious_activity(messages):
    """Detect suspicious patterns"""
    # High volume senders
    sender_count = defaultdict(int)
    for msg in messages.values():
        if 'from' in msg:
            sender_count[msg['from']] += 1
    
    print("High volume senders:")
    for sender, count in sorted(sender_count.items(), key=lambda x: x[1], reverse=True)[:10]:
        print(f"  {sender}: {count} messages")
    
    # Failed authentications
    failed_auth = [msg for msg in messages.values() if msg.get('status') == 'bounced']
    print(f"\nFailed deliveries: {len(failed_auth)}")
    
    # Large messages
    large_msgs = [msg for msg in messages.values() if msg.get('size', 0) > 10000000]
    print(f"Messages > 10MB: {len(large_msgs)}")

# Usage
messages = parse_postfix_log('/var/log/mail.log')
find_suspicious_activity(messages)
```

## DNS Server Logs (BIND, dnsmasq)

DNS logs reveal domain resolution patterns, potential data exfiltration via DNS tunneling, DGA (Domain Generation Algorithm) malware activity, and reconnaissance attempts. In CTF challenges, DNS logs frequently hide encoded flags or reveal infrastructure relationships.

### BIND (Berkeley Internet Name Domain)

**Default Log Locations:**

- Configured in `/etc/bind/named.conf` or `/etc/named.conf`
- Common: `/var/log/named/` or `/var/log/bind/`
- Query log: Often separate file like `queries.log`

**Configuration for Logging:**

```
logging {
    channel query_log {
        file "/var/log/named/queries.log" versions 3 size 20m;
        severity info;
        print-time yes;
        print-severity yes;
        print-category yes;
    };
    
    channel security_log {
        file "/var/log/named/security.log" versions 3 size 20m;
        severity info;
        print-time yes;
    };
    
    category queries { query_log; };
    category security { security_log; };
    category lame-servers { null; };  # Suppress lame server warnings
};

# Enable query logging (can also be done via rndc)
options {
    querylog yes;
};
```

**Runtime query logging control:**

```bash
# Enable query logging
rndc querylog on

# Disable query logging
rndc querylog off

# Check status
rndc status | grep "query logging"
```

**Log Format:**

Standard query log format:

```
28-Oct-2025 14:23:45.123 queries: info: client @0x7f8a4c001234 192.168.1.100#54321 (example.com): query: example.com IN A +E(0)DC (192.168.1.1)
```

Field breakdown:

- **28-Oct-2025 14:23:45.123**: Timestamp with milliseconds
- **queries**: Category
- **info**: Severity
- **client @0x7f8a4c001234**: Internal client object pointer
- **192.168.1.100#54321**: Source IP and port
- **(example.com)**: Queried domain
- **query**: Operation type
- **example.com**: Query name
- **IN**: Class (Internet)
- **A**: Record type (A, AAAA, MX, TXT, PTR, etc.)
- **+E(0)DC**: Query flags
    - `+`: Recursion desired
    - `E`: EDNS enabled
    - `(0)`: EDNS version
    - `D`: DNSSEC OK bit set
    - `C`: Checking disabled
- **(192.168.1.1)**: Server address that received query

**Common Record Types:**

- **A**: IPv4 address
- **AAAA**: IPv6 address
- **MX**: Mail exchange
- **NS**: Name server
- **CNAME**: Canonical name (alias)
- **TXT**: Text record
- **PTR**: Reverse lookup
- **SOA**: Start of authority
- **SRV**: Service record
- **ANY**: Request all records

**Analysis Commands:**

Extract queried domains:

```bash
grep "query:" /var/log/named/queries.log | awk '{print $8}' | sort | uniq -c | sort -rn
```

Query volume by source IP:

```bash
grep "query:" /var/log/named/queries.log | awk '{print $6}' | cut -d'#' -f1 | sort | uniq -c | sort -rn
```

Identify specific record type queries:

```bash
grep " A " /var/log/named/queries.log  # A records
grep " AAAA " /var/log/named/queries.log  # IPv6
grep " MX " /var/log/named/queries.log  # Mail servers
grep " TXT " /var/log/named/queries.log  # Text records
grep " ANY " /var/log/named/queries.log  # ANY queries (recon)
```

Failed queries (NXDOMAIN):

```bash
grep "NXDOMAIN" /var/log/named/queries.log
```

Reverse DNS lookups:

```bash
grep "PTR" /var/log/named/queries.log
grep "in-addr.arpa" /var/log/named/queries.log
grep "ip6.arpa" /var/log/named/queries.log
```

Time-based query analysis:

```bash
# Queries per hour
awk '{print $1, $2}' /var/log/named/queries.log | cut -d':' -f1 | sort | uniq -c

# Queries in specific timeframe
awk '/28-Oct-2025 14:/ {print $0}' /var/log/named/queries.log
```

Top level domains queried:

```bash
grep "query:" /var/log/named/queries.log | awk '{print $8}' | awk -F'.' '{print $NF}' | sort | uniq -c | sort -rn | head -20
```

### dnsmasq

**Default Log Locations:**

- Integrated with syslog: `/var/log/syslog` or `/var/log/messages`
- Facility: `daemon`
- Can be configured for separate log file

**Configuration:**

In `/etc/dnsmasq.conf`:

```
# Enable query logging
log-queries

# Additional debug logging
log-dhcp

# Specify log facility
log-facility=/var/log/dnsmasq.log

# Async logging
log-async=50
```

**Log Format:**

```
Oct 28 14:23:45 dnsmasq[12345]: query[A] example.com from 192.168.1.100
Oct 28 14:23:45 dnsmasq[12345]: forwarded example.com to 8.8.8.8
Oct 28 14:23:45 dnsmasq[12345]: reply example.com is 93.184.216.34
```

Query types:

- `query[A]` - A record query
- `query[AAAA]` - IPv6 query
- `query[PTR]` - Reverse lookup
- `query[MX]` - Mail exchange query

Response types:

- `reply ... is <IP>` - Successful resolution
- `reply ... is NODATA` - Domain exists but no records of requested type
- `reply ... is NXDOMAIN` - Domain doesn't exist
- `cached ... is <IP>` - Response from cache

**Analysis Commands:**

Extract all queried domains:

```bash
grep "query\[" /var/log/dnsmasq.log | awk '{print $6}' | sort | uniq -c | sort -rn
```

Track query sources:

```bash
grep "query\[" /var/log/dnsmasq.log | awk '{print $NF}' | sort | uniq -c | sort -rn
```

Identify cache hits vs forwards:

```bash
grep "cached" /var/log/dnsmasq.log | wc -l
grep "forwarded" /var/log/dnsmasq.log | wc -l
```

NXDOMAIN responses:

```bash
grep "NXDOMAIN" /var/log/dnsmasq.log
```

Blocked queries (if using blocklists):

```bash
grep "config" /var/log/dnsmasq.log | grep "is 0.0.0.0"
```

### CTF-Specific DNS Analysis

**DNS Tunneling Detection:**

DNS tunneling uses DNS queries to exfiltrate data or establish C2 channels. Characteristics:

- High volume of subdomain queries to same parent domain
- Long/random subdomain names
- High entropy in subdomain strings
- Unusual record types (TXT, NULL)
- Regular query intervals

Detection commands:

```bash
# Long subdomain names (potential Base64 encoded data)
grep "query:" /var/log/named/queries.log | awk '{print $8}' | awk -F'.' '{if(length($1) > 40) print}'

# High query volume to single domain
grep "query:" /var/log/named/queries.log | awk '{print $8}' | \
  awk -F'.' '{domain=$(NF-1)"."$NF; count[domain]++} END {for(d in count) if(count[d] > 1000) print d, count[d]}'

# Unusual TXT queries
grep "TXT" /var/log/named/queries.log | grep -v "spf\|dkim\|dmarc"

# Regular interval detection (potential beacon)
grep "query:" /var/log/named/queries.log | awk '{print $1, $2, $8}' | \
  awk '{domain=$3; if(last_domain==domain) print domain, $1, $2, last_time, "-->", $1, $2; last_domain=domain; last_time=$1" "$2}'
````

**Entropy Analysis for DNS Tunneling:**

```python
import re
import math
from collections import Counter

def calculate_entropy(string):
    """Calculate Shannon entropy of a string"""
    if not string:
        return 0
    entropy = 0
    for count in Counter(string).values():
        probability = count / len(string)
        entropy -= probability * math.log2(probability)
    return entropy

def analyze_dns_queries(logfile):
    """Detect potential DNS tunneling by entropy analysis"""
    suspicious_queries = []
    
    with open(logfile, 'r') as f:
        for line in f:
            # Extract domain from BIND query log
            match = re.search(r'query: ([^\s]+) IN', line)
            if not match:
                continue
            
            domain = match.group(1)
            parts = domain.split('.')
            
            if len(parts) < 2:
                continue
            
            # Analyze subdomain (everything except last 2 parts)
            if len(parts) > 2:
                subdomain = '.'.join(parts[:-2])
                entropy = calculate_entropy(subdomain)
                
                # High entropy subdomains are suspicious
                if entropy > 4.0 and len(subdomain) > 20:
                    suspicious_queries.append({
                        'domain': domain,
                        'subdomain': subdomain,
                        'entropy': entropy,
                        'length': len(subdomain)
                    })
    
    # Sort by entropy
    suspicious_queries.sort(key=lambda x: x['entropy'], reverse=True)
    
    print("Suspicious high-entropy DNS queries:")
    for q in suspicious_queries[:50]:
        print(f"  Entropy: {q['entropy']:.2f} | Length: {q['length']} | {q['domain']}")
    
    return suspicious_queries

# Usage
suspicious = analyze_dns_queries('/var/log/named/queries.log')
````

**DGA (Domain Generation Algorithm) Detection:**

Malware often uses DGAs to generate pseudo-random domains for C2 communication. Characteristics:

- Random-looking strings
- High proportion of consonants
- Unusual TLD combinations
- Short-lived domains (NXDOMAIN responses)

```bash
# Find domains with high consonant ratio
grep "query:" /var/log/named/queries.log | awk '{print $8}' | \
  grep -oP '^[a-z0-9-]+\.' | \
  awk '{
    consonants = gsub(/[bcdfghjklmnpqrstvwxyz]/, "")
    vowels = gsub(/[aeiou]/, "")
    if(consonants > vowels * 3) print $0, "Consonant ratio:", consonants/vowels
  }'

# Frequent NXDOMAIN for similar patterns (DGA probing)
grep "NXDOMAIN" /var/log/named/queries.log | awk '{print $8}' | \
  awk -F'.' '{print length($1), $1}' | sort -n | tail -50
```

**Python DGA Detection:**

```python
import re
from collections import defaultdict

def detect_dga(logfile):
    """Detect potential DGA-generated domains"""
    domains = defaultdict(int)
    nxdomains = set()
    
    with open(logfile, 'r') as f:
        for line in f:
            domain_match = re.search(r'query: ([^\s]+) IN', line)
            if domain_match:
                domain = domain_match.group(1).lower()
                domains[domain] += 1
            
            if 'NXDOMAIN' in line and domain_match:
                nxdomains.add(domain)
    
    suspicious = []
    
    for domain in domains:
        parts = domain.split('.')
        if len(parts) < 2:
            continue
        
        subdomain = parts[0]
        
        # DGA indicators
        has_digits = bool(re.search(r'\d', subdomain))
        length = len(subdomain)
        consonant_ratio = len(re.findall(r'[bcdfghjklmnpqrstvwxyz]', subdomain)) / max(len(subdomain), 1)
        is_nxdomain = domain in nxdomains
        
        # Scoring
        score = 0
        if length > 15:
            score += 2
        if consonant_ratio > 0.7:
            score += 3
        if has_digits:
            score += 1
        if is_nxdomain:
            score += 2
        if not re.search(r'[aeiou]{2,}', subdomain):  # No double vowels
            score += 1
        
        if score >= 5:
            suspicious.append({
                'domain': domain,
                'score': score,
                'length': length,
                'consonant_ratio': consonant_ratio,
                'nxdomain': is_nxdomain
            })
    
    suspicious.sort(key=lambda x: x['score'], reverse=True)
    
    print("Potential DGA domains:")
    for s in suspicious[:30]:
        print(f"  Score: {s['score']} | {s['domain']} | NXDOMAIN: {s['nxdomain']}")
    
    return suspicious

# Usage
dga_domains = detect_dga('/var/log/named/queries.log')
```

**Subdomain Enumeration Detection:**

Attackers often perform subdomain enumeration for reconnaissance:

```bash
# Detect rapid sequential queries to subdomains of same parent
grep "query:" /var/log/named/queries.log | \
  awk '{print $1, $2, $6, $8}' | \
  awk -F'[. ]' '{
    timestamp=$1" "$2
    ip=$3
    subdomain=$4
    parent=$(NF-1)"."$NF
    key=ip"-"parent
    if(last_key==key && timestamp==last_time) count[key]++
    else count[key]=1
    last_key=key
    last_time=timestamp
    if(count[key] > 10) print "Potential enumeration:", ip, "->", parent, "Count:", count[key]
  }' | sort -u
```

**Data Exfiltration via TXT Records:**

TXT records can hold arbitrary data, making them useful for exfiltration:

```bash
# Extract TXT query patterns
grep "TXT" /var/log/named/queries.log | \
  grep -v "spf\|dkim\|dmarc\|_acme-challenge" | \
  awk '{print $6, $8}' | column -t

# Look for Base64-like patterns in subdomains with TXT queries
grep "TXT" /var/log/named/queries.log | \
  awk '{print $8}' | \
  grep -E '^[A-Za-z0-9+/]{20,}='
```

**DNS Response Analysis:**

For BIND with response logging enabled:

```bash
# Extract responses (requires detailed logging)
grep "response:" /var/log/named/queries.log

# Track query-response pairs
awk '/query:/ {query=$0; getline; if(/response:/) print query" ||| "$0}' /var/log/named/queries.log
```

**Beaconing Detection:**

Malware often beacons at regular intervals:

```python
import re
from datetime import datetime
from collections import defaultdict

def detect_beaconing(logfile):
    """Detect regular DNS query intervals (beaconing)"""
    domain_timestamps = defaultdict(list)
    
    with open(logfile, 'r') as f:
        for line in f:
            # Parse timestamp and domain
            timestamp_match = re.search(r'^(\d{2}-\w{3}-\d{4} \d{2}:\d{2}:\d{2})', line)
            domain_match = re.search(r'query: ([^\s]+) IN', line)
            
            if timestamp_match and domain_match:
                timestamp = datetime.strptime(timestamp_match.group(1), '%d-%b-%Y %H:%M:%S')
                domain = domain_match.group(1)
                domain_timestamps[domain].append(timestamp)
    
    beacons = []
    
    for domain, timestamps in domain_timestamps.items():
        if len(timestamps) < 5:  # Need multiple samples
            continue
        
        timestamps.sort()
        
        # Calculate intervals
        intervals = []
        for i in range(1, len(timestamps)):
            interval = (timestamps[i] - timestamps[i-1]).total_seconds()
            intervals.append(interval)
        
        if not intervals:
            continue
        
        # Check for regular intervals (low standard deviation)
        avg_interval = sum(intervals) / len(intervals)
        variance = sum((x - avg_interval) ** 2 for x in intervals) / len(intervals)
        std_dev = variance ** 0.5
        
        # Coefficient of variation
        cv = std_dev / avg_interval if avg_interval > 0 else 0
        
        # Low CV indicates regular beaconing
        if cv < 0.3 and len(timestamps) > 10 and avg_interval < 3600:
            beacons.append({
                'domain': domain,
                'query_count': len(timestamps),
                'avg_interval': avg_interval,
                'std_dev': std_dev,
                'cv': cv
            })
    
    beacons.sort(key=lambda x: x['cv'])
    
    print("Potential beaconing domains:")
    for b in beacons[:20]:
        print(f"  {b['domain']}")
        print(f"    Queries: {b['query_count']}, Avg interval: {b['avg_interval']:.1f}s, CV: {b['cv']:.3f}")
    
    return beacons

# Usage
beacons = detect_beaconing('/var/log/named/queries.log')
```

**Geographical Anomaly Detection:**

Correlate DNS queries with unusual geographic patterns:

```bash
# Extract unique IPs querying DNS
grep "query:" /var/log/named/queries.log | awk '{print $6}' | cut -d'#' -f1 | sort -u > /tmp/dns_clients.txt

# Use external GeoIP lookup (requires geoiplookup or similar)
while read ip; do
    echo -n "$ip: "
    geoiplookup $ip 2>/dev/null | head -1
done < /tmp/dns_clients.txt

# Or with mmdb database and Python
```

**Fast Flux Detection:**

Fast flux networks rapidly change IP addresses:

```bash
# Track A record responses for same domain (requires response logging)
grep "reply.*is [0-9]" /var/log/dnsmasq.log | \
  awk '{domain=$2; ip=$NF; print domain, ip}' | \
  awk '{
    domain=$1
    ip=$2
    ips[domain]=ips[domain]" "ip
    count[domain]++
  } END {
    for(d in count) {
      if(count[d] > 10) {
        print d, "->", count[d], "different IPs:", ips[d]
      }
    }
  }'
```

### Log Correlation Across Services

**Cross-Service Analysis:**

Correlate DNS queries with FTP/SMTP activity:

```bash
# Extract IPs from DNS logs
grep "query:" /var/log/named/queries.log | awk '{print $6}' | cut -d'#' -f1 | sort -u > /tmp/dns_ips.txt

# Find same IPs in FTP logs
grep -f /tmp/dns_ips.txt /var/log/vsftpd.log

# Find same IPs in mail logs
grep -f /tmp/dns_ips.txt /var/log/mail.log
```

**Timeline Correlation Script:**

```python
import re
from datetime import datetime

def correlate_logs(dns_log, ftp_log, mail_log):
    """Correlate activities across service logs"""
    events = []
    
    # Parse DNS log
    with open(dns_log, 'r') as f:
        for line in f:
            match = re.search(r'^(\d{2}-\w{3}-\d{4} \d{2}:\d{2}:\d{2}).*client.*?([0-9.]+)#\d+.*query: ([^\s]+)', line)
            if match:
                timestamp = datetime.strptime(match.group(1), '%d-%b-%Y %H:%M:%S')
                ip = match.group(2)
                domain = match.group(3)
                events.append({
                    'timestamp': timestamp,
                    'service': 'DNS',
                    'ip': ip,
                    'detail': f"Query: {domain}"
                })
    
    # Parse FTP log (xferlog format)
    with open(ftp_log, 'r') as f:
        for line in f:
            parts = line.split()
            if len(parts) >= 14:
                try:
                    timestamp = datetime.strptime(' '.join(parts[0:4]), '%a %b %d %H:%M:%S %Y')
                    ip = parts[5]
                    filename = parts[7]
                    direction = 'download' if parts[10] == 'o' else 'upload'
                    events.append({
                        'timestamp': timestamp,
                        'service': 'FTP',
                        'ip': ip,
                        'detail': f"{direction}: {filename}"
                    })
                except:
                    continue
    
    # Parse mail log (Postfix)
    with open(mail_log, 'r') as f:
        for line in f:
            match = re.search(r'^(\w{3}\s+\d{1,2} \d{2}:\d{2}:\d{2}).*client=.*\[([0-9.]+)\].*from=<([^>]+)>', line)
            if match:
                try:
                    timestamp = datetime.strptime(match.group(1) + ' 2025', '%b %d %H:%M:%S %Y')
                    ip = match.group(2)
                    sender = match.group(3)
                    events.append({
                        'timestamp': timestamp,
                        'service': 'SMTP',
                        'ip': ip,
                        'detail': f"From: {sender}"
                    })
                except:
                    continue
    
    # Sort by timestamp
    events.sort(key=lambda x: x['timestamp'])
    
    # Group by IP for correlation
    ip_events = {}
    for event in events:
        ip = event['ip']
        if ip not in ip_events:
            ip_events[ip] = []
        ip_events[ip].append(event)
    
    # Print correlated activity
    print("=== Correlated Activity by IP ===\n")
    for ip, ip_event_list in sorted(ip_events.items(), key=lambda x: len(x[1]), reverse=True):
        if len(ip_event_list) > 1:  # Only show IPs with multiple service interactions
            print(f"IP: {ip} ({len(ip_event_list)} events)")
            for event in ip_event_list[:10]:  # Show first 10
                print(f"  {event['timestamp']} | {event['service']:5s} | {event['detail']}")
            print()

# Usage
correlate_logs('/var/log/named/queries.log', '/var/log/xferlog', '/var/log/mail.log')
```

### Advanced Analysis Tools

**Using Zeek (formerly Bro) for DNS Analysis:**

If Zeek is available, it provides structured DNS logging:

```bash
# Zeek DNS log location
cat /opt/zeek/logs/current/dns.log

# Parse with zeek-cut
cat dns.log | zeek-cut ts id.orig_h query qtype_name answers

# Find queries with no answers (potential C2)
cat dns.log | zeek-cut query answers | awk '$2 == "-"'

# Extract only A record queries
cat dns.log | zeek-cut query qtype_name | grep "\sA$"
```

**Using tshark for PCAP Analysis:**

If you have packet captures:

```bash
# Extract DNS queries from PCAP
tshark -r capture.pcap -Y "dns.qry.name" -T fields -e frame.time -e ip.src -e dns.qry.name

# DNS responses
tshark -r capture.pcap -Y "dns.flags.response == 1" -T fields -e dns.qry.name -e dns.a

# Filter by query type
tshark -r capture.pcap -Y "dns.qry.type == 16" -T fields -e dns.qry.name  # TXT records

# Extract DNS tunneling candidates (long queries)
tshark -r capture.pcap -Y "dns.qry.name && dns.qry.name.len > 50" -T fields -e dns.qry.name

# Query/response timing analysis
tshark -r capture.pcap -Y "dns" -T fields -e frame.time_relative -e dns.qry.name -e dns.flags.response
```

**Using dnstop for Real-Time Analysis:**

```bash
# Real-time DNS query monitoring (requires packet capture access)
dnstop -l 3 eth0

# Parameters:
# -l: Set refresh rate
# Various interactive keys:
#   1: Display sources
#   2: Display destinations
#   3: Display query types
#   4: Display top-level domains
#   5: Display second-level domains
#   6: Display third-level domains
```

### Log Retention and Management

**Log Rotation Configuration:**

For BIND (`/etc/logrotate.d/bind`):

```
/var/log/named/*.log {
    daily
    rotate 7
    compress
    delaycompress
    missingok
    notifempty
    create 0644 bind bind
    postrotate
        /usr/sbin/rndc reload > /dev/null 2>&1 || true
    endscript
}
```

For dnsmasq (`/etc/logrotate.d/dnsmasq`):

```
/var/log/dnsmasq.log {
    daily
    rotate 14
    compress
    missingok
    notifempty
    create 0644 dnsmasq dnsmasq
    postrotate
        systemctl reload dnsmasq > /dev/null 2>&1 || true
    endscript
}
```

**Storage Considerations:**

[Inference] DNS query logs can grow rapidly on busy servers, potentially generating several GB per day. For CTF analysis, focus on:

- Retaining logs for competition duration + analysis period
- Compressing older logs
- Using log aggregation tools for distributed analysis

### CTF Flag Discovery Patterns

**Common DNS-based CTF Flag Patterns:**

```bash
# Base64-encoded flags in subdomains
grep "query:" /var/log/named/queries.log | awk '{print $8}' | grep -oP '^[A-Za-z0-9+/=]{20,}\.' | base64 -d 2>/dev/null

# Flags in TXT record queries
grep "TXT" /var/log/named/queries.log | grep -i "flag\|ctf"

# Hex-encoded data in subdomains
grep "query:" /var/log/named/queries.log | awk '{print $8}' | grep -oP '^[0-9a-f]{32,}\.' | xxd -r -p 2>/dev/null

# Flags split across multiple queries (requires reconstruction)
grep "query:" /var/log/named/queries.log | awk '{print $1, $2, $8}' | sort -k1,2 | awk '{print $3}' | tr -d '\n'
```

### Important Subtopics

For comprehensive CTF log analysis mastery, consider exploring these related areas:

1. **SIEM Integration** - Aggregating network service logs into Splunk, ELK Stack, or Graylog for unified analysis
2. **Packet Capture Analysis** - Using Wireshark/tshark to correlate network traffic with service logs
3. **Automated Alerting** - Creating rules for suspicious patterns (OSSEC, Wazuh, Suricata)
4. **Log Forgery Detection** - Identifying tampered or injected log entries
5. **DNS Security Extensions** - DNSSEC validation logging and analysis

---

## DHCP Server Logs

DHCP logs map IP addresses to MAC addresses and hostnames, essential for correlating network activity with specific devices and tracking device movement across network segments.

### Key Log Locations

**ISC DHCP Server (dhcpd)**

```bash
# Primary log location (systemd-based systems)
/var/log/syslog
/var/log/messages

# Dedicated DHCP log (if configured)
/var/log/dhcpd.log

# Lease database (active/historical assignments)
/var/lib/dhcp/dhcpd.leases
/var/lib/dhcp/dhcpd.leases~  # Previous lease file
```

**dnsmasq (lightweight DHCP/DNS)**

```bash
/var/log/syslog
/var/log/dnsmasq.log  # If configured separately
```

### Log Analysis Techniques

**Extract DHCP transactions:**

```bash
# View all DHCP-related entries
grep -i dhcp /var/log/syslog

# Filter by transaction type
grep "DHCPDISCOVER" /var/log/syslog
grep "DHCPOFFER" /var/log/syslog
grep "DHCPREQUEST" /var/log/syslog
grep "DHCPACK" /var/log/syslog
grep "DHCPNAK" /var/log/syslog  # Denials (potential misconfiguration/attack)
grep "DHCPRELEASE" /var/log/syslog
```

**Correlate MAC to IP assignments:**

```bash
# Extract MAC-to-IP mappings with timestamps
grep "DHCPACK" /var/log/syslog | awk '{print $1, $2, $3, $8, $10}'

# Example output format: timestamp hostname IP MAC

# Parse current lease file for active assignments
cat /var/lib/dhcp/dhcpd.leases | grep -E "lease|hardware ethernet|client-hostname"
```

**Detect reconnaissance patterns:**

```bash
# Multiple DHCPDISCOVER from same MAC (scanning behavior)
grep "DHCPDISCOVER" /var/log/syslog | awk '{print $12}' | sort | uniq -c | sort -rn

# Rapid lease requests (potential DoS or pool exhaustion)
grep "DHCPREQUEST" /var/log/syslog | awk '{print $1, $2, $3}' | uniq -c
```

**Identify rogue DHCP servers:**

```bash
# Look for DHCPOFFER responses not from legitimate server IPs
grep "DHCPOFFER" /var/log/syslog | grep -v "from <legitimate_server_ip>"

# Cross-reference with network captures
tcpdump -i eth0 -n port 67 or port 68 -vv
```

### CTF-Specific Scenarios

**Scenario: Unauthorized device identification**

```bash
# Compare lease database against known asset inventory
awk '/hardware ethernet/ {mac=$3} /client-hostname/ {print mac, $2}' /var/lib/dhcp/dhcpd.leases | sort > current_macs.txt
comm -13 known_devices.txt current_macs.txt  # Show unauthorized MACs
```

**Scenario: IP address history for forensic timeline**

```bash
# Extract all IP assignments for specific MAC address
grep "00:11:22:33:44:55" /var/log/syslog* | grep DHCPACK | awk '{print $1, $2, $8}'

# Correlation with historical logs (requires log rotation retention)
zgrep "00:11:22:33:44:55" /var/log/syslog.*.gz | grep DHCPACK
```

### Parsing with Python

```python
#!/usr/bin/env python3
import re
from datetime import datetime

def parse_dhcp_log(logfile):
    dhcp_events = []
    pattern = r'(\w{3}\s+\d+\s+\d+:\d+:\d+).*dhcpd.*DHCP(\w+).*?(\d+\.\d+\.\d+\.\d+).*?([\da-f:]+)'
    
    with open(logfile, 'r') as f:
        for line in f:
            match = re.search(pattern, line, re.IGNORECASE)
            if match:
                timestamp, action, ip, mac = match.groups()
                dhcp_events.append({
                    'timestamp': timestamp,
                    'action': action,
                    'ip': ip,
                    'mac': mac
                })
    return dhcp_events

# Usage
events = parse_dhcp_log('/var/log/syslog')
for event in events:
    print(f"{event['timestamp']} - {event['action']}: {event['mac']} -> {event['ip']}")
```

---

## Proxy Server Logs (Squid)

Squid proxy logs provide detailed HTTP/HTTPS traffic visibility, including URLs accessed, user agents, response codes, and bandwidth usage. Critical for detecting data exfiltration, C2 communication, and web-based attacks.

### Key Log Locations

```bash
# Access log (primary analysis target)
/var/log/squid/access.log

# Cache log (object storage events)
/var/log/squid/cache.log

# Store log (cache operations detail)
/var/log/squid/store.log

# Configuration file (verify logging settings)
/etc/squid/squid.conf
```

### Log Format Analysis

**Default access.log format (native/squid format):**

```
timestamp elapsed client_ip code/status bytes method URL rfc931 peerstatus/peerhost type
```

**Common Log Format (CLF) - if configured:**

```
client_ip - username [timestamp] "method URL protocol" status bytes
```

**Example native format line:**

```
1698345678.123 456 192.168.1.100 TCP_MISS/200 5432 GET http://example.com/data.txt - DIRECT/93.184.216.34 text/plain
```

### Analysis Commands

**Basic traffic inspection:**

```bash
# View recent requests
tail -f /var/log/squid/access.log

# Count requests per client IP
awk '{print $3}' /var/log/squid/access.log | sort | uniq -c | sort -rn

# Extract unique domains accessed
awk '{print $7}' /var/log/squid/access.log | grep -oP 'https?://\K[^/]+' | sort -u
```

**Detect suspicious patterns:**

```bash
# Failed connection attempts (potential scanning)
grep "TCP_DENIED" /var/log/squid/access.log

# Large data transfers (exfiltration candidates)
awk '$5 > 10000000 {print $1, $3, $7, $5}' /var/log/squid/access.log

# Unusual HTTP methods
grep -E "PUT|DELETE|TRACE|CONNECT" /var/log/squid/access.log

# Access to IP addresses instead of domains (C2 indicator)
awk '$7 ~ /^https?:\/\/[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+/ {print}' /var/log/squid/access.log
```

**Identify C2 beaconing:**

```bash
# Regular interval connections (potential beaconing)
awk '{print $1, $3, $7}' /var/log/squid/access.log | \
  awk '{domain=$3; gsub(/https?:\/\//, "", domain); gsub(/\/.*/, "", domain); print $1, $2, domain}' | \
  awk '{intervals[$(2)"_"$(3)]++} END {for (i in intervals) if (intervals[i] > 50) print i, intervals[i]}'

# Uncommon user agents (tool signatures)
awk '{for(i=1;i<=NF;i++) if($i ~ /^Mozilla/ || $i ~ /curl/ || $i ~ /wget/) print $i}' /var/log/squid/access.log | sort | uniq -c | sort -rn
```

**Extract authentication data:**

```bash
# Authenticated user activity (if proxy auth enabled)
grep -v '^-' /var/log/squid/access.log | awk '{print $8}' | sort | uniq -c

# Failed authentication attempts
grep "TCP_DENIED/407" /var/log/squid/access.log
```

### Advanced Analysis with squid-analyzer

[Unverified] - squid-analyzer is commonly used but availability/functionality should be confirmed in your environment.

```bash
# Install squid-analyzer (Debian/Ubuntu)
apt-get install squidanalyzer

# Generate HTML report
squid-analyzer -c /etc/squidanalyzer/squidanalyzer.conf -b /var/www/html/squid-reports/

# View reports at http://<host>/squid-reports/
```

### Parsing with Python

```python
#!/usr/bin/env python3
import re
from urllib.parse import urlparse

def parse_squid_log(logfile):
    entries = []
    with open(logfile, 'r') as f:
        for line in f:
            parts = line.split()
            if len(parts) < 10:
                continue
            
            entry = {
                'timestamp': parts[0],
                'elapsed': parts[1],
                'client_ip': parts[2],
                'code': parts[3],
                'bytes': parts[4],
                'method': parts[5],
                'url': parts[6],
                'user': parts[7],
                'hierarchy': parts[8],
                'type': parts[9] if len(parts) > 9 else ''
            }
            
            # Extract domain
            parsed = urlparse(entry['url'])
            entry['domain'] = parsed.netloc
            
            entries.append(entry)
    return entries

# Usage: Detect data exfiltration
logs = parse_squid_log('/var/log/squid/access.log')
large_transfers = [log for log in logs if int(log['bytes']) > 50000000]
for transfer in large_transfers:
    print(f"{transfer['client_ip']} -> {transfer['domain']} : {transfer['bytes']} bytes")
```

### CTF-Specific Techniques

**Detect encoded/obfuscated URLs:**

```bash
# Base64 in URLs (common C2 technique)
grep -oP 'https?://[^\s]+' /var/log/squid/access.log | grep -E '[A-Za-z0-9+/]{20,}={0,2}'

# Excessive URL encoding
grep '%[0-9A-Fa-f][0-9A-Fa-f]' /var/log/squid/access.log | awk '{count=gsub(/%/,"&"); if(count>10) print}'
```

**Timeline correlation:**

```bash
# Convert Squid epoch timestamp to readable format
awk '{print strftime("%Y-%m-%d %H:%M:%S", $1), $3, $7}' /var/log/squid/access.log

# Filter by time window (epoch timestamp)
awk '$1 >= 1698345600 && $1 <= 1698349200' /var/log/squid/access.log
```

---

## VPN Logs (OpenVPN, WireGuard)

VPN logs track remote access sessions, authentication events, and tunnel traffic metadata. Essential for identifying unauthorized access, credential abuse, and pivot points into internal networks.

### OpenVPN Logs

**Log locations:**

```bash
# Systemd journal (default for recent installations)
journalctl -u openvpn@server -f

# Traditional syslog
/var/log/syslog
/var/log/messages

# Custom log file (check OpenVPN config)
grep "^log" /etc/openvpn/server.conf
# Common: /var/log/openvpn.log or /var/log/openvpn/openvpn.log

# Status file (active connections)
/var/log/openvpn/openvpn-status.log
/run/openvpn-server/status-server.log
```

**Configuration check:**

```bash
# Verify logging configuration
grep -E "^(log|verb|status)" /etc/openvpn/server.conf

# Increase verbosity for debugging (0-11, default 3)
# verb 4  # Connection-level detail
# verb 5  # Packet-level detail (very verbose)
```

### OpenVPN Log Analysis

**Connection tracking:**

```bash
# Client connection events
grep "Peer Connection Initiated" /var/log/openvpn.log

# Extract username and source IP
grep "Peer Connection Initiated" /var/log/openvpn.log | \
  grep -oP '(?<=with ).*?(?= \(via)' | sort | uniq -c

# Disconnection events
grep "SIGTERM received, sending exit notification" /var/log/openvpn.log
```

**Authentication analysis:**

```bash
# Successful authentications
grep "PLUGIN_CALL.*succeeded" /var/log/openvpn.log

# Failed authentication attempts
grep "TLS Auth Error" /var/log/openvpn.log
grep "AUTH_FAILED" /var/log/openvpn.log
grep "PLUGIN_CALL.*failed" /var/log/openvpn.log

# Username enumeration attempts (multiple failed auth)
grep "AUTH_FAILED" /var/log/openvpn.log | awk '{print $NF}' | sort | uniq -c | sort -rn
```

**Parse active connections (status file):**

```bash
# View current VPN clients
cat /var/log/openvpn/openvpn-status.log

# Extract client list with assigned IPs
awk '/CLIENT_LIST/,/ROUTING_TABLE/' /var/log/openvpn/openvpn-status.log | \
  grep -v "ROUTING_TABLE" | column -t
```

**Detect anomalous behavior:**

```bash
# Multiple connections from same user (session hijacking indicator)
grep "CLIENT_LIST" /var/log/openvpn/openvpn-status.log | \
  awk '{print $2}' | sort | uniq -c | awk '$1 > 1'

# Connections from unusual geographic locations (requires GeoIP)
grep "Peer Connection Initiated" /var/log/openvpn.log | \
  grep -oP '\d+\.\d+\.\d+\.\d+' | while read ip; do
    echo "$ip $(geoiplookup $ip)"
  done

# Short-lived connections (scanning behavior)
# [Inference] - requires timestamp analysis across connection/disconnection events
```

### WireGuard Logs

WireGuard has minimal logging by default. Analysis relies on kernel messages and custom logging configurations.

**Log locations:**

```bash
# Kernel ring buffer (immediate events)
dmesg | grep wireguard

# Systemd journal (primary source)
journalctl -k | grep wireguard
journalctl -u wg-quick@wg0 -f

# Traditional logs
/var/log/kern.log
/var/log/syslog
```

**Enable enhanced logging:**

```bash
# Increase kernel logging verbosity (temporary)
echo 'module wireguard +p' > /sys/kernel/debug/dynamic_debug/control

# Persistent (add to kernel parameters)
# Edit /etc/default/grub: GRUB_CMDLINE_LINUX="dyndbg=\"module wireguard +p\""
# Run: update-grub && reboot
```

### WireGuard Log Analysis

**Connection monitoring:**

```bash
# View handshake events (peer connections)
journalctl -k | grep "WireGuard" | grep "Receiving handshake initiation"

# Extract peer public keys from handshakes
dmesg | grep "wireguard: wg0: Handshake for peer" | grep -oP '[A-Za-z0-9+/]{43}='

# Check active peers
wg show wg0

# Parse active connections with traffic stats
wg show wg0 dump | column -t
```

**Detect suspicious activity:**

```bash
# Failed handshakes (invalid key attempts)
journalctl -k | grep "Invalid handshake initiation"

# Replay attack detection (built-in counter validation)
journalctl -k | grep "Packet has invalid nonce"

# Excessive handshake attempts (potential DoS)
journalctl -k --since "1 hour ago" | grep "Receiving handshake initiation" | wc -l
```

**Traffic analysis:**

```bash
# Monitor real-time traffic per peer
watch -n 1 'wg show wg0 transfer'

# Extract peer with highest data transfer
wg show wg0 transfer | sort -k3 -rn | head -1

# Identify inactive peers (no recent handshake)
wg show wg0 latest-handshakes | awk '{if ($2 < systime() - 300) print $1, "Last seen:", systime() - $2, "seconds ago"}'
```

### Correlation with Network Logs

**Cross-reference VPN and firewall logs:**

```bash
# Find internal IPs assigned to VPN clients (OpenVPN)
grep "MULTI: Learn" /var/log/openvpn.log | awk '{print $NF, $(NF-1)}'

# Match VPN IPs to firewall activity
VPN_IPS=$(grep "MULTI: Learn" /var/log/openvpn.log | awk '{print $(NF-1)}' | sort -u)
echo "$VPN_IPS" | while read ip; do
  echo "=== Activity for VPN client $ip ==="
  grep "$ip" /var/log/ufw.log
done
```

**Detect lateral movement post-VPN connection:**

```bash
# Timeline: VPN connection followed by internal SMB/RDP attempts
# [Inference] - requires correlation between VPN connect time and subsequent internal network logs

# Example: Extract VPN connect timestamp
VPN_TIME=$(grep "client_name,10.8.0.5" /var/log/openvpn/openvpn-status.log | awk '{print $NF}')

# Search firewall logs for outbound connections from that VPN IP after connection time
# (requires custom scripting based on log formats)
```

### Python Parsing Example (OpenVPN)

```python
#!/usr/bin/env python3
import re
from datetime import datetime

def parse_openvpn_log(logfile):
    connections = []
    pattern = r'(\w{3}\s+\d+\s+\d+:\d+:\d+).*Peer Connection Initiated with.*?(\d+\.\d+\.\d+\.\d+):\d+.*?(\S+)'
    
    with open(logfile, 'r') as f:
        for line in f:
            match = re.search(pattern, line)
            if match:
                timestamp, src_ip, username = match.groups()
                connections.append({
                    'timestamp': timestamp,
                    'source_ip': src_ip,
                    'username': username
                })
    return connections

# Detect multiple connections from different IPs (credential sharing)
logs = parse_openvpn_log('/var/log/openvpn.log')
user_ips = {}
for conn in logs:
    user = conn['username']
    if user not in user_ips:
        user_ips[user] = set()
    user_ips[user].add(conn['source_ip'])

for user, ips in user_ips.items():
    if len(ips) > 1:
        print(f"[!] User '{user}' connected from multiple IPs: {ips}")
```

---

## Important Related Topics

For comprehensive CTF log analysis involving network services, consider studying these interconnected areas:

- **Firewall Logs (iptables/nftables/ufw)** - Essential for correlating VPN/proxy traffic with permitted/blocked connections
- **DNS Logs (BIND/dnsmasq)** - Critical for resolving domain activity seen in proxy logs and detecting DNS tunneling
- **SIEM Integration** - Aggregating DHCP/proxy/VPN logs into centralized platforms (ELK, Splunk, Graylog)
- **Network Traffic Analysis (Wireshark/tcpdump)** - Deep packet inspection to validate log findings and uncover encrypted communications
- **Authentication Logs (PAM/RADIUS)** - Connecting VPN authentication events to broader identity management systems

---

# Firewall & Security Logs

## iptables/nftables logs

### Overview

iptables and nftables are Linux kernel-level firewall frameworks that can log dropped, accepted, or rejected packets based on configured rules. iptables uses the legacy netfilter framework, while nftables is the modern replacement (kernel 3.13+). Both can write logs to syslog or dedicated files for forensic analysis during CTF scenarios.

### Log Location & Structure

**Default log locations:**

- `/var/log/kern.log` - Primary kernel log containing firewall events
- `/var/log/syslog` - Combined system log (Debian/Ubuntu)
- `/var/log/messages` - Combined system log (RHEL/CentOS)
- `journalctl -k` - Systemd journal kernel messages

**Log entry anatomy (iptables):**

```
Oct 28 14:23:45 hostname kernel: [12345.678] IN=eth0 OUT= MAC=00:11:22:33:44:55:66:77:88:99:aa:bb:cc:dd SRC=192.168.1.100 DST=192.168.1.1 LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=54321 DF PROTO=TCP SPT=45678 DPT=22 WINDOW=29200 RES=0x00 SYN URGP=0
```

**Key fields:**

- `IN` - Incoming interface (blank if outgoing)
- `OUT` - Outgoing interface (blank if incoming)
- `SRC/DST` - Source/destination IP addresses
- `PROTO` - Protocol (TCP/UDP/ICMP)
- `SPT/DPT` - Source/destination ports
- `SYN/ACK/FIN/RST` - TCP flags

### iptables Logging Configuration

**View current rules:**

```bash
iptables -L -v -n --line-numbers
iptables -t nat -L -v -n  # NAT table
iptables -t mangle -L -v -n  # Mangle table
iptables-save  # Dump all rules
```

**Create logging rules:**

```bash
# Log dropped packets to specific chain
iptables -N LOGGING
iptables -A INPUT -j LOGGING
iptables -A LOGGING -m limit --limit 5/min -j LOG --log-prefix "IPT-DROPPED: " --log-level 4
iptables -A LOGGING -j DROP

# Log specific port attempts (e.g., SSH brute force)
iptables -A INPUT -p tcp --dport 22 -m state --state NEW -m recent --set --name SSH
iptables -A INPUT -p tcp --dport 22 -m state --state NEW -m recent --update --seconds 60 --hitcount 4 --rttl --name SSH -j LOG --log-prefix "SSH-BRUTE: "

# Log forwarded traffic
iptables -A FORWARD -j LOG --log-prefix "FWD: " --log-level 6

# Log with additional details
iptables -A INPUT -j LOG --log-prefix "CUSTOM: " --log-level 4 --log-tcp-sequence --log-tcp-options --log-ip-options
```

**Log levels:**

- 0-3: Emergency to Error (critical issues)
- 4: Warning (default for most logging)
- 5-7: Notice to Debug (verbose)

### nftables Logging Configuration

**View current rules:**

```bash
nft list ruleset
nft list table inet filter
nft list chain inet filter input
```

**Create logging rules:**

```bash
# Create table and chain with logging
nft add table inet filter
nft add chain inet filter input { type filter hook input priority 0\; policy accept\; }

# Log dropped packets
nft add rule inet filter input counter log prefix \"NFT-DROP: \" level warn drop

# Log specific protocol with rate limiting
nft add rule inet filter input tcp dport 22 limit rate 5/minute log prefix \"SSH-ATTEMPT: \" accept

# Log with additional metadata
nft add rule inet filter input log prefix \"INPUT: \" level info flags all

# Log to specific group for ulogd
nft add rule inet filter input log group 2 prefix \"NFLOG: \"
```

**nftables log flags:**

- `tcp sequence` - TCP sequence numbers
- `tcp options` - TCP options
- `ip options` - IP options
- `skuid` - Socket UID
- `ether` - Ethernet header
- `all` - All available information

### Analysis Techniques

**Real-time monitoring:**

```bash
# Tail kernel logs for firewall events
tail -f /var/log/kern.log | grep -i "IPT\|NFT"

# Using journalctl (systemd)
journalctl -kf | grep -E "IN=|OUT="

# Watch for specific prefix
dmesg -w | grep "SSH-BRUTE"
```

**Parse and analyze logs:**

```bash
# Extract unique source IPs attacking SSH
grep "SSH-BRUTE" /var/log/kern.log | grep -oP 'SRC=\K[0-9.]+' | sort -u

# Count drops per source IP
grep "IPT-DROPPED" /var/log/kern.log | grep -oP 'SRC=\K[0-9.]+' | sort | uniq -c | sort -rn

# Identify port scan patterns (multiple ports from same source)
grep "SYN" /var/log/kern.log | awk '{for(i=1;i<=NF;i++){if($i~/SRC=/){src=$i}if($i~/DPT=/){dpt=$i}}print src,dpt}' | sort | uniq -c | sort -rn

# Extract destination ports being targeted
grep "DPT=" /var/log/kern.log | grep -oP 'DPT=\K[0-9]+' | sort -n | uniq -c

# Timeline of attack attempts
grep "SSH-BRUTE" /var/log/kern.log | awk '{print $1, $2, $3}' | uniq -c

# Identify fragmented packets (potential evasion)
grep "FRAG:" /var/log/kern.log

# Find XMAS scans (FIN+PSH+URG flags)
grep -E "FIN.*PSH.*URG" /var/log/kern.log
```

**Advanced parsing with awk:**

```bash
# Create attack summary report
awk '/IPT-DROPPED/ {
    for(i=1; i<=NF; i++) {
        if($i ~ /SRC=/) src=$i;
        if($i ~ /DPT=/) dpt=$i;
        if($i ~ /PROTO=/) proto=$i;
    }
    print src, proto, dpt
}' /var/log/kern.log | sort | uniq -c | sort -rn | head -20
```

**Correlate with pcap data:**

```bash
# Extract IPs from firewall logs
grep "DROPPED" /var/log/kern.log | grep -oP 'SRC=\K[0-9.]+' > blocked_ips.txt

# Filter pcap for those IPs
tcpdump -r capture.pcap -n 'host X.X.X.X' -w filtered.pcap
```

### CTF-Specific Indicators

**Look for:**

- **Reverse shell attempts:** Outbound connections to unusual high ports
- **Data exfiltration:** Large outbound transfers to external IPs
- **Privilege escalation artifacts:** Local connections attempting privileged ports
- **Lateral movement:** Internal network scanning patterns (sequential port/IP probing)
- **C2 beaconing:** Regular interval connections to same destination

**Example queries:**

```bash
# Detect potential reverse shells (outbound to high ports)
grep "OUT=" /var/log/kern.log | grep -E "DPT=(4444|4445|8080|9001)" | grep -oP 'DST=\K[0-9.]+'

# Identify internal scanning
grep "SYN" /var/log/kern.log | grep "192.168" | awk '{for(i=1;i<=NF;i++){if($i~/DST=/){print $i}}}' | sort | uniq -c | sort -rn

# Find blocked legitimate tools (may indicate blue team activity)
grep -E "nmap|metasploit|sqlmap" /var/log/kern.log
```

### Persistence & Evasion Detection

[Inference] Common evasion techniques that may appear in logs:

- Fragmented packets to bypass rule matching
- Source IP spoofing (difficult in stateful inspection)
- Slow scans to avoid rate-limiting rules
- Using allowed protocols/ports (DNS, HTTP, HTTPS) for tunneling

**Detection commands:**

```bash
# Fragmentation analysis
grep "FRAG" /var/log/kern.log | wc -l

# Identify TTL anomalies (possible spoofing or OS fingerprint evasion)
grep "TTL=" /var/log/kern.log | grep -oP 'TTL=\K[0-9]+' | sort | uniq -c

# Detect packets with unusual flag combinations
grep -E "SYN.*FIN|FIN.*RST" /var/log/kern.log
```

## UFW (Uncomplicated Firewall) logs

### Overview

UFW is a frontend for iptables designed to simplify firewall management on Ubuntu/Debian systems. It maintains its own logging format while utilizing the underlying iptables/netfilter infrastructure.

### Log Location & Configuration

**Default log location:**

- `/var/log/ufw.log` - Dedicated UFW log file

**Enable/configure logging:**

```bash
# Enable UFW
ufw enable

# Set logging level
ufw logging off        # Disable logging
ufw logging low        # Log blocked packets
ufw logging medium     # Log blocked + allowed packets (limit logging)
ufw logging high       # Log all packets (with rate limiting)
ufw logging full       # Log all packets (no rate limiting)

# View current status
ufw status verbose

# View numbered rules
ufw status numbered
```

### Log Entry Structure

**Standard UFW log format:**

```
Oct 28 15:45:12 hostname kernel: [UFW BLOCK] IN=eth0 OUT= MAC=00:11:22:33:44:55:66:77:88:99:aa:bb:cc:dd SRC=203.0.113.45 DST=192.168.1.10 LEN=52 TOS=0x00 PREC=0x00 TTL=112 ID=34567 DF PROTO=TCP SPT=54321 DPT=445 WINDOW=8192 RES=0x00 SYN URGP=0
```

**Log types:**

- `[UFW BLOCK]` - Packet blocked by explicit rule
- `[UFW ALLOW]` - Packet allowed (medium/high/full logging)
- `[UFW AUDIT]` - Audit rule matched
- `[UFW LIMIT]` - Rate limit rule triggered

### Analysis Techniques

**Basic log parsing:**

```bash
# View recent blocks
tail -100 /var/log/ufw.log | grep BLOCK

# Count blocks by source IP
grep "BLOCK" /var/log/ufw.log | grep -oP 'SRC=\K[0-9.]+' | sort | uniq -c | sort -rn

# Identify most targeted ports
grep "BLOCK" /var/log/ufw.log | grep -oP 'DPT=\K[0-9]+' | sort -n | uniq -c | sort -rn

# Find allowed connections (if logging medium+)
grep "ALLOW" /var/log/ufw.log | tail -50

# Detect rate-limited attacks
grep "LIMIT" /var/log/ufw.log
```

**Protocol-specific analysis:**

```bash
# SSH attack attempts
grep "DPT=22" /var/log/ufw.log | grep "BLOCK" | wc -l

# SMB/CIFS reconnaissance
grep -E "DPT=(139|445)" /var/log/ufw.log | grep "BLOCK"

# RDP attempts
grep "DPT=3389" /var/log/ufw.log

# Web service targeting
grep -E "DPT=(80|443|8080|8443)" /var/log/ufw.log

# Database port scanning
grep -E "DPT=(3306|5432|1433|27017)" /var/log/ufw.log
```

**Temporal analysis:**

```bash
# Attack timeline (hourly summary)
grep "BLOCK" /var/log/ufw.log | awk '{print $1, $2, $3}' | cut -d: -f1 | uniq -c

# Daily attack volume
grep "BLOCK" /var/log/ufw.log | awk '{print $1, $2}' | uniq -c

# Rapid-fire attacks (same source, short timeframe)
grep "BLOCK" /var/log/ufw.log | grep -oP '\w{3}\s+\d+\s+\d+:\d+:\d+.*SRC=\K[0-9.]+' | uniq -c | awk '$1 > 10'
```

**Geographic correlation:** [Inference] Requires external tools or databases for IP geolocation:

```bash
# Extract top attacker IPs
grep "BLOCK" /var/log/ufw.log | grep -oP 'SRC=\K[0-9.]+' | sort | uniq -c | sort -rn | head -10 | awk '{print $2}' > top_attackers.txt

# Use whois or geoip tools
while read ip; do whois $ip | grep -i country; done < top_attackers.txt

# With geoiplookup (if installed)
while read ip; do echo -n "$ip: "; geoiplookup $ip; done < top_attackers.txt
```

### CTF-Specific Analysis

**Detect reconnaissance phases:**

```bash
# Port sweep detection (same source, multiple destinations)
awk '/BLOCK.*SYN/ {
    for(i=1; i<=NF; i++) {
        if($i ~ /SRC=/) src=$i;
        if($i ~ /DPT=/) dpt=$i;
    }
    print src, dpt
}' /var/log/ufw.log | sort -t= -k2 | uniq | cut -d= -f2 | sort | uniq -c | awk '$1 > 5'

# Host sweep detection (same source, multiple destinations)
awk '/BLOCK/ {
    for(i=1; i<=NF; i++) {
        if($i ~ /SRC=/) src=$i;
        if($i ~ /DST=/) dst=$i;
    }
    print src, dst
}' /var/log/ufw.log | sort -t= -k2 | uniq | cut -d= -f2 | sort | uniq -c | awk '$1 > 5'
```

**Identify successful attacks (absence of blocks):**

```bash
# Compare blocked ports vs. listening services
ss -tlnp | awk 'NR>1 {print $4}' | grep -oP ':\K[0-9]+$' | sort -u > listening_ports.txt
grep "BLOCK" /var/log/ufw.log | grep -oP 'DPT=\K[0-9]+' | sort -u > blocked_ports.txt
comm -23 listening_ports.txt blocked_ports.txt  # Ports not being attacked (may indicate knowledge)
```

**Correlate with authentication logs:**

```bash
# Cross-reference SSH blocks with auth attempts
join -t= -1 2 -2 2 \
  <(grep "DPT=22.*BLOCK" /var/log/ufw.log | grep -oP 'SRC=\K[0-9.]+' | sort -u) \
  <(grep "Failed password" /var/log/auth.log | grep -oP 'from \K[0-9.]+' | sort -u)
```

### Configuration Analysis

**Review UFW rules for misconfigurations:**

```bash
# List all rules
ufw status numbered

# Check for overly permissive rules
ufw show raw  # Display iptables rules created by UFW

# Identify default policies
ufw status verbose | grep "Default:"

# Review application profiles
ls /etc/ufw/applications.d/
ufw app list
ufw app info <app_name>
```

**Configuration files:**

- `/etc/ufw/ufw.conf` - Main configuration
- `/etc/ufw/before.rules` - Rules processed before UFW rules
- `/etc/ufw/after.rules` - Rules processed after UFW rules
- `/etc/ufw/user.rules` - User-defined rules
- `/etc/default/ufw` - UFW defaults

## fail2ban logs

### Overview

fail2ban is an intrusion prevention framework that monitors log files for malicious patterns (e.g., repeated failed login attempts) and creates temporary firewall rules to ban offending IP addresses. It operates through "jails" configured for specific services.

### Log Location & Structure

**Primary log file:**

- `/var/log/fail2ban.log` - Main fail2ban operational log

**Configuration locations:**

- `/etc/fail2ban/jail.conf` - Default jail configuration (do not edit)
- `/etc/fail2ban/jail.local` - Custom jail overrides
- `/etc/fail2ban/jail.d/` - Additional jail configurations
- `/etc/fail2ban/filter.d/` - Filter definitions (regex patterns)
- `/etc/fail2ban/action.d/` - Action definitions (ban/unban commands)

**Log entry types:**

```
2025-10-28 16:30:15,123 fail2ban.actions        [12345]: NOTICE  [sshd] Ban 198.51.100.23
2025-10-28 16:40:15,456 fail2ban.actions        [12345]: NOTICE  [sshd] Unban 198.51.100.23
2025-10-28 16:35:22,789 fail2ban.filter         [12345]: INFO    [apache-auth] Found 203.0.113.45 - 2025-10-28 16:35:22
```

**Key log components:**

- Timestamp in format: `YYYY-MM-DD HH:MM:SS,milliseconds`
- Module: `fail2ban.actions`, `fail2ban.filter`, `fail2ban.jail`, etc.
- PID: Process ID
- Level: NOTICE, INFO, WARNING, ERROR, CRITICAL
- Jail name: [sshd], [apache-auth], etc.
- Action: Ban, Unban, Found, Start, Stop

### Status & Management Commands

**Check fail2ban status:**

```bash
# Service status
systemctl status fail2ban

# View all jails
fail2ban-client status

# View specific jail status
fail2ban-client status sshd

# Get banned IPs for jail
fail2ban-client get sshd banned

# Get current ban count
fail2ban-client get sshd bantime
fail2ban-client get sshd maxretry
fail2ban-client get sshd findtime
```

**Manual ban/unban:**

```bash
# Ban IP manually
fail2ban-client set sshd banip 203.0.113.45

# Unban IP
fail2ban-client set sshd unbanip 203.0.113.45

# Unban all IPs in jail
fail2ban-client unban --all

# Reload jail
fail2ban-client reload sshd
```

### Log Analysis Techniques

**Basic monitoring:**

```bash
# Real-time monitoring
tail -f /var/log/fail2ban.log

# View recent bans
grep "Ban" /var/log/fail2ban.log | tail -50

# View recent unbans
grep "Unban" /var/log/fail2ban.log | tail -50

# Count total bans
grep " Ban " /var/log/fail2ban.log | wc -l

# Currently banned IPs across all jails
iptables -L f2b-sshd -v -n  # Replace 'sshd' with jail name
# or for nftables
nft list chain inet f2b-table f2b-sshd
```

**Jail-specific analysis:**

```bash
# SSH brute force attempts
grep "\[sshd\]" /var/log/fail2ban.log

# Apache/Nginx authentication failures
grep "\[apache-auth\]" /var/log/fail2ban.log
grep "\[nginx-http-auth\]" /var/log/fail2ban.log

# Postfix mail server attacks
grep "\[postfix\]" /var/log/fail2ban.log

# FTP brute force
grep "\[proftpd\]" /var/log/fail2ban.log
grep "\[vsftpd\]" /var/log/fail2ban.log
```

**Identify persistent attackers:**

```bash
# IPs banned multiple times
grep " Ban " /var/log/fail2ban.log | grep -oP 'Ban \K[0-9.]+' | sort | uniq -c | sort -rn

# IPs with frequent failed attempts (Found entries)
grep "Found" /var/log/fail2ban.log | grep -oP 'Found \K[0-9.]+' | sort | uniq -c | sort -rn

# Repeat offenders across different services
awk '/Ban/ {for(i=1;i<=NF;i++){if($i ~ /\[.*\]/){jail=$i} if($i ~ /[0-9]+\.[0-9]+/){ip=$i}} print jail, ip}' /var/log/fail2ban.log | sort | uniq -c | sort -rn
```

**Temporal analysis:**

```bash
# Ban activity timeline
grep " Ban " /var/log/fail2ban.log | awk '{print $1, $2}' | cut -d, -f1 | uniq -c

# Hourly ban patterns
grep " Ban " /var/log/fail2ban.log | awk '{print $1, $2}' | cut -d: -f1 | uniq -c

# Find coordinated attacks (multiple IPs, same timeframe)
grep " Ban " /var/log/fail2ban.log | awk '{print $1, $2}' | cut -d, -f1 | cut -d: -f1-2 | uniq -c | awk '$1 > 5'

# Time between ban and unban for IPs (detect ban durations)
awk '/Ban|Unban/ {print $1, $2, $NF, $(NF-1)}' /var/log/fail2ban.log | grep -A1 "Ban" | grep -B1 "Unban"
```

**Error and operational issues:**

```bash
# Identify configuration errors
grep "ERROR" /var/log/fail2ban.log

# Find warnings
grep "WARNING" /var/log/fail2ban.log

# Check for jail start/stop issues
grep -E "Starting|Stopping" /var/log/fail2ban.log

# Identify regex pattern failures
grep "Unable to find" /var/log/fail2ban.log
```

### Configuration Analysis for CTF

**Review jail configurations:**

```bash
# List enabled jails
fail2ban-client status | grep "Jail list"

# View jail parameters
fail2ban-client get sshd bantime
fail2ban-client get sshd findtime
fail2ban-client get sshd maxretry
fail2ban-client get sshd usedns
fail2ban-client get sshd action

# Dump full jail config
fail2ban-client -d

# Check which log files are monitored
fail2ban-client get sshd logpath
```

**Common jail configurations:**

```bash
# View default filter patterns
cat /etc/fail2ban/filter.d/sshd.conf
cat /etc/fail2ban/filter.d/apache-auth.conf

# Test filter against log file
fail2ban-regex /var/log/auth.log /etc/fail2ban/filter.d/sshd.conf

# Test with verbose output
fail2ban-regex /var/log/auth.log /etc/fail2ban/filter.d/sshd.conf --print-all-matched
```

**Key configuration parameters:**

- `bantime` - Duration of ban (seconds, -1 for permanent)
- `findtime` - Time window for counting failures (seconds)
- `maxretry` - Number of failures before ban
- `ignoreip` - Whitelisted IPs (never banned)
- `filter` - Regex pattern to match malicious behavior
- `action` - What to do when match occurs (typically iptables/nftables ban)

### CTF-Specific Indicators

**Detect evasion attempts:**

```bash
# IPs with failures just below maxretry threshold
# [Inference] Requires parsing monitored service logs alongside fail2ban logs
grep "Found" /var/log/fail2ban.log | grep -oP 'Found \K[0-9.]+' | sort | uniq -c | awk '$1 >= <maxretry-1>'

# Check for IP rotation (many unique IPs, low failure counts)
grep "Found" /var/log/fail2ban.log | grep -oP 'Found \K[0-9.]+' | sort -u | wc -l
```

**Identify whitelisted IPs (potential backdoors):**

```bash
# Check ignore lists
grep "ignoreip" /etc/fail2ban/jail.local
grep "ignoreip" /etc/fail2ban/jail.d/*.conf

# Find IPs that should be banned but aren't
comm -23 \
  <(grep "Found" /var/log/fail2ban.log | grep -oP 'Found \K[0-9.]+' | sort -u) \
  <(grep " Ban " /var/log/fail2ban.log | grep -oP 'Ban \K[0-9.]+' | sort -u)
```

**Correlate with actual service logs:**

```bash
# Cross-reference SSH failed attempts with fail2ban bans
grep "Failed password" /var/log/auth.log | grep -oP 'from \K[0-9.]+' | sort -u > failed_ssh.txt
grep "\[sshd\] Ban" /var/log/fail2ban.log | grep -oP 'Ban \K[0-9.]+' | sort -u > banned_ssh.txt
comm -23 failed_ssh.txt banned_ssh.txt  # IPs with failures but not banned

# Check if banned IPs attempted connections after ban
banned_ip="203.0.113.45"
ban_time=$(grep "$banned_ip" /var/log/fail2ban.log | grep " Ban " | head -1 | awk '{print $1, $2}')
grep "$banned_ip" /var/log/auth.log | awk -v bt="$ban_time" '$0 > bt'
```

**Detect disabled or ineffective jails:**

```bash
# Find jails with no bans
for jail in $(fail2ban-client status | grep "Jail list" | cut -d: -f2 | tr ',' ' '); do
    banned=$(fail2ban-client status $jail | grep "Currently banned" | grep -oP '\d+$')
    total=$(fail2ban-client status $jail | grep "Total banned" | grep -oP '\d+$')
    echo "$jail: Total=$total, Current=$banned"
done

# Identify jails that aren't monitoring expected logs
grep "Unable to open" /var/log/fail2ban.log
```

### Database Integration for Persistent Tracking

[Unverified] fail2ban can be configured to use a SQLite database for persistent ban tracking:

**Check if database is enabled:**

```bash
# Look for dbfile or dbpurgeage in configuration
grep -r "dbfile" /etc/fail2ban/

# Default database location
ls -lh /var/lib/fail2ban/fail2ban.sqlite3
```

**Query SQLite database (if exists):**

```bash
sqlite3 /var/lib/fail2ban/fail2ban.sqlite3 "SELECT * FROM bans ORDER BY timeofban DESC LIMIT 20;"
sqlite3 /var/lib/fail2ban/fail2ban.sqlite3 "SELECT ip, COUNT(*) as ban_count FROM bans GROUP BY ip ORDER BY ban_count DESC;"
```

---

## Cross-Log Correlation Strategies

When analyzing firewall and security logs in CTF scenarios, correlation across different log sources reveals the complete attack narrative:

**Workflow example:**

1. Identify banned IPs in fail2ban logs
2. Search for those IPs in iptables/UFW logs to see blocked connection attempts
3. Check authentication logs (`/var/log/auth.log`) for successful logins from banned IPs before the ban
4. Review application logs (`/var/log/apache2/access.log`, etc.) for successful exploitation
5. Examine outbound firewall logs for C2 beaconing or data exfiltration

**Combined analysis script example:**

```bash
#!/bin/bash
# Extract top attacker IP
TOP_IP=$(grep " Ban " /var/log/fail2ban.log | grep -oP 'Ban \K[0-9.]+' | sort | uniq -c | sort -rn | head -1 | awk '{print $2}')

echo "Analyzing IP: $TOP_IP"
echo "=== Fail2ban Events ==="
grep "$TOP_IP" /var/log/fail2ban.log

echo "=== Firewall Blocks ==="
grep "$TOP_IP" /var/log/ufw.log

echo "=== Authentication Attempts ==="
grep "$TOP_IP" /var/log/auth.log

echo "=== Web Access ==="
grep "$TOP_IP" /var/log/apache2/access.log
```

---

**Important subtopics for deeper exploration:**

- Syslog configuration and remote logging (rsyslog/syslog-ng)
- Advanced iptables/nftables rule syntax and connection tracking
- Custom fail2ban filters for application-specific attacks
- IDS/IPS log correlation (Snort, Suricata) with firewall logs

---

## SELinux Audit Logs

### Architecture & Components

SELinux (Security-Enhanced Linux) operates as a Mandatory Access Control (MAC) system that generates audit logs through the Linux Audit Framework. All SELinux policy violations and decisions are logged via the `auditd` daemon.

**Primary Log Location:**

```bash
/var/log/audit/audit.log
```

**Key Components:**

- `auditd` - Audit daemon that writes to disk
- `audisp` - Audit dispatcher for real-time event processing
- `ausearch` - Primary search tool for audit logs
- `aureport` - Summary report generation tool

### Log Structure & Message Types

SELinux audit entries follow a structured format:

```
type=AVC msg=audit(timestamp:serial): avc: denied { permission } for pid=XXXX comm="process" name="file" dev="device" ino=inode scontext=source_context tcontext=target_context tclass=object_class permissive=0/1
```

**Critical Message Types:**

- `AVC` (Access Vector Cache) - Policy denials/grants
- `SELINUX_ERR` - SELinux internal errors
- `USER_AVC` - Userspace AVC denials (dbus, systemd)
- `MAC_POLICY_LOAD` - Policy loading events

### Practical Analysis Commands

**Search for AVC denials:**

```bash
ausearch -m avc -ts recent
ausearch -m avc -ts today
ausearch -m avc -ts 10:00:00 -te 11:00:00
```

**Filter by process/executable:**

```bash
ausearch -m avc -c httpd
ausearch -m avc -x /usr/bin/python3
```

**Filter by denied permission:**

```bash
ausearch -m avc | grep denied
ausearch -m avc -i  # Interpret UIDs/GIDs to names
```

**Generate human-readable reports:**

```bash
aureport -a  # AVC report
aureport --avc --summary  # Summarized AVC denials
aureport -i -ts today  # Today's events with interpretation
```

### CTF Exploitation Scenarios

**Identifying privilege escalation opportunities:**

```bash
# Look for denials that might indicate misconfigurations
ausearch -m avc --success no | grep -E "write|execute|transition"

# Find processes attempting capability usage
ausearch -m avc | grep capability

# Identify file access denials that may reveal sensitive paths
ausearch -m avc | grep -oP 'name="[^"]+' | sort -u
```

**Analyzing context mismatches:**

```bash
# Extract source and target contexts
ausearch -m avc -i | grep -E "scontext|tcontext"

# Find domain transitions being blocked
ausearch -m avc | grep type_transition
```

**Using audit2allow for reconnaissance:**

```bash
# Generate policy module from denials (reveals system behavior)
ausearch -m avc -ts recent | audit2allow -M custom_policy

# Analyze what would be allowed
cat custom_policy.te  # View Type Enforcement rules
```

### Key Artifacts in CTF Context

**Timestamps reveal activity patterns:**

- Repeated denials may indicate automated exploitation attempts
- Time gaps between denials suggest manual enumeration
- Correlated denials across services indicate lateral movement

**Process context reveals execution flow:**

```bash
# Track process execution chain
ausearch -m avc -c bash --format text | grep -oP 'pid=\d+'
```

**File contexts expose sensitive resources:**

```bash
# Extract all denied file paths
ausearch -m avc | grep -oP 'path="[^"]+' | sort -u > denied_paths.txt
```

### Common Pitfalls & Notes

[Inference] SELinux logs may not capture all security events if `auditd` is stopped or if policies are in permissive mode. Always check:

```bash
getenforce  # Current SELinux mode
systemctl status auditd  # Daemon status
```

**Permissive mode caveat:** When `permissive=1` in AVC messages, the denial was logged but **not enforced** - the action actually succeeded.

---

## AppArmor Logs

### Architecture & Integration

AppArmor is a path-based MAC system integrated into the Linux Security Module (LSM) framework. Unlike SELinux's label-based approach, AppArmor uses filesystem paths for access control.

**Primary Log Locations:**

```bash
/var/log/syslog          # Debian/Ubuntu systems
/var/log/messages        # RHEL/CentOS systems  
/var/log/audit/audit.log # When auditd integration enabled
/var/log/kern.log        # Kernel-level denials
```

### Log Format & Identification

AppArmor entries are prefixed with `audit` or `apparmor` in syslog format:

```
audit: type=1400 audit(timestamp): apparmor="DENIED" operation="open" profile="profile_name" name="/path/to/file" pid=XXXX comm="process" requested_mask="r" denied_mask="r" fsuid=XXX ouid=XXX
```

**Key Fields:**

- `apparmor="DENIED/ALLOWED/AUDIT"` - Decision type
- `operation=` - System call or operation attempted
- `profile=` - AppArmor profile name
- `name=` - Filesystem path accessed
- `requested_mask=` - Permissions requested (r/w/x/l/k/a/m)
- `denied_mask=` - Permissions actually denied
- `comm=` - Command/process name

### Analysis Commands & Techniques

**Grep-based filtering (most common in CTF):**

```bash
# All AppArmor denials
grep -i apparmor /var/log/syslog | grep DENIED

# Specific profile violations
grep 'profile="/usr/bin/firefox"' /var/log/syslog

# Recent denials (last hour)
grep apparmor /var/log/syslog | tail -n 100 | grep DENIED
```

**Filtering by operation type:**

```bash
grep 'operation="open"' /var/log/syslog | grep DENIED
grep 'operation="exec"' /var/log/syslog | grep DENIED
grep 'operation="connect"' /var/log/syslog  # Network operations
```

**When auditd integration is enabled:**

```bash
ausearch -m apparmor
ausearch -m apparmor --success no  # Denials only
aureport --apparmor
```

**Using aa-notify (if available):**

```bash
aa-notify -s 1h  # Show denials from last hour
aa-notify -p     # Show denials since last notification
```

### CTF Exploitation Patterns

**Profile enumeration:**

```bash
# List all loaded profiles
aa-status

# Extract active profile names from logs
grep apparmor /var/log/syslog | grep -oP 'profile="[^"]+' | sort -u

# Identify profiles in complain mode (permissive)
aa-status | grep complain
```

**Path discovery for privilege escalation:**

```bash
# Find all denied file access attempts
grep DENIED /var/log/syslog | grep -oP 'name="[^"]+' | sort -u

# Identify write denials to system directories
grep DENIED /var/log/syslog | grep 'requested_mask=".*w' | grep -oP 'name="[^"]+' 

# Discover executable paths being blocked
grep DENIED /var/log/syslog | grep 'operation="exec"' | grep -oP 'name="[^"]+' 
```

**Capability and network analysis:**

```bash
# Denied capabilities (privilege indicators)
grep DENIED /var/log/syslog | grep capability

# Network connection attempts
grep apparmor /var/log/syslog | grep -E "connect|socket"
```

**Temporal correlation:**

```bash
# Extract timestamps of denials
grep DENIED /var/log/syslog | awk '{print $1,$2,$3}'

# Count denials by profile
grep DENIED /var/log/syslog | grep -oP 'profile="[^"]+' | sort | uniq -c | sort -rn
```

### Profile Analysis for CTF

**Generate policy from denials:**

```bash
# Extract denials to create permissive policy
grep DENIED /var/log/syslog | aa-logprof  # Interactive profile generator

# Manual extraction for analysis
grep 'profile="target_profile"' /var/log/syslog | grep DENIED > profile_denials.txt
```

**Profile file locations:**

```bash
/etc/apparmor.d/          # Profile definitions
/etc/apparmor.d/abstractions/  # Reusable rule fragments
/etc/apparmor.d/tunables/      # Path variables
```

### Differences from SELinux Logging

|Aspect|AppArmor|SELinux|
|---|---|---|
|Path specificity|Exact paths or globs|Context labels|
|Log verbosity|Generally less verbose|Highly detailed|
|Default logging|syslog integration|Dedicated audit.log|
|Transitional states|Complain mode|Permissive mode|

[Inference] In CTF scenarios, AppArmor profiles in complain mode will log violations without blocking them, potentially revealing intended system behavior without triggering defensive responses.

---

## IDS/IPS Logs (Snort & Suricata)

### Snort Architecture & Log Types

Snort operates as a packet-based network intrusion detection system with multiple output formats optimized for different analysis scenarios.

**Primary Log Locations:**

```bash
/var/log/snort/alert      # Fast alert format (default)
/var/log/snort/snort.log  # Binary pcap format
/var/log/snort/alert.fast # Single-line alerts
/var/log/snort/alert.full # Full alert details
/var/log/snort/alert.csv  # CSV format output
```

**Configuration locations:**

```bash
/etc/snort/snort.conf     # Main configuration
/etc/snort/rules/         # Rule definitions
```

### Snort Alert Format Analysis

**Fast alert format (most common):**

```
[**] [GID:SID:REV] Rule Message [**]
[Classification: category] [Priority: N]
MM/DD-HH:MM:SS.microseconds SRC_IP:PORT -> DST_IP:PORT
Protocol TTL:X TOS:X ID:X IpLen:X DgmLen:X
```

**Example:**

```
[**] [1:1000001:1] Potential SQL Injection Attempt [**]
[Classification: Web Application Attack] [Priority: 1]
10/28-14:23:45.123456 192.168.1.100:54321 -> 192.168.1.50:80
TCP TTL:64 TOS:0x0 ID:12345 IpLen:20 DgmLen:512
```

### Snort Analysis Commands

**Basic log reading:**

```bash
# View alert log
tail -f /var/log/snort/alert

# Filter by priority
grep "Priority: 1" /var/log/snort/alert

# Filter by classification
grep "Web Application Attack" /var/log/snort/alert
```

**Reading binary logs:**

```bash
# Convert pcap log to readable format
snort -r /var/log/snort/snort.log.XXXXX

# Read with specific output format
snort -r /var/log/snort/snort.log.XXXXX -K ascii

# Extract to pcap for Wireshark analysis
tcpdump -r /var/log/snort/snort.log.XXXXX -w output.pcap
```

**Using barnyard2 for unified2 format:**

```bash
# Process unified2 logs to database or other formats
barnyard2 -c /etc/snort/barnyard2.conf -d /var/log/snort -f snort.log
```

**Statistical analysis:**

```bash
# Count alerts by signature
grep -oP '\[\*\*\] \[.*?\] \K[^[]*' /var/log/snort/alert | sort | uniq -c | sort -rn

# Extract unique source IPs
grep -oP '\d+\.\d+\.\d+\.\d+:\d+ ->' /var/log/snort/alert | awk '{print $1}' | cut -d: -f1 | sort -u

# Time-based filtering
grep "10/28-14:" /var/log/snort/alert  # Specific hour
```

### Suricata Architecture & EVE JSON Logging

Suricata uses a multi-threaded architecture with structured JSON output via the EVE (Extensible Event Format) logging system, providing significantly more context than Snort.

**Primary Log Locations:**

```bash
/var/log/suricata/eve.json       # Primary EVE JSON log
/var/log/suricata/fast.log       # Fast alert format (Snort-compatible)
/var/log/suricata/stats.log      # Performance statistics
/var/log/suricata/suricata.log   # Daemon logs
```

**Configuration:**

```bash
/etc/suricata/suricata.yaml      # Main configuration
/etc/suricata/rules/             # Rule files
/var/lib/suricata/rules/         # Managed rules (suricata-update)
```

### EVE JSON Structure & Fields

**Standard EVE event format:**

```json
{
  "timestamp": "2025-10-28T14:23:45.123456+0000",
  "flow_id": 123456789,
  "event_type": "alert",
  "src_ip": "192.168.1.100",
  "src_port": 54321,
  "dest_ip": "192.168.1.50",
  "dest_port": 80,
  "proto": "TCP",
  "alert": {
    "action": "allowed",
    "gid": 1,
    "signature_id": 2100498,
    "rev": 7,
    "signature": "GPL ATTACK_RESPONSE id check returned root",
    "category": "Potentially Bad Traffic",
    "severity": 2
  },
  "http": {
    "hostname": "example.com",
    "url": "/admin/login.php",
    "http_method": "POST",
    "protocol": "HTTP/1.1",
    "status": 200,
    "length": 1234
  },
  "payload_printable": "POST /admin/login.php HTTP/1.1..."
}
```

**Key Event Types:**

- `alert` - IDS rule matches
- `http` - HTTP transaction logs
- `dns` - DNS queries/responses
- `tls` - TLS/SSL handshake data
- `ssh` - SSH protocol events
- `flow` - Network flow records
- `fileinfo` - Extracted file metadata
- `anomaly` - Protocol anomalies

### Suricata Analysis with jq

**Basic alert filtering:**

```bash
# All alerts
jq 'select(.event_type=="alert")' /var/log/suricata/eve.json

# High severity alerts
jq 'select(.event_type=="alert" and .alert.severity<=2)' /var/log/suricata/eve.json

# Alerts from specific source
jq 'select(.event_type=="alert" and .src_ip=="192.168.1.100")' /var/log/suricata/eve.json
```

**Extracting specific fields:**

```bash
# List all signature messages
jq -r 'select(.event_type=="alert") | .alert.signature' /var/log/suricata/eve.json | sort -u

# Create IP pair list
jq -r 'select(.event_type=="alert") | "\(.src_ip):\(.src_port) -> \(.dest_ip):\(.dest_port)"' /var/log/suricata/eve.json

# Extract payloads for analysis
jq -r 'select(.event_type=="alert") | .payload_printable' /var/log/suricata/eve.json
```

**HTTP traffic analysis:**

```bash
# All HTTP requests
jq 'select(.event_type=="http")' /var/log/suricata/eve.json

# POST requests with specific path
jq 'select(.event_type=="http" and .http.http_method=="POST" and .http.url | contains("admin"))' /var/log/suricata/eve.json

# HTTP responses by status code
jq -r 'select(.event_type=="http") | "\(.http.status) \(.http.url)"' /var/log/suricata/eve.json | sort | uniq -c
```

**DNS enumeration:**

```bash
# DNS queries
jq -r 'select(.event_type=="dns" and .dns.type=="query") | .dns.rrname' /var/log/suricata/eve.json | sort -u

# DNS responses with IPs
jq 'select(.event_type=="dns" and .dns.type=="answer")' /var/log/suricata/eve.json
```

**TLS/SSL intelligence gathering:**

```bash
# Extract TLS SNI (Server Name Indication)
jq -r 'select(.event_type=="tls") | .tls.sni' /var/log/suricata/eve.json | sort -u

# Identify certificate subjects
jq -r 'select(.event_type=="tls") | .tls.subject' /var/log/suricata/eve.json

# Find weak ciphers or versions
jq 'select(.event_type=="tls" and .tls.version | contains("TLS 1.0"))' /var/log/suricata/eve.json
```

### CTF Exploitation Techniques

**Timeline reconstruction:**

```bash
# Extract chronological alert sequence
jq -r 'select(.event_type=="alert") | "\(.timestamp) [\(.alert.signature_id)] \(.alert.signature)"' /var/log/suricata/eve.json | sort

# Correlate events by flow_id
jq --arg flow "123456789" 'select(.flow_id==($flow|tonumber))' /var/log/suricata/eve.json
```

**Signature analysis for attack vectors:**

```bash
# Group alerts by category
jq -r 'select(.event_type=="alert") | .alert.category' /var/log/suricata/eve.json | sort | uniq -c | sort -rn

# Identify exploit attempts
jq 'select(.event_type=="alert" and (.alert.signature | contains("exploit") or contains("shellcode")))' /var/log/suricata/eve.json
```

**Payload extraction and analysis:**

```bash
# Extract base64 payloads
jq -r 'select(.event_type=="alert" and .payload) | .payload' /var/log/suricata/eve.json | while read p; do echo $p | base64 -d; done

# Hex payload analysis
jq -r 'select(.event_type=="alert" and .payload_printable) | .payload_printable' /var/log/suricata/eve.json | grep -oP '[0-9a-fA-F]{2,}' 
```

**Network mapping:**

```bash
# Build connection matrix
jq -r 'select(.event_type=="flow") | "\(.src_ip) -> \(.dest_ip):\(.dest_port)"' /var/log/suricata/eve.json | sort -u

# Identify scanning behavior
jq -r 'select(.event_type=="flow") | .src_ip' /var/log/suricata/eve.json | sort | uniq -c | sort -rn
```

**File extraction tracking:**

```bash
# List extracted files
jq 'select(.event_type=="fileinfo")' /var/log/suricata/eve.json

# Malicious file indicators
jq 'select(.event_type=="fileinfo" and .fileinfo.md5)' /var/log/suricata/eve.json | jq -r '.fileinfo.md5' | sort -u
```

### Rule Analysis & Development

**Snort/Suricata rule structure:**

```
action protocol src_ip src_port direction dst_ip dst_port (rule-options)
```

**Example rule:**

```
alert tcp any any -> $HOME_NET 80 (msg:"SQL Injection Attempt"; flow:to_server,established; content:"UNION"; nocase; content:"SELECT"; nocase; distance:0; classtype:web-application-attack; sid:1000001; rev:1;)
```

**Finding triggered rules:**

```bash
# Snort - match SID to rule file
grep -r "sid:1000001" /etc/snort/rules/

# Suricata - same approach
grep -r "sid:2100498" /etc/suricata/rules/

# Extract rule from logs and locate definition
jq -r 'select(.event_type=="alert") | .alert.signature_id' /var/log/suricata/eve.json | sort -u | while read sid; do grep -r "sid:$sid" /etc/suricata/rules/; done
```

### Snort vs Suricata: Key Differences

|Feature|Snort|Suricata|
|---|---|---|
|Processing|Single-threaded|Multi-threaded|
|Log format|Text-based, binary pcap|JSON (EVE), text-compatible|
|Protocol parsing|Basic|Deep packet inspection with protocol parsers|
|File extraction|Via Snort++ only|Native support|
|TLS inspection|Limited|JA3 fingerprinting, full TLS logging|
|Rule compatibility|Snort format|Snort-compatible + Suricata-specific|
|Performance|Lower on multi-core|Scales with CPU cores|

### Performance & Troubleshooting

**Check Snort stats:**

```bash
# During runtime (if configured)
kill -USR1 $(pidof snort)  # Dumps stats to log

# Analyze packet statistics
grep -i "packet" /var/log/snort/snort.log.*
```

**Suricata statistics analysis:**

```bash
# Live stats
tail -f /var/log/suricata/stats.log

# JSON stats parsing
jq 'select(.event_type=="stats")' /var/log/suricata/eve.json

# Packet drop analysis
jq 'select(.event_type=="stats") | .stats.capture.kernel_drops' /var/log/suricata/eve.json
```

**Common issues in CTF environments:**

- **Dropped packets**: Indicates insufficient resources or misconfigurations
- **Missing alerts**: Check rule syntax, variables (`$HOME_NET`, `$EXTERNAL_NET`), and ensure rules are enabled
- **False positives**: Review rule thresholds and suppression lists

[Inference] In CTF scenarios, IDS/IPS logs frequently contain intentional false positives or decoy alerts to obscure the actual attack vector. Always correlate multiple event types and validate alert context before assuming malicious activity.

### Rule Management

**Suricata rule updates:**

```bash
# Update rules via suricata-update
suricata-update
suricata-update list-sources  # Show available sources
suricata-update enable-source et/open  # Enable ET Open rules

# Force reload after updates
suricatasc -c reload-rules
```

**Snort rule updates (PulledPork):**

```bash
# Traditional tool for Snort 2.x rule management
pulledpork.pl -c /etc/snort/pulledpork.conf

# Manual download from Snort.org (requires registration)
# Place rules in /etc/snort/rules/
```

---

### Important Subtopics for Further Study

- **Log correlation techniques**: Combining SELinux/AppArmor MAC logs with IDS/IPS network logs to identify multi-stage attacks
- **SIEM integration**: Parsing these logs into Elastic Stack, Splunk, or Graylog for centralized analysis
- **Custom rule development**: Writing IDS signatures based on CTF-specific attack patterns discovered in audit logs
- **Performance tuning**: Optimizing auditd, AppArmor, and IDS/IPS configurations for high-volume CTF traffic scenarios

---

# Database Logs

Database logs are critical artifacts in CTF scenarios for discovering credentials, identifying injection points, reconstructing attacker actions, and finding hidden data. Database logging mechanisms vary significantly across implementations, each providing different levels of verbosity and forensic value.

## MySQL/MariaDB logs

MySQL and MariaDB share similar logging architectures with multiple log types serving different investigative purposes.

### Log Types and Locations

**Error Log** (default: enabled)

- **Purpose**: Records startup/shutdown events, critical errors, warnings
- **Default location**: `/var/log/mysql/error.log` (Debian/Ubuntu), `/var/log/mysqld.log` (RHEL/CentOS)
- **Configuration**: `log_error` parameter in `/etc/mysql/my.cnf` or `/etc/my.cnf`

```bash
# View error log
sudo tail -f /var/log/mysql/error.log

# Search for authentication failures
grep "Access denied" /var/log/mysql/error.log
```

**General Query Log** (default: disabled - performance impact)

- **Purpose**: Records all client connections and executed SQL statements
- **CTF value**: Highest forensic value - shows exact queries including injection attempts
- **Location**: Defined by `general_log_file` parameter
- **Enable temporarily**:

```sql
SET GLOBAL general_log = 'ON';
SET GLOBAL general_log_file = '/var/log/mysql/general.log';
```

**Analyze general query log**:

```bash
# Extract unique SQL queries
grep "Query" /var/log/mysql/general.log | awk '{$1=$2=$3=""; print $0}' | sort -u

# Find potential SQL injection patterns
grep -E "(UNION|SELECT.*FROM|' OR |admin'--|1=1)" /var/log/mysql/general.log

# Identify suspicious table access
grep -i "information_schema\|mysql.user" /var/log/mysql/general.log
```

**Slow Query Log** (default: disabled)

- **Purpose**: Records queries exceeding `long_query_time` threshold
- **CTF value**: Identifies resource-intensive injection attempts, time-based blind SQLi
- **Configuration**:

```sql
SET GLOBAL slow_query_log = 'ON';
SET GLOBAL slow_query_log_file = '/var/log/mysql/slow.log';
SET GLOBAL long_query_time = 2;  -- seconds
```

**Analyze slow query log**:

```bash
# Parse with mysqldumpslow
mysqldumpslow -s t -t 10 /var/log/mysql/slow.log  # Top 10 by query time
mysqldumpslow -s c -t 20 /var/log/mysql/slow.log  # Top 20 by count

# Find SLEEP() usage (time-based SQLi indicator)
grep -i "sleep(" /var/log/mysql/slow.log
```

**Binary Log** (binlog)

- **Purpose**: Records all data modification statements for replication/recovery
- **CTF value**: Reconstructs database state changes, identifies data exfiltration
- **Location**: Defined by `log_bin` parameter, typically `/var/log/mysql/mysql-bin.*`
- **Enable**: Add to `my.cnf`: `log_bin = /var/log/mysql/mysql-bin`

**Extract binary log contents**:

```bash
# List binary logs
mysqlbinlog --base64-output=DECODE-ROWS /var/log/mysql/mysql-bin.000001

# Filter by database
mysqlbinlog --database=target_db /var/log/mysql/mysql-bin.000001

# Extract between timestamps
mysqlbinlog --start-datetime="2025-10-28 10:00:00" \
            --stop-datetime="2025-10-28 11:00:00" \
            /var/log/mysql/mysql-bin.000001

# Search for specific table operations
mysqlbinlog /var/log/mysql/mysql-bin.* | grep -A 5 "users"
```

### Audit Plugin (MariaDB/MySQL Enterprise)

For granular activity tracking:

```sql
-- Install audit plugin (MariaDB)
INSTALL SONAME 'server_audit';

-- Configure audit logging
SET GLOBAL server_audit_logging = ON;
SET GLOBAL server_audit_events = 'CONNECT,QUERY,TABLE';
SET GLOBAL server_audit_file_path = '/var/log/mysql/audit.log';
```

**Parse audit logs**:

```bash
# Extract connection attempts
grep "CONNECT" /var/log/mysql/audit.log

# Find failed authentication
grep "FAILED_CONNECT" /var/log/mysql/audit.log

# Track specific user activity
grep "user='suspicious_user'" /var/log/mysql/audit.log
```

### Live Query Monitoring

```bash
# Monitor queries in real-time (requires admin privileges)
mysql -e "SHOW FULL PROCESSLIST\G" | grep -A 10 "Query"

# Watch for long-running queries
watch -n 1 'mysql -e "SHOW PROCESSLIST" | grep -v Sleep'

# Kill suspicious query
mysql -e "KILL QUERY <process_id>"
```

### Configuration File Analysis

```bash
# Locate configuration files
find /etc -name "my.cnf" 2>/dev/null
find /etc -name "*.cnf" -path "*/mysql/*" 2>/dev/null

# Check logging configuration
grep -E "log_error|general_log|slow_query_log|log_bin" /etc/mysql/my.cnf
```

## PostgreSQL logs

PostgreSQL provides extensive logging capabilities with highly configurable verbosity levels.

### Log Types and Locations

**Primary Log File**

- **Default location**: `/var/log/postgresql/postgresql-<version>-main.log` (Debian/Ubuntu)
- **RHEL/CentOS**: `/var/lib/pgsql/<version>/data/log/`
- **Configuration file**: `/etc/postgresql/<version>/main/postgresql.conf`

### Critical Configuration Parameters

```bash
# View current logging configuration
sudo -u postgres psql -c "SHOW log_destination;"
sudo -u postgres psql -c "SHOW logging_collector;"
sudo -u postgres psql -c "SHOW log_directory;"
sudo -u postgres psql -c "SHOW log_filename;"
```

**Key parameters in postgresql.conf**:

```conf
# Enable logging collector
logging_collector = on
log_directory = 'log'
log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'

# Statement logging (CTF-critical)
log_statement = 'all'  # none, ddl, mod, all
log_min_duration_statement = 0  # Log all queries with duration

# Connection logging
log_connections = on
log_disconnections = on
log_hostname = on

# Authentication failures
log_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '
```

**Enable comprehensive logging temporarily**:

```sql
-- As superuser
ALTER SYSTEM SET log_statement = 'all';
ALTER SYSTEM SET log_connections = 'on';
ALTER SYSTEM SET log_disconnections = 'on';
SELECT pg_reload_conf();  -- Apply without restart
```

### Log Analysis Techniques

**Basic parsing**:

```bash
# View live logs
sudo tail -f /var/log/postgresql/postgresql-*.log

# Extract authentication failures
grep "FATAL.*password authentication failed" /var/log/postgresql/*.log

# Find SQL injection patterns
grep -E "UNION.*SELECT|' OR '|admin'--" /var/log/postgresql/*.log

# Identify privilege escalation attempts
grep -i "alter.*role\|grant\|create.*user" /var/log/postgresql/*.log
```

**Advanced filtering**:

```bash
# Extract queries from specific database
grep "db=target_db" /var/log/postgresql/*.log | grep "statement:"

# Find queries by specific user
grep "user=webapp" /var/log/postgresql/*.log | grep "LOG:"

# Identify connection source IPs
grep "connection authorized" /var/log/postgresql/*.log | \
  awk -F'host=' '{print $2}' | awk '{print $1}' | sort -u

# Extract duration of slow queries
grep "duration:" /var/log/postgresql/*.log | \
  awk -F'duration: ' '{print $2}' | awk '{print $1}' | sort -n
```

### Query Statistics (pg_stat_statements)

**Enable extension**:

```sql
-- Add to postgresql.conf
shared_preload_libraries = 'pg_stat_statements'
pg_stat_statements.track = all

-- Restart PostgreSQL, then:
CREATE EXTENSION pg_stat_statements;
```

**Analyze query patterns**:

```sql
-- Most executed queries
SELECT calls, total_exec_time, query 
FROM pg_stat_statements 
ORDER BY calls DESC 
LIMIT 20;

-- Slowest queries
SELECT mean_exec_time, calls, query 
FROM pg_stat_statements 
ORDER BY mean_exec_time DESC 
LIMIT 20;

-- Find potential injection attempts (contains common patterns)
SELECT query, calls 
FROM pg_stat_statements 
WHERE query ILIKE '%union%select%' 
   OR query ILIKE '%'' or ''%';
```

### CSV Log Format (Structured Parsing)

**Enable CSV logging**:

```conf
log_destination = 'csvlog'
logging_collector = on
```

**Parse with tools**:

```bash
# Import to temporary table for analysis
psql -d postgres -c "
CREATE TEMP TABLE postgres_log (
  log_time timestamp,
  user_name text,
  database_name text,
  process_id integer,
  connection_from text,
  session_id text,
  session_line_num bigint,
  command_tag text,
  session_start_time timestamp,
  virtual_transaction_id text,
  transaction_id bigint,
  error_severity text,
  sql_state_code text,
  message text,
  detail text,
  hint text,
  internal_query text,
  internal_query_pos integer,
  context text,
  query text,
  query_pos integer,
  location text,
  application_name text
);

COPY postgres_log FROM '/var/lib/postgresql/13/main/log/postgresql-2025-10-28.csv' CSV;

SELECT * FROM postgres_log WHERE message ILIKE '%authentication failed%';
"
```

### Connection Auditing

```bash
# Track active connections
sudo -u postgres psql -c "
SELECT pid, usename, application_name, client_addr, state, query 
FROM pg_stat_activity 
WHERE state = 'active';
"

# Identify connection spikes (potential brute force)
grep "connection authorized" /var/log/postgresql/*.log | \
  awk '{print $1, $2}' | uniq -c | sort -n
```

## MongoDB logs

MongoDB uses a unified log file with structured JSON output in newer versions.

### Log Types and Locations

**System Log**

- **Default location**: `/var/log/mongodb/mongod.log`
- **Configuration**: `/etc/mongod.conf`
- **Format**: JSON (4.4+) or plaintext (older versions)

### Configuration

**Key mongod.conf settings**:

```yaml
systemLog:
  destination: file
  path: /var/log/mongodb/mongod.log
  logAppend: true
  verbosity: 0  # 0-5, higher = more verbose
  component:
    accessControl:
      verbosity: 2
    command:
      verbosity: 1

# Enable audit logging (Enterprise/Percona)
auditLog:
  destination: file
  format: JSON
  path: /var/log/mongodb/audit.json
```

**Enable verbose logging at runtime**:

```javascript
// Connect to MongoDB
mongosh

// Set verbosity levels
db.setLogLevel(2)  // Global verbosity
db.setLogLevel(2, "query")  // Query-specific
db.setLogLevel(3, "accessControl")  // Authentication attempts
```

### Log Analysis Techniques

**Parse plaintext logs (pre-4.4)**:

```bash
# View live logs
sudo tail -f /var/log/mongodb/mongod.log

# Extract authentication failures
grep "Authentication failed" /var/log/mongodb/mongod.log

# Find NoSQL injection patterns
grep -E "\$where.*function|\$ne|javascript:" /var/log/mongodb/mongod.log

# Identify slow queries (>100ms)
grep "durationMillis" /var/log/mongodb/mongod.log | \
  awk -F'durationMillis:' '{print $2}' | awk '{print $1}' | \
  awk '$1 > 100' 

# Extract connection attempts
grep "connection accepted" /var/log/mongodb/mongod.log
```

**Parse JSON logs (4.4+)**:

```bash
# Extract authentication failures with jq
grep "Authentication failed" /var/log/mongodb/mongod.log | jq .

# Find slow operations
jq 'select(.attr.durationMillis > 100)' /var/log/mongodb/mongod.log

# Extract unique source IPs
grep "connection accepted" /var/log/mongodb/mongod.log | \
  jq -r '.attr.remote' | sort -u

# Identify administrative commands
jq 'select(.attr.commandName == "createUser" or .attr.commandName == "grantRolesToUser")' \
  /var/log/mongodb/mongod.log
```

### Profiler (Query Performance Analysis)

MongoDB profiler captures detailed query execution data.

**Enable profiling**:

```javascript
// Level 0: Off
// Level 1: Slow operations only (>slowms)
// Level 2: All operations
db.setProfilingLevel(2)

// Profile only slow queries (>50ms)
db.setProfilingLevel(1, { slowms: 50 })

// Check profiling status
db.getProfilingStatus()
```

**Analyze profiler data**:

```javascript
// View profiled operations
db.system.profile.find().pretty()

// Find slowest queries
db.system.profile.find().sort({ millis: -1 }).limit(10)

// Identify queries on specific collection
db.system.profile.find({ ns: "testdb.users" }).pretty()

// Find operations by user
db.system.profile.find({ user: "webapp@admin" }).pretty()

// Export for external analysis
mongoexport --db admin --collection system.profile \
  --out /tmp/profile.json
```

**Parse exported profiler data**:

```bash
# Find injection-like patterns
jq 'select(.command.filter | tostring | contains("$ne") or contains("$where"))' \
  /tmp/profile.json

# Identify excessive collection scans
jq 'select(.planSummary == "COLLSCAN")' /tmp/profile.json
```

### Audit Log (Enterprise/Percona)

**Configuration** (requires Enterprise or Percona Server):

```yaml
auditLog:
  destination: file
  format: JSON
  path: /var/log/mongodb/audit.json
  filter: '{ atype: { $in: [ "authenticate", "createCollection", "dropCollection" ] } }'
```

**Analyze audit logs**:

```bash
# Authentication events
jq 'select(.atype == "authenticate")' /var/log/mongodb/audit.json

# Failed authentications
jq 'select(.atype == "authenticate" and .result != 0)' /var/log/mongodb/audit.json

# Administrative actions
jq 'select(.atype == "createUser" or .atype == "grantRolesToUser")' \
  /var/log/mongodb/audit.json
```

### Real-time Monitoring

```javascript
// Current operations
db.currentOp()

// Filter active queries
db.currentOp({ "active": true, "secs_running": { "$gt": 5 } })

// Kill long-running operation
db.killOp(<opid>)

// Monitor connections
db.serverStatus().connections
```

## Redis logs

Redis provides minimal logging by default, focusing on server events rather than command-level auditing.

### Log Configuration

**redis.conf settings**:

```conf
# Log location
logfile /var/log/redis/redis-server.log

# Verbosity levels
# debug: Very verbose, useful for development/testing
# verbose: Contains many rarely useful info
# notice: Moderately verbose (default)
# warning: Only very important/critical messages
loglevel notice

# Slow log configuration (in-memory)
slowlog-log-slower-than 10000  # microseconds (10ms)
slowlog-max-len 128  # entries to keep
```

**Runtime configuration**:

```bash
# Connect to Redis
redis-cli

# Set log level
CONFIG SET loglevel debug

# Configure slow log
CONFIG SET slowlog-log-slower-than 5000  # 5ms
CONFIG SET slowlog-max-len 256
```

### Log Analysis Techniques

**Basic parsing**:

```bash
# View live logs
sudo tail -f /var/log/redis/redis-server.log

# Extract authentication failures
grep "Authentication failed" /var/log/redis/redis-server.log

# Find connection attempts
grep "Accepted" /var/log/redis/redis-server.log

# Identify crashed/terminated commands
grep "killed\|timeout\|crashed" /var/log/redis/redis-server.log

# Monitor configuration changes
grep "CONFIG" /var/log/redis/redis-server.log
```

### Slow Log Analysis

Redis maintains an in-memory slow query log accessible via commands.

**Access slow log**:

```bash
redis-cli

# View all slow log entries
SLOWLOG GET 100

# View recent slow commands
SLOWLOG GET 10

# Get slow log length
SLOWLOG LEN

# Reset slow log
SLOWLOG RESET
```

**Parse slow log output**:

```bash
# Extract slow commands with timestamps
redis-cli SLOWLOG GET 100 | grep -A 5 "1)" | \
  awk '/unix time/ {time=$0} /command/ {print time, $0}'

# Identify specific command patterns
redis-cli SLOWLOG GET 100 | grep -i "KEYS\|FLUSHALL"
```

**Export slow log for analysis**:

```bash
# Simple extraction
redis-cli SLOWLOG GET 1000 > /tmp/redis-slowlog.txt

# Parse to CSV format (requires custom script)
redis-cli --csv SLOWLOG GET 100 | \
  awk -F',' '{print $2","$3","$4}' > /tmp/slowlog.csv
```

### Monitor Command Execution

Redis MONITOR command streams all commands in real-time (**significant performance impact**).

```bash
# Start monitoring (Ctrl+C to stop)
redis-cli MONITOR

# Filter specific patterns
redis-cli MONITOR | grep -i "AUTH\|CONFIG\|FLUSHALL"

# Save monitored commands
redis-cli MONITOR | tee /tmp/redis-monitor.log

# Monitor with timestamps
redis-cli MONITOR | while read line; do echo "$(date '+%Y-%m-%d %H:%M:%S') $line"; done
```

[Inference] Common attack patterns to watch for:

- `CONFIG SET dir /var/www/html` + `CONFIG SET dbfilename shell.php` (webshell upload attempts)
- `KEYS *` (database enumeration)
- `FLUSHALL` (data destruction)
- Repeated AUTH failures (credential brute force)
- `EVAL` commands with suspicious Lua code

### Client Connection Tracking

```bash
redis-cli

# List connected clients
CLIENT LIST

# Get client connection count
INFO clients

# Kill specific client
CLIENT KILL <ip:port>

# Monitor connection/disconnection events (in logs)
grep -E "Accepted|Client closed" /var/log/redis/redis-server.log
```

### INFO Command Analysis

The INFO command provides server statistics including command execution counts.

```bash
redis-cli INFO commandstats

# Extract most-used commands
redis-cli INFO commandstats | grep "calls" | sort -t'=' -k2 -n

# Check authentication attempts
redis-cli INFO stats | grep rejected_connections
```

### Audit Logging [Unverified]

**Note**: Redis does not have native comprehensive audit logging. [Inference] For CTF scenarios requiring command-level auditing:

1. **Use MONITOR with log rotation**:

```bash
# Run MONITOR with automatic log rotation
redis-cli MONITOR | rotatelogs /var/log/redis/audit-%Y%m%d.log 86400 &
```

2. **Redis Enterprise** [Unverified]: May include audit logging features not available in open-source Redis
    
3. **Proxy-based auditing** [Inference]: Tools like `redis-audit` or custom proxy scripts can intercept and log commands
    

### Security Event Detection

```bash
# Detect configuration manipulation attempts
grep "CONFIG SET" /var/log/redis/redis-server.log

# Find dangerous commands
redis-cli MONITOR | grep -i "FLUSHALL\|FLUSHDB\|CONFIG\|SCRIPT\|EVAL"

# Identify authenticated connections
grep "Client accepted" /var/log/redis/redis-server.log

# Track AUTH command usage
redis-cli MONITOR | grep "AUTH"
```

---

## Critical CTF Considerations

**Important subtopics for further study**:

- **Log forwarding/centralization**: Shipping database logs to SIEM systems (rsyslog, filebeat, fluentd)
- **Log injection attacks**: Exploiting log parsing vulnerabilities through crafted input
- **Anti-forensics**: Detecting log tampering, deletion, or gaps in timeline reconstruction
- **Performance impact**: Understanding logging overhead in production vs. CTF/lab environments
- **Log correlation**: Cross-referencing database logs with web server logs, system logs for complete attack chain reconstruction

---

## Query Logs

Query logs record all SQL statements executed against the database. These logs are invaluable for reconstructing attacker actions, identifying injection points, and discovering credentials or sensitive data patterns.

### MySQL/MariaDB Query Logs

**Enabling General Query Log:**

```bash
# Temporary enable (session-specific)
mysql> SET GLOBAL general_log = 'ON';
mysql> SET GLOBAL general_log_file = '/var/log/mysql/query.log';

# Persistent configuration in /etc/mysql/mysql.conf.d/mysqld.cnf
[mysqld]
general_log = 1
general_log_file = /var/log/mysql/query.log
```

**Analysis Commands:**

```bash
# Real-time monitoring
tail -f /var/log/mysql/query.log

# Search for SQL injection patterns
grep -iE "(union|select|concat|load_file|into outfile|information_schema)" /var/log/mysql/query.log

# Extract authentication attempts
grep "Connect" /var/log/mysql/query.log | awk '{print $3, $4, $5}'

# Find queries from specific user
grep "user@host" /var/log/mysql/query.log | grep -v "root"

# Identify data exfiltration (large result sets)
grep -E "SELECT.*FROM.*WHERE" /var/log/mysql/query.log | wc -l
```

**Log Format Example:**

```
2025-10-28T10:15:23.123456Z    42 Query    SELECT * FROM users WHERE username='admin'
2025-10-28T10:15:24.987654Z    42 Query    SELECT password FROM users WHERE id=1
```

### PostgreSQL Query Logs

**Configuration in postgresql.conf:**

```bash
log_statement = 'all'              # none, ddl, mod, all
log_directory = 'pg_log'
log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'
log_duration = on
log_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '
```

**Analysis Commands:**

```bash
# Standard location
cd /var/lib/postgresql/[VERSION]/main/pg_log/

# Parse connection attempts
grep "connection authorized" postgresql-*.log

# Extract failed authentication
grep "FATAL.*password authentication failed" postgresql-*.log

# Identify privilege escalation attempts
grep -iE "(alter user|grant|create user)" postgresql-*.log

# Find queries with errors (potential injection points)
grep "ERROR" postgresql-*.log | grep -v "relation.*does not exist"
```

### MSSQL Query Logs

**Enabling via SQL Profiler (GUI) or Extended Events:**

```sql
-- Create trace for query logging
DECLARE @traceid INT;
EXEC sp_trace_create @traceid OUTPUT, 0, N'C:\temp\sqltrace';
EXEC sp_trace_setevent @traceid, 10, 1, 1;  -- RPC:Completed
EXEC sp_trace_setevent @traceid, 12, 1, 1;  -- SQL:BatchCompleted
EXEC sp_trace_setstatus @traceid, 1;

-- Query existing traces
SELECT * FROM sys.traces;
```

**Windows Log Analysis:**

```powershell
# Application Event Log contains SQL errors
Get-EventLog -LogName Application -Source MSSQL* | Select-Object TimeGenerated,Message

# Parse SQL Server error log
Get-Content "C:\Program Files\Microsoft SQL Server\MSSQL15.MSSQLSERVER\MSSQL\Log\ERRORLOG" | Select-String "Login"
```

### SQLite Query Logs

[Inference] SQLite does not have native query logging; instrumentation must be implemented at the application layer or via wrapper tools.

```bash
# Enable trace callback in code (example)
sqlite3_trace_v2(db, SQLITE_TRACE_STMT, trace_callback, NULL);

# Analyze .sqlite-wal (Write-Ahead Log) files
strings database.db-wal | grep -i "INSERT\|UPDATE\|DELETE"

# Examine journal files
hexdump -C database.db-journal | less
```

## Slow Query Logs

Slow query logs identify performance bottlenecks and potential Denial of Service (DoS) vectors. In CTF contexts, these logs may reveal resource-intensive injection payloads or timing-based attack patterns.

### MySQL/MariaDB Slow Query Logs

**Configuration:**

```bash
# In /etc/mysql/mysql.conf.d/mysqld.cnf
[mysqld]
slow_query_log = 1
slow_query_log_file = /var/log/mysql/slow-query.log
long_query_time = 2                    # Threshold in seconds
log_queries_not_using_indexes = 1
```

**Analysis Tools:**

```bash
# mysqldumpslow - summarize slow query logs
mysqldumpslow -s t -t 10 /var/log/mysql/slow-query.log
# -s t: sort by query time
# -t 10: top 10 queries

# pt-query-digest (Percona Toolkit)
pt-query-digest /var/log/mysql/slow-query.log > report.txt

# Manual analysis
grep "Query_time:" /var/log/mysql/slow-query.log | awk '{print $3}' | sort -n | tail -n 20
```

**Log Format:**

```
# Time: 2025-10-28T10:23:45.123456Z
# User@Host: webapp[webapp] @ localhost []
# Query_time: 15.234567  Lock_time: 0.000123 Rows_sent: 1000000  Rows_examined: 5000000
SET timestamp=1730109825;
SELECT * FROM orders WHERE user_id IN (SELECT id FROM users WHERE status='active');
```

### PostgreSQL Slow Query Logs

**Configuration:**

```bash
log_min_duration_statement = 1000      # Log queries taking >1000ms
auto_explain.log_min_duration = 1000   # Auto-explain slow queries
auto_explain.log_analyze = true
```

**Analysis:**

```bash
# Extract slow queries with execution times
grep "duration:" /var/lib/postgresql/*/main/pg_log/*.log | \
awk -F'duration: ' '{print $2}' | \
sort -n | tail -n 50

# Find queries with table scans (potential index issues)
grep -A5 "Seq Scan" /var/lib/postgresql/*/main/pg_log/*.log
```

### MSSQL Query Store

**Enable Query Store (SQL Server 2016+):**

```sql
ALTER DATABASE [YourDB] SET QUERY_STORE = ON;
ALTER DATABASE [YourDB] SET QUERY_STORE (OPERATION_MODE = READ_WRITE);

-- Query slow statements
SELECT 
    q.query_id,
    qt.query_sql_text,
    rs.avg_duration / 1000 AS avg_duration_ms,
    rs.max_duration / 1000 AS max_duration_ms
FROM sys.query_store_query q
JOIN sys.query_store_query_text qt ON q.query_text_id = qt.query_text_id
JOIN sys.query_store_plan p ON q.query_id = p.query_id
JOIN sys.query_store_runtime_stats rs ON p.plan_id = rs.plan_id
WHERE rs.avg_duration > 5000000  -- 5 seconds in microseconds
ORDER BY rs.avg_duration DESC;
```

## Error Logs

Error logs contain authentication failures, constraint violations, syntax errors from injection attempts, and system-level database issues. These are high-value targets for identifying attack vectors.

### MySQL/MariaDB Error Logs

**Default Location:**

```bash
/var/log/mysql/error.log
/var/lib/mysql/[hostname].err
```

**Critical Patterns to Investigate:**

```bash
# Access denied (brute force indicators)
grep "Access denied for user" /var/log/mysql/error.log | \
awk -F"'" '{print $2}' | sort | uniq -c | sort -rn

# Plugin loading errors (potential backdoor attempts)
grep -i "plugin" /var/log/mysql/error.log

# Table crashes (exploitation aftermath)
grep -E "crashed|marked as crashed" /var/log/mysql/error.log

# Out of memory (DoS indicators)
grep -i "out of memory" /var/log/mysql/error.log

# Syntax errors (injection attempts)
grep "You have an error in your SQL syntax" /var/log/mysql/error.log
```

**Extract Failed Login IP Addresses:**

```bash
grep "Access denied" /var/log/mysql/error.log | \
grep -oP "\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}" | \
sort | uniq -c | sort -rn | head -n 20
```

### PostgreSQL Error Logs

**Critical Error Patterns:**

```bash
# Authentication failures
grep "FATAL.*authentication failed" /var/lib/postgresql/*/main/pg_log/*.log

# Permission denied (privilege escalation attempts)
grep "ERROR.*permission denied" /var/lib/postgresql/*/main/pg_log/*.log

# Syntax errors from injection
grep "ERROR.*syntax error at or near" /var/lib/postgresql/*/main/pg_log/*.log

# Connection limit exhaustion (DoS)
grep "FATAL.*too many connections" /var/lib/postgresql/*/main/pg_log/*.log

# File access attempts (potential LFI/RFI)
grep -E "ERROR.*(pg_read_file|COPY.*FROM PROGRAM)" /var/lib/postgresql/*/main/pg_log/*.log
```

**Correlation Script Example:**

```bash
#!/bin/bash
# Correlate errors with specific IPs
for ip in $(grep "FATAL" /var/lib/postgresql/*/main/pg_log/*.log | \
grep -oP "\d{1,3}(\.\d{1,3}){3}" | sort -u); do
    echo "=== IP: $ip ==="
    grep "$ip" /var/lib/postgresql/*/main/pg_log/*.log | \
    grep -E "FATAL|ERROR" | tail -n 5
done
```

### MSSQL Error Logs

**Default Location:**

```
C:\Program Files\Microsoft SQL Server\MSSQL15.MSSQLSERVER\MSSQL\Log\ERRORLOG
```

**PowerShell Analysis:**

```powershell
# Read current error log
Get-Content "C:\...\ERRORLOG" | Select-String "Error:|Failed"

# Login failures
Get-Content "C:\...\ERRORLOG" | Select-String "Login failed"

# Permission errors
Get-Content "C:\...\ERRORLOG" | Select-String "permission"

# Parse multiple archived logs
Get-ChildItem "C:\...\ERRORLOG*" | ForEach-Object {
    Write-Host "`n=== $($_.Name) ==="
    Get-Content $_.FullName | Select-String "Error:" | Select-Object -First 10
}
```

**T-SQL Log Reader:**

```sql
-- Read error log via T-SQL
EXEC xp_readerrorlog 0, 1, N'error';  -- Current log
EXEC xp_readerrorlog 1, 1, N'failed'; -- Previous log

-- Search for specific pattern
EXEC xp_readerrorlog 0, 1, N'Login failed', N'sa';
```

### Oracle Error Logs

**Alert Log Location:**

```bash
$ORACLE_BASE/diag/rdbms/$ORACLE_SID/$ORACLE_SID/trace/alert_$ORACLE_SID.log
```

**Analysis Commands:**

```bash
# ORA errors
grep "ORA-" alert_*.log | awk '{print $3}' | sort | uniq -c | sort -rn

# Authentication failures (ORA-01017)
grep "ORA-01017" alert_*.log

# Audit trail entries
grep "AUDIT" alert_*.log
```

## CTF-Specific Analysis Workflows

### Identifying SQL Injection Evidence

**Combined Log Analysis:**

```bash
# Search for common SQLi signatures across all logs
for log in /var/log/mysql/*.log /var/lib/postgresql/*/main/pg_log/*.log; do
    echo "=== Analyzing $log ==="
    grep -iE "(union select|' or '1'='1|' or 1=1|concat\(|load_file\(|into outfile)" "$log"
done
```

### Timeline Reconstruction

```bash
# Merge and sort logs by timestamp
(grep "^[0-9]\{4\}-[0-9]\{2\}-[0-9]\{2\}" /var/log/mysql/query.log | \
 awk '{print $1" "$2" [MySQL] "$0}';
 grep "^[0-9]\{4\}-[0-9]\{2\}-[0-9]\{2\}" /var/lib/postgresql/*/main/pg_log/*.log | \
 awk '{print $1" "$2" [PostgreSQL] "$0}') | \
sort -k1,2 > timeline.txt
```

### Automated Flag Extraction

```bash
# Pattern matching for CTF flags
grep -rEoh "[A-Za-z0-9]{32}|flag\{[^}]+\}|CTF\{[^}]+\}" \
/var/log/mysql/ /var/lib/postgresql/*/main/pg_log/
```

## Important Subtopics for Further Study

- **Binary log analysis** (MySQL replication logs containing all database modifications)
- **Audit plugins** (MariaDB audit plugin, PostgreSQL pgAudit for compliance-grade logging)
- **Log injection attacks** (exploiting unsanitized log entries for SIEM/log analyzer compromise)
- **Database forensics tools** (Sleuth Kit Autopsy modules, commercial database forensic suites)

---

# Windows Event Logs

Windows Event Logs are critical artifacts for forensic analysis, incident response, and CTF challenges involving Windows systems. They provide detailed records of system activities, security events, and application behaviors stored in `.evtx` format (Windows Vista+) or `.evt` format (legacy systems).

## Log Storage Locations

**Default Path (Windows Vista+):**

```
C:\Windows\System32\winevt\Logs\
```

**Legacy Path (Windows XP/2003):**

```
C:\Windows\System32\config\
```

**Key Files:**

- `Security.evtx` - Security and audit events
- `System.evtx` - System component events
- `Application.evtx` - Application-level events

## Security Event Logs

Security logs track authentication, authorization, and audit policy events. These are primary sources for detecting intrusions, privilege escalations, and lateral movement.

### Critical Event IDs

**Authentication Events:**

- **4624** - Successful logon
    - Type 2: Interactive (console)
    - Type 3: Network (SMB, RDP)
    - Type 7: Unlock
    - Type 10: RemoteInteractive (RDP)
- **4625** - Failed logon attempt (brute force detection)
- **4634/4647** - Logoff events
- **4648** - Logon using explicit credentials (runas, PsExec)

**Account Management:**

- **4720** - User account created
- **4722** - User account enabled
- **4724** - Password reset attempt
- **4728/4732** - User added to privileged group (Administrators, Domain Admins)
- **4738** - User account changed
- **4740** - Account locked out

**Privilege Escalation:**

- **4672** - Special privileges assigned to new logon (Admin logon)
- **4673** - Sensitive privilege use
- **4674** - Operation attempted on privileged object

**Object Access:**

- **4663** - Attempt to access an object (requires audit policy)
- **4656** - Handle to object requested
- **5140** - Network share accessed
- **5145** - Shared object access check

**Process Tracking:**

- **4688** - Process creation (requires audit policy enabled)
- **4689** - Process termination

**Policy Changes:**

- **4719** - System audit policy changed
- **4904** - Security event source attempt

### Analysis Commands (Kali Linux)

**Using python-evtx:**

```bash
# Install
pip3 install python-evtx

# Parse EVTX to XML
evtx_dump.py Security.evtx > security.xml

# Extract specific Event ID
evtx_dump.py Security.evtx | grep -A 20 "EventID>4624"
```

**Using evtxexport (libevtx):**

```bash
# Install
apt-get install libevtx-utils

# Export to text
evtxexport -f text Security.evtx

# Export to XML
evtxexport -f xml Security.evtx > security.xml
```

**Chainsaw (Rust-based EVTX parser):**

```bash
# Install
wget https://github.com/WithSecureLabs/chainsaw/releases/latest/download/chainsaw_x86_64-unknown-linux-gnu.tar.gz
tar -xzf chainsaw_x86_64-unknown-linux-gnu.tar.gz

# Hunt for threats using Sigma rules
./chainsaw hunt Security.evtx -s sigma/ --mapping mappings/sigma-event-logs-all.yml

# Search for specific Event IDs
./chainsaw search Security.evtx -e 4624 -e 4625

# Timeline generation
./chainsaw dump Security.evtx --json | jq -r '[.timestamp, .event_id, .event_data] | @csv'
```

**Parsing with jq (JSON output):**

```bash
# Convert EVTX to JSON then filter
evtx_dump.py --json Security.evtx | jq '.[] | select(.event_id == 4624)'

# Extract failed logons with usernames
evtx_dump.py --json Security.evtx | jq '.[] | select(.event_id == 4625) | {time: .timestamp, user: .event_data.TargetUserName, ip: .event_data.IpAddress}'
```

### Common Attack Patterns in Security Logs

**Brute Force Detection:**

```bash
# Multiple 4625 events from same source
evtx_dump.py Security.evtx | grep "EventID>4625" | grep -o "IpAddress>[^<]*" | sort | uniq -c | sort -rn
```

**Lateral Movement (Pass-the-Hash):**

- Look for Event ID 4624 Type 3 with NTLM authentication
- Suspicious account names (service accounts used interactively)

**Privilege Escalation Timeline:**

```bash
# Track specific user's privilege changes
evtx_dump.py Security.evtx | grep -E "(4672|4728|4732)" | grep "target_username"
```

## System Event Logs

System logs record events from Windows components, drivers, and system services. Critical for identifying system instability, malware installation, and persistence mechanisms.

### Critical Event IDs

**Service Events:**

- **7034** - Service crashed unexpectedly
- **7035** - Service start/stop control sent
- **7036** - Service started/stopped
- **7040** - Service start type changed (manual to auto = persistence)
- **7045** - New service installed (malware persistence)

**System Boot/Shutdown:**

- **1074** - System shutdown initiated (who/why)
- **6005** - Event Log service started (boot time)
- **6006** - Event Log service stopped (shutdown time)
- **6008** - Unexpected shutdown (crash/power loss)

**Time Changes:**

- **1** - System time changed (anti-forensics technique)

**Kernel/Driver Events:**

- **219** - Kernel power event (unexpected reboots)

**Windows Update:**

- **19** - Installation successful
- **20** - Installation failure

### Analysis Techniques

**Service Installation Detection:**

```bash
# Find newly installed services
evtx_dump.py System.evtx | grep "EventID>7045" | grep -A 5 "ServiceName"

# With Chainsaw
./chainsaw search System.evtx -e 7045 --json | jq -r '.[] | {time: .timestamp, service: .event_data.ServiceName, path: .event_data.ImagePath}'
```

**Boot Timeline Reconstruction:**

```bash
# Extract boot events
evtx_dump.py System.evtx | grep -E "(EventID>6005|EventID>6006|EventID>6008)"

# Calculate uptime periods
evtx_dump.py System.evtx --json | jq '.[] | select(.event_id == 6005 or .event_id == 6006) | {time: .timestamp, event: .event_id}'
```

**Persistence Mechanism Detection:**

```bash
# Services changed to auto-start
evtx_dump.py System.evtx | grep "EventID>7040" | grep -B 2 -A 2 "auto start"
```

**Unexpected Shutdown Analysis:**

```bash
# Find crashes vs. clean shutdowns
evtx_dump.py System.evtx | grep -E "(EventID>6008|EventID>1074)" | grep -o "TimeCreated SystemTime='[^']*'" | sort
```

## Application Event Logs

Application logs capture events from installed software and Windows features. Valuable for tracking application-specific exploits, malware behavior, and data exfiltration.

### Critical Event IDs

**Windows Defender (WinDefend):**

- **1116** - Malware detected
- **1117** - Malware action taken
- **1118** - Malware detected but action incomplete
- **5001** - Real-time protection disabled

**PowerShell (PowerShell-Operational):**

- Located: `Microsoft-Windows-PowerShell/Operational.evtx`
- **4103** - Module logging (command execution)
- **4104** - Script block logging (full script content)

**Windows Error Reporting:**

- **1000** - Application crash
- **1001** - Application hang

**MSI Installer:**

- **1033** - Application installation
- **1034** - Application removal
- **11707** - Installation success
- **11708** - Installation failure

**Task Scheduler (Microsoft-Windows-TaskScheduler):**

- **106** - Task registered
- **140** - Task updated
- **141** - Task deleted
- **200** - Task executed
- **201** - Task completed

### PowerShell Log Analysis

[Inference: PowerShell logs may contain obfuscated or encoded commands that require decoding]

**Extract PowerShell commands:**

```bash
# Parse PowerShell Operational log
evtx_dump.py Microsoft-Windows-PowerShell%4Operational.evtx | grep "EventID>4104" -A 30

# Extract script blocks
evtx_dump.py Microsoft-Windows-PowerShell%4Operational.evtx --json | jq '.[] | select(.event_id == 4104) | .event_data.ScriptBlockText'
```

**Decode Base64 PowerShell:**

```bash
# Extract and decode encoded commands
echo "encoded_string_here" | base64 -d

# Common PowerShell obfuscation indicators
grep -iE "(invoke-expression|iex|downloadstring|encodedcommand)" powershell_extracted.txt
```

### Application-Specific Analysis

**Malware Detection Timeline:**

```bash
# Windows Defender detections
evtx_dump.py Application.evtx | grep -E "(EventID>1116|EventID>1117)" | grep -o "TimeCreated SystemTime='[^']*'"
```

**Installed Software Tracking:**

```bash
# Track MSI installations
evtx_dump.py Application.evtx | grep "EventID>11707" | grep -o "Data Name='ProductName'>[^<]*"
```

**Scheduled Task Abuse:**

```bash
# Parse Task Scheduler log
evtx_dump.py Microsoft-Windows-TaskScheduler%4Operational.evtx | grep "EventID>106" -A 10 | grep "TaskName"
```

**Application Crashes (Exploitation Indicators):**

```bash
# Find crash patterns
evtx_dump.py Application.evtx | grep "EventID>1000" | grep -o "Data Name='AppName'>[^<]*" | sort | uniq -c | sort -rn
```

## Cross-Log Correlation Techniques

**Timeline Creation:**

```bash
# Merge multiple logs by timestamp
for log in Security System Application; do
    evtx_dump.py ${log}.evtx --json >> combined.json
done
jq -s 'sort_by(.timestamp)' combined.json > timeline.json
```

**User Activity Reconstruction:**

```bash
# Correlate logon (Security 4624) with service starts (System 7036)
# Filter Security and System logs for specific timeframe
jq '.[] | select(.timestamp >= "2024-01-01" and .timestamp <= "2024-01-02")' timeline.json
```

## Tools Summary

|Tool|Format|Speed|Features|
|---|---|---|---|
|python-evtx|XML/JSON|Moderate|Python library, scriptable|
|libevtx-utils|Text/XML|Fast|C-based, lightweight|
|Chainsaw|JSON|Very Fast|Sigma rules, threat hunting|
|EvtxECmd (Windows)|CSV|Fast|Timeline generation|

## CTF-Specific Notes

**Common CTF Scenarios:**

1. **Brute Force Detection** - Count Event ID 4625, identify source IPs
2. **Malicious Service** - Event ID 7045 with suspicious ImagePath
3. **Privilege Escalation** - Event ID 4728/4732 + 4672 correlation
4. **Data Exfiltration** - Event ID 5140/5145 large file access patterns
5. **PowerShell Attacks** - Decode Event ID 4104 script blocks

**Flag Locations:**

- Embedded in EventData fields (usernames, file paths, command lines)
- Base64 encoded in PowerShell script blocks
- Hex encoded in binary data fields
- Timestamps forming coordinates or codes

**Important Subtopics:**

- **Sysmon Logs** (Microsoft-Windows-Sysmon/Operational) - Enhanced process, network, and file monitoring
- **Windows Filtering Platform (WFP)** - Firewall and network connection logs
- **RDP Connection Logs** (TerminalServices-RemoteConnectionManager) - Remote access forensics

---

## Event ID Interpretation

### Core Logging Architecture

Windows Event Logs are stored in `.evtx` format and organized into channels. The primary log locations are:

**Critical Security Channels:**

- `Security.evtx` - Authentication, privilege use, object access
- `System.evtx` - Service/driver events, system errors
- `Application.evtx` - Application-specific events
- `Microsoft-Windows-PowerShell/Operational.evtx` - PowerShell execution
- `Microsoft-Windows-Sysmon/Operational.evtx` - Sysmon telemetry (if installed)

**Log File Locations:**

```
C:\Windows\System32\winevt\Logs\*.evtx
```

### Accessing Event Logs on Kali Linux

**Using evtx_dump (Rust-based parser):**

```bash
# Install
cargo install evtx

# Parse to JSON
evtx_dump -o json Security.evtx > security.json

# Parse to XML
evtx_dump -o xml Security.evtx > security.xml

# Filter by Event ID
evtx_dump Security.evtx | grep -A 20 "EventID: 4624"
```

**Using python-evtx:**

```bash
# Install
pip3 install python-evtx

# Parse and extract
evtx_dump.py Security.evtx > parsed_security.xml

# Convert to CSV for analysis
evtx_dump.py --csv Security.evtx > security.csv
```

**Using log2timeline/plaso:**

```bash
# Create timeline
log2timeline.py timeline.plaso Security.evtx

# Export to CSV
psort.py -o l2tcsv -w timeline.csv timeline.plaso
```

### Critical Security Event IDs

**Authentication Events:**

**Event ID 4624 - Successful Logon**

```
LogonType values:
2  = Interactive (console)
3  = Network (SMB, RPC)
4  = Batch (scheduled task)
5  = Service
7  = Unlock
9  = NewCredentials (runas)
10 = RemoteInteractive (RDP)
11 = CachedInteractive (cached creds, no DC contact)
```

**Detection Focus:**

- LogonType 3 with abnormal source IPs (lateral movement)
- LogonType 10 from unusual sources (RDP brute force)
- LogonType 9 with SYSTEM account (privilege escalation)
- Account names ending in `$` for LogonType 3 (Pass-the-Hash indicators)

**Parsing Example:**

```bash
# Extract all RDP logons
evtx_dump Security.evtx | grep -B 5 -A 15 '"EventID": 4624' | grep -A 15 '"LogonType": 10'

# Find external RDP attempts
jq 'select(.Event.System.EventID == 4624 and .Event.EventData.LogonType == "10") | {Time: .Event.System.TimeCreated.SystemTime, User: .Event.EventData.TargetUserName, IP: .Event.EventData.IpAddress}' security.json
```

**Event ID 4625 - Failed Logon**

```
SubStatus codes:
0xC0000064 = User does not exist
0xC000006A = Correct user, wrong password
0xC000006D = Bad username or password
0xC000006E = Account restriction
0xC0000070 = Violation of logon hours
0xC0000071 = Password expired
0xC0000072 = Account disabled
0xC0000193 = Account expired
0xC0000224 = Password must be changed
0xC0000234 = Account locked out
```

**Brute Force Detection:**

```bash
# Count failed logons per user
jq -r 'select(.Event.System.EventID == 4625) | .Event.EventData.TargetUserName' security.json | sort | uniq -c | sort -rn

# Failed logons with source IPs
jq 'select(.Event.System.EventID == 4625) | {Time: .Event.System.TimeCreated.SystemTime, User: .Event.EventData.TargetUserName, IP: .Event.EventData.IpAddress, Status: .Event.EventData.SubStatus}' security.json
```

**Event ID 4672 - Special Privileges Assigned**

- Logged when sensitive privileges (SeDebugPrivilege, SeBackupPrivilege, etc.) are assigned
- Always follows a 4624 event
- Filter out routine admin logons to find anomalies

**Event ID 4688 - Process Creation**

```
Critical Fields:
- NewProcessName: Executable path
- CommandLine: Full command (requires audit policy)
- ParentProcessName: Spawning process
- TokenElevationType: Privilege level
```

**Enable CommandLine Logging (Windows):**

```
Requires: Computer Configuration > Policies > Administrative Templates > System > Audit Process Creation > Include command line in process creation events = Enabled
```

**Parsing Process Trees:**

```bash
# Extract process creation with command lines
jq 'select(.Event.System.EventID == 4688) | {Time: .Event.System.TimeCreated.SystemTime, Process: .Event.EventData.NewProcessName, CommandLine: .Event.EventData.CommandLine, Parent: .Event.EventData.ParentProcessName}' security.json

# Find PowerShell executions
jq 'select(.Event.System.EventID == 4688 and (.Event.EventData.NewProcessName | contains("powershell")))' security.json
```

**Event ID 4720 - User Account Created** **Event ID 4726 - User Account Deleted** **Event ID 4732 - Member Added to Security-Enabled Local Group** **Event ID 4728 - Member Added to Security-Enabled Global Group**

**Privilege Escalation Indicators:**

```bash
# Find additions to Administrators group
jq 'select(.Event.System.EventID == 4732 and (.Event.EventData.TargetUserName | contains("Administrators"))) | {Time: .Event.System.TimeCreated.SystemTime, AddedUser: .Event.EventData.MemberName, By: .Event.EventData.SubjectUserName}' security.json
```

**Event ID 4698 - Scheduled Task Created** **Event ID 4702 - Scheduled Task Updated**

**Persistence Detection:**

```bash
# Extract scheduled task creation
jq 'select(.Event.System.EventID == 4698) | {Time: .Event.System.TimeCreated.SystemTime, TaskName: .Event.EventData.TaskName, TaskContent: .Event.EventData.TaskContent}' security.json
```

**Event ID 5140 - Network Share Accessed** **Event ID 5145 - Network Share Object Accessed**

**Lateral Movement Detection:**

```bash
# Find unusual share access
jq 'select(.Event.System.EventID == 5140) | {Time: .Event.System.TimeCreated.SystemTime, Share: .Event.EventData.ShareName, User: .Event.EventData.SubjectUserName, IP: .Event.EventData.IpAddress}' security.json
```

### System Log Event IDs

**Event ID 7045 - Service Installed**

```bash
# Extract service installations
evtx_dump System.evtx | grep -B 5 -A 15 '"EventID": 7045'

# Identify suspicious service names/paths
jq 'select(.Event.System.EventID == 7045) | {Time: .Event.System.TimeCreated.SystemTime, ServiceName: .Event.EventData.ServiceName, ImagePath: .Event.EventData.ImagePath, ServiceType: .Event.EventData.ServiceType}' system.json
```

**Event ID 1074 - System Shutdown/Restart** **Event ID 6005 - Event Log Service Started (boot)** **Event ID 6006 - Event Log Service Stopped (shutdown)** **Event ID 6013 - System Uptime**

### Application Log Event IDs

**Event ID 1000 - Application Crash** **Event ID 1001 - Windows Error Reporting** **Event ID 1002 - Application Hang**

**Exploit Detection:**

```bash
# Find frequent crashes (possible exploitation)
jq 'select(.Event.System.EventID == 1000) | {Time: .Event.System.TimeCreated.SystemTime, App: .Event.EventData.AppName, Exception: .Event.EventData.ExceptionCode}' application.json | head -20
```

## PowerShell Logging

### PowerShell Log Sources

**1. PowerShell Operational Log**

```
Location: Microsoft-Windows-PowerShell/Operational.evtx
Path: C:\Windows\System32\winevt\Logs\Microsoft-Windows-PowerShell%4Operational.evtx
```

**Key Event IDs:**

**Event ID 4103 - Module Logging**

- Records pipeline execution details
- Captures commands sent to modules
- Contains de-obfuscated code

**Enable Module Logging (Windows):**

```
Computer Configuration > Policies > Administrative Templates > Windows Components > Windows PowerShell > Turn on Module Logging = Enabled
Module Names: * (log all modules)
```

**Event ID 4104 - Script Block Logging**

- Captures entire script content
- Logs de-obfuscated/decoded commands
- Includes warning level for suspicious patterns

**Enable Script Block Logging:**

```
Computer Configuration > Policies > Administrative Templates > Windows Components > Windows PowerShell > Turn on PowerShell Script Block Logging = Enabled
Log script block invocation start/stop events = Enabled (optional, creates 4105/4106)
```

**Suspicious Pattern Detection:** [Inference] Windows assigns warning level based on patterns like `Invoke-Expression`, `iex`, `-EncodedCommand`, but specific detection logic is not publicly documented:

```bash
# Extract Script Block logs
jq 'select(.Event.System.EventID == 4104) | {Time: .Event.System.TimeCreated.SystemTime, ScriptBlock: .Event.EventData.ScriptBlockText, Path: .Event.EventData.Path, MessageNumber: .Event.EventData.MessageNumber, MessageTotal: .Event.EventData.MessageTotal}' powershell-operational.json
```

**Handling Multi-Part Script Blocks:**

```bash
# Large scripts are split across multiple events
# MessageNumber and MessageTotal track fragments
# Filter and reconstruct:
jq -r 'select(.Event.System.EventID == 4104) | "\(.Event.EventData.MessageNumber)/\(.Event.EventData.MessageTotal): \(.Event.EventData.ScriptBlockText)"' powershell-operational.json | grep "^1/" -A 100
```

**Event ID 4105/4106 - Script Block Invocation Start/Stop**

- Tracks execution start and completion
- Links to 4104 events via ActivityId

**2. PowerShell Transcript Logs**

**Enable Transcription:**

```
Computer Configuration > Policies > Administrative Templates > Windows Components > Windows PowerShell > Turn on PowerShell Transcription = Enabled
Transcript output directory: C:\Transcripts (or preferred path)
Include invocation headers = Enabled
```

**Default Transcript Locations:**

```
Per-User: C:\Users\<username>\Documents\PowerShell_transcript.*
System: C:\Transcripts\PowerShell_transcript.<date>.<random>.txt
```

**Parsing Transcripts:**

```bash
# Search for commands
grep -i "Invoke-" PowerShell_transcript.*.txt

# Extract download commands
grep -iE "(IWR|Invoke-WebRequest|wget|curl|DownloadFile)" *.txt

# Find encoded commands
grep -i "encodedcommand\|-enc\|-e " *.txt
```

**3. Windows PowerShell Log (Legacy)**

```
Location: Windows PowerShell.evtx
Path: C:\Windows\System32\winevt\Logs\Windows PowerShell.evtx
```

**Event ID 400 - Engine Lifecycle** **Event ID 403 - Engine health** **Event ID 600 - Provider Lifecycle**

### PowerShell Attack Pattern Detection

**Obfuscated Commands:**

```bash
# Base64-encoded commands
grep -i "encodedcommand\|-enc\|-e " *.json
jq 'select(.Event.System.EventID == 4104 and (.Event.EventData.ScriptBlockText | contains("-enc")))' powershell-operational.json

# Decode Base64
echo "<base64_string>" | base64 -d

# Character-level obfuscation
grep -iE "(\^|`|'|\+)" powershell-operational.json
```

**Download Cradles:**

```bash
# Common download patterns
jq 'select(.Event.System.EventID == 4104 and (.Event.EventData.ScriptBlockText | test("(IWR|Invoke-WebRequest|Net\\.WebClient|DownloadString|DownloadFile|wget|curl)"; "i")))' powershell-operational.json

# Fileless execution
jq 'select(.Event.System.EventID == 4104 and (.Event.EventData.ScriptBlockText | test("IEX.*\\(.*\\)|Invoke-Expression"; "i")))' powershell-operational.json
```

**Invoke-Mimikatz / Credential Dumping:**

```bash
# Mimikatz indicators
jq 'select(.Event.System.EventID == 4104 and (.Event.EventData.ScriptBlockText | test("(mimikatz|DumpCreds|sekurlsa|lsadump)"; "i")))' powershell-operational.json

# LSASS access
jq 'select(.Event.System.EventID == 4104 and (.Event.EventData.ScriptBlockText | test("lsass|process.*memory"; "i")))' powershell-operational.json
```

**PowerShell Remoting:**

```bash
# Remote execution
jq 'select(.Event.System.EventID == 4104 and (.Event.EventData.ScriptBlockText | test("Invoke-Command.*-ComputerName|Enter-PSSession|New-PSSession"; "i")))' powershell-operational.json
```

**Empire/Cobalt Strike Patterns:**

```bash
# Common C2 indicators
jq 'select(.Event.System.EventID == 4104 and (.Event.EventData.ScriptBlockText | test("(Start-Job.*-ScriptBlock|System\\.Net\\.Sockets|IO\\.Compression|Reflection\\.Assembly.*Load)"; "i")))' powershell-operational.json
```

**AMSI Bypass Attempts:**

```bash
# Anti-Malware Scan Interface evasion
jq 'select(.Event.System.EventID == 4104 and (.Event.EventData.ScriptBlockText | test("(amsi|AmsiUtils|amsiInitFailed|amsiContext)"; "i")))' powershell-operational.json
```

## Windows Defender Logs

### Defender Log Locations

**Event Viewer Logs:**

```
Microsoft-Windows-Windows Defender/Operational.evtx
Path: C:\Windows\System32\winevt\Logs\Microsoft-Windows-Windows Defender%4Operational.evtx
```

**MPLog Files (Diagnostic):**

```
Path: C:\ProgramData\Microsoft\Windows Defender\Support\
Files: MPLog-*.log
```

**Detection History:**

```
Path: C:\ProgramData\Microsoft\Windows Defender\Scans\History\Service\
Quarantine: C:\ProgramData\Microsoft\Windows Defender\Quarantine\
```

### Critical Defender Event IDs

**Event ID 1116 - Malware Detected**

```
Fields:
- Threat Name
- Severity (Low, Medium, High, Severe)
- Category (Trojan, Backdoor, etc.)
- Path
- Detection User
- Process Name
```

**Parsing Detections:**

```bash
# Extract all malware detections
jq 'select(.Event.System.EventID == 1116) | {Time: .Event.System.TimeCreated.SystemTime, Threat: .Event.EventData.ThreatName, Severity: .Event.EventData.SeverityName, Path: .Event.EventData.Path, Process: .Event.EventData.ProcessName}' defender-operational.json

# Find specific threat categories
jq 'select(.Event.System.EventID == 1116 and (.Event.EventData.CategoryName | contains("Backdoor")))' defender-operational.json
```

**Event ID 1117 - Action Taken**

```
Actions:
- 2 = Quarantine
- 3 = Remove
- 6 = Allow
- 8 = User Allowed
- 9 = No Action
- 10 = Block
```

**Event ID 1118 - Action Failed**

- Indicates remediation failure
- Possible indicators of evasion or persistence

**Event ID 1119 - Critical Error**

**Event ID 5001 - Real-time Protection Disabled** **Event ID 5010 - Scanning Disabled** **Event ID 5012 - Tamper Protection**

**Defense Evasion Detection:**

```bash
# Find Defender disablement
jq 'select(.Event.System.EventID == 5001 or .Event.System.EventID == 5010 or .Event.System.EventID == 5012)' defender-operational.json

# Correlate with user activity
jq 'select(.Event.System.EventID == 5001) | {Time: .Event.System.TimeCreated.SystemTime, User: .Event.System.Security.UserID}' defender-operational.json
```

**Event ID 1006/1007 - Scan Started/Completed** **Event ID 1000/1001 - Scan Canceled/Failed**

**Event ID 2000-2050 - Update Events**

- 2000: Signature update started
- 2001: Signature update failed
- 2004: Engine update failed

### MPLog Analysis

**Parsing MPLog Files:**

```bash
# Extract from Kali
cat MPLog-*.log | grep "DETECTION"

# Find threat names
grep "Threat:" MPLog-*.log

# Extract file paths
grep "file:" MPLog-*.log | cut -d':' -f3-

# Timeline reconstruction
grep "ProcessImageName:" MPLog-*.log | awk '{print $1, $2, $NF}'
```

**MPLog Structure:**

```
Example entry:
2025-10-28T14:32:11.123Z DETECTION Threat:Trojan:Win32/Meterpreter.A ProcessImageName:C:\Windows\System32\rundll32.exe file:C:\Users\victim\AppData\Local\Temp\payload.dll
```

**Quarantine File Extraction:**

```bash
# Quarantine files are encrypted
# Location: C:\ProgramData\Microsoft\Windows Defender\Quarantine\ResourceData\

# [Unverified] Decryption requires Windows Defender tooling or reverse engineering; no standard Kali tool exists for automated decryption
```

### Cross-Correlation Analysis

**Timeline Event Correlation:**

```bash
# Combine Security + PowerShell + Defender logs
log2timeline.py --parsers "winevtx" combined.plaso Security.evtx System.evtx Microsoft-Windows-PowerShell%4Operational.evtx Microsoft-Windows-Windows\ Defender%4Operational.evtx

# Export and filter
psort.py -o l2tcsv -w full_timeline.csv combined.plaso

# Analyze in timeline explorer or grep
grep -i "mimikatz\|payload\|meterpreter" full_timeline.csv
```

**Process Execution + Defender Detection:**

```bash
# Find process (4688) followed by detection (1116)
# Requires scripting or SIEM-like correlation

# Example: Extract timestamps and compare
jq -r 'select(.Event.System.EventID == 4688) | "\(.Event.System.TimeCreated.SystemTime) PROCESS \(.Event.EventData.NewProcessName)"' security.json > proc.txt
jq -r 'select(.Event.System.EventID == 1116) | "\(.Event.System.TimeCreated.SystemTime) DETECT \(.Event.EventData.Path)"' defender-operational.json > detect.txt

# Manual timeline analysis
sort -m proc.txt detect.txt | less
```

**Important related topics:** For comprehensive log analysis, you should also study Sysmon event correlation (Event IDs 1-22), Registry/File System monitoring for persistence mechanisms, and Network log analysis (Zeek/Suricata) for lateral movement detection.

---

# Command-Line Tools

## grep, egrep, fgrep

These pattern-matching tools are essential for filtering log entries, extracting indicators of compromise (IOCs), and identifying attack patterns in CTF challenges.

### grep (Global Regular Expression Print)

**Basic Syntax:**

```bash
grep [OPTIONS] PATTERN [FILE...]
```

**Critical Options for CTF:**

```bash
-i          # Case-insensitive search
-v          # Invert match (show non-matching lines)
-r/-R       # Recursive directory search
-n          # Show line numbers
-c          # Count matching lines
-l          # List filenames with matches
-h          # Suppress filename prefix
-A NUM      # Show NUM lines after match
-B NUM      # Show NUM lines before match
-C NUM      # Show NUM lines of context (before and after)
-o          # Show only matching part of line
-E          # Extended regex (same as egrep)
-F          # Fixed strings (same as fgrep)
-w          # Match whole words only
-x          # Match whole lines only
-P          # Perl-compatible regex (PCRE)
```

**Common CTF Patterns:**

_Finding IP addresses:_

```bash
grep -E '\b([0-9]{1,3}\.){3}[0-9]{1,3}\b' access.log
grep -oE '\b([0-9]{1,3}\.){3}[0-9]{1,3}\b' access.log | sort -u
```

_Finding URLs with parameters (potential SQLi/XSS):_

```bash
grep -E '\?[^[:space:]]+' access.log
grep -iE '(union|select|script|alert|onerror)' access.log
```

_Filtering HTTP status codes:_

```bash
grep ' 200 ' access.log          # Successful requests
grep -E ' (4[0-9]{2}|5[0-9]{2}) ' access.log  # Errors
```

_Finding authentication failures:_

```bash
grep -i 'failed\|failure\|invalid' auth.log
grep -i 'authentication failure' /var/log/auth.log
```

_Extracting base64 encoded data:_

```bash
grep -oE '[A-Za-z0-9+/]{20,}={0,2}' suspicious.log
```

_Recursive search in log directories:_

```bash
grep -r "malicious_payload" /var/log/
grep -rn "192.168.1.100" /var/log/ 2>/dev/null
```

**Performance Considerations:**

```bash
# Faster for simple string searches (no regex)
grep -F "exact_string" large.log

# Multiple pattern file
grep -f patterns.txt access.log

# Exclude binary files
grep -rI "pattern" /var/log/
```

### egrep (Extended grep)

Equivalent to `grep -E`, supports extended regular expressions without escaping special characters.

**Syntax:**

```bash
egrep [OPTIONS] PATTERN [FILE...]
# Identical to: grep -E [OPTIONS] PATTERN [FILE...]
```

**Extended Regex Advantages:**

```bash
# Alternation (OR) without escaping
egrep 'admin|root|sudo' auth.log
# Instead of: grep 'admin\|root\|sudo'

# Grouping
egrep '(GET|POST|PUT|DELETE) /api/' access.log

# Quantifiers
egrep '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}' log.txt

# Character classes and ranges
egrep '[A-Z]{3,}[0-9]{6,}' data.log
```

**CTF-Specific Examples:**

_Finding SQLi patterns:_

```bash
egrep -i '(union.*select|select.*from|or.*1=1|drop.*table)' access.log
```

_Command injection attempts:_

```bash
egrep '(;|\||`|\$\(|<\(|>\()' access.log
egrep -o '\|(cat|ls|whoami|id|wget|curl)' access.log
```

_Path traversal:_

```bash
egrep '\.\./|\.\.\\' access.log
egrep -c '(\.\./%2e%2e/|\.\.%5c)' access.log
```

_Multiple simultaneous conditions:_

```bash
egrep '(POST|PUT).*(\.(php|asp|jsp))' access.log
```

### fgrep (Fixed/Fast grep)

Equivalent to `grep -F`, treats patterns as fixed strings (no regex interpretation). Significantly faster for literal string matching.

**Syntax:**

```bash
fgrep [OPTIONS] PATTERN [FILE...]
# Identical to: grep -F [OPTIONS] PATTERN [FILE...]
```

**When to Use fgrep:**

- Searching for strings containing regex special characters (`.*[]^$`)
- Performance-critical searches on large files
- Searching for exact IOCs (hashes, IPs, domains)

**Examples:**

_Searching for exact strings with special characters:_

```bash
fgrep '192.168.1.1' access.log          # No need to escape dots
fgrep '$_POST["cmd"]' suspicious.php
fgrep 'C:\Windows\System32' event.log
```

_Multiple pattern file (IOC list):_

```bash
fgrep -f ioc_list.txt access.log
# ioc_list.txt contains:
# malicious.domain.com
# 1.2.3.4
# /evil/path.php
```

_Case-insensitive IOC matching:_

```bash
fgrep -if malware_hashes.txt file_hashes.log
```

## awk

A powerful text processing language for pattern scanning and field extraction. Essential for structured log analysis.

**Basic Syntax:**

```bash
awk 'PATTERN { ACTION }' file
awk -F'delimiter' 'PATTERN { ACTION }' file
```

**Field Variables:**

```bash
$0      # Entire line
$1      # First field
$2      # Second field
$NF     # Last field
$(NF-1) # Second-to-last field
NR      # Current line number
NF      # Number of fields in current line
```

**Common CTF Operations:**

_Extracting specific fields from Apache/Nginx logs:_

```bash
# Apache common log format: IP - - [timestamp] "request" status size
awk '{print $1}' access.log                    # Extract IPs
awk '{print $7}' access.log                    # Extract requested URLs
awk '{print $9}' access.log                    # Extract status codes
awk '{print $1, $7, $9}' access.log           # Multiple fields
```

_Using different field delimiters:_

```bash
awk -F':' '{print $1}' /etc/passwd            # Usernames
awk -F',' '{print $2, $3}' csv_log.txt        # CSV fields
awk -F'[:|]' '{print $1, $3}' mixed.log       # Multiple delimiters
```

_Conditional filtering:_

```bash
# Show only lines where status code is 404
awk '$9 == 404' access.log

# Show requests with size > 10000 bytes
awk '$10 > 10000' access.log

# Show requests from specific IP
awk '$1 == "192.168.1.100"' access.log

# Multiple conditions (AND)
awk '$9 >= 400 && $9 < 500' access.log

# Multiple conditions (OR)
awk '$9 == 404 || $9 == 403' access.log
```

_Pattern matching with regex:_

```bash
# Lines containing "admin" in URL field
awk '$7 ~ /admin/' access.log

# Lines NOT containing pattern
awk '$7 !~ /\.jpg$/' access.log

# Case-insensitive matching
awk 'tolower($0) ~ /union.*select/' access.log
```

**Advanced CTF Techniques:**

_Counting occurrences:_

```bash
# Count requests per IP
awk '{ip[$1]++} END {for (i in ip) print i, ip[i]}' access.log

# Count HTTP status codes
awk '{status[$9]++} END {for (s in status) print s, status[s]}' access.log

# Count requests per URL
awk '{url[$7]++} END {for (u in url) print url[u], u}' access.log | sort -rn
```

_Time-based analysis:_

```bash
# Extract hour from Apache timestamp [DD/Mon/YYYY:HH:MM:SS]
awk -F'[: []' '{print $6}' access.log | sort | uniq -c

# Filter logs within specific timeframe
awk -F'[: []' '$6 >= 10 && $6 <= 14' access.log

# Extract full timestamp
awk '{match($0, /\[.*\]/); print substr($0, RSTART+1, RLENGTH-2)}' access.log
```

_Statistical analysis:_

```bash
# Calculate average response size
awk '{sum+=$10; count++} END {print sum/count}' access.log

# Find maximum value
awk 'BEGIN{max=0} {if($10>max) max=$10} END{print max}' access.log

# Response size distribution
awk '{
    if ($10 < 1000) small++
    else if ($10 < 10000) medium++
    else large++
} END {
    print "Small:", small
    print "Medium:", medium
    print "Large:", large
}' access.log
```

_Combining with shell commands:_

```bash
# Extract IPs, sort, count unique
awk '{print $1}' access.log | sort | uniq -c | sort -rn

# Top 10 requesting IPs
awk '{print $1}' access.log | sort | uniq -c | sort -rn | head -10

# Extract and process suspicious patterns
awk '/union|select/ {print $1, $7}' access.log | sort -u
```

_Multi-line processing:_

```bash
# Print lines and their line numbers
awk '{print NR, $0}' file.log

# Print every 5th line
awk 'NR % 5 == 0' file.log

# Process ranges
awk 'NR==10,NR==20' file.log           # Lines 10-20
awk '/START/,/END/' file.log            # Between patterns
```

_Output formatting:_

```bash
# Printf formatting
awk '{printf "%-15s %s\n", $1, $7}' access.log

# Custom delimiters in output
awk -F' ' '{print $1 "," $7 "," $9}' access.log

# Add headers
awk 'BEGIN{print "IP,URL,Status"} {print $1","$7","$9}' access.log
```

**Built-in Functions:**

```bash
length($0)              # String length
substr($1, 1, 5)       # Substring extraction
tolower($7)            # Convert to lowercase
toupper($1)            # Convert to uppercase
split($7, arr, "/")    # Split string into array
gsub(/pattern/, "replacement", $7)  # Global substitution
```

## sed (Stream Editor)

Non-interactive text editor for filtering and transforming text streams. Ideal for log modification, extraction, and sanitization.

**Basic Syntax:**

```bash
sed [OPTIONS] 'COMMAND' file
sed -e 'COMMAND1' -e 'COMMAND2' file
```

**Essential Options:**

```bash
-n          # Suppress automatic output (use with p command)
-i          # Edit files in-place (use cautiously)
-i.bak      # In-place edit with backup
-e          # Multiple commands
-f file     # Read commands from file
-r/-E       # Extended regex
```

**Core Commands:**

_Substitution (most common):_

```bash
# Basic: s/pattern/replacement/
sed 's/old/new/' file              # Replace first occurrence per line
sed 's/old/new/g' file             # Replace all occurrences
sed 's/old/new/2' file             # Replace second occurrence
sed 's/old/new/gi' file            # Case-insensitive global replace

# Using different delimiters
sed 's|/var/log|/tmp|g' file       # Useful for paths
sed 's#http://##g' file
```

_CTF-Specific Substitutions:_

_IP address obfuscation/extraction:_

```bash
# Mask last octet
sed 's/\([0-9]\{1,3\}\.\)\{3\}[0-9]\{1,3\}/\1xxx/g' access.log

# Extract IPs
sed -n 's/.*\([0-9]\{1,3\}\.[0-9]\{1,3\}\.[0-9]\{1,3\}\.[0-9]\{1,3\}\).*/\1/p' file
```

_URL decoding:_

```bash
# Decode common URL encodings
sed 's/%20/ /g; s/%2F/\//g; s/%3A/:/g' encoded.log
```

_Remove HTML/XML tags:_

```bash
sed 's/<[^>]*>//g' file
```

_Clean log formatting:_

```bash
# Remove ANSI color codes
sed 's/\x1b\[[0-9;]*m//g' colorized.log

# Remove carriage returns
sed 's/\r$//' dos_file.txt

# Normalize whitespace
sed 's/[[:space:]]\+/ /g' file
```

**Deletion Commands:**

```bash
# Delete lines
sed '5d' file                      # Delete line 5
sed '5,10d' file                   # Delete lines 5-10
sed '/pattern/d' file              # Delete matching lines
sed '/^$/d' file                   # Delete empty lines
sed '/^#/d' file                   # Delete comments

# Delete first/last line
sed '1d' file
sed '$d' file

# Delete everything except pattern
sed -n '/pattern/p' file
# or
sed '/pattern/!d' file
```

_CTF Log Cleaning:_

```bash
# Remove noise from logs
sed '/DEBUG/d; /INFO/d' application.log

# Keep only errors
sed -n '/ERROR\|CRITICAL/p' application.log

# Remove successful requests
sed '/\" 200 /d' access.log
```

**Print Commands:**

```bash
# Print specific lines
sed -n '10p' file                  # Print line 10
sed -n '10,20p' file               # Print lines 10-20
sed -n '$p' file                   # Print last line

# Print matching lines
sed -n '/pattern/p' file           # Like grep
sed -n '/start/,/end/p' file       # Print range between patterns
```

**Insertion and Append:**

```bash
# Insert before line
sed '5i\New line text' file        # Insert before line 5
sed '/pattern/i\New line' file     # Insert before matching line

# Append after line
sed '5a\New line text' file        # Append after line 5
sed '/pattern/a\New line' file     # Append after matching line

# Add header/footer
sed '1i\=== LOG START ===' file
sed '$a\=== LOG END ===' file
```

**Advanced Techniques:**

_Multiple commands:_

```bash
sed -e 's/foo/bar/g' -e 's/baz/qux/g' file

# Or using semicolon
sed 's/foo/bar/g; s/baz/qux/g' file

# Or using -f for complex scripts
cat > sed_script.txt << EOF
/^#/d
s/ERROR/[ERROR]/g
s/WARNING/[WARN]/g
EOF
sed -f sed_script.txt application.log
```

_Address ranges:_

```bash
# Apply command only to range
sed '10,20s/old/new/g' file        # Lines 10-20
sed '/START/,/END/s/old/new/g' file # Between patterns
sed '1,/pattern/d' file            # From start to first match

# Every nth line
sed -n '1~5p' file                 # Print every 5th line starting from 1
```

_Backreferences and capture groups:_

```bash
# Extract date from Apache log
sed -n 's/.*\[\([^:]*\).*/\1/p' access.log

# Reorder fields
sed 's/\([^,]*\),\([^,]*\)/\2,\1/' file  # Swap first two CSV fields

# Extract quoted strings
sed -n 's/.*"\([^"]*\)".*/\1/p' file
```

_Hold space operations (advanced):_

```bash
# Reverse lines
sed '1!G;h;$!d' file

# Remove duplicate consecutive lines
sed '$!N; /^\(.*\)\n\1$/!P; D'

# Join lines
sed ':a;N;$!ba;s/\n/ /g' file
```

## cut

Extracts sections from each line of files, particularly useful for delimited data.

**Basic Syntax:**

```bash
cut -d'delimiter' -f fields file
cut -c characters file
cut -b bytes file
```

**Options:**

```bash
-d      # Specify delimiter (default is TAB)
-f      # Select fields
-c      # Select characters
-b      # Select bytes
-s      # Suppress lines with no delimiters
--complement  # Invert selection
--output-delimiter  # Specify output delimiter
```

**Field Selection Examples:**

_Basic field extraction:_

```bash
cut -d':' -f1 /etc/passwd          # First field (usernames)
cut -d':' -f1,6 /etc/passwd        # Fields 1 and 6
cut -d':' -f1-3 /etc/passwd        # Fields 1 through 3
cut -d':' -f3- /etc/passwd         # Field 3 to end
```

_CTF Log Analysis:_

_Apache/Nginx logs (space-delimited):_

```bash
# Extract IPs (field 1)
cut -d' ' -f1 access.log

# Extract URLs (field 7)
cut -d' ' -f7 access.log

# Extract status codes (field 9)
cut -d' ' -f9 access.log

# Multiple fields with custom output delimiter
cut -d' ' -f1,7,9 --output-delimiter=',' access.log
```

_CSV processing:_

```bash
cut -d',' -f2 data.csv             # Second column
cut -d',' -f1,3,5 data.csv         # Multiple columns
cut -d',' -f2- data.csv            # All except first column
```

_Extracting from structured logs:_

```bash
# Syslog format: timestamp hostname process[pid]: message
cut -d' ' -f5- /var/log/syslog     # Extract message only
cut -d':' -f2- auth.log            # Everything after first colon
```

**Character/Byte Selection:**

```bash
# Extract first 10 characters
cut -c1-10 file

# Extract specific character positions
cut -c1,5,10 file

# Extract character ranges
cut -c1-5,10-15 file

# Extract from position to end
cut -c20- file
```

_CTF Applications:_

```bash
# Extract hash from hash:password format
cut -c1-32 hashes.txt              # First 32 chars (MD5)

# Extract timestamps
cut -c1-19 timestamps.log          # First 19 chars

# Fixed-width log parsing
cut -c1-15,20-30,40-50 fixed_width.log
```

**Practical Combinations:**

```bash
# Most frequent IPs in access log
cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head

# Unique URLs accessed
cut -d' ' -f7 access.log | sort -u

# Extract usernames attempting SSH login
grep "Failed password" /var/log/auth.log | cut -d' ' -f11 | sort | uniq -c

# Extract domains from URLs
cut -d'/' -f3 urls.txt | cut -d':' -f1 | sort -u
```

**Limitations:**

- Only works with single-character delimiters
- Cannot handle multiple consecutive delimiters as one (unlike awk)
- For complex field extraction, use `awk` instead

## sort

Orders lines of text files, essential for organizing log data and preparing for unique counts.

**Basic Syntax:**

```bash
sort [OPTIONS] file
```

**Key Options:**

```bash
-n          # Numeric sort
-r          # Reverse order
-u          # Remove duplicates (unique)
-k N        # Sort by field N
-t DELIM    # Field delimiter
-h          # Human-numeric sort (1K, 1M, 1G)
-g          # General numeric sort (handles scientific notation)
-V          # Version number sort
-f          # Case-insensitive
-b          # Ignore leading blanks
-s          # Stable sort (preserve original order of equal lines)
-c          # Check if already sorted
-m          # Merge already sorted files
-o FILE     # Output to file
```

**Common CTF Operations:**

_Basic sorting:_

```bash
sort access.log                    # Alphabetical
sort -r access.log                 # Reverse alphabetical
sort -u access.log                 # Alphabetical, unique only
sort -n numbers.txt                # Numeric
sort -rn numbers.txt               # Reverse numeric
```

_Field-based sorting:_

```bash
# Sort Apache logs by IP (field 1)
sort -k1 access.log

# Sort by status code (field 9), numeric
sort -k9n access.log

# Sort by response size (field 10), reverse numeric
sort -k10rn access.log

# Multiple sort keys: primary by field 9, secondary by field 10
sort -k9n -k10rn access.log
```

_Delimiter-specific sorting:_

```bash
# CSV sorting by second field
sort -t',' -k2 data.csv

# /etc/passwd by UID (field 3), numeric
sort -t':' -k3n /etc/passwd

# Sort by field, then by subfield
sort -t' ' -k1,1 -k7,7 access.log  # By IP, then by URL
```

_CTF-Specific Examples:_

_Frequency analysis:_

```bash
# Most common IPs
cut -d' ' -f1 access.log | sort | uniq -c | sort -rn

# Most accessed URLs
cut -d' ' -f7 access.log | sort | uniq -c | sort -rn

# Most common status codes
cut -d' ' -f9 access.log | sort | uniq -c | sort -rn
```

_Time-based sorting:_

```bash
# Sort by timestamp (assuming ISO 8601 format)
sort -k1 timestamped.log

# Sort by date field in Apache logs (field 4, between brackets)
sort -t'[' -k2 access.log
```

_Human-readable sizes:_

```bash
# Sort file sizes
ls -lh | sort -k5h                 # 5th field, human-readable

# Sort by transfer size in logs
awk '{print $10, $0}' access.log | sort -k1h | cut -d' ' -f2-
```

_Version sorting:_

```bash
# Sort version numbers correctly
sort -V versions.txt
# Example: 1.9 comes before 1.10 (correct)
# Regular sort would put 1.10 before 1.9 (incorrect)
```

**Advanced Techniques:**

_Case handling:_

```bash
sort -f file                       # Case-insensitive sort
sort -f -u file                    # Case-insensitive unique
```

_Sorting ranges:_

```bash
# Sort by character range
sort -k1.5,1.8 file               # Sort by characters 5-8 of field 1
```

_Checking sort order:_

```bash
sort -c file                       # Check if sorted, report first unsorted line
sort -C file                       # Check if sorted, silent mode
```

_Merging sorted files:_

```bash
sort -m sorted1.log sorted2.log sorted3.log
```

_Parallel sorting (faster on large files):_

```bash
sort --parallel=4 large_file.log
```

## uniq

Reports or filters out repeated lines in a file. **[CRITICAL]: Input must be sorted for uniq to work correctly.**

**Basic Syntax:**

```bash
uniq [OPTIONS] input
```

**Essential Options:**

```bash
-c          # Prefix lines with count
-d          # Only print duplicate lines
-u          # Only print unique lines (appear once)
-i          # Case-insensitive comparison
-f N        # Skip first N fields
-s N        # Skip first N characters
-w N        # Compare only first N characters
```

**Common CTF Workflows:**

_Basic duplicate removal:_

```bash
sort file | uniq                   # Remove adjacent duplicates
sort file | uniq -c                # Count occurrences
sort file | uniq -u                # Show lines that appear once
sort file | uniq -d                # Show lines that appear multiple times
```

_Frequency analysis (most common pattern):_

```bash
# Find most common IPs
cut -d' ' -f1 access.log | sort | uniq -c | sort -rn

# Top 10 most common
cut -d' ' -f1 access.log | sort | uniq -c | sort -rn | head -10

# Find least common
cut -d' ' -f1 access.log | sort | uniq -c | sort -n | head -10
```

_Finding unique/duplicate entries:_

```bash
# IPs that appear only once
cut -d' ' -f1 access.log | sort | uniq -u

# IPs that appear multiple times (potential scanners)
cut -d' ' -f1 access.log | sort | uniq -d

# Count of unique IPs
cut -d' ' -f1 access.log | sort -u | wc -l
```

_Field-based uniqueness:_

```bash
# Ignore first field, check uniqueness on remaining
sort file | uniq -f 1

# Compare only first 10 characters
sort file | uniq -w 10

# Skip first 5 characters
sort file | uniq -s 5
```

**CTF-Specific Examples:**

_Detecting scan patterns:_

```bash
# IPs with more than 100 requests
cut -d' ' -f1 access.log | sort | uniq -c | awk '$1 > 100'

# URLs accessed by only one IP (targeted attacks)
cut -d' ' -f7 access.log | sort | uniq -c | awk '$1 == 1 {print $2}'
```

_Finding anomalies:_

```bash
# Rare status codes
cut -d' ' -f9 access.log | sort | uniq -c | sort -n | head

# User agents that appear only once (suspicious)
grep -oP 'User-Agent: \K.*' access.log | sort | uniq -u
```

_Comparison operations:_

```bash
# Lines in file1 but not in file2
sort file1 file2 file2 | uniq -u

# Lines common to both files
sort file1 file2 | uniq -d

# All unique lines from both files
sort file1 file2 | uniq
```

_Case-insensitive operations:_

```bash
# Case-insensitive unique count
sort file | uniq -ci
```

**Combined Pipelines for CTF:**

```bash
# Complete IP frequency analysis
cat access.log | \
  awk '{print $1}' | \
  sort | \
  uniq -c | \
  sort -rn | \
  awk '{printf "%s requests from %s\n", $1, $2}'

# Find IPs attempting SQL injection
grep -i 'union.*select' access.log | \
  awk '{print $1}' | \
  sort | \
  uniq -c | \
  sort -rn

# Extract unique malicious payloads
grep -oP '\?(.*?)\s' access.log | \
  sort -u | \
  grep -iE '(union|select|script|exec)'

# User agents sorted by frequency
awk -F'"' '/GET|POST/ {print $6}' access.log | \
  sort | \
  uniq -c | \
  sort -rn

# Timeline of unique attack patterns
awk '{print $4, $7}' access.log | \
  sort -u | \
  grep -iE '(union|sleep|benchmark)'
```

**Performance Notes:**

- `sort -u` is faster than `sort | uniq` for simple duplicate removal
- For very large files, consider using `sort -u --parallel=4`
- `uniq` only compares adjacent lines; always sort first unless input is pre-sorted

**Common Pitfalls:**

```bash
# INCORRECT: uniq without sort
uniq unsorted.log              # Will miss non-adjacent duplicates

# CORRECT: sort before uniq
sort unsorted.log | uniq

# INCORRECT: Assuming uniq -c sorts output
cut -d' ' -f1 access.log | sort | uniq -c  # Output not sorted by count

# CORRECT: Sort the count output
cut -d' ' -f1 access.log | sort | uniq -c | sort -rn
```

---

**Recommended Subtopics**

For comprehensive CTF log analysis mastery, proceed with these related topics:

- **Regular Expressions**: Advanced pattern matching for grep/awk/sed
- **Log Format Structures**: Apache, Nginx, Syslog, Windows Event Logs
- **Process Monitoring Tools**: ps, top, lsof, netstat for live analysis
- **Time/Date Manipulation**: date, timedate, timezone handling in logs

---

## head, tail, less

Essential tools for navigating and sampling log files, particularly when dealing with large datasets in CTF scenarios.

### head

Displays the first part of files. Critical for quick file inspection and sampling.

**Basic Syntax:**

```bash
head [OPTIONS] [FILE...]
```

**Key Options:**

```bash
-n NUM      # Show first NUM lines (default: 10)
-c NUM      # Show first NUM bytes
-q          # Quiet mode (suppress headers with multiple files)
-v          # Verbose mode (always show headers)
```

**Common CTF Operations:**

_Basic usage:_

```bash
head access.log                    # First 10 lines
head -n 20 access.log              # First 20 lines
head -5 access.log                 # First 5 lines (shorthand)
head -n -50 access.log             # All except last 50 lines
```

_Byte-based extraction:_

```bash
head -c 100 file                   # First 100 bytes
head -c 1K file                    # First 1 kilobyte
head -c 1M large.log               # First 1 megabyte
```

_Multiple files:_

```bash
head -n 5 *.log                    # First 5 lines of each log file
head -q -n 5 *.log                 # Without filename headers
```

**CTF-Specific Examples:**

_Quick log format identification:_

```bash
head -n 1 access.log               # Check log format
head -n 3 *.log                    # Compare formats across files
```

_Extract log header/metadata:_

```bash
head -n 20 application.log | grep -i "version\|config\|start"
```

_Sampling for pattern development:_

```bash
# Get sample before developing complex regex
head -n 100 access.log > sample.log
# Develop patterns on sample, then apply to full log
```

_Combined with other tools:_

```bash
# First 100 unique IPs
head -n 1000 access.log | awk '{print $1}' | sort -u

# Check first entries after attack timestamp
grep -m 1 "2024-10-15" access.log | head -n 50
```

### tail

Displays the last part of files. Essential for monitoring recent activity and following live logs.

**Basic Syntax:**

```bash
tail [OPTIONS] [FILE...]
```

**Key Options:**

```bash
-n NUM      # Show last NUM lines (default: 10)
-c NUM      # Show last NUM bytes
-f          # Follow mode (watch file for new lines)
-F          # Follow with retry (recreates if file is rotated)
-q          # Quiet mode
-v          # Verbose mode
--pid=PID   # With -f, terminate after process PID dies
-s NUM      # With -f, sleep NUM seconds between iterations
```

**Common CTF Operations:**

_Basic usage:_

```bash
tail access.log                    # Last 10 lines
tail -n 50 access.log              # Last 50 lines
tail -20 access.log                # Last 20 lines (shorthand)
tail -n +50 access.log             # From line 50 to end
```

_Follow mode (real-time monitoring):_

```bash
tail -f access.log                 # Watch for new entries
tail -f /var/log/auth.log          # Monitor authentication attempts
tail -F rotating.log               # Follow even if log rotates
tail -f access.log | grep "error"  # Filter while following
```

_Multiple files simultaneously:_

```bash
tail -f /var/log/{access,error}.log
tail -f *.log
```

**CTF-Specific Examples:**

_Finding recent activity:_

```bash
# Last 100 requests
tail -n 100 access.log

# Recent authentication failures
tail -n 500 /var/log/auth.log | grep -i "failed"

# Latest error entries
tail -n 200 error.log | grep -E "CRITICAL|ERROR"
```

_Extracting time ranges:_

```bash
# Get last hour of activity (approximate)
tail -n 10000 access.log | grep "$(date -d '1 hour ago' '+%H:')"

# Recent suspicious activity
tail -n 5000 access.log | grep -iE "union|select|script"
```

_Live monitoring during CTF:_

```bash
# Monitor for attack patterns
tail -f access.log | grep --line-buffered -iE "\.\.\/|union|exec"

# Watch for specific IP
tail -f access.log | grep --line-buffered "192.168.1.100"

# Color-coded monitoring
tail -f access.log | grep --color=always -E "40[0-9]|50[0-9]|$"
```

_Combined operations:_

```bash
# Skip header, get last entries
tail -n +2 data.csv | tail -n 100

# Last 1000 lines, unique IPs
tail -n 1000 access.log | awk '{print $1}' | sort -u

# Recent failed logins with count
tail -n 1000 /var/log/auth.log | grep "Failed" | awk '{print $11}' | sort | uniq -c
```

_Byte-based operations:_

```bash
tail -c 1K file                    # Last 1 kilobyte
tail -c +1000 file                 # From byte 1000 to end
```

**Performance Considerations:**

```bash
# Efficient for large files (reads from end)
tail -n 100 100GB.log              # Fast

# Following multiple files can be resource-intensive
tail -f file1.log file2.log file3.log

# Limit output rate when following
tail -f access.log | head -n 1000  # Stop after 1000 new lines
```

### less

Interactive file viewer with searching and navigation capabilities. Superior to `more` for log analysis.

**Basic Syntax:**

```bash
less [OPTIONS] [FILE...]
```

**Key Options:**

```bash
-N          # Show line numbers
-S          # Chop long lines (no wrap)
-i          # Case-insensitive search
-I          # Case-insensitive search (ignores case even with pattern)
-F          # Quit if entire file fits on one screen
-X          # Don't clear screen on exit
-R          # Display ANSI color codes correctly
+F          # Start in follow mode (like tail -f)
+/pattern   # Start at first occurrence of pattern
+NUM        # Start at line NUM
```

**Navigation Commands (within less):**

```bash
# Movement
Space, f         # Forward one screen
b                # Backward one screen
d                # Forward half screen
u                # Backward half screen
Down arrow, j    # Forward one line
Up arrow, k      # Backward one line
g                # Go to first line
G                # Go to last line
NUM g            # Go to line NUM
```

```bash
# Search
/pattern         # Search forward
?pattern         # Search backward
n                # Next match
N                # Previous match
&pattern         # Display only matching lines
```

```bash
# Marking positions
m letter         # Mark position with letter (a-z)
' letter         # Go to marked position
```

```bash
# Other commands
q                # Quit
h                # Help
v                # Edit current file in $EDITOR
F                # Follow mode (like tail -f)
Ctrl+C           # Exit follow mode
-N               # Toggle line numbers
-S               # Toggle line wrapping
```

**Common CTF Operations:**

_Basic viewing:_

```bash
less access.log                    # Interactive view
less -N access.log                 # With line numbers
less -S access.log                 # No line wrapping (better for wide logs)
less +G access.log                 # Start at end
```

_Searching within less:_

```bash
# After opening file:
/192.168.1.100                     # Find IP forward
?error                             # Find "error" backward
/union.*select                     # Regex search
n                                  # Next occurrence
N                                  # Previous occurrence
```

_Filter view (show only matching lines):_

```bash
# After opening file:
&ERROR                             # Show only ERROR lines
&                                  # Remove filter
&404|500                           # Show only lines with 404 or 500
```

_CTF-Specific Examples:_

_Opening with pre-search:_

```bash
less +/malicious access.log        # Open at first "malicious"
less +/2024-10-15 access.log       # Jump to date
less +1000 access.log              # Start at line 1000
```

_Follow mode:_

```bash
less +F access.log                 # Start following (like tail -f)
# Press Ctrl+C to stop following, scroll/search
# Press F to resume following
```

_Multiple files:_

```bash
less *.log
# Within less:
:n           # Next file
:p           # Previous file
:e file.log  # Examine specific file
```

_Color preservation:_

```bash
# View logs with ANSI colors
less -R colored_output.log

# View with grep highlighting
grep --color=always "pattern" file.log | less -R
```

**Advanced Techniques:**

_Pipeline viewing:_

```bash
# View output from complex command
grep -i "error" *.log | less

# Preserve formatting
ps aux | less -S

# Search results
find /var/log -name "*.log" -exec grep -l "error" {} \; | less
```

_Environment configuration:_

```bash
# Set default options
export LESS="-N -S -i -R"

# Custom prompt showing filename and line
export LESS="-PM?f%f .?m(file %i of %m) .?ltlines %lt-%lb?L/%L. :byte %bB?s/%s. .?e(END) ?x- Next\: %x.:?pB%pB\%..%t"
```

_Comparing files side-by-side:_

```bash
# View differences
diff file1.log file2.log | less

# Better: use vim diff mode for actual side-by-side
vimdiff file1.log file2.log
```

**CTF Workflow Examples:**

```bash
# Investigate specific IP activity
less +/192.168.1.100 access.log
# Then within less: &192.168.1.100 (filter to show only this IP)

# Find all SQL injection attempts
less access.log
# Within less: /union.*select
# Press 'n' repeatedly to investigate each occurrence

# Monitor live while searching history
less +F access.log
# Ctrl+C to pause
# /suspicious_pattern to search
# F to resume following

# Quick line number reference
less -N access.log
# Find interesting entry, note line number
# Exit and extract: sed -n '1500,1600p' access.log
```

**Comparison: head/tail/less Selection Guide:**

```bash
# Use head when:
# - Quick peek at file structure/format
# - Sampling first N entries
# - Extracting file headers

# Use tail when:
# - Recent activity analysis
# - Live monitoring (tail -f)
# - Last N entries extraction

# Use less when:
# - Interactive investigation needed
# - Searching through logs
# - Need to navigate back and forth
# - Filtering views dynamically
# - Files too large for text editors
```

## wc (word count)

Counts lines, words, characters, and bytes in files. Essential for quick statistical analysis of log files.

**Basic Syntax:**

```bash
wc [OPTIONS] [FILE...]
```

**Key Options:**

```bash
-l          # Count lines only
-w          # Count words only
-c          # Count bytes only
-m          # Count characters only
-L          # Length of longest line
```

**Common CTF Operations:**

_Basic counts:_

```bash
wc access.log                      # Lines, words, bytes
wc -l access.log                   # Line count only
wc -l *.log                        # Line count per file + total
```

**Output format:**

```bash
# Default output: lines words bytes filename
$ wc access.log
  1542  18504  234567 access.log
```

**CTF-Specific Examples:**

_Log volume analysis:_

```bash
# Total log entries
wc -l access.log

# Compare log sizes
wc -l *.log | sort -n

# Entries per log type
wc -l /var/log/{access,error,auth}.log
```

_Attack volume measurement:_

```bash
# Count suspicious requests
grep -i "union select" access.log | wc -l

# Failed login attempts
grep "Failed password" /var/log/auth.log | wc -l

# Requests from specific IP
grep "192.168.1.100" access.log | wc -l

# HTTP errors (4xx, 5xx)
grep -E " (4[0-9]{2}|5[0-9]{2}) " access.log | wc -l
```

_Unique value counting:_

```bash
# Unique IPs
awk '{print $1}' access.log | sort -u | wc -l

# Unique URLs accessed
awk '{print $7}' access.log | sort -u | wc -l

# Unique user agents
awk -F'"' '{print $6}' access.log | sort -u | wc -l
```

_Data validation:_

```bash
# Verify expected record count
wc -l data.csv
# Compare to expected value

# Check if processing lost records
wc -l input.log output.log

# Verify filtering worked
wc -l original.log
grep "pattern" original.log | wc -l
```

_Character/byte analysis:_

```bash
# File size in bytes
wc -c large.log

# Character count (useful for encoded data)
wc -m encoded_payload.txt

# Longest line length (detect anomalies)
wc -L access.log
```

**Advanced CTF Techniques:**

_Timeline analysis:_

```bash
# Requests per hour
for hour in {00..23}; do
  echo -n "$hour:00 - "
  grep "/$hour:" access.log | wc -l
done

# Daily activity over time period
for day in {01..31}; do
  echo -n "Oct $day: "
  grep "Oct/$day" access.log | wc -l
done
```

_Comparative analysis:_

```bash
# Before vs after incident
echo "Before: $(grep -c "Oct/27" access.log)"
echo "After: $(grep -c "Oct/28" access.log)"

# Success vs failure rates
echo "Success (200): $(grep -c ' 200 ' access.log)"
echo "Client errors (4xx): $(grep -cE ' 4[0-9]{2} ' access.log)"
echo "Server errors (5xx): $(grep -cE ' 5[0-9]{2} ' access.log)"
```

_Attack pattern quantification:_

```bash
# Different attack types
echo "SQLi attempts: $(grep -ci 'union.*select' access.log)"
echo "XSS attempts: $(grep -ci 'script.*alert' access.log)"
echo "Path traversal: $(grep -c '\.\.\/' access.log)"
echo "Command injection: $(grep -cE '\||;|`' access.log)"
```

_Reporting:_

```bash
# Generate summary report
cat << EOF
=== Log Analysis Summary ===
Total entries: $(wc -l < access.log)
Unique IPs: $(awk '{print $1}' access.log | sort -u | wc -l)
Unique URLs: $(awk '{print $7}' access.log | sort -u | wc -l)
404 errors: $(grep -c ' 404 ' access.log)
500 errors: $(grep -c ' 500 ' access.log)
Suspicious: $(grep -ciE 'union|select|script|exec' access.log)
EOF
```

_Percentage calculations:_

```bash
# Percentage of failed requests
total=$(wc -l < access.log)
failed=$(grep -cE ' (4[0-9]{2}|5[0-9]{2}) ' access.log)
echo "scale=2; $failed * 100 / $total" | bc
# Output: 15.42 (meaning 15.42% failed)
```

_Progress monitoring:_

```bash
# Watch log growth during attack
watch -n 1 'wc -l access.log'

# Monitor specific pattern growth
watch -n 5 'grep -c "malicious" access.log'
```

**Performance Notes:**

```bash
# Fast line counting (stops at first match if only counting)
wc -l file                         # Counts all lines
grep -c pattern file               # Counts matching lines

# wc -l < file is slightly faster (no filename output)
wc -l < access.log

# For multiple operations, store in variable
count=$(wc -l < access.log)
echo "Total: $count"
echo "Average: $(echo "$count / 24" | bc)"
```

**Common Patterns:**

```bash
# Count files matching criteria
find /var/log -name "*.log" | wc -l

# Count running processes
ps aux | wc -l

# Network connections
netstat -an | wc -l

# Count grep matches across multiple files
grep -h "pattern" *.log | wc -l
```

## find and locate

Tools for searching the filesystem. `find` performs real-time searches with extensive filtering, while `locate` queries a pre-built database for fast lookups.

### find

Searches for files and directories in real-time based on multiple criteria. Essential for discovering hidden logs, suspicious files, and artifacts.

**Basic Syntax:**

```bash
find [PATH...] [EXPRESSION]
```

**Primary Expressions:**

```bash
# Name-based
-name pattern       # Case-sensitive filename match
-iname pattern      # Case-insensitive filename match
-path pattern       # Match full path
-regex pattern      # Match path using regex
-iregex pattern     # Case-insensitive regex

# Type-based
-type f             # Regular file
-type d             # Directory
-type l             # Symbolic link
-type s             # Socket
-type p             # Named pipe

# Time-based (n = number)
-mtime n            # Modified n*24 hours ago
-atime n            # Accessed n*24 hours ago
-ctime n            # Changed n*24 hours ago
-mmin n             # Modified n minutes ago
-amin n             # Accessed n minutes ago
-cmin n             # Changed n minutes ago
-newer file         # Modified more recently than file

# Size-based
-size n[cwbkMG]     # File size (c=bytes, k=KB, M=MB, G=GB)
-empty              # Empty files/directories

# Permission-based
-perm mode          # Exact permission match
-perm -mode         # All specified bits set
-perm /mode         # Any specified bits set
-user username      # Owned by user
-group groupname    # Owned by group
-uid n              # Owned by UID
-gid n              # Owned by GID

# Other
-depth              # Process directory contents before directory itself
-maxdepth n         # Descend at most n levels
-mindepth n         # At least n levels deep
-nouser             # No valid user owner (suspicious)
-nogroup            # No valid group owner (suspicious)
```

**Logical Operators:**

```bash
-and, -a            # AND (default between expressions)
-or, -o             # OR
-not, !             # NOT
( )                 # Grouping (escape with \( \))
```

**Actions:**

```bash
-print              # Print pathname (default)
-print0             # Print null-separated (for xargs -0)
-ls                 # List in ls -dils format
-printf format      # Custom format
-delete             # Delete matched files (use with caution!)
-exec cmd {} \;     # Execute command on each match
-exec cmd {} +      # Execute command with multiple matches
-ok cmd {} \;       # Like -exec but asks confirmation
```

**Common CTF Operations:**

_Finding log files:_

```bash
# All log files in system
find /var/log -name "*.log"

# Case-insensitive (catches .LOG, .Log, etc.)
find /var/log -iname "*.log"

# Log files modified in last 24 hours
find /var/log -name "*.log" -mtime -1

# Large log files (>100MB)
find /var/log -name "*.log" -size +100M

# Recently modified logs (last hour)
find /var/log -name "*.log" -mmin -60
```

_Discovering suspicious files:_

```bash
# SUID files (potential privilege escalation)
find / -type f -perm -4000 -ls 2>/dev/null

# SGID files
find / -type f -perm -2000 -ls 2>/dev/null

# World-writable files (security risk)
find / -type f -perm -002 -ls 2>/dev/null

# World-writable directories
find / -type d -perm -002 -ls 2>/dev/null

# Files with no owner (orphaned, suspicious)
find / -nouser -ls 2>/dev/null

# Files with no group
find / -nogroup -ls 2>/dev/null
```

_Time-based forensics:_

```bash
# Files modified in last 7 days
find /home -type f -mtime -7

# Files accessed today
find /var/www -type f -atime 0

# Files modified between 2-5 days ago
find / -type f -mtime +2 -mtime -5

# Files modified more recently than reference file
find /var/log -newer /tmp/incident_timestamp

# Files modified in specific time window (requires touch reference files)
touch -t 202410150900 /tmp/start_time
touch -t 202410151700 /tmp/end_time
find /var/www -newer /tmp/start_time ! -newer /tmp/end_time
```

_Finding backdoors and web shells:_

```bash
# PHP files in uploads directory (shouldn't normally be there)
find /var/www/html/uploads -name "*.php"

# Recently modified PHP files in webroot
find /var/www -name "*.php" -mtime -7

# PHP files with suspicious permissions
find /var/www -name "*.php" -perm -002

# Hidden PHP files
find /var/www -name ".*\.php"

# Scripts in temp directories
find /tmp /var/tmp -type f -executable

# Recently created files in webroot
find /var/www -type f -ctime -1
```

_Search by content (combined with grep):_

```bash
# Files containing specific string
find /var/www -name "*.php" -exec grep -l "eval(" {} \;

# Case-insensitive content search
find /var/www -name "*.php" -exec grep -li "base64_decode" {} \;

# Multiple patterns
find /var/log -name "*.log" -exec grep -l "192.168.1.100" {} \;

# Count matches per file
find /var/www -name "*.php" -exec grep -c "system(" {} +
```

_Size analysis:_

```bash
# Files larger than 1GB
find / -type f -size +1G 2>/dev/null

# Files smaller than 1KB (suspicious logs)
find /var/log -name "*.log" -size -1k

# Empty files
find /var/log -type f -empty

# Size range (between 1MB and 10MB)
find /var/log -size +1M -size -10M
```

**Advanced CTF Techniques:**

_Complex logical expressions:_

```bash
# PHP or ASP files modified recently
find /var/www \( -name "*.php" -o -name "*.asp" \) -mtime -7

# Executable files that are NOT in standard locations
find / -type f -executable ! -path "/bin/*" ! -path "/usr/bin/*" ! -path "/sbin/*" 2>/dev/null

# World-writable files that are NOT in /tmp
find / -type f -perm -002 ! -path "/tmp/*" 2>/dev/null

# Large logs NOT in /var/log
find / -name "*.log" -size +100M ! -path "/var/log/*" 2>/dev/null
```

_Efficient execution:_

```bash
# Execute command on each file individually
find /var/log -name "*.log" -exec wc -l {} \;

# Execute command with multiple files at once (faster)
find /var/log -name "*.log" -exec wc -l {} +

# Null-separated for filenames with spaces
find /var/log -name "*.log" -print0 | xargs -0 wc -l

# Parallel execution (faster for intensive operations)
find /var/log -name "*.log" -print0 | xargs -0 -P 4 -I {} sh -c 'grep "error" {}'
```

_Permission-specific searches:_

```bash
# Exactly 777 permissions
find / -perm 777 2>/dev/null

# At least 755 permissions (all bits set)
find / -perm -755 2>/dev/null

# Any executable bit set
find / -perm /111 2>/dev/null

# Files owned by specific user
find /home -user apache -ls

# Files owned by root but world-writable (dangerous)
find / -user root -perm -002 2>/dev/null
```

_CTF Challenge Patterns:_

_Finding hidden flag files:_

```bash
# Hidden files
find / -name ".*" -type f 2>/dev/null

# Files with "flag" in name
find / -iname "*flag*" 2>/dev/null

# Files in unusual locations
find /dev /proc -type f 2>/dev/null | grep -v "^/proc"

# Files with specific extensions in odd places
find / -name "*.txt" ! -path "/home/*" ! -path "/var/*" 2>/dev/null
```

_Lateral movement detection:_

```bash
# SSH keys
find / -name "id_rsa" -o -name "id_dsa" -o -name "authorized_keys" 2>/dev/null

# Recently accessed SSH configurations
find /home -name ".ssh" -type d -exec find {} -mtime -1 \;

# History files
find /home -name ".*history" -type f

# Recently modified scripts
find / -name "*.sh" -mtime -1 2>/dev/null
```

_Malware hunting:_

```bash
# Executables in tmp
find /tmp /var/tmp -type f -executable -ls

# Files owned by www-data (web compromise)
find / -user www-data -type f -ls 2>/dev/null

# Recently created binaries
find / -type f -executable -ctime -7 2>/dev/null | grep -v "/bin\|/sbin\|/usr"

# Files with multiple extensions (suspicious)
find /var/www -name "*.php.*" -o -name "*.jpg.php"
```

**Output Formatting:**

```bash
# Custom printf format
find /var/log -name "*.log" -printf "%p %s bytes %TY-%Tm-%Td\n"

# Format codes:
# %p - pathname
# %f - filename
# %s - size in bytes
# %T@ - timestamp (seconds since epoch)
# %TY-%Tm-%Td - date (YYYY-MM-DD)
# %u - username
# %g - group name
# %m - permission bits (octal)

# Detailed listing with size
find /var/log -name "*.log" -printf "%10s %p\n" | sort -n
```

**Performance Optimization:**

```bash
# Limit search depth
find /var -maxdepth 3 -name "*.log"

# Skip specific directories
find / -path /proc -prune -o -path /sys -prune -o -name "*.log" -print

# Better: use multiple -path prunes
find / \( -path /proc -o -path /sys -o -path /dev \) -prune -o -name "*.log" -print

# Redirect errors to suppress noise
find / -name "*.log" 2>/dev/null

# Use -quit to stop after first match
find / -name "flag.txt" -quit 2>/dev/null
```

### locate

Searches a pre-built database of filenames. Much faster than `find` but requires updated database and doesn't support as many search criteria.

**Basic Syntax:**

```bash
locate [OPTIONS] PATTERN
```

**Key Options:**

```bash
-i          # Case-insensitive
-c          # Count matches only
-l N        # Limit output to N entries
-r          # Use regex instead of pattern
-b          # Match only basename (not full path)
-e          # Only show existing files (check if still exists)
-S          # Show database statistics
```

**Database Management:**

```bash
# Update locate database (requires root)
sudo updatedb

# Update specific paths only
sudo updatedb --prunepaths="/path/to/skip"

# Show database info
locate -S
```

**Common CTF Operations:**

_Basic searches:_

```bash
# Find all instances of filename
locate access.log

# Case-insensitive
locate -i FLAG.TXT

# Limit results
locate -l 10 ".log"

# Count matches
locate -c "*.log"
```

_Pattern matching:_

```bash
# Wildcard patterns
locate "*.conf"
locate "passwd"
locate "/etc/*.conf"

# Multiple extensions
locate -r "\.php$|\.asp$"

# Basename only (ignore path)
locate -b "\access.log"
```

**CTF-Specific Examples:**

_Finding configuration files:_

```bash
locate -i config | grep -E "\.(conf|cfg|ini)$"
locate ".conf" | grep apache
locate "wp-config"
```

_Finding log files quickly:_

```bash
locate ".log" | wc -l
locate -i "error.log"
locate "access.log" | grep -v "/var/log"  # Non-standard locations
```

_Finding backup files (potential info disclosure):_

```bash
locate ".bak"
locate ".backup"
locate -r "\.sql$"
locate -r "\.old$|\.bak$|~$"
```

_Finding scripts:_

```bash
locate ".sh" | head -20
locate ".php" | grep upload
locate ".py" | grep -i admin
```

**Comparison: find vs locate**

```bash
# Speed comparison
time locate "*.log" | wc -l        # Very fast (seconds)
time find / -name "*.log" 2>/dev/null | wc -l  # Slow (minutes)

# Currency
locate flag.txt                     # May show deleted files
find / -name "flag.txt"            # Shows only current files

# Accuracy
locate flag                         # Matches "flagstone.txt", "conflag.conf"
find / -name "flag.txt" -exact     # Exact match only

# Flexibility
locate "*.log"                      # Simple patterns only
find / -name "*.log" -mtime -1     # Complex criteria supported
```

**When to use locate:**

- Quick filename lookups
- Known filename searches
- System-wide searches where speed matters
- Finding files across multiple directories

**When to use find:**

- Need current filesystem state
- Complex search criteria (time, size, permissions)
- Content-based searches (with -exec grep)
- Suspicious file discovery
- Recently created/modified files
- Database not available or outdated

## strings

Extracts printable character sequences from binary files. Critical for analyzing executables, memory dumps, network captures, and obfuscated data.

**Basic Syntax:**

```bash
strings [OPTIONS] [FILE...]
```

**Key Options:**

```bash
-a          # Scan entire file (default: only loadable sections)
-n NUM      # Minimum string length (default: 4)
-t FORMAT   # Print offset (o=octal, x=hex, d=decimal)
-e ENCODING # Character encoding
-o          # Alias for -t o (octal offset)
-f          # Print filename before each string
-w          # Include whitespace in strings (default: printable only)
```

**Character Encodings:**

```bash
-e s        # 7-bit ASCII (default)
-e S        # 8-bit ASCII
-e b        # 16-bit big-endian
-e l        # 16-bit little-endian
-e B        # 32-bit big-endian

-e L # 32-bit little-endian
````

**Common CTF Operations:**

*Basic string extraction:*
```bash
# Extract all strings
strings binary_file

# Minimum length 8 characters (filter noise)
strings -n 8 suspicious.exe

# All printable strings (scan whole file)
strings -a memory.dump

# With hexadecimal offsets
strings -t x binary_file
````

**CTF-Specific Examples:**

_Finding flags in binaries:_

```bash
# Look for flag format
strings challenge.bin | grep -i "flag"
strings -n 6 challenge.bin | grep -E "CTF\{.*\}"
strings challenge.bin | grep -oE "[A-Za-z0-9+/]{20,}={0,2}"  # Base64

# Case-insensitive flag search
strings -a challenge.bin | grep -iE "flag|key|password"
```

_Analyzing executables:_

```bash
# Find hardcoded credentials
strings suspicious.exe | grep -iE "pass|user|login|admin"

# Find URLs and IPs
strings malware.bin | grep -oE "https?://[^\s]+"
strings malware.bin | grep -oE "\b([0-9]{1,3}\.){3}[0-9]{1,3}\b"

# Find file paths
strings rootkit.ko | grep -E "^/|^\.\./|^C:\\"

# Find function names
strings -a binary | grep -E "^[a-z_][a-z0-9_]+$"
```

_Memory dump analysis:_

```bash
# Extract from memory dump
strings -a memory.dmp > extracted_strings.txt

# Find commands executed
strings memory.dmp | grep -E "(cmd\.exe|powershell|bash|sh -c)"

# Find registry keys (Windows)
strings memory.dmp | grep "HKEY_"

# Find environment variables
strings memory.dmp | grep -E "^[A-Z_]+=" | sort -u
```

_Network capture analysis:_

```bash
# Extract from pcap
strings capture.pcap | grep -i "http"

# Find HTTP headers
strings capture.pcap | grep -E "^(GET|POST|HTTP|Host:|User-Agent:)"

# Extract credentials from network traffic
strings capture.pcap | grep -iE "user|pass|auth|login"

# Find domains
strings capture.pcap | grep -oE "[a-z0-9-]+\.[a-z]{2,}" | sort -u
```

_Document analysis:_

```bash
# Extract from PDF
strings document.pdf | grep -i "author\|title\|subject"

# Extract from Office documents
strings document.docx | grep -a "word/document.xml"

# Find metadata
strings image.jpg | grep -i "exif\|comment\|software"

# Extract hidden data
strings -n 10 stego_image.png
```

**Advanced CTF Techniques:**

_Encoding-specific extraction:_

```bash
# Little-endian Unicode (Windows strings)
strings -e l suspicious.exe | grep -i "flag"

# Big-endian Unicode
strings -e b data.bin

# Extract all encoding types
for enc in s S b l B L; do
  echo "=== Encoding: $enc ==="
  strings -e $enc binary | head -20
done
```

_Offset-based analysis:_

```bash
# Show hex offsets (useful for binary patching)
strings -t x -n 8 binary.exe

# Show decimal offsets
strings -t d binary.exe

# Find string at specific offset
strings -t x binary.exe | grep "^00001a40"

# Extract with context (using dd based on offset)
# If strings shows flag at offset 0x1a40:
dd if=binary.exe bs=1 skip=$((0x1a40)) count=100 2>/dev/null | strings
```

_Multi-file analysis:_

```bash
# Extract from all binaries in directory
strings -f *.exe | grep -i "password"

# Recursive directory search
find /path -type f -exec strings {} \; | grep -i "flag"

# Compare strings across files
strings file1.bin | sort > strings1.txt
strings file2.bin | sort > strings2.txt
diff strings1.txt strings2.txt
```

_Filtering and analysis:_

```bash
# Remove short/common strings
strings binary | awk 'length($0) > 10' | sort -u

# Find only uppercase strings (constants, macros)
strings binary | grep -E "^[A-Z_]+$"

# Find only lowercase (function names)
strings binary | grep -E "^[a-z_][a-z0-9_]*$"

# Find mixed case (likely variable names)
strings binary | grep -E "[a-z].*[A-Z]|[A-Z].*[a-z]"

# Find numeric strings
strings binary | grep -E "^[0-9]+$"

# Statistical analysis
strings binary | awk '{print length}' | sort -n | uniq -c
```

_Obfuscation detection:_

```bash
# Find base64 encoded data
strings binary | grep -E "^[A-Za-z0-9+/]{40,}={0,2}$"

# Hex-encoded strings (alternating patterns)
strings binary | grep -E "^([0-9a-fA-F]{2})+$"

# URL-encoded strings
strings binary | grep "%[0-9A-F][0-9A-F]"

# ROT13 or Caesar cipher (repeated character patterns)
strings -n 20 binary | grep -E "(.)\1{3,}"
```

**Combination with Other Tools:**

_With grep for pattern extraction:_

```bash
# Extract email addresses
strings file | grep -oE "\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b"

# Extract phone numbers
strings file | grep -oE "\+?[0-9]{1,3}[-. ]?[(]?[0-9]{3}[)]?[-. ]?[0-9]{3}[-. ]?[0-9]{4}"

# Extract Bitcoin addresses
strings file | grep -oE "\b[13][a-km-zA-HJ-NP-Z1-9]{25,34}\b"

# Extract MAC addresses
strings file | grep -oE "([0-9A-Fa-f]{2}[:-]){5}([0-9A-Fa-f]{2})"
```

_With sort and uniq:_

```bash
# Most common strings
strings binary | sort | uniq -c | sort -rn | head -20

# Unique strings only (no duplicates)
strings binary | sort -u

# Strings appearing multiple times (potential constants)
strings binary | sort | uniq -d
```

_With awk for processing:_

```bash
# Strings longer than 50 characters
strings binary | awk 'length > 50'

# Format output
strings -t x binary | awk '{printf "0x%s: %s\n", $1, substr($0, index($0,$2))}'

# Extract specific string types
strings binary | awk '/^\/.*\// {print}'  # Path-like strings
```

_With sed for cleaning:_

```bash
# Remove non-ASCII characters
strings binary | sed 's/[^[:print:]]//g'

# Remove leading/trailing whitespace
strings binary | sed 's/^[[:space:]]*//;s/[[:space:]]*$//'

# Extract after keyword
strings binary | sed -n 's/.*password: \(.*\)/\1/p'
```

**Specialized Analysis:**

_Extracting shellcode indicators:_

```bash
# Common shellcode patterns
strings -a exploit | grep -E "(cmd\.exe|/bin/sh|socket|connect|exec)"

# Syscall numbers (Linux)
strings -a binary | grep -oE "\$0x[0-9a-f]{1,2}" | sort -u

# NOP sleds (repeated 0x90 or similar)
strings -a binary | grep -E "(\\x90){10,}"
```

_Cryptographic artifacts:_

```bash
# Find crypto algorithm names
strings binary | grep -iE "aes|des|rsa|sha|md5|rc4|blowfish"

# Find key-related strings
strings binary | grep -iE "key|iv|nonce|salt"

# Potential keys (random-looking base64)
strings -n 32 binary | grep -E "^[A-Za-z0-9+/]{32,}={0,2}$"
```

_Malware analysis:_

```bash
# C2 indicators
strings malware.exe | grep -oE "(http|https|ftp)://[^\s]+"

# DLL names (Windows)
strings malware.exe | grep -iE "\.dll$"

# API function names
strings malware.exe | grep -E "^(Get|Set|Create|Open|Read|Write|Load|Reg)"

# Mutex names (malware synchronization)
strings malware.exe | grep -E "^(Global\\|Local\\)"
```

**Performance and Optimization:**

```bash
# Fast extraction (default sections only)
strings binary > output.txt

# Thorough extraction (entire file)
strings -a binary > output.txt

# Large file processing (stream to grep)
strings large_file.bin | grep "pattern" > matches.txt

# Parallel processing multiple files
find . -type f -print0 | xargs -0 -P 4 -I {} sh -c 'strings "{}" | grep -H --label="{}" "flag"'
```

**CTF Challenge Patterns:**

_Flag extraction workflows:_

```bash
# Standard flag format
strings challenge | grep -E "flag\{.*\}|CTF\{.*\}|FLAG\{.*\}"

# Base64 encoded flag
strings challenge | grep -oE "[A-Za-z0-9+/]{40,}={0,2}" | while read line; do
  echo "$line" | base64 -d 2>/dev/null
done

# Hex encoded flag
strings challenge | grep -oE "([0-9a-fA-F]{2})+" | while read line; do
  echo "$line" | xxd -r -p 2>/dev/null
done

# ROT13 encoded
strings challenge | tr 'A-Za-z' 'N-ZA-Mn-za-m' | grep -i flag
```

_Hidden data discovery:_

```bash
# Strings in unusual encoding
strings -e l challenge.exe | grep -v "^$"  # Unicode

# Strings at end of file (often appended data)
strings challenge | tail -50

# Strings in specific section (using offset)
strings -t x challenge | awk '$1 > "100000"'  # After offset 0x100000

# Compare with known good binary
diff <(strings legitimate.exe | sort) <(strings suspicious.exe | sort)
```

_Reverse engineering support:_

```bash
# Function name extraction
strings binary | grep -E "^_Z" | c++filt  # Demangle C++ names

# Debug symbols
strings -a binary | grep -E "\.cpp|\.c|\.h|debug|DWARF"

# Interesting strings for RE
strings binary | grep -iE "(main|init|fini|constructor|destructor)"
```

**Binary File Type Identification:**

```bash
# Identify file type from strings
strings unknown_file | head -5

# Look for magic bytes (as strings)
strings -n 2 file | head -1

# ELF binaries
strings binary | grep -E "^(ELF|\.text|\.data|\.rodata)"

# PE executables
strings binary.exe | grep -E "^(MZ|PE|This program)"

# Scripts
strings file | head -1 | grep -E "^#!/"
```

**Common Pitfalls and Solutions:**

```bash
# Problem: Too much output
strings large.bin | wc -l              # Count first
strings -n 10 large.bin | less         # Increase minimum length

# Problem: Non-ASCII strings missed
strings -e l windows.exe               # Try Unicode

# Problem: Need context around string
strings -t x binary | grep "keyword"   # Get offset
# Then use hexdump at that offset

# Problem: Binary file detection
file binary                            # Check file type first
strings -a binary                      # Force scan if needed
```

---

## Recommended Subtopics

For comprehensive CTF log and file analysis mastery, proceed with:

- **Text Processing Tools**: `tr`, `paste`, `join`, `column` for advanced data manipulation
- **Binary Analysis Tools**: `hexdump`, `xxd`, `od` for raw binary examination
- **Compression and Archives**: `tar`, `gzip`, `gunzip`, `unzip`, `7z` for handling compressed logs
- **File Analysis**: `file`, `stat`, `md5sum`, `sha256sum` for file identification and integrity checking

---

# Advanced Parsing Tools

Structured data formats (JSON, XML, CSV) are ubiquitous in modern CTF scenariosfrom API responses and configuration files to exported logs and data dumps. Mastering specialized parsing tools enables rapid extraction, transformation, and analysis of complex datasets during time-constrained competitions.

## jq (JSON Processor)

jq is a lightweight, flexible command-line JSON processor that enables filtering, mapping, and transforming JSON data using a domain-specific language. Essential for parsing application logs, API responses, container configurations, and modern security tool outputs.

### Installation and Basic Syntax

```bash
# Installation
apt-get install jq          # Debian/Ubuntu
dnf install jq              # Fedora/RHEL
pacman -S jq                # Arch

# Verify installation
jq --version

# Basic syntax
jq [options] 'filter' [file...]
cat file.json | jq 'filter'
```

### Core Filtering Operations

**Identity and pretty-printing:**

```bash
# Pretty-print JSON (default behavior)
jq '.' input.json

# Compact output (single line)
jq -c '.' input.json

# Raw output (no quotes for strings)
jq -r '.' input.json
```

**Accessing fields:**

```bash
# Top-level field
echo '{"name":"alice","uid":1001}' | jq '.name'
# Output: "alice"

# Nested field access
echo '{"user":{"name":"alice","role":"admin"}}' | jq '.user.name'
# Output: "alice"

# Array element access (zero-indexed)
echo '{"users":["alice","bob","charlie"]}' | jq '.users[1]'
# Output: "bob"

# Optional field access (no error if missing)
echo '{"name":"alice"}' | jq '.email?'
# Output: null
```

**Array operations:**

```bash
# Iterate array elements
echo '[{"name":"alice"},{"name":"bob"}]' | jq '.[]'

# Extract specific field from array objects
echo '[{"name":"alice","uid":1001},{"name":"bob","uid":1002}]' | jq '.[].name'
# Output: "alice" \n "bob"

# Array length
echo '["a","b","c"]' | jq 'length'
# Output: 3

# Array slicing
echo '[0,1,2,3,4]' | jq '.[1:3]'
# Output: [1,2]

# First/last element
echo '[1,2,3]' | jq 'first'
echo '[1,2,3]' | jq 'last'
```

### Filtering and Selection

**Conditional filtering:**

```bash
# Select objects where field equals value
jq '.[] | select(.status == "failed")' auth.log.json

# Numeric comparison
jq '.[] | select(.port > 1024)' connections.json

# String matching with test (regex)
jq '.[] | select(.username | test("admin"))' users.json

# Multiple conditions (AND)
jq '.[] | select(.status == "failed" and .attempts > 3)' logs.json

# Multiple conditions (OR)
jq '.[] | select(.port == 22 or .port == 3389)' connections.json

# Check if field exists
jq '.[] | select(has("error"))' logs.json
```

**Type checking:**

```bash
# Filter by type
jq '.[] | select(type == "string")' mixed.json

# Available types: null, boolean, number, string, array, object
```

### Transformation and Construction

**Object construction:**

```bash
# Create new object with selected fields
jq '.[] | {user: .username, ip: .source_ip}' logs.json

# Rename fields
jq '.[] | {name: .username, identifier: .uid}' users.json

# Computed fields
jq '.[] | {user: .username, total: (.sent + .received)}' traffic.json
```

**String manipulation:**

```bash
# String interpolation
jq '.[] | "\(.username) logged in from \(.ip)"' logs.json

# Split string
echo '{"path":"/var/log/auth.log"}' | jq '.path | split("/") | last'
# Output: "auth.log"

# String concatenation
jq '.[] | .first_name + " " + .last_name' users.json

# Case conversion
jq '.username | ascii_upcase' user.json
jq '.username | ascii_downcase' user.json

# Substring extraction
echo '{"text":"Hello World"}' | jq '.text[0:5]'
# Output: "Hello"
```

**Array transformation:**

```bash
# Map function (transform each element)
echo '[1,2,3]' | jq 'map(. * 2)'
# Output: [2,4,6]

# Map over objects
jq 'map({name: .username, uid: .id})' users.json

# Flatten nested arrays
echo '[[1,2],[3,4]]' | jq 'flatten'
# Output: [1,2,3,4]

# Unique values
echo '[1,2,2,3,3,3]' | jq 'unique'
# Output: [1,2,3]

# Sort array
echo '[3,1,2]' | jq 'sort'
# Output: [1,2,3]

# Reverse array
echo '[1,2,3]' | jq 'reverse'
# Output: [3,2,1]
```

### Aggregation and Statistics

**Counting and grouping:**

```bash
# Count array elements
jq '. | length' array.json

# Group by field
jq 'group_by(.status)' logs.json

# Count occurrences
jq 'group_by(.username) | map({username: .[0].username, count: length})' logs.json

# Unique values
jq '[.[].country] | unique' users.json
```

**Mathematical operations:**

```bash
# Sum values
echo '[1,2,3,4,5]' | jq 'add'
# Output: 15

# Sum specific field
jq '[.[].bytes] | add' transfers.json

# Min/max
echo '[5,2,8,1,9]' | jq 'min'
echo '[5,2,8,1,9]' | jq 'max'

# Average (requires calculation)
jq '[.[].value] | add / length' data.json
```

### CTF-Specific Use Cases

**Parse Docker container logs (JSON format):**

```bash
# Extract error messages
docker logs container_id --since 1h 2>&1 | \
  grep '^{' | \
  jq -r 'select(.level == "error") | .message'

# Find privileged container configurations
docker inspect $(docker ps -aq) | \
  jq '.[] | select(.HostConfig.Privileged == true) | {name: .Name, id: .Id}'
```

**Analyze web server JSON logs:**

```bash
# Extract failed login attempts with usernames
jq -r 'select(.status == 401) | "\(.timestamp) \(.request.username) from \(.client_ip)"' access.log.json

# Top 10 requested URLs
jq -r '.request.url' access.log.json | sort | uniq -c | sort -rn | head -10

# Filter suspicious user agents
jq -r 'select(.user_agent | test("sqlmap|nikto|nmap", "i")) | {ip: .client_ip, agent: .user_agent}' access.log.json
```

**Parse cloud service logs (AWS CloudTrail, Azure Activity):**

```bash
# Extract failed authentication events
jq '.Records[] | select(.errorCode == "AccessDenied") | {time: .eventTime, user: .userIdentity.principalId, action: .eventName}' cloudtrail.json

# Find privilege escalation attempts
jq '.Records[] | select(.eventName | test("Attach.*Policy|Put.*Policy")) | {user: .userIdentity.userName, action: .eventName, resource: .requestParameters}' cloudtrail.json
```

**Kubernetes configuration analysis:**

```bash
# Find pods with hostNetwork enabled (container escape risk)
kubectl get pods -A -o json | \
  jq '.items[] | select(.spec.hostNetwork == true) | {namespace: .metadata.namespace, name: .metadata.name}'

# Extract environment variables from pod specs
kubectl get pod pod_name -o json | \
  jq '.spec.containers[].env[]? | {name: .name, value: .value}'
```

**Parse JWT tokens:**

```bash
# Decode JWT payload (base64url decoding)
echo "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWV9.signature" | \
  cut -d'.' -f2 | \
  base64 -d 2>/dev/null | \
  jq '.'

# Extract specific claims
echo $JWT_TOKEN | cut -d'.' -f2 | base64 -d 2>/dev/null | jq -r '.sub'
```

### Advanced Techniques

**Recursive descent:**

```bash
# Find all values for a key recursively
jq '.. | .password? | select(. != null)' config.json

# Extract all IP addresses from nested structure
jq '.. | strings | select(test("^\\d{1,3}(\\.\\d{1,3}){3}$"))' complex.json
```

**Variables and functions:**

```bash
# Define variable
jq --arg threshold "1000" '.[] | select(.bytes > ($threshold | tonumber))' logs.json

# Multiple variables
jq --arg user "admin" --arg ip "10.0.0.1" '.[] | select(.username == $user and .source_ip == $ip)' logs.json

# Define function
jq 'def double: . * 2; [1,2,3] | map(double)'
```

**Processing multiple files:**

```bash
# Combine multiple JSON files
jq -s 'add' file1.json file2.json file3.json

# Process each file separately but output as single array
jq -s '.' file1.json file2.json

# Merge objects from multiple files
jq -s '.[0] * .[1]' config1.json config2.json
```

**Streaming parser for large files:**

```bash
# Process large JSON without loading entire file
jq --stream 'select(length == 2) | .[0] |= join(".")' huge.json

# [Inference] - Streaming mode processes JSON incrementally, useful for files exceeding available RAM
```

### Common CTF Patterns

**Extract credentials from configuration:**

```bash
# Find password fields recursively
jq '.. | objects | select(has("password")) | {context: ., password: .password}' config.json

# Extract database connection strings
jq -r '.. | strings | select(test("mysql://|postgres://|mongodb://"))' app-config.json
```

**Timeline analysis:**

```bash
# Sort events by timestamp
jq 'sort_by(.timestamp)' events.json

# Filter time range (Unix epoch)
jq --arg start "1698345600" --arg end "1698349200" \
  '.[] | select(.timestamp >= ($start | tonumber) and .timestamp <= ($end | tonumber))' logs.json

# Convert epoch to human-readable
jq '.[] | .timestamp_readable = (.timestamp | strftime("%Y-%m-%d %H:%M:%S"))' logs.json
```

**Detect anomalies:**

```bash
# Find outliers (example: unusually large values)
jq '[.[].bytes] | add / length as $avg | $avg * 3 as $threshold | .. | select(.bytes? > $threshold)' logs.json

# Identify rare events (frequency analysis)
jq 'group_by(.event_type) | map({event: .[0].event_type, count: length}) | sort_by(.count)' events.json
```

---

## xmllint (XML Parser)

xmllint is part of the libxml2 library and provides validation, formatting, and XPath-based querying of XML documents. Critical for parsing SOAP responses, configuration files, and structured exports.

### Installation and Basic Usage

```bash
# Installation (usually pre-installed)
apt-get install libxml2-utils  # Debian/Ubuntu
dnf install libxml2             # Fedora/RHEL

# Verify installation
xmllint --version

# Basic syntax
xmllint [options] xmlfile
```

### Validation and Formatting

**Well-formedness check:**

```bash
# Check if XML is well-formed
xmllint --noout document.xml
# No output = valid; Error messages = invalid

# Display line numbers in errors
xmllint --noout --debug document.xml
```

**Pretty-printing:**

```bash
# Format XML with proper indentation
xmllint --format messy.xml

# Format and save to new file
xmllint --format input.xml --output formatted.xml

# Remove blank nodes
xmllint --format --noblanks document.xml
```

**Validation against DTD/Schema:**

```bash
# Validate against DTD
xmllint --valid --noout document.xml

# Validate against XML Schema (XSD)
xmllint --schema schema.xsd --noout document.xml

# Validate against RELAX NG schema
xmllint --relaxng schema.rng --noout document.xml
```

### XPath Querying

XPath is the primary mechanism for extracting data from XML documents. xmllint supports XPath 1.0.

**Basic XPath syntax:**

```bash
# Extract nodes matching XPath
xmllint --xpath 'expression' document.xml

# Example XML for following queries:
# <users>
#   <user id="1">
#     <name>alice</name>
#     <role>admin</role>
#   </user>
#   <user id="2">
#     <name>bob</name>
#     <role>user</role>
#   </user>
# </users>
```

**Node selection:**

```bash
# Select all user nodes
xmllint --xpath '//user' users.xml

# Select specific user by attribute
xmllint --xpath '//user[@id="1"]' users.xml

# Select nested element
xmllint --xpath '//user/name' users.xml

# Output: <name>alice</name><name>bob</name>

# Extract text content only
xmllint --xpath '//user/name/text()' users.xml
# Output: alicebob (concatenated)
```

**Attribute extraction:**

```bash
# Get attribute value
xmllint --xpath 'string(//user/@id)' users.xml

# Get all id attributes
xmllint --xpath '//user/@id' users.xml
```

**Predicates and filtering:**

```bash
# Select user where role is admin
xmllint --xpath '//user[role="admin"]/name/text()' users.xml

# Select first user
xmllint --xpath '//user[1]/name/text()' users.xml

# Select last user
xmllint --xpath '//user[last()]/name/text()' users.xml

# Select users with id > 1
xmllint --xpath '//user[@id>1]/name/text()' users.xml

# Combine conditions
xmllint --xpath '//user[@id="1" and role="admin"]/name/text()' users.xml
```

**XPath functions:**

```bash
# Count nodes
xmllint --xpath 'count(//user)' users.xml

# Check if node exists (returns boolean)
xmllint --xpath 'boolean(//user[@id="999"])' users.xml

# String concatenation
xmllint --xpath 'concat(//user[1]/name, " is ", //user[1]/role)' users.xml

# Contains function
xmllint --xpath '//user[contains(name, "ali")]/name/text()' users.xml

# Starts-with function
xmllint --xpath '//user[starts-with(name, "a")]/@id' users.xml
```

### Namespace Handling

**Extract data with namespaces:**

```bash
# Example XML with namespace:
# <root xmlns:app="http://example.com/app">
#   <app:user>alice</app:user>
# </root>

# Query with namespace (requires shell mode)
echo 'setns app=http://example.com/app
cat //app:user/text()' | xmllint --shell document.xml

# Alternative: use local-name() to ignore namespace
xmllint --xpath '//*[local-name()="user"]/text()' document.xml
```

### CTF-Specific Applications

**Parse SOAP responses:**

```bash
# Example SOAP response structure:
# <soap:Envelope xmlns:soap="http://schemas.xmlsoap.org/soap/envelope/">
#   <soap:Body>
#     <GetUserResponse>
#       <User>
#         <Username>admin</Username>
#         <Password>secret123</Password>
#       </User>
#     </GetUserResponse>
#   </soap:Body>
# </soap:Envelope>

# Extract credentials (ignoring namespace)
xmllint --xpath '//*[local-name()="Username"]/text()' response.xml
xmllint --xpath '//*[local-name()="Password"]/text()' response.xml
```

**Analyze configuration files:**

```bash
# Parse Nmap XML output
xmllint --xpath '//host[status/@state="up"]/address/@addr' scan.xml

# Extract open ports
xmllint --xpath '//port[@protocol="tcp"]/state[@state="open"]/../@portid' scan.xml

# Get service versions
xmllint --xpath '//port/service/@version' scan.xml

# Parse Android AndroidManifest.xml
xmllint --xpath '//manifest/@package' AndroidManifest.xml
xmllint --xpath '//uses-permission/@*[local-name()="name"]' AndroidManifest.xml
```

**Extract SVG embedded data:**

```bash
# SVG files can contain embedded scripts or data
xmllint --xpath '//script/text()' image.svg
xmllint --xpath '//metadata/text()' image.svg
```

**Parse web.config (IIS configuration):**

```bash
# Extract connection strings
xmllint --xpath '//connectionStrings/add/@connectionString' web.config

# Find authentication settings
xmllint --xpath '//authentication/@mode' web.config

# Check if customErrors is off (information disclosure)
xmllint --xpath 'string(//customErrors/@mode)' web.config
```

### Advanced Techniques

**Shell mode for complex queries:**

```bash
# Enter interactive shell
xmllint --shell document.xml

# Common shell commands:
# - cat XPATH : Display nodes matching XPath
# - cd XPATH : Change context to node
# - ls : List current context
# - pwd : Show current XPath location
# - quit : Exit shell

# Example shell session:
xmllint --shell users.xml << EOF
cd //user[1]
cat name/text()
quit
EOF
```

**XSLT transformation:**

```bash
# Apply XSLT stylesheet
xmllint --xinclude --xslt transform.xsl input.xml

# [Unverified] - xmllint supports XSLT 1.0; availability of specific XSLT features should be confirmed
```

**Combine with other tools:**

```bash
# Extract and process with grep
xmllint --xpath '//user/name/text()' users.xml | grep -o 'admin'

# Pretty-print and search
xmllint --format large.xml | grep -A 5 "<password>"

# Convert to line-based format for processing
xmllint --format users.xml | sed 's/></>\n</g' | grep '<name>'
```

### Common Patterns

**Batch extraction to JSON:**

```bash
# Manual conversion (simple cases)
echo "["
xmllint --xpath '//user' users.xml | \
sed 's/<user id="\([^"]*\)"><name>\([^<]*\)<\/name><role>\([^<]*\)<\/role><\/user>/{"id":"\1","name":"\2","role":"\3"},/g' | \
sed '$ s/,$//'
echo "]"

# [Inference] - For complex XML-to-JSON conversion, dedicated tools like xml2json may be more reliable
```

**Find all unique element names:**

```bash
# Useful for exploring unknown XML structures
xmllint --format document.xml | grep -oP '<\K[^/> ]+' | sort -u
```

**Extract comments (potential information disclosure):**

```bash
# XML comments format: <!-- comment -->
xmllint --format document.xml | grep -oP '<!--\K.*(?=-->)'
```

---

## csvkit

csvkit is a suite of command-line tools for converting and working with CSV files. Essential for analyzing exported data, processing tabular logs, and manipulating delimited datasets.

### Installation

```bash
# Install via pip (Python 3)
pip install csvkit

# Verify installation
csvstat --version

# Core tools included:
# - in2csv : Convert Excel, JSON, etc. to CSV
# - csvclean : Fix common CSV errors
# - csvcut : Select columns
# - csvgrep : Filter rows
# - csvjoin : Join multiple CSVs
# - csvlook : Render CSV as table
# - csvsort : Sort CSV data
# - csvstat : Generate statistics
# - csvsql : Execute SQL queries on CSV
# - csvformat : Reformat CSV (delimiters, quotes)
# - csvstack : Merge multiple CSVs
```

### in2csv - Format Conversion

**Convert Excel to CSV:**

```bash
# Convert first sheet
in2csv data.xlsx > output.csv

# Convert specific sheet by name
in2csv --sheet "Users" workbook.xlsx > users.csv

# Convert specific sheet by index (0-based)
in2csv --sheet 0 workbook.xlsx > first_sheet.csv

# List available sheets
in2csv --names workbook.xlsx
```

**Convert JSON to CSV:**

```bash
# Simple JSON array
in2csv data.json > output.csv

# JSON with nested objects (flattens structure)
in2csv --key "results" nested.json > output.csv
```

**Convert other formats:**

```bash
# ndjson (newline-delimited JSON)
in2csv --format ndjson data.ndjson > output.csv

# DBF (dBase database)
in2csv data.dbf > output.csv
```

### csvcut - Column Selection

**Select specific columns:**

```bash
# By column name
csvcut -c username,ip_address logs.csv

# By column number (1-indexed)
csvcut -c 1,3,5 data.csv

# Exclude columns
csvcut -C 4,5 data.csv  # Remove columns 4 and 5

# Reorder columns
csvcut -c email,username,uid users.csv
```

**Column inspection:**

```bash
# List column names with indices
csvcut -n data.csv

# Example output:
# 1: username
# 2: email
# 3: last_login
```

### csvgrep - Row Filtering

**Pattern matching:**

```bash
# Filter rows where column contains string
csvgrep -c username -m "admin" users.csv

# Multiple patterns (OR logic)
csvgrep -c status -m "failed" -m "error" logs.csv

# Regex matching
csvgrep -c email -r ".*@example\.com$" users.csv

# Invert match (exclude rows)
csvgrep -c status -m "success" -i logs.csv
```

**Numeric filtering:**

```bash
# Not directly supported; use csvsql for complex filtering
csvsql --query "SELECT * FROM stdin WHERE port > 1024" connections.csv
```

### csvjoin - Merging Data

**Join CSVs on common column:**

```bash
# Inner join (default)
csvjoin -c user_id users.csv actions.csv

# Left outer join
csvjoin --left -c user_id users.csv actions.csv

# Join on different column names
csvjoin --left -c "users.id,actions.user_id" users.csv actions.csv

# Specify columns to include
csvjoin -c user_id users.csv actions.csv | csvcut -c username,action,timestamp
```

### csvsort - Sorting

**Sort by columns:**

```bash
# Sort by single column (ascending)
csvsort -c timestamp logs.csv

# Sort descending
csvsort -r -c bytes_transferred traffic.csv

# Multi-column sort
csvsort -c country,city,population cities.csv

# Numeric sort (auto-detected, but can force)
csvsort -c port connections.csv
```

### csvstat - Statistical Analysis

**Generate summary statistics:**

```bash
# Full statistics for all columns
csvstat data.csv

# Statistics for specific columns
csvstat -c bytes_sent,bytes_received traffic.csv

# Output includes:
# - Type inference
# - Nulls count
# - Unique values
# - Min/max
# - Mean, median, stdev (numeric)
# - Most common values

# Example output:
# Column: bytes_sent
# Type: Number
# Nulls: 0
# Min: 64
# Max: 1048576
# Mean: 52341.23
# Median: 8192
```

**Specific statistics:**

```bash
# Count rows
csvstat --count data.csv

# Only show column names and types
csvstat --names data.csv
```

### csvsql - SQL Queries

**Query CSV with SQL:**

```bash
# Basic SELECT
csvsql --query "SELECT username, ip FROM stdin WHERE failed_attempts > 3" auth.csv

# Aggregate functions
csvsql --query "SELECT country, COUNT(*) as count FROM stdin GROUP BY country ORDER BY count DESC" users.csv

# Join multiple files with SQL
csvsql --query "SELECT u.username, a.action FROM users u JOIN actions a ON u.id = a.user_id" users.csv actions.csv

# Use SQLite syntax for complex queries
csvsql --query "SELECT * FROM stdin WHERE datetime(timestamp) > datetime('2024-01-01')" logs.csv
```

**Insert into database:**

```bash
# Create table in SQLite
csvsql --db sqlite:///database.db --insert data.csv

# Specify table name
csvsql --db sqlite:///database.db --insert --table custom_name data.csv

# PostgreSQL
csvsql --db postgresql://user:pass@localhost/dbname --insert data.csv
```

### csvlook - Pretty Display

**Render as formatted table:**

```bash
# Basic display (uses table formatting)
csvlook data.csv

# Limit rows displayed
csvlook data.csv | head -20

# Combine with other tools
csvgrep -c status -m "failed" logs.csv | csvlook
```

### csvclean - Data Cleaning

**Fix common CSV issues:**

```bash
# Remove malformed rows
csvclean problematic.csv
# Outputs: problematic_out.csv (clean data)
#          problematic_err.csv (removed rows)

# Check what would be removed
csvclean -n problematic.csv
```

### csvstack - Vertical Merge

**Combine multiple CSVs:**

```bash
# Stack CSVs with same columns
csvstack file1.csv file2.csv file3.csv > combined.csv

# Add grouping column to identify source
csvstack -g "Jan","Feb","Mar" jan.csv feb.csv mar.csv > quarterly.csv

# Stack with different column orders (aligns by name)
csvstack users_a.csv users_b.csv
```

### csvformat - Format Conversion

**Change delimiters:**

```bash
# Convert to tab-separated
csvformat -T data.csv

# Use custom delimiter
csvformat -D "|" data.csv

# Convert to Unix line endings
csvformat -U data.csv
```

**Change quoting style:**

```bash
# Quote all fields
csvformat -Q data.csv

# Minimal quoting (only when necessary)
csvformat data.csv
```

### CTF-Specific Use Cases

**Analyze exported user databases:**

```bash
# Find administrators
csvgrep -c role -m "admin" users.csv | csvlook

# Extract credentials
csvcut -c username,password_hash users.csv > creds.txt

# Count users by role
csvsql --query "SELECT role, COUNT(*) as count FROM stdin GROUP BY role" users.csv | csvlook

# Find weak passwords (assuming exported plaintext)
csvgrep -c password -r "^[0-9]{4,8}$" users.csv
```

**Process network connection logs:**

```bash
# Top talkers by bytes transferred
csvsql --query "SELECT src_ip, SUM(bytes) as total FROM stdin GROUP BY src_ip ORDER BY total DESC LIMIT 10" netflow.csv | csvlook

# Find connections to suspicious ports
csvgrep -c dst_port -r "^(4444|31337|1337)$" connections.csv

# Timeline of connections
csvsort -c timestamp connections.csv | csvcut -c timestamp,src_ip,dst_ip,dst_port | csvlook
```

**Correlate multiple log sources:**

```bash
# Join authentication and access logs
csvjoin -c timestamp auth.csv access.csv | \
  csvsql --query "SELECT * FROM stdin WHERE auth_status='failed' AND access_granted='true'"

# [Inference] - This pattern reveals privilege escalation where authentication failed but access was granted
```

**Data exfiltration detection:**

```bash
# Find large outbound transfers
csvgrep -c direction -m "outbound" traffic.csv | \
  csvsql --query "SELECT src_ip, dst_ip, SUM(bytes) as total FROM stdin GROUP BY src_ip, dst_ip HAVING total > 1000000000" | \
  csvlook

# Statistical outlier detection
csvstat -c bytes_transferred traffic.csv
# Use mean + 3*stdev as threshold for anomalies
```

**Convert Excel artifacts:**

```bash
# Extract and analyze Excel spreadsheet found on target
in2csv suspicious.xlsx | csvlook

# Search for sensitive data patterns
in2csv documents.xlsx | csvgrep -c -r "(?i)(password|credential|secret|api.?key)"

# Extract specific worksheet
in2csv --sheet "Configuration" app_backup.xlsx | csvcut -c setting,value | csvlook
```

### Advanced Patterns

**Chaining csvkit tools:**

```bash
# Complex pipeline example
in2csv data.xlsx | \
  csvcut -c username,email,last_login | \
  csvgrep -c last_login -r "2024-10" | \
  csvsort -r -c last_login | \
  csvlook | \
  head -20

# Multi-step analysis
csvgrep -c status -m "failed" auth.csv | \
  csvsql --query "SELECT username, COUNT(*) as attempts FROM stdin GROUP BY username HAVING attempts > 5" | \
  csvjoin -c username - users.csv | \
  csvcut -c username,email,attempts | \
  csvlook
```

**Handling large files:**

```bash
# csvkit tools stream data, efficient for large files
# Process multi-GB CSV without loading into memory

# Sample large file
head -10000 huge.csv | csvstat

# Filter then process
csvgrep -c important_field -m "target_value" huge.csv | csvstat
```

**Date/time processing:**

```bash
# csvsql supports SQLite date functions
csvsql --query "SELECT *, date(timestamp) as date FROM stdin" logs.csv | \
  csvsql --query "SELECT date, COUNT(*) FROM stdin GROUP BY date" | \
  csvlook

# Filter by date range
csvsql --query "SELECT * FROM stdin WHERE date(timestamp) BETWEEN '2024-10-01' AND '2024-10-31'" logs.csv
```

### Integration with Other Tools

**Combine with jq for JSON output:**

```bash
# Convert CSV to JSON
csvjson data.csv > output.json

# Then process with jq
csvjson users.csv | jq '.[] | select(.role == "admin")'
```

**Pipe to visualization tools:**

```bash
# Generate data for gnuplot
csvsql --query "SELECT timestamp, value FROM stdin ORDER BY timestamp" metrics.csv > plot_data.csv

# Create simple ASCII chart (requires external tools like spark)
csvcut -c response_time logs.csv | tail -n +2 | spark
```

**Database integration:**

```bash
# Import CSV into PostgreSQL for complex analysis
csvsql --db postgresql://localhost/ctfdb --insert --create-if-not-exists large_dataset.csv

# Then query with full SQL capabilities
psql ctfdb -c "SELECT src_ip, COUNT(_) FROM large_dataset WHERE port IN (22, 3389) GROUP BY src_ip HAVING COUNT(_) > 100;"

# Export query results back to CSV
psql ctfdb -c "COPY (SELECT * FROM large_dataset WHERE suspicious = true) TO STDOUT CSV HEADER" > suspicious_entries.csv

````

**Combine with awk for performance:**
```bash
# csvkit is Python-based and can be slow on massive files
# For simple filtering, awk may be faster

# csvkit approach:
csvgrep -c 3 -m "admin" huge.csv

# Equivalent awk (faster for simple patterns):
awk -F',' '$3 == "admin"' huge.csv

# Best practice: Use awk for simple filtering, csvkit for complex operations
awk -F',' '$5 > 10000' huge.csv | csvsql --query "SELECT col1, col2, SUM(col5) FROM stdin GROUP BY col1, col2"
````

### Error Handling and Data Quality

**Detect encoding issues:**

```bash
# Check file encoding
file -i data.csv

# Convert encoding if needed
iconv -f ISO-8859-1 -t UTF-8 data.csv > data_utf8.csv

# csvkit assumes UTF-8; specify encoding if different
csvcut -c 1,2 -e iso-8859-1 data.csv
```

**Handle missing values:**

```bash
# csvstat shows null counts
csvstat -c important_column data.csv

# Filter out rows with nulls
csvsql --query "SELECT * FROM stdin WHERE column_name IS NOT NULL" data.csv

# Replace nulls with default value
csvsql --query "SELECT COALESCE(column_name, 'UNKNOWN') as column_name FROM stdin" data.csv
```

**Fix delimiter detection issues:**

```bash
# Force specific delimiter (semicolon)
csvformat -d ";" data.csv | csvlook

# Convert tabs to commas
csvformat -t data.tsv > data.csv

# Handle pipe-delimited files
csvformat -D "|" data.txt | csvcut -c 1,2,3
```

**Validate data integrity:**

```bash
# Check for duplicate rows
csvstat --unique data.csv

# Find duplicate primary keys
csvsql --query "SELECT id, COUNT(*) as count FROM stdin GROUP BY id HAVING count > 1" data.csv

# Verify expected row count
ROW_COUNT=$(csvstat --count data.csv)
EXPECTED=1000
if [ "$ROW_COUNT" -ne "$EXPECTED" ]; then
    echo "Data integrity issue: expected $EXPECTED rows, got $ROW_COUNT"
fi
```

### Performance Optimization

**Speed up large file processing:**

```bash
# Use --no-inference to skip type detection (faster but less accurate)
csvcut --no-inference -c 1,2,3 huge.csv

# Process in chunks with split
split -l 100000 huge.csv chunk_
for chunk in chunk_*; do
    csvgrep -c status -m "failed" "$chunk" >> results.csv
done

# Parallel processing with GNU parallel
cat huge.csv | parallel --pipe --block 10M 'csvgrep -c status -m "failed"' > results.csv
```

**Memory-efficient operations:**

```bash
# Stream processing without loading entire file
csvcut -c 1,2,3 huge.csv | csvgrep -c 2 -m "target" | csvsort -c 1 > output.csv

# Avoid csvstat on extremely large files (loads into memory for calculations)
# Use database approach instead:
csvsql --db sqlite:///temp.db --insert huge.csv
sqlite3 temp.db "SELECT AVG(column_name) FROM huge;"
```

### CTF Competition Patterns

**Rapid data reconnaissance:**

```bash
#!/bin/bash
# Quick CSV analysis script for CTF

FILE=$1

echo "=== File Preview ==="
head -5 "$FILE" | csvlook

echo -e "\n=== Column Structure ==="
csvcut -n "$FILE"

echo -e "\n=== Row Count ==="
csvstat --count "$FILE"

echo -e "\n=== Statistics ==="
csvstat "$FILE"

echo -e "\n=== Unique Values (First Column) ==="
csvcut -c 1 "$FILE" | tail -n +2 | sort -u | head -20
```

**Extract password patterns:**

```bash
# Find credentials in CSV exports
csvcut -c password,hash,pwd,pass,secret data.csv 2>/dev/null || \
csvcut -n data.csv | grep -iE "(pass|pwd|secret|key|cred)" | \
awk '{print $2}' | xargs -I {} csvcut -c {} data.csv | head -20

# Analyze password complexity
csvcut -c password users.csv | tail -n +2 | while read pass; do
    echo "$pass" | grep -q '[[:upper:]]' && UPPER=1 || UPPER=0
    echo "$pass" | grep -q '[[:lower:]]' && LOWER=1 || LOWER=0
    echo "$pass" | grep -q '[[:digit:]]' && DIGIT=1 || DIGIT=0
    echo "$pass" | grep -q '[^[:alnum:]]' && SPECIAL=1 || SPECIAL=0
    LEN=${#pass}
    echo "$pass,$LEN,$UPPER,$LOWER,$DIGIT,$SPECIAL"
done | csvsql --query "SELECT AVG(col2) as avg_length, SUM(col3) as has_upper, SUM(col5) as has_digit FROM stdin"
```

**Correlate timestamps across logs:**

```bash
# Normalize timestamps to common format
csvformat auth.csv | csvsql --query "SELECT datetime(timestamp) as ts, * FROM stdin" > auth_normalized.csv
csvformat access.csv | csvsql --query "SELECT datetime(timestamp) as ts, * FROM stdin" > access_normalized.csv

# Join on timestamp window (within 5 seconds)
csvsql --query "
SELECT a.username, a.ip, b.resource 
FROM auth_normalized a 
JOIN access_normalized b 
ON ABS(julianday(a.ts) - julianday(b.ts)) * 86400 < 5
WHERE a.status = 'failed' AND b.status = 'success'
" auth_normalized.csv access_normalized.csv
```

**Pivot table analysis:**

```bash
# Create pivot table (users by country)
csvsql --query "
SELECT country,
    SUM(CASE WHEN role='admin' THEN 1 ELSE 0 END) as admins,
    SUM(CASE WHEN role='user' THEN 1 ELSE 0 END) as users,
    SUM(CASE WHEN role='guest' THEN 1 ELSE 0 END) as guests
FROM stdin
GROUP BY country
ORDER BY admins DESC
" users.csv | csvlook

# Time-based aggregation
csvsql --query "
SELECT strftime('%Y-%m-%d', timestamp) as date,
    strftime('%H', timestamp) as hour,
    COUNT(*) as events
FROM stdin
GROUP BY date, hour
ORDER BY date, hour
" events.csv | csvlook
```

### Forensic Analysis Workflows

**User activity timeline:**

```bash
# Merge multiple activity logs
csvstack -g "login","file_access","network" logins.csv files.csv network.csv > all_activity.csv

# Sort by timestamp and filter by user
csvgrep -c username -m "suspicious_user" all_activity.csv | \
    csvsort -c timestamp | \
    csvcut -c timestamp,activity_type,details | \
    csvlook

# Generate activity heatmap data
csvsql --query "
SELECT strftime('%H', timestamp) as hour,
    strftime('%w', timestamp) as day_of_week,
    COUNT(*) as activity_count
FROM stdin
WHERE username = 'suspicious_user'
GROUP BY hour, day_of_week
" all_activity.csv | csvlook
```

**Network flow analysis:**

```bash
# Identify top conversations
csvsql --query "
SELECT src_ip || ' -> ' || dst_ip as conversation,
    COUNT(*) as connections,
    SUM(bytes) as total_bytes
FROM stdin
GROUP BY src_ip, dst_ip
ORDER BY total_bytes DESC
LIMIT 20
" netflow.csv | csvlook

# Detect port scanning
csvsql --query "
SELECT src_ip,
    COUNT(DISTINCT dst_port) as unique_ports,
    COUNT(*) as total_attempts,
    GROUP_CONCAT(DISTINCT dst_port) as ports
FROM stdin
GROUP BY src_ip
HAVING unique_ports > 20
ORDER BY unique_ports DESC
" connections.csv | csvlook
```

**Anomaly detection via statistics:**

```bash
# Calculate z-scores for numeric columns
csvsql --query "
WITH stats AS (
    SELECT AVG(response_time) as mean,
        (AVG(response_time * response_time) - AVG(response_time) * AVG(response_time)) as variance
    FROM stdin
)
SELECT timestamp, url, response_time,
    (response_time - stats.mean) / SQRT(stats.variance) as z_score
FROM stdin, stats
ORDER BY ABS(z_score) DESC
LIMIT 20
" web_logs.csv | csvlook

# [Inference] - Z-scores above 3 or below -3 typically indicate statistical anomalies
```

### Integration with Scripting

**Python script using csvkit programmatically:**

```python
#!/usr/bin/env python3
import subprocess
import json

def csv_to_dict(csv_file, filter_column=None, filter_value=None):
    """Convert CSV to list of dictionaries with optional filtering"""
    cmd = ['csvjson', csv_file]
    if filter_column and filter_value:
        grep_cmd = ['csvgrep', '-c', filter_column, '-m', filter_value, csv_file]
        grep_result = subprocess.run(grep_cmd, capture_output=True, text=True)
        cmd = ['csvjson']
        result = subprocess.run(cmd, input=grep_result.stdout, 
                              capture_output=True, text=True)
    else:
        result = subprocess.run(cmd, capture_output=True, text=True)
    
    return json.loads(result.stdout)

def query_csv(csv_file, sql_query):
    """Execute SQL query on CSV file"""
    cmd = ['csvsql', '--query', sql_query, csv_file]
    result = subprocess.run(cmd, capture_output=True, text=True)
    return result.stdout

# Usage example
users = csv_to_dict('users.csv', 'role', 'admin')
for user in users:
    print(f"Admin: {user['username']} ({user['email']})")

# Complex query
suspicious = query_csv('access.csv', 
    "SELECT ip, COUNT(*) as attempts FROM stdin WHERE status=401 GROUP BY ip HAVING attempts > 10")
print(suspicious)
```

**Bash automation for CTF:**

```bash
#!/bin/bash
# Automated CSV log analysis for CTF

analyze_csv() {
    local file=$1
    local output_dir="analysis_$(basename $file .csv)"
    
    mkdir -p "$output_dir"
    
    # Basic info
    echo "Analyzing $file..."
    csvstat "$file" > "$output_dir/statistics.txt"
    
    # Extract potential credentials
    csvcut -n "$file" | grep -iE "(user|pass|email|key)" | \
        awk '{print $2}' | xargs -I {} csvcut -c {} "$file" > "$output_dir/credentials.csv"
    
    # Find anomalies (assuming numeric column 'value')
    if csvcut -n "$file" | grep -q "value"; then
        MEAN=$(csvstat -c value "$file" | grep "Mean:" | awk '{print $2}')
        STDEV=$(csvstat -c value "$file" | grep "StDev:" | awk '{print $2}')
        THRESHOLD=$(echo "$MEAN + 3 * $STDEV" | bc)
        
        csvsql --query "SELECT * FROM stdin WHERE value > $THRESHOLD" "$file" > "$output_dir/anomalies.csv"
    fi
    
    # Timeline if timestamp column exists
    if csvcut -n "$file" | grep -iq "timestamp\|time\|date"; then
        TIMECOL=$(csvcut -n "$file" | grep -iE "timestamp|time|date" | head -1 | awk '{print $2}')
        csvsort -c "$TIMECOL" "$file" > "$output_dir/timeline.csv"
    fi
    
    echo "Analysis complete. Results in $output_dir/"
}

# Process all CSV files in current directory
for csv in *.csv; do
    [ -f "$csv" ] && analyze_csv "$csv"
done
```

### Common Pitfalls and Solutions

**Issue: Header row confusion**

```bash
# Problem: First data row treated as header
csvlook data.csv  # Shows wrong column names

# Solution: Add header if missing
echo "col1,col2,col3" > temp.csv
cat data.csv >> temp.csv
csvlook temp.csv

# Or skip header with tail
tail -n +2 data.csv | csvlook --no-header-row
```

**Issue: Quote handling problems**

```bash
# Problem: Embedded quotes breaking parsing
# Data: username,"O"Reilly",email

# Solution: Use csvclean to fix
csvclean problematic.csv
csvlook problematic_out.csv

# Or specify quote character
csvformat -q '"' data.csv
```

**Issue: Mixed delimiters**

```bash
# Problem: File uses both commas and tabs

# Solution: Normalize to single delimiter
csvformat -D "," mixed.csv > normalized.csv

# Or convert tabs first
expand -t 4 file_with_tabs.txt | tr '\t' ',' > converted.csv
```

**Issue: Unicode and special characters**

```bash
# Problem: csvkit crashes on non-UTF8 data

# Solution: Convert encoding first
iconv -f LATIN1 -t UTF-8 data.csv | csvlook

# Remove non-ASCII characters if necessary
iconv -f UTF-8 -t ASCII//TRANSLIT data.csv | csvlook
```

### Cross-Tool Workflows

**CSV  JSON  XML pipeline:**

```bash
# CSV to JSON
csvjson users.csv > users.json

# JSON to XML (requires external tool like yq or python)
python3 << 'EOF'
import json
import xml.etree.ElementTree as ET

with open('users.json') as f:
    data = json.load(f)

root = ET.Element('users')
for user in data:
    user_el = ET.SubElement(root, 'user')
    for key, val in user.items():
        child = ET.SubElement(user_el, key)
        child.text = str(val)

tree = ET.ElementTree(root)
tree.write('users.xml', encoding='utf-8', xml_declaration=True)
EOF

# Verify with xmllint
xmllint --format users.xml
```

**Combined parsing scenario:**

```bash
# Scenario: JSON config contains CSV filename, process that CSV
CONFIG_FILE="config.json"
CSV_FILE=$(jq -r '.log_file' "$CONFIG_FILE")

if [ -f "$CSV_FILE" ]; then
    echo "Processing $CSV_FILE from config..."
    
    # Extract filter criteria from JSON
    FILTER_COL=$(jq -r '.filter.column' "$CONFIG_FILE")
    FILTER_VAL=$(jq -r '.filter.value' "$CONFIG_FILE")
    
    # Apply filter and analyze
    csvgrep -c "$FILTER_COL" -m "$FILTER_VAL" "$CSV_FILE" | csvstat
fi
```

**Multi-format log aggregation:**

```bash
#!/bin/bash
# Aggregate logs from different formats into single CSV

OUTPUT="aggregated_logs.csv"
echo "timestamp,source,level,message" > "$OUTPUT"

# Process JSON logs
jq -r '.[] | [.timestamp, "json_source", .level, .message] | @csv' logs.json >> "$OUTPUT"

# Process XML logs
xmllint --xpath '//log' logs.xml | \
    sed -n 's/.*<timestamp>\(.*\)<\/timestamp>.*<level>\(.*\)<\/level>.*<message>\(.*\)<\/message>.*/\1,xml_source,\2,\3/p' >> "$OUTPUT"

# Process existing CSV logs
tail -n +2 logs.csv | sed 's/^/csv_source,/' >> "$OUTPUT"

# Analyze aggregated data
csvstat "$OUTPUT"
csvsort -c timestamp "$OUTPUT" | csvlook | less
```

---

## Important Related Topics

For comprehensive CTF log parsing and data analysis, consider exploring these interconnected areas:

- **Regular Expressions (grep, sed, awk)** - Fundamental for pattern matching and text extraction before/after structured parsing
- **Python Data Analysis (pandas, numpy)** - More powerful alternative to csvkit for complex data manipulation and statistical analysis
- **SQL and Database Systems** - Essential for querying large datasets efficiently; csvkit provides gateway but full SQL knowledge enables advanced analysis
- **Log Aggregation Platforms (ELK Stack, Splunk)** - Enterprise-scale solutions for parsing and correlating diverse log formats in real-time
- **Data Visualization (matplotlib, gnuplot, D3.js)** - Converting parsed data into visual insights for pattern recognition
- **Binary Data Analysis (xxd, hexdump, binwalk)** - Complementary skills for parsing non-text formats often encountered in CTFs

---

## awk advanced patterns

### Overview

awk is a pattern-scanning and text-processing language particularly suited for structured log analysis. It processes input line-by-line, splitting each line into fields automatically, making it ideal for CTF log parsing where you need to extract, filter, and transform data rapidly.

### Core Syntax & Execution Model

**Basic structure:**

```bash
awk 'BEGIN {initialization} pattern {action} END {finalization}' file

# Execution order:
# 1. BEGIN block executes once before processing
# 2. Pattern-action pairs execute for each line
# 3. END block executes once after all lines processed
```

**Built-in variables:**

- `$0` - Entire current line
- `$1, $2, $3, ..., $NF` - Individual fields (1-indexed)
- `NF` - Number of fields in current line
- `NR` - Current line number (global across all files)
- `FNR` - Current line number (resets per file)
- `FS` - Input field separator (default: whitespace)
- `OFS` - Output field separator (default: space)
- `RS` - Input record separator (default: newline)
- `ORS` - Output record separator (default: newline)
- `FILENAME` - Current input filename

### Field Manipulation & Extraction

**Basic field operations:**

```bash
# Print specific fields from auth.log
awk '{print $1, $2, $3, $11}' /var/log/auth.log

# Print last field
awk '{print $NF}' /var/log/syslog

# Print all except first two fields
awk '{$1=$2=""; print $0}' /var/log/syslog

# Swap field order
awk '{print $3, $2, $1}' file

# Print with custom separator
awk 'BEGIN{OFS="|"} {print $1, $5, $9}' /var/log/apache2/access.log
```

**Dynamic field selection:**

```bash
# Extract IP from variable position (searches for SRC= pattern)
awk '{for(i=1;i<=NF;i++) if($i~/^SRC=/) print $i}' /var/log/kern.log

# Print fields matching regex
awk '{for(i=1;i<=NF;i++) if($i~/DPT=[0-9]+/) print $i}' /var/log/ufw.log

# Extract key=value pairs
awk '{for(i=1;i<=NF;i++) if($i~/^(SRC|DST|DPT)=/) print $i}' /var/log/kern.log
```

### Pattern Matching & Filtering

**Regex patterns:**

```bash
# Lines matching regex
awk '/Failed password/ {print}' /var/log/auth.log

# Lines NOT matching regex
awk '!/kernel/ {print}' /var/log/syslog

# Multiple patterns (OR logic)
awk '/Failed password|Invalid user/ {print}' /var/log/auth.log

# Pattern with field condition
awk '/Failed/ && $11~/^[0-9]/ {print $11}' /var/log/auth.log

# Case-insensitive matching
awk 'tolower($0) ~ /error/ {print}' /var/log/syslog
```

**Field-based conditions:**

```bash
# Numeric comparisons
awk '$10 > 1000 {print}' /var/log/apache2/access.log  # Response size > 1000
awk '$9 >= 400 && $9 < 500 {print}' /var/log/apache2/access.log  # 4xx errors

# String comparisons
awk '$6 == "22" {print}' formatted_firewall.log  # Exact match
awk '$6 != "ESTABLISHED" {print}' connection.log  # Not equal

# Field existence check
awk 'NF > 10 {print}' /var/log/syslog  # Lines with more than 10 fields

# Multiple conditions (AND)
awk '$1=="Oct" && $2=="28" && /BLOCK/ {print}' /var/log/ufw.log

# Multiple conditions (OR)
awk '$9==404 || $9==403 {print}' /var/log/apache2/access.log
```

**Range patterns:**

```bash
# Process lines between two patterns
awk '/START/,/END/ {print}' /var/log/syslog

# From pattern to end of file
awk '/ERROR/,0 {print}' /var/log/application.log

# Line number ranges
awk 'NR==10,NR==20 {print}' /var/log/syslog  # Lines 10-20

# Date range (assuming date in first fields)
awk '/Oct 28 14:/,/Oct 28 15:/ {print}' /var/log/syslog
```

### Associative Arrays for Aggregation

**Counting and frequency analysis:**

```bash
# Count failed login attempts per IP
awk '/Failed password/ {ip=$(NF-3); count[ip]++} END {for(i in count) print i, count[i]}' /var/log/auth.log

# Count HTTP status codes
awk '{status[$9]++} END {for(s in status) print s, status[s]}' /var/log/apache2/access.log

# Count unique values with sorting
awk '/DPT=/ {for(i=1;i<=NF;i++) if($i~/DPT=/) {split($i,a,"="); ports[a[2]]++}} END {for(p in ports) print ports[p], p}' /var/log/ufw.log | sort -rn

# Top 10 attacking IPs
awk '/BLOCK/ {for(i=1;i<=NF;i++) if($i~/^SRC=/) {split($i,a,"="); ips[a[2]]++}} END {for(ip in ips) print ips[ip], ip}' /var/log/kern.log | sort -rn | head -10
```

**Multi-dimensional arrays:**

```bash
# Track attempts per IP per hour
awk '/Failed password/ {
    hour = substr($3, 1, 2)
    ip = $(NF-3)
    attacks[hour][ip]++
}
END {
    for(h in attacks) {
        print "Hour:", h
        for(i in attacks[h]) {
            print "  ", i, attacks[h][i]
        }
    }
}' /var/log/auth.log

# Source IP to destination port mapping
awk '{
    for(i=1;i<=NF;i++) {
        if($i~/SRC=/) split($i,src,"=")
        if($i~/DPT=/) split($i,dpt,"=")
    }
    if(src[2] && dpt[2]) map[src[2]][dpt[2]]++
}
END {
    for(ip in map) {
        print ip":"
        for(port in map[ip]) print "  port", port":", map[ip][port]
    }
}' /var/log/kern.log
```

**Statistical calculations:**

```bash
# Calculate average, min, max of response sizes
awk '{
    size=$10
    if(size ~ /^[0-9]+$/) {
        sum+=size
        count++
        if(size>max || max=="") max=size
        if(size<min || min=="") min=size
    }
}
END {
    print "Count:", count
    print "Sum:", sum
    print "Average:", sum/count
    print "Min:", min
    print "Max:", max
}' /var/log/apache2/access.log

# Standard deviation calculation
awk '{
    values[NR]=$10
    sum+=$10
}
END {
    avg=sum/NR
    for(i in values) {
        diff=values[i]-avg
        sq_diff+=diff*diff
    }
    variance=sq_diff/NR
    stddev=sqrt(variance)
    print "Avg:", avg, "StdDev:", stddev
}' response_times.log
```

### Advanced String Processing

**String functions:**

```bash
# Extract substring
awk '{print substr($3, 1, 2)}' /var/log/syslog  # First 2 chars of field 3

# String length
awk '{print length($0)}' /var/log/syslog  # Line length

# String replacement
awk '{gsub(/Failed/, "FAIL"); print}' /var/log/auth.log  # Global substitution
awk '{sub(/Failed/, "FAIL"); print}' /var/log/auth.log   # First occurrence only

# Split string into array
awk '{
    split($11, parts, ":")  # Split field 11 by colon
    print parts[1], parts[2]
}' /var/log/auth.log

# Case conversion
awk '{print toupper($0)}' file  # Uppercase
awk '{print tolower($0)}' file  # Lowercase

# String concatenation
awk '{combined=$1 "-" $2 "-" $3; print combined}' /var/log/syslog
```

**Regular expression functions:**

```bash
# Match and extract with regex
awk 'match($0, /SRC=([0-9.]+)/, arr) {print arr[1]}' /var/log/kern.log

# Test if field matches pattern
awk '{if($9 ~ /^[45]/) print "Error:", $0}' /var/log/apache2/access.log

# Extract IP addresses
awk '{
    if(match($0, /[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+/)) {
        ip=substr($0, RSTART, RLENGTH)
        print ip
    }
}' /var/log/syslog

# Multiple captures
awk 'match($0, /SRC=([0-9.]+).*DPT=([0-9]+)/, a) {print a[1], a[2]}' /var/log/kern.log
```

### Control Flow & Logic

**Conditional statements:**

```bash
# If-else logic
awk '{
    if($9 >= 500) 
        print "Server error:", $0
    else if($9 >= 400)
        print "Client error:", $0
    else if($9 >= 200)
        print "Success:", $0
}' /var/log/apache2/access.log

# Ternary operator
awk '{status = ($9>=400) ? "ERROR" : "OK"; print status, $0}' /var/log/apache2/access.log

# Multiple conditions
awk '{
    if($1=="Oct" && $2>=28 && /Failed/) {
        print "Recent attack:", $0
    }
}' /var/log/auth.log
```

**Loops:**

```bash
# For loop through fields
awk '{
    for(i=1; i<=NF; i++) {
        if($i ~ /^DPT=/) {
            print "Found port at field", i":", $i
        }
    }
}' /var/log/ufw.log

# While loop
awk '{
    i=1
    while(i<=NF) {
        if($i ~/error/i) print "Error in field", i
        i++
    }
}' /var/log/syslog

# Do-while loop
awk '{
    i=NF
    do {
        print $i
        i--
    } while(i>0)
}' file  # Print fields in reverse
```

### Time & Date Processing

**Parse and filter by time:**

```bash
# Extract hour from timestamp
awk '{split($3, time, ":"); print time[1]}' /var/log/syslog

# Filter by time range
awk '{
    split($3, t, ":")
    hour=t[1]
    if(hour>=14 && hour<=16) print
}' /var/log/syslog

# Convert to epoch time (requires mktime)
awk '{
    # Assuming format: Oct 28 14:30:45
    months["Jan"]=1; months["Feb"]=2; months["Mar"]=3; months["Apr"]=4
    months["May"]=5; months["Jun"]=6; months["Jul"]=7; months["Aug"]=8
    months["Sep"]=9; months["Oct"]=10; months["Nov"]=11; months["Dec"]=12
    
    month=months[$1]
    day=$2
    split($3, t, ":")
    
    # mktime format: YYYY MM DD HH MM SS
    timestamp=mktime("2025 " month " " day " " t[1] " " t[2] " " t[3])
    print timestamp, $0
}' /var/log/syslog

# Calculate time differences
awk '{
    split($3, t, ":")
    current_sec = t[1]*3600 + t[2]*60 + t[3]
    if(NR>1) {
        diff = current_sec - prev_sec
        print "Time diff:", diff, "seconds"
    }
    prev_sec = current_sec
}' /var/log/syslog
```

**Aggregate by time windows:**

```bash
# Count events per minute
awk '{
    time=substr($3, 1, 5)  # Extract HH:MM
    events[time]++
}
END {
    for(t in events) print t, events[t]
}' /var/log/syslog | sort

# Events per hour with date
awk '{
    timestamp=$1" "$2" "substr($3,1,2)":00"
    hourly[timestamp]++
}
END {
    for(h in hourly) print h, hourly[h]
}' /var/log/syslog | sort
```

### Multi-File Processing

**Compare multiple files:**

```bash
# Process multiple files with filename tracking
awk '{print FILENAME, NR, FNR, $0}' /var/log/auth.log /var/log/syslog

# File-specific actions
awk 'FILENAME=="/var/log/auth.log" {auth[$NF]++}
     FILENAME=="/var/log/fail2ban.log" {banned[$NF]++}
     END {
         print "Unique auth IPs:", length(auth)
         print "Banned IPs:", length(banned)
     }' /var/log/auth.log /var/log/fail2ban.log

# Cross-reference between files
awk 'NR==FNR {ips[$1]=1; next}  # First file: store IPs
     $1 in ips {print "Match found:", $0}  # Second file: check matches
' blocked_ips.txt /var/log/apache2/access.log
```

**FNR vs NR pattern (classic two-pass technique):**

```bash
# First pass: collect data; Second pass: use data
awk 'NR==FNR {
        # First file processing
        banned_ips[$1]=1
        next  # Skip to next line, don't process second block
    }
    {
        # Second file processing
        if($1 in banned_ips) {
            print "Banned IP accessed:", $0
        }
    }' banned.txt access.log
```

### CTF-Specific Patterns

**Exploit detection patterns:**

```bash
# SQL injection attempts in web logs
awk '$7 ~ /(union|select|insert|update|delete|drop|exec|script)/i {
    print "SQLi attempt:", $1, $7
}' /var/log/apache2/access.log

# Command injection patterns
awk '$7 ~ /(\||;|`|\$\(|%0a|%0d)/  {
    print "Command injection:", $1, $7
}' /var/log/apache2/access.log

# Path traversal attempts
awk '$7 ~ /\.\.\/|\.\.\\/ {
    print "Path traversal:", $1, $7
}' /var/log/apache2/access.log

# Suspicious user agents
awk '$0 ~ /(sqlmap|nikto|nmap|masscan|metasploit|burp)/i {
    match($0, /"[^"]*"[^"]*"[^"]*"/)
    ua=substr($0, RSTART, RLENGTH)
    print $1, ua
}' /var/log/apache2/access.log
```

**Attack timeline reconstruction:**

```bash
# Build attack timeline for specific IP
awk -v target="192.168.1.100" '
BEGIN {
    print "Timeline for", target
    print "================================"
}
{
    found=0
    for(i=1; i<=NF; i++) {
        if($i ~ target) {
            found=1
            break
        }
    }
    if(found) {
        # Extract timestamp
        timestamp=$1" "$2" "$3
        
        # Determine event type
        if($0 ~ /Failed/) event="AUTH_FAIL"
        else if($0 ~ /Accepted/) event="AUTH_SUCCESS"
        else if($0 ~ /BLOCK/) event="FW_BLOCK"
        else if($0 ~ /Ban/) event="BANNED"
        else event="OTHER"
        
        print timestamp, event, FILENAME
    }
}' /var/log/auth.log /var/log/ufw.log /var/log/fail2ban.log | sort
```

**Protocol analysis:**

```bash
# TCP flag analysis from firewall logs
awk '{
    flags=""
    for(i=1;i<=NF;i++) {
        if($i=="SYN") flags=flags"S"
        if($i=="ACK") flags=flags"A"
        if($i=="FIN") flags=flags"F"
        if($i=="RST") flags=flags"R"
        if($i=="PSH") flags=flags"P"
        if($i=="URG") flags=flags"U"
    }
    if(flags!="") {
        for(i=1;i<=NF;i++) {
            if($i~/SRC=/) src=$i
            if($i~/DPT=/) dpt=$i
        }
        print flags, src, dpt
    }
}' /var/log/kern.log | sort | uniq -c | sort -rn

# Port scan detection (unique ports per source)
awk '{
    for(i=1;i<=NF;i++) {
        if($i~/SRC=/) {split($i,s,"="); src=s[2]}
        if($i~/DPT=/) {split($i,d,"="); dpt=d[2]}
    }
    if(src && dpt) {
        ports[src][dpt]=1
    }
}
END {
    for(ip in ports) {
        count=0
        for(p in ports[ip]) count++
        if(count > 10) {
            print ip, "scanned", count, "ports"
        }
    }
}' /var/log/ufw.log
```

**Data exfiltration indicators:**

```bash
# Large outbound transfers
awk '$7 ~ /POST|PUT/ && $10 > 1000000 {
    print "Large upload:", $1, $7, $10, "bytes"
}' /var/log/apache2/access.log

# Detect beaconing (regular interval connections)
awk '{
    split($3, t, ":")
    timestamp=t[1]*3600 + t[2]*60 + t[3]
    
    if(last_time[$1]) {
        interval=timestamp - last_time[$1]
        intervals[$1][interval]++
    }
    last_time[$1]=timestamp
}
END {
    for(ip in intervals) {
        # Check if intervals are suspiciously regular
        for(i in intervals[ip]) {
            if(intervals[ip][i] > 5) {  # Same interval 5+ times
                print ip, "beaconing every", i, "seconds"
            }
        }
    }
}' connection.log
```

### Performance Optimization

**Efficient patterns:**

```bash
# Use next to skip processing
awk '/^#/ {next} {print}' file  # Skip comments

# Early exit with exit
awk '/CRITICAL/ {print; exit}' /var/log/syslog  # Stop at first match

# Limit processing with NR
awk 'NR > 1000 {exit} {print}' large_file.log  # Process first 1000 lines

# Pre-compile regex patterns (automatically done, but be aware)
awk 'BEGIN{pattern="Failed password"} $0 ~ pattern {count++} END{print count}' /var/log/auth.log
```

**Memory considerations:**

```bash
# For huge files, avoid storing everything in arrays
# Bad (memory intensive):
awk '{lines[NR]=$0} END {for(i in lines) print lines[i]}' huge_file.log

# Good (streaming):
awk '{print}' huge_file.log

# If you must aggregate, use delete for cleanup
awk '{
    data[NR]=$0
    if(NR % 10000 == 0) {
        # Process and clear
        for(i in data) {
            # Process data[i]
            delete data[i]
        }
    }
}' huge_file.log
```

### Custom Functions

**User-defined functions:**

```bash
# Define and use custom functions
awk '
function ip_to_int(ip) {
    split(ip, octets, ".")
    return (octets[1]*16777216 + octets[2]*65536 + octets[3]*256 + octets[4])
}

function is_private(ip) {
    int_ip = ip_to_int(ip)
    # 10.0.0.0/8
    if(int_ip >= 167772160 && int_ip <= 184549375) return 1
    # 172.16.0.0/12
    if(int_ip >= 2886729728 && int_ip <= 2887778303) return 1
    # 192.168.0.0/16
    if(int_ip >= 3232235520 && int_ip <= 3232301055) return 1
    return 0
}

{
    for(i=1;i<=NF;i++) {
        if($i ~/SRC=/) {
            split($i, arr, "=")
            if(!is_private(arr[2])) {
                print "External IP:", arr[2]
            }
        }
    }
}' /var/log/kern.log
```

### Output Formatting

**Pretty printing:**

```bash
# Formatted table output
awk 'BEGIN {
    printf "%-15s %-10s %-10s\n", "IP", "Attempts", "Status"
    printf "%-15s %-10s %-10s\n", "---------------", "----------", "----------"
}
{
    printf "%-15s %-10s %-10s\n", $1, $2, $3
}' attack_summary.txt

# CSV output
awk 'BEGIN{OFS=","} {print $1, $2, $3}' file

# JSON-like output
awk '{
    print "{"
    print "  \"ip\": \"" $1 "\","
    print "  \"count\": " $2 ","
    print "  \"date\": \"" $3 "\""
    print "},"
}' data.txt

# HTML table
awk 'BEGIN{print "<table>"}
     {print "<tr><td>"$1"</td><td>"$2"</td></tr>"}
     END{print "</table>"}' data.txt
```

## Python one-liners

### Overview

Python one-liners leverage Python's standard library and expressive syntax for rapid log analysis. While not as concise as awk for simple field extraction, Python excels at complex data structures, JSON/XML parsing, and cryptographic operations common in CTF scenarios.

### Basic Execution Patterns

**Command-line invocation:**

```bash
# Execute Python code directly
python3 -c 'print("Hello from Python")'

# Process stdin line-by-line
cat /var/log/syslog | python3 -c 'import sys; [print(line.strip()) for line in sys.stdin]'

# Import multiple modules
python3 -c 'import sys, re, json; ...'

# Use semicolons for multiple statements
python3 -c 'import sys; data=sys.stdin.read(); print(len(data))'
```

**List comprehensions for filtering:**

```bash
# Print lines containing "Failed"
cat /var/log/auth.log | python3 -c 'import sys; [print(l) for l in sys.stdin if "Failed" in l]'

# Extract IPs with regex
python3 -c 'import sys, re; [print(ip) for line in sys.stdin for ip in re.findall(r"\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b", line)]' < /var/log/syslog

# Filter and transform
cat access.log | python3 -c 'import sys; [print(l.split()[0]) for l in sys.stdin if "404" in l]'
```

### Regular Expression Processing

**Pattern matching and extraction:**

```bash
# Extract all IPs from log
python3 -c 'import sys, re; [print(m.group()) for line in sys.stdin for m in re.finditer(r"\d{1,3}(?:\.\d{1,3}){3}", line)]' < /var/log/syslog

# Extract key=value pairs
python3 -c 'import sys, re; [print(dict(re.findall(r"(\w+)=([^\s]+)", line))) for line in sys.stdin if "BLOCK" in line]' < /var/log/ufw.log

# Named capture groups
python3 -c 'import sys, re; pat=re.compile(r"SRC=(?P<src>[\d.]+).*DPT=(?P<dpt>\d+)"); [print(m.groupdict()) for line in sys.stdin for m in [pat.search(line)] if m]' < /var/log/kern.log

# Replace patterns
python3 -c 'import sys, re; [print(re.sub(r"\d{1,3}(?:\.\d{1,3}){3}", "[IP]", line), end="") for line in sys.stdin]' < /var/log/auth.log

# Multiple patterns
python3 -c 'import sys, re; patterns=["Failed password", "Invalid user"]; [print(line, end="") for line in sys.stdin if any(re.search(p, line) for p in patterns)]' < /var/log/auth.log
```

**Advanced regex techniques:**

```bash
# Lookahead/lookbehind assertions
python3 -c 'import sys, re; [print(m.group()) for line in sys.stdin for m in re.finditer(r"(?<=SRC=)[\d.]+", line)]' < /var/log/kern.log

# Non-greedy matching
python3 -c 'import sys, re; [print(re.findall(r"\"(.*?)\"", line)) for line in sys.stdin]' < access.log

# Multiline mode with flags
python3 -c 'import sys, re; text=sys.stdin.read(); print(re.findall(r"^ERROR.*$", text, re.MULTILINE | re.IGNORECASE))' < /var/log/application.log
```

### Data Structure Manipulation

**Counting and frequency analysis:**

```bash
# Count occurrences using Counter
python3 -c 'import sys; from collections import Counter; ips=[l.split()[-4] for l in sys.stdin if "Failed" in l]; print(Counter(ips).most_common(10))' < /var/log/auth.log

# Count HTTP status codes
python3 -c 'import sys; from collections import Counter; codes=[l.split()[8] for l in sys.stdin if len(l.split())>8]; [print(k,v) for k,v in Counter(codes).items()]' < access.log

# Group by field
python3 -c 'import sys; from collections import defaultdict; d=defaultdict(list); [d[l.split()[0]].append(l) for l in sys.stdin]; [print(k, len(v)) for k,v in d.items()]' < /var/log/syslog
```

**Dictionary operations:**

```bash
# Build IP to port mapping
python3 -c 'import sys, re; from collections import defaultdict; m=defaultdict(set); [m[re.search(r"SRC=([\d.]+)", l).group(1)].add(re.search(r"DPT=(\d+)", l).group(1)) for l in sys.stdin if "SRC=" in l and "DPT=" in l]; [print(k, len(v), "ports") for k,v in m.items() if len(v)>10]' < /var/log/ufw.log

# Merge multiple dictionaries
python3 -c 'import sys, json; dicts=[json.loads(l) for l in sys.stdin]; merged={}; [merged.update(d) for d in dicts]; print(json.dumps(merged, indent=2))' < data.jsonl

# Dictionary comprehension for filtering
python3 -c 'import sys, json; data=json.loads(sys.stdin.read()); filtered={k:v for k,v in data.items() if v>100}; print(json.dumps(filtered))' < stats.json
```

**Set operations for correlation:**

```bash
# Find common IPs between two files
python3 -c 'import sys; f1=set(open(sys.argv[1]).read().splitlines()); f2=set(open(sys.argv[2]).read().splitlines()); print("\n".join(f1 & f2))' file1.txt file2.txt

# Unique IPs
python3 -c 'import sys, re; print("\n".join(set(ip for line in sys.stdin for ip in re.findall(r"\b\d{1,3}(?:\.\d{1,3}){3}\b", line))))' < /var/log/syslog

# Difference between sets
python3 -c 'import sys; blocked=set(open("blocked.txt").read().splitlines()); all_ips=set(ip for line in sys.stdin for ip in line.split() if "." in ip); print("\n".join(all_ips - blocked))' < access.log
```

### JSON/XML Processing

**JSON parsing and manipulation:**

```bash
# Pretty-print JSON
echo '{"key":"value","nested":{"data":123}}' | python3 -c 'import sys, json; print(json.dumps(json.load(sys.stdin), indent=2))'

# Extract specific field from JSON lines
python3 -c 'import sys, json; [print(json.loads(l)["ip"]) for l in sys.stdin if l.strip()]' < logs.jsonl

# Filter JSON objects
python3 -c 'import sys, json; [print(json.dumps(obj)) for line in sys.stdin if line.strip() for obj in [json.loads(line)] if obj.get("status")==404]' < logs.jsonl

# Modify JSON field
python3 -c 'import sys, json; data=json.load(sys.stdin); data["modified"]=True; print(json.dumps(data))' < config.json

# Nested field extraction
python3 -c 'import sys, json; data=json.load(sys.stdin); [print(item["user"]["name"]) for item in data["users"]]' < users.json

# JSON to CSV
python3 -c 'import sys, json, csv; data=json.load(sys.stdin); w=csv.DictWriter(sys.stdout, data[0].keys()); w.writeheader(); w.writerows(data)' < data.json
```

**XML parsing:**

```bash
# Extract XML elements
python3 -c 'import sys; from xml.etree import ElementTree as ET; tree=ET.parse(sys.stdin); [print(elem.text) for elem in tree.findall(".//user")]' < data.xml

# Extract attributes
python3 -c 'import sys; from xml.etree import ElementTree as ET; tree=ET.parse(sys.stdin); [print(elem.get("id"), elem.text) for elem in tree.findall(".//user")]' < data.xml

# XML to JSON conversion
python3 -c 'import sys, json; from xml.etree import ElementTree as ET; tree=ET.parse(sys.stdin); root=tree.getroot(); data={child.tag: child.text for child in root}; print(json.dumps(data, indent=2))' < data.xml

# XPath-like queries
python3 -c 'import sys; from xml.etree import ElementTree as ET; tree=ET.parse(sys.stdin); [print(elem.text) for elem in tree.findall(".//record[@type="error"]")]' < log.xml
````

### Encoding and Decoding

**Base64 operations:**
```bash
# Decode base64
echo "SGVsbG8gV29ybGQ=" | python3 -c 'import sys, base64; print(base64.b64decode(sys.stdin.read().strip()).decode())'

# Encode to base64
echo "Hello World" | python3 -c 'import sys, base64; print(base64.b64encode(sys.stdin.read().encode()).decode())'

# Decode base64 in log lines
python3 -c 'import sys, base64, re; [print(base64.b64decode(m.group()).decode(errors="ignore")) for line in sys.stdin for m in re.finditer(r"[A-Za-z0-9+/]{20,}={0,2}", line)]' < encoded.log

# URL decode
echo "param=%2Fpath%2Fto%2Ffile" | python3 -c 'import sys; from urllib.parse import unquote; print(unquote(sys.stdin.read()))'

# URL encode
echo "/path/to/file" | python3 -c 'import sys; from urllib.parse import quote; print(quote(sys.stdin.read().strip()))'
````

**Hex operations:**

```bash
# Hex to ASCII
echo "48656c6c6f" | python3 -c 'import sys; print(bytes.fromhex(sys.stdin.read().strip()).decode())'

# ASCII to hex
echo "Hello" | python3 -c 'import sys; print(sys.stdin.read().strip().encode().hex())'

# Extract hex strings from logs
python3 -c 'import sys, re; [print(bytes.fromhex(m.group()).decode(errors="ignore")) for line in sys.stdin for m in re.finditer(r"\b[0-9a-fA-F]{8,}\b", line)]' < data.log

# Hex dump format
python3 -c 'import sys; data=sys.stdin.buffer.read(); [print(f"{i:08x}  {data[i:i+16].hex():32s}  {data[i:i+16].decode(errors=\"replace\")}") for i in range(0, len(data), 16)]' < binary.dat
```

**Hashing and checksums:**

```bash
# MD5 hash
echo "password123" | python3 -c 'import sys, hashlib; print(hashlib.md5(sys.stdin.read().strip().encode()).hexdigest())'

# SHA256 hash
echo "password123" | python3 -c 'import sys, hashlib; print(hashlib.sha256(sys.stdin.read().strip().encode()).hexdigest())'

# Hash each line in file
python3 -c 'import sys, hashlib; [print(hashlib.md5(line.strip().encode()).hexdigest(), line.strip()) for line in sys.stdin]' < passwords.txt

# Verify checksums
python3 -c 'import sys, hashlib; expected="5d41402abc4b2a76b9719d911017c592"; actual=hashlib.md5(open(sys.argv[1], "rb").read()).hexdigest(); print("MATCH" if expected==actual else "MISMATCH")' file.bin
```

### Date and Time Processing

**Parse and format timestamps:**

```bash
# Parse syslog timestamp
python3 -c 'import sys; from datetime import datetime; [print(datetime.strptime(line[:15], "%b %d %H:%M:%S")) for line in sys.stdin]' < /var/log/syslog

# Convert to epoch time
python3 -c 'import sys; from datetime import datetime; [print(int(datetime.strptime(line[:15], "%b %d %H:%M:%S").replace(year=2025).timestamp())) for line in sys.stdin]' < /var/log/syslog

# Parse ISO 8601 format
echo "2025-10-28T14:30:45Z" | python3 -c 'import sys; from datetime import datetime; print(datetime.fromisoformat(sys.stdin.read().strip().replace("Z", "+00:00")))'

# Time difference calculation
python3 -c 'import sys; from datetime import datetime; lines=sys.stdin.readlines(); times=[datetime.strptime(l[:15], "%b %d %H:%M:%S") for l in lines]; diffs=[(times[i+1]-times[i]).total_seconds() for i in range(len(times)-1)]; print(f"Avg: {sum(diffs)/len(diffs):.2f}s")' < /var/log/syslog

# Group events by hour
python3 -c 'import sys; from datetime import datetime; from collections import Counter; hours=[datetime.strptime(l[:15], "%b %d %H:%M:%S").hour for l in sys.stdin]; [print(f"{h:02d}:00 - {c}") for h,c in sorted(Counter(hours).items())]' < /var/log/syslog

# Filter by time range
python3 -c 'import sys; from datetime import datetime, time; start=time(14,0); end=time(16,0); [print(line, end="") for line in sys.stdin if start <= datetime.strptime(line.split()[2], "%H:%M:%S").time() <= end]' < /var/log/syslog
```

**Unix timestamp conversion:**

```bash
# Epoch to human-readable
echo "1730124645" | python3 -c 'import sys; from datetime import datetime; print(datetime.fromtimestamp(int(sys.stdin.read().strip())))'

# Human-readable to epoch
echo "2025-10-28 14:30:45" | python3 -c 'import sys; from datetime import datetime; print(int(datetime.strptime(sys.stdin.read().strip(), "%Y-%m-%d %H:%M:%S").timestamp()))'

# Extract and convert timestamps in logs
python3 -c 'import sys, re; from datetime import datetime; [print(datetime.fromtimestamp(int(m.group()))) for line in sys.stdin for m in re.finditer(r"\b\d{10}\b", line)]' < app.log
```

### IP Address Operations

**IP address manipulation:**

```bash
# Validate IP addresses
python3 -c 'import sys, ipaddress; [print(line.strip()) for line in sys.stdin if all(ipaddress.ip_address(ip) for ip in line.split() if "." in ip or ":" in ip)]' < ips.txt

# Check if IP is private
python3 -c 'import sys, ipaddress; [print(ip, "private" if ipaddress.ip_address(ip).is_private else "public") for line in sys.stdin for ip in line.split() if "." in ip]' < ips.txt

# Sort IPs properly
python3 -c 'import sys, ipaddress; ips=sorted([ipaddress.ip_address(l.strip()) for l in sys.stdin]); [print(ip) for ip in ips]' < ips.txt

# Check if IP in subnet
python3 -c 'import sys, ipaddress; subnet=ipaddress.ip_network("192.168.1.0/24"); [print(line.strip()) for line in sys.stdin if ipaddress.ip_address(line.strip()) in subnet]' < ips.txt

# Generate IP range
python3 -c 'import ipaddress; [print(ip) for ip in ipaddress.ip_network("192.168.1.0/28")]'

# Extract IPs and check geolocation category
python3 -c 'import sys, ipaddress, re; [print(ip, "internal" if ipaddress.ip_address(ip).is_private else "external") for line in sys.stdin for ip in re.findall(r"\b\d{1,3}(?:\.\d{1,3}){3}\b", line)]' < /var/log/syslog
```

**CIDR operations:**

```bash
# Check overlap between networks
python3 -c 'import ipaddress; n1=ipaddress.ip_network("192.168.1.0/24"); n2=ipaddress.ip_network("192.168.1.128/25"); print("Overlap" if n1.overlaps(n2) else "No overlap")'

# Subnet division
python3 -c 'import ipaddress; net=ipaddress.ip_network("192.168.1.0/24"); [print(subnet) for subnet in net.subnets(prefixlen_diff=2)]'
```

### Statistical Analysis

**Basic statistics:**

```bash
# Calculate mean, median, stddev
python3 -c 'import sys, statistics; values=[float(line.strip()) for line in sys.stdin]; print(f"Mean: {statistics.mean(values):.2f}"); print(f"Median: {statistics.median(values):.2f}"); print(f"StdDev: {statistics.stdev(values):.2f}")' < numbers.txt

# Percentiles
python3 -c 'import sys, statistics; values=sorted([float(l.strip()) for l in sys.stdin]); n=len(values); print(f"P50: {values[n//2]}"); print(f"P95: {values[int(n*0.95)]}"); print(f"P99: {values[int(n*0.99)]}")' < response_times.txt

# Frequency distribution
python3 -c 'import sys; from collections import Counter; values=[int(l.strip()) for l in sys.stdin]; c=Counter(values); [print(f"{k}: {\"#\"*v}") for k,v in sorted(c.items())]' < data.txt

# Moving average
python3 -c 'import sys; window=5; values=[float(l.strip()) for l in sys.stdin]; [print(sum(values[max(0,i-window+1):i+1])/min(i+1,window)) for i in range(len(values))]' < series.txt
```

**Correlation and analysis:**

```bash
# Detect anomalies (values beyond 2 standard deviations)
python3 -c 'import sys, statistics; values=[float(l.strip()) for l in sys.stdin]; mean=statistics.mean(values); sd=statistics.stdev(values); [print(f"Anomaly: {v}") for v in values if abs(v-mean) > 2*sd]' < metrics.txt

# Entropy calculation (password strength, data randomness)
python3 -c 'import sys, math; from collections import Counter; data=sys.stdin.read(); freq=Counter(data); total=len(data); entropy=-sum((count/total)*math.log2(count/total) for count in freq.values()); print(f"Entropy: {entropy:.2f} bits")' < data.txt
```

### CTF-Specific Techniques

**Web log analysis:**

```bash
# Extract URLs with SQLi indicators
python3 -c 'import sys, re; from urllib.parse import unquote; [print(unquote(m.group())) for line in sys.stdin for m in re.finditer(r"\"GET ([^\"]+)\"", line) if any(k in m.group().lower() for k in ["union","select","drop","insert"])]' < access.log

# Decode double-encoded payloads
python3 -c 'import sys; from urllib.parse import unquote; [print(unquote(unquote(line.strip()))) for line in sys.stdin]' < payloads.txt

# Extract suspicious user agents
python3 -c 'import sys, re; tools=["sqlmap","nikto","burp","nmap","metasploit","dirb","gobuster"]; [print(m.group(1)) for line in sys.stdin for m in [re.search(r"\"([^\"]*" + "|".join(tools) + r"[^\"]*)\"", line, re.I)] if m]' < access.log

# Parse POST data from logs
python3 -c 'import sys, re; from urllib.parse import parse_qs, unquote; [print(parse_qs(unquote(m.group(1)))) for line in sys.stdin for m in re.finditer(r"POST [^\"]*\" \d+ \d+ \"[^\"]*\" \"[^\"]*\" \"([^\"]+)\"", line)]' < access.log
```

**Binary data extraction:**

```bash
# Extract printable strings from binary logs
python3 -c 'import sys, re; data=sys.stdin.buffer.read(); [print(m.group().decode()) for m in re.finditer(rb"[\x20-\x7e]{4,}", data)]' < binary.log

# Find embedded IPs in binary data
python3 -c 'import sys, re; data=sys.stdin.buffer.read().decode(errors="ignore"); [print(m.group()) for m in re.finditer(r"\b\d{1,3}(?:\.\d{1,3}){3}\b", data)]' < packet.dump

# Extract base64 from mixed content
python3 -c 'import sys, re, base64; data=sys.stdin.read(); [print(base64.b64decode(m.group()).decode(errors="ignore")) for m in re.finditer(r"[A-Za-z0-9+/]{40,}={0,2}", data)]' < mixed.log
```

**Credential extraction:**

```bash
# Find potential passwords in logs
python3 -c 'import sys, re; [print(m.group(1)) for line in sys.stdin for m in re.finditer(r"(?:password|passwd|pwd)[=:\s]+([^\s&\"]+)", line, re.I)]' < app.log

# Extract email addresses
python3 -c 'import sys, re; [print(m.group()) for line in sys.stdin for m in re.finditer(r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b", line)]' < data.log

# Find API keys/tokens (high entropy strings)
python3 -c 'import sys, re, math; from collections import Counter; [print(m.group()) for line in sys.stdin for m in re.finditer(r"\b[A-Za-z0-9]{32,}\b", line) if -sum((c/len(m.group()))*math.log2(c/len(m.group())) for c in Counter(m.group()).values()) > 4]' < config.log
```

**Steganography helpers:**

```bash
# Check for hidden data in LSB
python3 -c 'import sys; data=sys.stdin.buffer.read(); lsb_bytes=bytes([b & 1 for b in data]); print(lsb_bytes.hex())' < image.png

# Extract bits at specific positions
python3 -c 'import sys; data=sys.stdin.buffer.read(); bits="".join(str((b >> 0) & 1) for b in data); print(bits[:100])' < file.bin
```

### Complex Multi-Step Operations

**Chained analysis pipeline:**

```bash
# Parse -> Filter -> Aggregate -> Report
python3 -c '
import sys, re
from collections import Counter, defaultdict

# Parse logs
events = []
for line in sys.stdin:
    m = re.search(r"(\w+\s+\d+\s+\S+).*SRC=([\d.]+).*DPT=(\d+)", line)
    if m:
        events.append({"time": m.group(1), "src": m.group(2), "port": m.group(3)})

# Aggregate by source
attacks = defaultdict(lambda: {"ports": set(), "count": 0})
for e in events:
    attacks[e["src"]]["ports"].add(e["port"])
    attacks[e["src"]]["count"] += 1

# Report
print("=== Attack Summary ===")
for ip, data in sorted(attacks.items(), key=lambda x: x[1]["count"], reverse=True)[:10]:
    print(f"{ip}: {data[\"count\"]} attempts, {len(data[\"ports\"])} unique ports")
' < /var/log/kern.log
```

**Log correlation across sources:**

```bash
# Correlate authentication and firewall logs
python3 -c '
import sys, re
from collections import defaultdict

# Parse file1: failed auths
failed = defaultdict(list)
with open("/var/log/auth.log") as f:
    for line in f:
        if "Failed password" in line:
            m = re.search(r"from ([\d.]+)", line)
            if m:
                failed[m.group(1)].append(line.split()[2])

# Parse file2: firewall blocks
blocked = set()
with open("/var/log/ufw.log") as f:
    for line in f:
        m = re.search(r"SRC=([\d.]+)", line)
        if m:
            blocked.add(m.group(1))

# Find IPs with failed auth but not blocked
print("=== Failed auth but not blocked ===")
for ip in failed:
    if ip not in blocked:
        print(f"{ip}: {len(failed[ip])} attempts")
'
```

## Perl one-liners

### Overview

Perl excels at text processing with powerful regex capabilities, built-in command-line options for line processing, and extensive string manipulation functions. While less common in modern CTF scenarios than Python, Perl remains valuable for its conciseness and regex power.

### Basic Execution Patterns

**Command-line options:**

```bash
# -e: Execute code
perl -e 'print "Hello\n"'

# -n: Loop over input lines (implicit while(<>){...})
echo -e "line1\nline2" | perl -ne 'print'

# -p: Like -n but prints $_ automatically
echo -e "line1\nline2" | perl -pe 's/line/LINE/'

# -l: Auto-chomps input and adds newline to output
echo "test" | perl -lne 'print length($_)'

# -a: Autosplit mode (splits $_ into @F array)
echo "field1 field2 field3" | perl -lane 'print $F[1]'

# -F: Specify autosplit delimiter
echo "field1:field2:field3" | perl -F: -lane 'print $F[1]'

# -i: In-place editing
perl -i.bak -pe 's/old/new/g' file.txt

# -0: Set record separator (octal)
perl -0777 -ne 'print' file.txt  # Slurp entire file
```

**Combining options:**

```bash
# Print second field of colon-separated file
perl -F: -lane 'print $F[1]' /etc/passwd

# Replace in-place with backup
perl -i.bak -pe 's/PATTERN/REPLACEMENT/g' file.txt

# Process entire file as one string
perl -0777 -pe 's/pattern/replacement/gs' file.txt
```

### Regular Expression Processing

**Pattern matching:**

```bash
# Print lines matching pattern
perl -ne 'print if /Failed password/' /var/log/auth.log

# Case-insensitive match
perl -ne 'print if /error/i' /var/log/syslog

# Negative match
perl -ne 'print unless /kernel/' /var/log/syslog

# Multiple patterns (OR)
perl -ne 'print if /Failed|Invalid|Illegal/' /var/log/auth.log

# Multiple patterns (AND)
perl -ne 'print if /Failed/ and /password/' /var/log/auth.log

# Match with capture groups
perl -ne 'print "$1\n" if /SRC=([\d.]+)/' /var/log/kern.log

# Named capture groups
perl -ne 'print "$+{src} -> $+{dst}\n" if /SRC=(?<src>[\d.]+).*DST=(?<dst>[\d.]+)/' /var/log/kern.log
```

**Substitution:**

```bash
# Basic substitution
echo "hello world" | perl -pe 's/world/universe/'

# Global substitution
echo "test test test" | perl -pe 's/test/TEST/g'

# Case-insensitive substitution
echo "Error ERROR error" | perl -pe 's/error/FAIL/gi'

# Backreferences
echo "192.168.1.1" | perl -pe 's/(\d+)\.(\d+)\.(\d+)\.(\d+)/$4.$3.$2.$1/'

# Substitution with expressions
echo "value=100" | perl -pe 's/(\d+)/$1*2/e'

# Multiple substitutions
perl -pe 's/cat/dog/g; s/blue/red/g' file.txt

# Conditional substitution
perl -pe 's/old/new/ if /pattern/' file.txt
```

**Advanced regex patterns:**

```bash
# Lookahead
perl -ne 'print "$1\n" if /password(?=\s*=\s*)/' config.log

# Lookbehind
perl -ne 'print "$1\n" if /(?<=SRC=)([\d.]+)/' /var/log/kern.log

# Non-capturing groups
perl -ne 'print if /(?:Failed|Invalid)\s+password/' /var/log/auth.log

# Greedy vs non-greedy
perl -ne 'print "$1\n" if /"(.*?)"/' access.log  # Non-greedy

# Word boundaries
perl -ne 'print if /\berror\b/i' /var/log/syslog

# Possessive quantifiers (no backtracking)
perl -ne 'print if /\d++/' file.txt
```

### Field Processing with Autosplit

**Basic field extraction:**

```bash
# Print specific fields (whitespace-delimited)
perl -lane 'print "$F[0] $F[2]"' /var/log/syslog

# Print last field
perl -lane 'print $F[-1]' /var/log/auth.log

# Print all fields except first
perl -lane 'shift @F; print "@F"' /var/log/syslog

# Custom delimiter
perl -F: -lane 'print "$F[0]: $F[2]"' /etc/passwd

# Multiple delimiters
perl -F'[:\s]+' -lane 'print $F[1]' mixed_delim.log

# Regex delimiter
perl -F'/\s*=\s*/' -lane 'print "$F[0] => $F[1]"' config.txt
```

**Field manipulation:**

```bash
# Swap fields
perl -lane 'print "$F[1] $F[0]"' data.txt

# Conditional field printing
perl -lane 'print $F[2] if $F[0] eq "ERROR"' /var/log/application.log

# Field arithmetic
perl -lane 'print $F[0], " ", $F[1]*2' numbers.txt

# Join fields with custom separator
perl -F: -lane 'print join("|", @F)' /etc/passwd

# Filter based on field value
perl -lane 'print if $F[8] >= 400' /var/log/apache2/access.log
```

### Data Structures and Aggregation

**Hashes for counting:**

```bash
# Count occurrences
perl -lne '$count{$_}++; END{print "$_: $count{$_}" for keys %count}' file.txt

# Count by field
perl -lane '$count{$F[0]}++; END{print "$_: $count{$_}" for sort keys %count}' /var/log/syslog

# Top N most frequent
perl -lane '$c{$F[0]}++; END{print "$_: $c{$_}" for (sort {$c{$b}<=>$c{$a}} keys %c)[0..9]}' /var/log/syslog

# Count unique values
perl -lane 'push @{$data{$F[0]}}, $F[1]; END{print "$_: ", scalar(@{$data{$_}}) for keys %data}' data.txt

# Group by key
perl -F: -lane 'push @{$users{$F[2]}}, $F[0]; END{print "$_: @{$users{$_}}" for keys %users}' /etc/passwd
```

**Arrays for collection:**

```bash
# Collect unique IPs
perl -lne 'if(/(\d+\.\d+\.\d+\.\d+)/){$ips{$1}=1} END{print for keys %ips}' /var/log/syslog

# Store and sort
perl -lne 'push @lines, $_; END{print for sort @lines}' unsorted.txt

# Collect matches
perl -lne 'push @matches, $1 if /DPT=(\d+)/; END{print "@matches"}' /var/log/kern.log

# Array operations
perl -lane 'push @all, @F; END{print "Total fields: ", scalar(@all)}' data.txt
```

### String Manipulation

**Built-in functions:**

```bash
# Length
perl -lne 'print if length($_) > 100' /var/log/syslog

# Substring extraction
perl -lne 'print substr($_, 0, 15)' /var/log/syslog  # First 15 chars

# Split and join
perl -lne '@parts=split(/:/); print join("|", @parts)' data.txt

# Case conversion
perl -pe '$_=uc($_)' file.txt  # Uppercase
perl -pe '$_=lc($_)' file.txt  # Lowercase
perl -pe 's/(\w+)/\u$1/g' file.txt  # Capitalize each word

# Trimming whitespace
perl -pe 's/^\s+|\s+$//g' file.txt

# Reverse string
perl -lne 'print scalar reverse' file.txt

# Character translation (tr///)
perl -pe 'tr/a-z/A-Z/' file.txt  # Uppercase
perl -pe 'tr/ /_/' file.txt  # Replace spaces with underscores
perl -pe 'tr/a-zA-Z//cd' file.txt  # Delete non-letters
```

**Advanced string operations:**

```bash
# Padding
perl -lne 'printf "%-20s %s\n", $F[0], $F[1]' data.txt

# String repetition
perl -le 'print "=" x 50'

# Index and rindex (find position)
perl -lne 'print index($_, "error")' /var/log/syslog

# Quote meta (escape regex characters)
perl -le '$str="test.log"; print quotemeta($str)'
```

### Encoding and Decoding

**Base64:**

```bash
# Encode
echo "Hello World" | perl -MMIME::Base64 -ne 'print encode_base64($_)'

# Decode
echo "SGVsbG8gV29ybGQ=" | perl -MMIME::Base64 -ne 'print decode_base64($_)'

# Decode base64 in logs
perl -MMIME::Base64 -lne 'print decode_base64($1) if /data=([A-Za-z0-9+\/]+=*)/' app.log
```

**URL encoding:**

```bash
# URL encode
echo "/path/to/file" | perl -MURI::Escape -lne 'print uri_escape($_)'

# URL decode
echo "%2Fpath%2Fto%2Ffile" | perl -MURI::Escape -lne 'print uri_unescape($_)'

# Decode URLs in access logs
perl -MURI::Escape -lne 'if(/"GET ([^"]+)"/){print uri_unescape($1)}' /var/log/apache2/access.log
```

**Hex operations:**

```bash
# Hex to ASCII
echo "48656c6c6f" | perl -lne 'print pack("H*", $_)'

# ASCII to hex
echo "Hello" | perl -lne 'print unpack("H*", $_)'

# Extract and decode hex strings
perl -lne 'print pack("H*", $1) while /\b([0-9a-fA-F]{8,})\b/g' data.log
```

### CTF-Specific Patterns

**Log analysis:**

```bash
# Extract IPs from failed SSH attempts
perl -lne 'print $1 if /Failed password.*from ([\d.]+)/' /var/log/auth.log | sort -u

# Count attacks per IP
perl -lne '$ip{$1}++ if /Failed.*from ([\d.]+)/; END{print "$_: $ip{$_}" for sort {$ip{$b}<=>$ip{$a}} keys %ip}' /var/log/auth.log

# Parse firewall logs for port scans
perl -lne '
    /SRC=([\d.]+)/ and $src=$1;
    /DPT=(\d+)/ and $dpt=$1;
    push @{$scan{$src}}, $dpt if $src and $dpt;
    END {
        for(keys %scan) {
            $ports = scalar(@{$scan{$_}});
            print "$_: $ports ports" if $ports > 10;
        }
    }
' /var/log/ufw.log

# Extract SQL injection attempts
perl -MURI::Escape -lne '
    if(/"GET ([^"]+)"/) {
        $url = uri_unescape($1);
        print $url if $url =~ /(union|select|insert|drop|exec)/i;
    }
' /var/log/apache2/access.log
```

**Data extraction:**

```bash
# Extract potential credentials
perl -lne 'print "$1:$2" if /(user|username|login)[=:\s]+([^\s&]+).*(?:pass|password|pwd)[=:\s]+([^\s&]+)/i' app.log

# Find base64-encoded data
perl -MMIME::Base64 -lne '
    while(/([A-Za-z0-9+\/]{20,}={0,2})/g) {
        eval { $decoded = decode_base64($1); print $decoded if $decoded =~ /^[\x20-\x7e]+$/; };
    }
' data.log

# Extract email addresses
perl -lne 'print $& while /\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b/g' data.log

# Find API keys (high entropy strings)
perl -lne '
    while(/\b([A-Za-z0-9]{32,})\b/g) {
        $key = $1;
        %freq = ();
        $freq{$_}++ for split //, $key;
        $entropy = 0;
        $len = length($key);
        $entropy -= ($freq{$_}/$len) * log($freq{$_}/$len)/log(2) for keys %freq;
        print $key if $entropy > 4;
    }' config.log
````

**Binary data processing:**
```bash
# Extract printable strings from binary
perl -ne 'print "$1\n" while /([\x20-\x7e]{4,})/g' binary.dat

# Hex dump with ASCII
perl -e '
    $addr=0;
    while(read(STDIN, $buf, 16)) {
        $hex = unpack("H*", $buf);
        $hex =~ s/(..)/$1 /g;
        $ascii = $buf;
        $ascii =~ tr/\x20-\x7e/./c;
        printf "%08x  %-48s  %s\n", $addr, $hex, $ascii;
        $addr += 16;
    }
' < binary.dat

# Extract specific byte patterns
perl -0777 -ne 'print pos()-4, "\n" while /\x89PNG/g' file.bin  # Find PNG headers

# Bit manipulation
perl -e '$byte=0b11010010; printf "Binary: %08b, Hex: %02x, Decimal: %d\n", $byte, $byte, $byte'
````

**Network data parsing:**

```bash
# Parse pcap hex dumps
perl -ne '
    if(/^\s*0x[0-9a-f]+:\s+((?:[0-9a-f]{2}\s*)+)/) {
        $hex = $1;
        $hex =~ s/\s+//g;
        print pack("H*", $hex);
    }
' pcap_hex.txt

# Extract MAC addresses
perl -lne 'print $& while /\b[0-9a-fA-F]{2}(?::[0-9a-fA-F]{2}){5}\b/g' network.log

# Parse netstat output
perl -lane 'print "$F[3] -> $F[4]" if $F[5] eq "ESTABLISHED"' netstat.txt
```

**Web application analysis:**

```bash
# Decode multiple URL encoding layers
perl -MURI::Escape -lne '
    $decoded = $_;
    while($decoded ne ($prev = $decoded)) {
        $decoded = uri_unescape($decoded);
    }
    print $decoded;
' encoded.txt

# Extract cookies from headers
perl -lne 'print "$1=$2" while /(\w+)=([^;]+)/g if /^Cookie:/' headers.txt

# Parse JWT tokens (base64url decode)
perl -MMIME::Base64 -lne '
    if(/^([^.]+)\.([^.]+)\.([^.]+)$/) {
        for($1, $2) {
            tr{-_}{+/};
            print decode_base64($_ . "=" x (4 - length($_) % 4));
        }
    }
' jwt.txt

# Extract POST parameters
perl -MURI::Escape -lne '
    if(/"POST ([^"]+)".*?"([^"]+)"$/) {
        $post_data = uri_unescape($2);
        print "$1 => $post_data";
    }
' access.log
```

### Advanced Techniques

**Stateful processing:**

```bash
# Track connection states
perl -ne '
    if(/(\d+\.\d+\.\d+\.\d+).*SYN/) {
        $syn{$1}++;
    }
    elsif(/(\d+\.\d+\.\d+\.\d+).*ACK/) {
        $ack{$1}++;
    }
    END {
        for(keys %syn) {
            $completed = $ack{$_} || 0;
            print "$_: SYN=$syn{$_}, ACK=$completed";
            print " [Incomplete handshake]" if $syn{$_} > $completed;
            print "\n";
        }
    }
' connection.log

# Session reconstruction
perl -ne '
    if(/session=(\w+).*user=(\w+)/) {
        $sessions{$1}{user} = $2;
        $sessions{$1}{start} = $. unless exists $sessions{$1}{start};
    }
    if(/session=(\w+).*action=(\w+)/) {
        push @{$sessions{$1}{actions}}, $2;
    }
    END {
        for(keys %sessions) {
            print "Session $_: User=$sessions{$_}{user}, Actions=@{$sessions{$_}{actions}}\n";
        }
    }
' app.log
```

**Time-based analysis:**

```bash
# Calculate time deltas between events
perl -lne '
    if(/^(\S+\s+\S+\s+\S+)/) {
        $time = $1;
        if($prev_time) {
            # Simple timestamp comparison
            print "Event at $time (previous: $prev_time)";
        }
        $prev_time = $time;
    }
' /var/log/syslog

# Detect beaconing patterns
perl -lne '
    if(/(\d+\.\d+\.\d+\.\d+).*(\d+:\d+:\d+)/) {
        ($ip, $time) = ($1, $2);
        ($h, $m, $s) = split /:/, $time;
        $seconds = $h*3600 + $m*60 + $s;
        
        if(exists $last{$ip}) {
            $interval = $seconds - $last{$ip};
            $intervals{$ip}{$interval}++;
        }
        $last{$ip} = $seconds;
    }
    END {
        for $ip (keys %intervals) {
            for $int (keys %{$intervals{$ip}}) {
                $count = $intervals{$ip}{$int};
                print "$ip: Interval ${int}s occurred $count times" if $count > 5;
            }
        }
    }
' connection.log
```

**Multi-file correlation:**

```bash
# Correlate between auth and firewall logs
perl -ne '
    BEGIN {
        # Read banned IPs from fail2ban
        open(F2B, "/var/log/fail2ban.log");
        while(<F2B>) {
            $banned{$1}++ if /Ban ([\d.]+)/;
        }
        close(F2B);
    }
    
    # Check failed auths against banned list
    if(/Failed.*from ([\d.]+)/) {
        $status = exists $banned{$1} ? "BANNED" : "NOT_BANNED";
        print "$1: $status\n";
    }
' /var/log/auth.log
```

**Stream processing with state machines:**

```bash
# Parse multi-line log entries
perl -ne '
    if(/^==== START/) {
        $in_block = 1;
        $block = "";
        next;
    }
    if(/^==== END/) {
        $in_block = 0;
        print "Block: $block\n" if $block =~ /ERROR/;
        next;
    }
    $block .= $_ if $in_block;
' multi_line.log

# Track attack phases
perl -ne '
    if(/(\d+\.\d+\.\d+\.\d+).*scan/) {
        $phase{$1} = "recon";
    }
    elsif(/(\d+\.\d+\.\d+\.\d+).*exploit/) {
        if($phase{$1} eq "recon") {
            $phase{$1} = "exploit";
            print "Full attack chain from $1\n";
        }
    }
' attack.log
```

### Performance and Optimization

**Efficient patterns:**

```bash
# Use compiled regex for repeated matching
perl -ne 'BEGIN{$re=qr/Failed password/} print if /$re/' /var/log/auth.log

# Early exit for large files
perl -ne 'print and exit if /CRITICAL/' /var/log/syslog

# Process only N lines
perl -ne 'print; exit if $. > 1000' large_file.log

# Skip lines efficiently
perl -ne 'next if /^#/; print' file.txt

# Use study() for multiple regex operations on same string
perl -ne 'study; print if /pattern1/ or /pattern2/ or /pattern3/' file.txt
```

**Memory-efficient processing:**

```bash
# Don't store everything in memory
perl -ne 'print if /pattern/' huge_file.log  # Good (streaming)

# Bad (loads entire file):
# perl -0777 -ne '...' huge_file.log

# Incremental statistics
perl -lne '
    $sum += $_;
    $count++;
    $min = $_ if !defined($min) || $_ < $min;
    $max = $_ if !defined($max) || $_ > $max;
    END {
        print "Count: $count, Sum: $sum, Avg: ", $sum/$count;
        print "Min: $min, Max: $max";
    }
' numbers.txt
```

### Useful Modules and One-Liners

**JSON processing:**

```bash
# Parse JSON lines
perl -MJSON -lne '$data=decode_json($_); print $data->{ip}' logs.jsonl

# Pretty print JSON
perl -MJSON -0777 -ne 'print JSON->new->pretty->encode(decode_json($_))' compact.json

# Extract nested fields
perl -MJSON -lne '$d=decode_json($_); print $d->{user}{name} if exists $d->{user}' logs.jsonl
```

**CSV processing:**

```bash
# Parse CSV with Text::CSV
perl -MText::CSV -lne 'BEGIN{$csv=Text::CSV->new()} $csv->parse($_); @f=$csv->fields(); print $f[2]' data.csv

# Simple CSV parsing (no quoted fields)
perl -F, -lane 'print $F[1]' simple.csv
```

**Date/Time:**

```bash
# Parse dates with Time::Piece
perl -MTime::Piece -lne '$t=Time::Piece->strptime($_, "%Y-%m-%d %H:%M:%S"); print $t->epoch' dates.txt

# Simple date extraction
perl -lne 'print $1 if /(\d{4}-\d{2}-\d{2})/' logs.txt
```

## Cross-Tool Comparison and Best Practices

### When to Use Each Tool

**awk - Best for:**

- Field-based log parsing (whitespace or delimiter-separated)
- Quick aggregations and counting
- Numeric calculations on structured data
- Built-in field splitting without regex

**Python - Best for:**

- Complex data structures (nested dicts, sets)
- JSON/XML parsing
- Cryptographic operations (hashing, encoding)
- Statistical analysis
- When you need extensive standard library

**Perl - Best for:**

- Heavy regex operations
- Quick text transformations
- When conciseness is critical
- Binary data manipulation
- Legacy system compatibility

### Performance Considerations

**Speed comparison (general order, fastest to slowest):** [Inference] Based on typical use cases, not guaranteed for all scenarios:

1. awk - Fastest for simple field operations
2. Perl - Fast for regex and text processing
3. Python - Slower startup, but competitive for complex operations

**Memory usage:**

- awk: Most memory-efficient for streaming
- Perl: Efficient for text processing
- Python: Higher memory overhead but manageable

### Chaining Tools Effectively

**Combine strengths:**

```bash
# Use awk for initial filtering, Python for complex analysis
awk '/Failed password/ {print $11}' /var/log/auth.log | python3 -c 'import sys; from collections import Counter; print(Counter(sys.stdin).most_common(10))'

# Use grep for fast filtering, then awk for fields
grep "BLOCK" /var/log/ufw.log | awk '{for(i=1;i<=NF;i++) if($i~/SRC=/) print $i}'

# Perl for extraction, Python for JSON formatting
perl -lne 'print "$1,$2" if /user=(\w+).*ip=([\d.]+)/' app.log | python3 -c 'import sys, json; data=[{"user":l.split(",")[0],"ip":l.split(",")[1].strip()} for l in sys.stdin]; print(json.dumps(data, indent=2))'
```

### CTF Workflow Recommendations

**Initial reconnaissance:**

```bash
# Quick overview with awk
awk '{print $5}' /var/log/syslog | sort | uniq -c | sort -rn | head -20

# Extract suspicious IPs
grep -E "Failed|Invalid|Illegal" /var/log/auth.log | awk '{print $(NF-3)}' | sort -u

# Check for exploit patterns
perl -lne 'print if /(union|select|\.\.\/|%00|<script)/i' /var/log/apache2/access.log
```

**Deep analysis:**

```bash
# Complex correlation with Python
python3 -c '
import sys, re
from collections import defaultdict

# Build attack timeline
events = defaultdict(list)
for line in open("/var/log/auth.log"):
    if m := re.search(r"(\w+\s+\d+\s+\S+).*from ([\d.]+)", line):
        events[m.group(2)].append(m.group(1))

for ip, times in sorted(events.items(), key=lambda x: len(x[1]), reverse=True)[:10]:
    print(f"{ip}: {len(times)} events")
    print(f"  First: {times[0]}, Last: {times[-1]}")
'
```

**Report generation:**

```bash
# Formatted output with awk
awk 'BEGIN {
    print "="*60
    print "Security Analysis Report"
    print "="*60
    printf "%-15s %-10s %-10s\n", "IP Address", "Attempts", "Status"
    print "-"*60
}
/Failed/ {
    ip=$(NF-3)
    count[ip]++
}
END {
    for(ip in count) {
        status = count[ip] > 10 ? "CRITICAL" : "WARNING"
        printf "%-15s %-10d %-10s\n", ip, count[ip], status
    }
    print "="*60
}' /var/log/auth.log
```

---

**Important related topics:**

- Shell scripting integration (combining tools in bash scripts)
- Log parsing with specialized tools (jq for JSON, xmlstarlet for XML)
- Regular expression engines and differences (PCRE, POSIX ERE, Python re)
- Performance profiling and optimization for large datasets
- Stream editing with sed for simple transformations

---

# Log Aggregation Tools

## Logstash Basics

### Architecture & Data Flow

Logstash operates as an ETL (Extract, Transform, Load) pipeline with three core stages:

**Pipeline Structure:**

```
INPUT  FILTER  OUTPUT
```

- **Input plugins**: Ingest data from various sources (files, network, APIs)
- **Filter plugins**: Parse, transform, and enrich data
- **Output plugins**: Send processed data to destinations (Elasticsearch, files, databases)

**Installation locations (Kali/Debian-based):**

```bash
/usr/share/logstash/          # Installation directory
/etc/logstash/                # Configuration files
/etc/logstash/conf.d/         # Pipeline configurations
/var/log/logstash/            # Logstash logs
/var/lib/logstash/            # Data directory
```

### Configuration File Structure

Logstash uses a declarative configuration syntax:

```ruby
input {
  # Input plugin configuration
}

filter {
  # Filter plugin configuration
}

output {
  # Output plugin configuration
}
```

**Basic configuration file example (`/etc/logstash/conf.d/basic.conf`):**

```ruby
input {
  file {
    path => "/var/log/syslog"
    start_position => "beginning"
    sincedb_path => "/dev/null"
  }
}

filter {
  grok {
    match => { "message" => "%{SYSLOGLINE}" }
  }
}

output {
  stdout {
    codec => rubydebug
  }
}
```

### Essential Input Plugins

**File input (most common in CTF):**

```ruby
input {
  file {
    path => ["/var/log/auth.log", "/var/log/apache2/access.log"]
    start_position => "beginning"        # Start from file beginning
    sincedb_path => "/dev/null"          # Don't track position (reprocess each run)
    codec => "plain"                     # Raw text input
    type => "syslog"                     # Add type field for filtering
    tags => ["auth", "security"]         # Add tags for categorization
  }
}
```

**TCP/UDP network input:**

```ruby
input {
  tcp {
    port => 5000
    type => "syslog"
  }
  
  udp {
    port => 5514
    type => "syslog"
  }
}
```

**Beats input (for Filebeat integration):**

```ruby
input {
  beats {
    port => 5044
    ssl => false
  }
}
```

**Standard input (testing/CTF):**

```ruby
input {
  stdin { }
}
```

**HTTP input (webhook/API ingestion):**

```ruby
input {
  http {
    port => 8080
    codec => "json"
  }
}
```

### Critical Filter Plugins for CTF

**Grok - Pattern matching and parsing:**

Grok uses regex patterns to extract structured data from unstructured text.

```ruby
filter {
  grok {
    match => { "message" => "%{COMBINEDAPACHELOG}" }
  }
}
```

**Common Grok patterns:**

```ruby
%{SYSLOGLINE}                 # Standard syslog format
%{COMBINEDAPACHELOG}          # Apache combined log format
%{COMMONAPACHELOG}            # Apache common log format
%{IP:client_ip}               # Extract IP to field 'client_ip'
%{TIMESTAMP_ISO8601:timestamp} # ISO timestamp
%{WORD:action}                # Extract single word
%{USERNAME:user}              # Extract username
%{PATH:file_path}             # File path
```

**Custom Grok patterns:**

```ruby
filter {
  grok {
    match => { 
      "message" => "%{IP:src_ip} \[%{HTTPDATE:timestamp}\] \"%{WORD:method} %{URIPATHPARAM:request} HTTP/%{NUMBER:http_version}\" %{NUMBER:status_code} %{NUMBER:bytes}"
    }
  }
}
```

**Testing Grok patterns (critical for CTF efficiency):**

```bash
# Use Grok Debugger in Kibana (if available)
# Or test locally:
/usr/share/logstash/bin/logstash -f test.conf --config.test_and_exit
```

**Multiple pattern matching:**

```ruby
filter {
  grok {
    match => {
      "message" => [
        "%{SYSLOGLINE}",
        "%{COMBINEDAPACHELOG}",
        ".*%{IP:ip_addr}.*"     # Fallback pattern
      ]
    }
  }
}
```

**KV (Key-Value) parser:**

Extracts key-value pairs from strings:

```ruby
filter {
  kv {
    source => "message"
    field_split => "&"          # Split on ampersand
    value_split => "="          # Split key/value on equals
    target => "params"          # Store in 'params' field
  }
}
```

Example input: `user=admin&action=login&status=success`  
Output: `params.user=admin`, `params.action=login`, `params.status=success`

**JSON parser:**

```ruby
filter {
  json {
    source => "message"
    target => "parsed_json"
  }
}
```

**Mutate - Field manipulation:**

```ruby
filter {
  mutate {
    # Add fields
    add_field => { "source_system" => "webserver01" }
    
    # Rename fields
    rename => { "old_field" => "new_field" }
    
    # Remove fields
    remove_field => [ "unwanted_field", "temp_field" ]
    
    # Convert types
    convert => {
      "status_code" => "integer"
      "response_time" => "float"
    }
    
    # String manipulation
    lowercase => [ "username" ]
    uppercase => [ "hostname" ]
    strip => [ "message" ]          # Remove whitespace
    
    # Replace content
    gsub => [
      "message", "/", "_"           # Replace / with _
    ]
  }
}
```

**Date parsing (critical for timestamp normalization):**

```ruby
filter {
  date {
    match => [ "timestamp", "ISO8601", "dd/MMM/yyyy:HH:mm:ss Z" ]
    target => "@timestamp"
  }
}
```

**GeoIP enrichment:**

```ruby
filter {
  geoip {
    source => "client_ip"
    target => "geoip"
    database => "/usr/share/logstash/GeoLite2-City.mmdb"
  }
}
```

**Conditional filtering (essential for multi-log parsing):**

```ruby
filter {
  if [type] == "apache" {
    grok {
      match => { "message" => "%{COMBINEDAPACHELOG}" }
    }
  } else if [type] == "syslog" {
    grok {
      match => { "message" => "%{SYSLOGLINE}" }
    }
  }
  
  # Conditional based on field content
  if [status_code] >= 400 {
    mutate {
      add_tag => [ "error" ]
    }
  }
}
```

### Output Plugins

**Elasticsearch output (most common):**

```ruby
output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "logstash-%{+YYYY.MM.dd}"
    document_type => "_doc"
  }
}
```

**File output (useful for CTF log extraction):**

```ruby
output {
  file {
    path => "/tmp/processed_logs.json"
    codec => json_lines
  }
}
```

**Stdout (debugging):**

```ruby
output {
  stdout {
    codec => rubydebug           # Pretty-printed output
  }
}
```

**Conditional outputs:**

```ruby
output {
  if [type] == "security" {
    file {
      path => "/var/log/security_events.log"
    }
  }
  
  if "error" in [tags] {
    file {
      path => "/var/log/errors.log"
    }
  }
}
```

### Practical CTF Pipeline Examples

**Apache access log processing:**

```ruby
input {
  file {
    path => "/var/log/apache2/access.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
  }
}

filter {
  grok {
    match => { "message" => "%{COMBINEDAPACHELOG}" }
  }
  
  mutate {
    convert => { "response" => "integer" }
    convert => { "bytes" => "integer" }
  }
  
  geoip {
    source => "clientip"
  }
  
  if [response] >= 400 {
    mutate {
      add_tag => [ "http_error" ]
    }
  }
  
  if [request] =~ /\.\./ or [request] =~ /union.*select/i {
    mutate {
      add_tag => [ "potential_attack" ]
    }
  }
}

output {
  if "potential_attack" in [tags] {
    file {
      path => "/tmp/attacks.log"
      codec => json_lines
    }
  }
  
  stdout { codec => rubydebug }
}
```

**Auth.log processing (privilege escalation detection):**

```ruby
input {
  file {
    path => "/var/log/auth.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
  }
}

filter {
  grok {
    match => { 
      "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_host} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}"
    }
  }
  
  if [syslog_program] == "sudo" {
    grok {
      match => {
        "syslog_message" => "%{DATA:sudo_user} : TTY=%{DATA:tty} ; PWD=%{DATA:pwd} ; USER=%{DATA:target_user} ; COMMAND=%{GREEDYDATA:command}"
      }
    }
    mutate {
      add_tag => [ "sudo_usage" ]
    }
  }
  
  if [syslog_message] =~ /Failed password/ {
    grok {
      match => {
        "syslog_message" => "Failed password for %{DATA:failed_user} from %{IP:src_ip}"
      }
    }
    mutate {
      add_tag => [ "auth_failure" ]
    }
  }
}

output {
  if "sudo_usage" in [tags] {
    file {
      path => "/tmp/sudo_commands.log"
      codec => json_lines
    }
  }
  
  if "auth_failure" in [tags] {
    file {
      path => "/tmp/failed_logins.log"
      codec => json_lines
    }
  }
}
```

**Suricata EVE JSON processing:**

```ruby
input {
  file {
    path => "/var/log/suricata/eve.json"
    codec => "json"
    start_position => "beginning"
    sincedb_path => "/dev/null"
  }
}

filter {
  if [event_type] == "alert" {
    mutate {
      add_tag => [ "ids_alert" ]
    }
    
    if [alert][severity] <= 2 {
      mutate {
        add_tag => [ "high_severity" ]
      }
    }
  }
  
  # Extract payload if present
  if [payload_printable] {
    mutate {
      copy => { "payload_printable" => "payload_analysis" }
    }
  }
}

output {
  if "high_severity" in [tags] {
    file {
      path => "/tmp/critical_alerts.json"
      codec => json_lines
    }
  }
}
```

### Running Logstash

**Command-line execution:**

```bash
# Run with specific config file
/usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/myconfig.conf

# Test configuration syntax without running
/usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/myconfig.conf --config.test_and_exit

# Debug mode (verbose output)
/usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/myconfig.conf --log.level=debug

# Reload config automatically on changes
/usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/myconfig.conf --config.reload.automatic
```

**Service management:**

```bash
systemctl start logstash
systemctl enable logstash
systemctl status logstash
journalctl -u logstash -f
```

**Quick testing with stdin/stdout:**

```bash
# Interactive testing
/usr/share/logstash/bin/logstash -e 'input { stdin { } } filter { grok { match => { "message" => "%{IP:ip}" } } } output { stdout { codec => rubydebug } }'
```

### Performance Tuning

**Pipeline configuration (`/etc/logstash/logstash.yml`):**

```yaml
pipeline.workers: 4              # Number of parallel filter threads
pipeline.batch.size: 125         # Events per batch
pipeline.batch.delay: 50         # Milliseconds to wait for batch fill
```

**JVM heap settings (`/etc/logstash/jvm.options`):**

```
-Xms1g    # Minimum heap
-Xmx1g    # Maximum heap (should match Xms)
```

### Troubleshooting

**Check pipeline status:**

```bash
curl -XGET 'localhost:9600/_node/stats/pipelines?pretty'
```

**Monitor processing:**

```bash
tail -f /var/log/logstash/logstash-plain.log
```

**Common issues in CTF environments:**

- **Sincedb preventing reprocessing**: Set `sincedb_path => "/dev/null"` to reprocess files
- **Grok pattern failures**: Use `_grokparsefailure` tag to identify unparsed logs
- **Type conversion errors**: Check `_dateparsefailure` and similar tags

---

## Filebeat

### Architecture & Purpose

Filebeat is a lightweight log shipper designed to forward logs to Logstash, Elasticsearch, or other outputs with minimal resource overhead. It's part of the Elastic Beats family.

**Key advantages over Logstash for collection:**

- Lower memory footprint (~10MB vs ~500MB)
- Built-in backpressure handling
- Automatic log rotation detection
- Registry-based position tracking

**Installation locations:**

```bash
/usr/share/filebeat/          # Installation directory
/etc/filebeat/                # Configuration
/var/lib/filebeat/            # Registry (tracking file positions)
/var/log/filebeat/            # Filebeat logs
```

### Configuration Structure

Filebeat uses YAML configuration (`/etc/filebeat/filebeat.yml`):

```yaml
filebeat.inputs:
  # Input configurations

filebeat.modules:
  # Pre-built module configurations

output:
  # Output destination

processors:
  # Data transformation
```

### Input Configuration

**Basic file input:**

```yaml
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/syslog
    - /var/log/auth.log
  fields:
    log_type: system
  fields_under_root: false
```

**Multiple inputs with different configurations:**

```yaml
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/apache2/access.log
  fields:
    service: apache
    log_type: access
  tags: ["apache", "web"]

- type: log
  enabled: true
  paths:
    - /var/log/apache2/error.log
  fields:
    service: apache
    log_type: error
  tags: ["apache", "web", "errors"]
  
- type: log
  enabled: true
  paths:
    - /var/log/suricata/eve.json
  json.keys_under_root: true
  json.add_error_key: true
  tags: ["suricata", "ids"]
```

**Wildcards and recursive paths:**

```yaml
filebeat.inputs:
- type: log
  paths:
    - /var/log/*.log                    # All .log files
    - /var/log/**/*.log                 # Recursive
    - /var/log/app-*/application.log    # Pattern matching
```

**Multiline configuration (critical for stack traces, errors):**

```yaml
filebeat.inputs:
- type: log
  paths:
    - /var/log/application.log
  multiline.pattern: '^[0-9]{4}-[0-9]{2}-[0-9]{2}'  # Lines starting with date
  multiline.negate: true
  multiline.match: after
```

**Example - Java stack trace:**

```yaml
multiline.pattern: '^[[:space:]]+(at|\.{3})[[:space:]]|^Caused by:'
multiline.negate: false
multiline.match: after
```

**Excluding lines:**

```yaml
filebeat.inputs:
- type: log
  paths:
    - /var/log/syslog
  exclude_lines: ['^DBG', 'DEBUG']      # Exclude debug messages
  include_lines: ['^ERR', '^WARN']      # Only include errors/warnings
```

**JSON parsing:**

```yaml
filebeat.inputs:
- type: log
  paths:
    - /var/log/app/output.json
  json.keys_under_root: true       # Place JSON fields at root level
  json.overwrite_keys: true        # Overwrite existing fields
  json.add_error_key: true         # Add json_error field on parse failure
  json.message_key: log            # Field containing main message
```

### Filebeat Modules

Modules provide pre-configured parsing for common log formats.

**List available modules:**

```bash
filebeat modules list
```

**Enable modules:**

```bash
filebeat modules enable apache
filebeat modules enable system
filebeat modules enable suricata
```

**Module configuration (`/etc/filebeat/modules.d/apache.yml`):**

```yaml
- module: apache
  access:
    enabled: true
    var.paths: ["/var/log/apache2/access.log*"]
  error:
    enabled: true
    var.paths: ["/var/log/apache2/error.log*"]
```

**Common CTF-relevant modules:**

- `system` - syslog, auth.log
- `apache` - Apache web server
- `nginx` - Nginx web server
- `suricata` - Suricata IDS
- `iptables` - Firewall logs
- `mysql` - MySQL logs
- `auditd` - Linux audit logs

### Output Configuration

**Elasticsearch output:**

```yaml
output.elasticsearch:
  hosts: ["localhost:9200"]
  index: "filebeat-%{[agent.version]}-%{+yyyy.MM.dd}"
  username: "elastic"
  password: "changeme"
```

**Logstash output (most common in CTF for additional processing):**

```yaml
output.logstash:
  hosts: ["localhost:5044"]
  compression_level: 3
  loadbalance: true              # If multiple Logstash instances
```

**File output (debugging/CTF analysis):**

```yaml
output.file:
  path: "/tmp/filebeat"
  filename: filebeat
  rotate_every_kb: 10000
  number_of_files: 7
  codec.json:
    pretty: true
```

**Console output (testing):**

```yaml
output.console:
  pretty: true
```

### Processors (Data Transformation)

**Add fields:**

```yaml
processors:
  - add_fields:
      target: ''
      fields:
        environment: production
        datacenter: dc01
```

**Drop fields:**

```yaml
processors:
  - drop_fields:
      fields: ["agent.ephemeral_id", "agent.id", "ecs.version"]
```

**Rename fields:**

```yaml
processors:
  - rename:
      fields:
        - from: "source_ip"
          to: "src_ip"
```

**Decode JSON:**

```yaml
processors:
  - decode_json_fields:
      fields: ["message"]
      target: "json"
      overwrite_keys: true
```

**Extract key-value pairs:**

```yaml
processors:
  - extract_array:
      field: message
      mappings:
        user: 0
        action: 1
```

**Add tags conditionally:**

```yaml
processors:
  - add_tags:
      tags: ["production"]
      when.equals:
        environment: "prod"
```

**GeoIP enrichment:**

```yaml
processors:
  - add_geoip:
      field: "client.ip"
      target: "client.geo"
```

**Drop events (filtering):**

```yaml
processors:
  - drop_event:
      when.regexp:
        message: "^DEBUG"
```

### CTF-Specific Filebeat Configurations

**Multi-source CTF log collection:**

```yaml
filebeat.inputs:
# System authentication
- type: log
  paths:
    - /var/log/auth.log
  tags: ["auth", "system"]
  fields:
    category: authentication

# Web server logs
- type: log
  paths:
    - /var/log/apache2/access.log
  tags: ["apache", "access"]
  fields:
    category: web

# IDS alerts
- type: log
  paths:
    - /var/log/suricata/eve.json
  json.keys_under_root: true
  tags: ["ids", "suricata"]
  fields:
    category: security

# Application logs
- type: log
  paths:
    - /var/log/app/*.log
  multiline.pattern: '^[0-9]{4}-[0-9]{2}-[0-9]{2}'
  multiline.negate: true
  multiline.match: after
  tags: ["application"]

processors:
  - drop_fields:
      fields: ["agent", "ecs", "host.name"]
  - add_host_metadata: ~

output.logstash:
  hosts: ["localhost:5044"]
```

**Filtering sensitive data:**

```yaml
processors:
  - drop_event:
      when.contains:
        message: "password"
  
  - dissect:
      tokenizer: "%{key}=%{value}"
      field: "message"
      target_prefix: "params"
  
  # Remove credit card patterns
  - script:
      lang: javascript
      source: >
        function process(event) {
          var msg = event.Get("message");
          msg = msg.replace(/\d{4}-\d{4}-\d{4}-\d{4}/g, "XXXX-XXXX-XXXX-XXXX");
          event.Put("message", msg);
        }
```

### Running Filebeat

**Test configuration:**

```bash
filebeat test config
filebeat test output
```

**Run in foreground (debugging):**

```bash
filebeat -e -c /etc/filebeat/filebeat.yml
```

**Service management:**

```bash
systemctl start filebeat
systemctl enable filebeat
systemctl status filebeat
journalctl -u filebeat -f
```

**Setup dashboards (if using Elasticsearch/Kibana):**

```bash
filebeat setup --dashboards
```

### Registry Management

Filebeat tracks file positions in a registry to prevent duplicate processing.

**Registry location:**

```bash
/var/lib/filebeat/registry/filebeat/
```

**Reset registry (reprocess all files - useful in CTF):**

```bash
systemctl stop filebeat
rm -rf /var/lib/filebeat/registry
systemctl start filebeat
```

**View registry contents:**

```bash
cat /var/lib/filebeat/registry/filebeat/data.json | jq
```

### Performance & Monitoring

**Check internal metrics:**

```bash
curl http://localhost:5066/stats | jq
```

**Monitoring configuration:**

```yaml
http.enabled: true
http.host: localhost
http.port: 5066
```

**Adjust batch size for performance:**

```yaml
filebeat.config.inputs:
  reload.enabled: false

queue.mem:
  events: 4096
  flush.min_events: 512
  flush.timeout: 1s

output.logstash:
  hosts: ["localhost:5044"]
  worker: 2
  bulk_max_size: 2048
  compression_level: 3
```

---

## rsyslog Configuration

### Architecture & Role

rsyslog is a high-performance syslog implementation that processes, filters, and forwards system logs. It's the default syslog daemon on most Linux distributions and provides the foundation for centralized logging.

**Key capabilities:**

- Rule-based log routing
- Log filtering and transformation
- Remote log forwarding/reception
- Protocol support: UDP, TCP, TLS/SSL
- Template-based formatting
- Database output support

**Configuration locations:**

```bash
/etc/rsyslog.conf                    # Main configuration
/etc/rsyslog.d/*.conf                # Modular configurations
/var/log/                            # Default log directory
/var/spool/rsyslog/                  # Queue directory
```

### Configuration Syntax

rsyslog uses two syntax formats:

**Legacy BSD syntax (basic):**

```
facility.priority    action
```

**RainerScript syntax (advanced):**

```
ruleset(name="ruleset_name") {
  action(type="omfile" file="/var/log/custom.log")
}
```

### Facilities and Priorities

**Standard facilities:**

```
auth, authpriv    # Authentication/authorization
cron              # Cron daemon
daemon            # System daemons
kern              # Kernel messages
mail              # Mail system
user              # User-level messages
local0-local7     # Custom applications
```

**Priorities (severity levels):**

```
emerg     # 0 - System unusable
alert     # 1 - Action must be taken immediately
crit      # 2 - Critical conditions
err       # 3 - Error conditions
warning   # 4 - Warning conditions
notice    # 5 - Normal but significant
info      # 6 - Informational
debug     # 7 - Debug messages
```

### Basic rsyslog Configuration

**Simple rule examples:**

```bash
# Log all kernel messages to /var/log/kern.log
kern.*                          /var/log/kern.log

# Log authentication messages to /var/log/auth.log
auth,authpriv.*                 /var/log/auth.log

# Log all cron messages
cron.*                          /var/log/cron.log

# Log emergency messages to all logged-in users
*.emerg                         :omusrmsg:*

# Discard specific messages
:msg, contains, "Connection reset by peer"  stop
```

**Wildcard usage:**

```bash
*.*                 # All facilities, all priorities
mail.*              # All mail priorities
*.crit              # All facilities at critical or higher
kern.!err           # Kernel messages excluding errors and higher
```

### Advanced Filtering

**Property-based filters (RainerScript):**

```bash
# Filter by hostname
:hostname, isequal, "webserver01"    /var/log/webserver01.log

# Filter by program name
:programname, isequal, "sshd"        /var/log/sshd.log

# Regex matching
:msg, regex, "Failed password.*"     /var/log/auth_failures.log

# Contains check
:msg, contains, "error"              /var/log/errors.log

# Start/end with
:msg, startswith, "kernel:"          /var/log/kernel_specific.log
```

**Expression-based filters:**

```bash
# Combine multiple conditions
if $programname == 'sshd' and $msg contains 'Failed password' then {
    action(type="omfile" file="/var/log/ssh_failures.log")
    stop
}

# Numeric comparisons
if $syslogseverity <= 3 then {
    action(type="omfile" file="/var/log/critical.log")
}
```

### Templates (Custom Log Formatting)

**Template syntax:**

```bash
# Define template
template(name="CustomFormat" type="string" 
    string="%TIMESTAMP% %HOSTNAME% %syslogtag%%msg:::sp-if-no-1st-sp%%msg:::drop-last-lf%\n")

# Use template
*.* action(type="omfile" file="/var/log/custom_format.log" template="CustomFormat")
```

**Common template examples:**

**Traditional syslog format:**

```bash
template(name="TraditionalFormat" type="string"
    string="%TIMESTAMP% %HOSTNAME% %syslogtag%%msg%\n")
```

**JSON format (excellent for CTF analysis):**

```bash
template(name="JsonFormat" type="list") {
    constant(value="{")
    constant(value="\"timestamp\":\"")      property(name="timereported" dateFormat="rfc3339")
    constant(value="\",\"host\":\"")        property(name="hostname")
    constant(value="\",\"severity\":\"")    property(name="syslogseverity-text")
    constant(value="\",\"facility\":\"")    property(name="syslogfacility-text")
    constant(value="\",\"program\":\"")     property(name="programname")
    constant(value="\",\"message\":\"")     property(name="msg" format="json")
    constant(value="\"}\n")
}

*.* action(type="omfile" file="/var/log/syslog.json" template="JsonFormat")
```

**CEE/Lumberjack JSON format:**

```bash
template(name="CEEFormat" type="string" string="@cee: %jsonmesg%\n")
```

### Remote Logging Configuration

**Send logs to remote server (client configuration):**

**UDP (fast, unreliable):**

```bash
*.* @remote-server:514
# Or with RainerScript:
*.* action(type="omfwd" target="remote-server" port="514" protocol="udp")
```

**TCP (reliable):**

```bash
*.* @@remote-server:514
# Or with RainerScript:
*.* action(type="omfwd" target="remote-server" port="514" protocol="tcp")
```

**TCP with queue (recommended for CTF central logging):**

```bash
*.* action(
    type="omfwd"
    target="192.168.1.100"
    port="514"
    protocol="tcp"
    queue.type="LinkedList"
    queue.filename="remote"
    queue.maxdiskspace="1g"
    queue.saveonshutdown="on"
    action.resumeRetryCount="-1"
)
```

**TLS-encrypted forwarding:**

```bash
# Load required modules
module(load="imtcp")

# Configure TLS
global(
    DefaultNetstreamDriver="gtls"
    DefaultNetstreamDriverCAFile="/etc/rsyslog.d/ca.pem"
    DefaultNetstreamDriverCertFile="/etc/rsyslog.d/client-cert.pem"
    DefaultNetstreamDriverKeyFile="/etc/rsyslog.d/client-key.pem"
)

*.* action(
    type="omfwd"
    target="remote-server"
    port="6514"
    protocol="tcp"
    StreamDriver="gtls"
    StreamDriverMode="1"
    StreamDriverAuthMode="x509/name"
    StreamDriverPermittedPeers="remote-server"
)
```

**Receive logs from remote systems (server configuration):**

**UDP reception:**

```bash
module(load="imudp")
input(type="imudp" port="514")
````

**TCP reception:**
```bash
module(load="imtcp")
input(type="imtcp" port="514")
````

**TLS reception:**

```bash
module(load="imtcp"
    StreamDriver.Name="gtls"
    StreamDriver.Mode="1"
    StreamDriver.Authmode="x509/name"
)

input(
    type="imtcp"
    port="6514"
    StreamDriver.Name="gtls"
    StreamDriver.Mode="1"
    StreamDriver.AuthMode="x509/name"
    PermittedPeer=["*.example.com", "192.168.1.*"]
)
```

**Separate logs by source host:**

```bash
# Template with hostname in path
template(name="RemoteHost" type="string" string="/var/log/remote/%HOSTNAME%/%PROGRAMNAME%.log")

# Apply to remote logs
if $fromhost-ip != '127.0.0.1' then {
    action(type="omfile" dynaFile="RemoteHost")
    stop
}
```

### CTF-Specific rsyslog Configurations

**Centralized CTF log collection server:**

```bash
# /etc/rsyslog.conf or /etc/rsyslog.d/ctf-collector.conf

# Enable reception modules
module(load="imudp")
module(load="imtcp")
input(type="imudp" port="514")
input(type="imtcp" port="514")

# JSON template for parsing
template(name="CTFJsonFormat" type="list") {
    constant(value="{")
    constant(value="\"timestamp\":\"")      property(name="timereported" dateFormat="rfc3339")
    constant(value="\",\"source_host\":\"") property(name="hostname")
    constant(value="\",\"source_ip\":\"")   property(name="fromhost-ip")
    constant(value="\",\"facility\":\"")    property(name="syslogfacility-text")
    constant(value="\",\"severity\":\"")    property(name="syslogseverity-text")
    constant(value="\",\"program\":\"")     property(name="programname")
    constant(value="\",\"pid\":\"")         property(name="procid")
    constant(value="\",\"message\":\"")     property(name="msg" format="json")
    constant(value="\"}\n")
}

# Separate logs by category
if $programname == 'sshd' then {
    action(type="omfile" file="/var/log/ctf/auth/ssh.log" template="CTFJsonFormat")
}

if $programname == 'sudo' then {
    action(type="omfile" file="/var/log/ctf/auth/sudo.log" template="CTFJsonFormat")
}

if $syslogfacility-text == 'authpriv' then {
    action(type="omfile" file="/var/log/ctf/auth/all_auth.log" template="CTFJsonFormat")
}

# Web server logs (if forwarded via logger command)
if $programname contains 'apache' or $programname contains 'nginx' then {
    action(type="omfile" file="/var/log/ctf/web/access.log" template="CTFJsonFormat")
}

# Store everything by host
template(name="ByHost" type="string" string="/var/log/ctf/hosts/%HOSTNAME%/syslog.log")
if $fromhost-ip != '127.0.0.1' then {
    action(type="omfile" dynaFile="ByHost" template="CTFJsonFormat")
}

# Critical events to separate file
if $syslogseverity <= 3 then {
    action(type="omfile" file="/var/log/ctf/critical.log" template="CTFJsonFormat")
}
```

**Attack pattern detection and filtering:**

```bash
# Detect and isolate SSH brute force attempts
if $programname == 'sshd' and $msg contains 'Failed password' then {
    action(type="omfile" file="/var/log/ctf/attacks/ssh_bruteforce.log")
}

# Detect privilege escalation attempts
if $programname == 'sudo' and ($msg contains 'NOT in sudoers' or $msg contains 'incorrect password') then {
    action(type="omfile" file="/var/log/ctf/attacks/sudo_attempts.log")
}

# Detect potential command injection
if $msg contains '../' or $msg contains '/etc/passwd' or $msg contains '/etc/shadow' then {
    action(type="omfile" file="/var/log/ctf/attacks/path_traversal.log")
}

# Detect SQL injection patterns
if $msg regex '(union.*select|drop.*table|insert.*into)' then {
    action(type="omfile" file="/var/log/ctf/attacks/sqli.log")
}
```

**Rate limiting (prevent log flooding):**

```bash
# Limit messages from single source
module(load="imuxsock" 
    SysSock.RateLimit.Interval="5"     # Seconds
    SysSock.RateLimit.Burst="500"      # Messages per interval
)

# Rate limit specific rules
if $programname == 'noisy_app' then {
    action(type="omfile" file="/var/log/noisy.log"
        action.execOnlyWhenPreviousIsSuspended="on"
    )
} else {
    action(type="omfile" file="/var/log/syslog")
}
```

### Filtering and Discarding Logs

**Stop processing after match:**

```bash
# Discard debug messages entirely
if $syslogseverity == 7 then stop

# Process and stop
:programname, isequal, "cron" /var/log/cron.log
& stop
```

**Discard specific messages:**

```bash
# Discard DHCP messages
:msg, contains, "DHCPACK" stop
:msg, contains, "DHCPREQUEST" stop

# Discard kernel USB messages
:msg, regex, "usb [0-9]-[0-9]" stop
```

**Conditional discard:**

```bash
if $programname == 'systemd' and $msg contains 'Reached target' then stop
if $hostname == 'test-server' and $syslogseverity >= 6 then stop
```

### Output Modules and Actions

**File output with rotation:**

```bash
# Output with automatic file creation
*.* action(
    type="omfile"
    file="/var/log/custom.log"
    fileOwner="root"
    fileGroup="adm"
    fileCreateMode="0640"
    dirCreateMode="0755"
)
```

**Database output (MySQL example):**

```bash
module(load="ommysql")

*.* action(
    type="ommysql"
    server="localhost"
    db="syslog"
    uid="rsyslog"
    pwd="password"
)
```

**Pipe output (to script/program):**

```bash
*.* action(type="omprog" binary="/usr/local/bin/log_processor.sh")
```

**Mail output (alerts):**

```bash
module(load="ommail")

if $syslogseverity <= 2 then {
    action(
        type="ommail"
        server="smtp.example.com"
        port="25"
        mailfrom="rsyslog@example.com"
        mailto="admin@example.com"
        subject.template="CriticalAlert"
        body.enable="on"
    )
}
```

**Execute command:**

```bash
if $msg contains 'INTRUSION' then {
    action(type="omshell" binary="/usr/local/bin/alert.sh")
}
```

### Properties and Variables

**Common properties for filtering/formatting:**

```
$timestamp / timereported       # Message timestamp
$hostname                       # Sending hostname
$fromhost / fromhost-ip        # Source host/IP
$syslogtag                     # Tag (program + PID)
$programname                   # Program name only
$procid                        # Process ID
$msg                           # Message content
$rawmsg                        # Complete raw message
$syslogfacility                # Facility number
$syslogfacility-text           # Facility name
$syslogseverity                # Severity number
$syslogseverity-text           # Severity name
```

**Property replacer examples:**

```bash
# Extract only digits from PID
%procid:::R,ERE,0,DFLT:[0-9]+--end%

# Convert to lowercase
%programname:::lowercase%

# Drop last character
%msg:::drop-last-lf%

# JSON escape
%msg:::json%

# First N characters
%hostname:1:10%
```

### Rulesets (Modular Configuration)

**Define custom rulesets:**

```bash
# Define ruleset
ruleset(name="remote") {
    action(type="omfile" file="/var/log/remote.log")
}

# Bind input to ruleset
input(type="imtcp" port="514" ruleset="remote")
```

**Multiple rulesets for different sources:**

```bash
ruleset(name="web_logs") {
    if $programname == 'apache2' then {
        action(type="omfile" file="/var/log/apache_centralized.log")
    }
    stop
}

ruleset(name="db_logs") {
    if $programname == 'mysql' then {
        action(type="omfile" file="/var/log/mysql_centralized.log")
    }
    stop
}

# Bind different ports to different rulesets
input(type="imtcp" port="5140" ruleset="web_logs")
input(type="imtcp" port="5141" ruleset="db_logs")
```

### Queues and Reliability

**Disk-assisted queue configuration:**

```bash
action(
    type="omfwd"
    target="remote-server"
    port="514"
    protocol="tcp"
    queue.type="LinkedList"              # In-memory queue
    queue.filename="fwdqueue"            # Disk queue name
    queue.maxdiskspace="1g"              # Max disk space
    queue.size="10000"                   # Queue size
    queue.discardmark="9500"             # Discard threshold
    queue.discardseverity="5"            # Discard priority >=5
    queue.checkpointinterval="100"       # Checkpoint frequency
    queue.saveonshutdown="on"            # Persist on shutdown
    action.resumeRetryCount="-1"         # Infinite retries
    action.resumeInterval="30"           # Retry interval (seconds)
)
```

**Memory queue:**

```bash
action(
    type="omfile"
    file="/var/log/important.log"
    queue.type="FixedArray"
    queue.size="50000"
)
```

### Debugging rsyslog

**Enable debug mode:**

```bash
# In configuration file
module(load="builtin:omfile" Template="RSYSLOG_DebugFormat")

# Or command line
rsyslogd -d -n -f /etc/rsyslog.conf
```

**Test configuration:**

```bash
rsyslogd -N1 -f /etc/rsyslog.conf
```

**View statistics:**

```bash
# Enable impstats module
module(load="impstats" interval="60" severity="7")

# View runtime statistics
rsyslogd -i /var/run/rsyslogd.pid -v
```

**Check message flow:**

```bash
# Send test message
logger -p local0.info "Test message"

# Monitor live
tail -f /var/log/syslog
```

### Performance Tuning

**Optimize main queue:**

```bash
# Global configuration
global(
    maxMessageSize="64k"
    workDirectory="/var/spool/rsyslog"
)

# Main message queue
main_queue(
    queue.type="LinkedList"
    queue.size="50000"
    queue.workerThreads="4"
    queue.workerThreadMinimumMessages="1000"
)
```

**Async file writing:**

```bash
# Enable async I/O
global(processInternalMessages="on")

*.* action(
    type="omfile"
    file="/var/log/async.log"
    asyncWriting="on"
    flushInterval="1"
    ioBufferSize="64k"
)
```

**Batch processing:**

```bash
action(
    type="omfwd"
    target="remote"
    protocol="tcp"
    queue.dequeuebatchsize="1000"      # Process in batches
)
```

### Integration with CTF Workflow

**Forwarding application logs to rsyslog:**

From command line:

```bash
# Send to syslog
logger -p local0.info "Application event: User login successful"

# With tag
logger -t myapp -p local0.warn "Suspicious activity detected"

# From file
logger -f /var/log/app.log -t myapp
```

From Python:

```python
import syslog

syslog.openlog('myapp', syslog.LOG_PID, syslog.LOG_LOCAL0)
syslog.syslog(syslog.LOG_INFO, 'Application started')
syslog.syslog(syslog.LOG_ERR, 'Critical error occurred')
```

From Apache (using mod_syslog or piped logs):

```apache
# Apache config
CustomLog "|/usr/bin/logger -t apache -p local6.info" combined
ErrorLog syslog:local6
```

**CTF log aggregation pipeline example:**

```
[Target Systems] 
     (forward logs)
[rsyslog Collector]  Parse/Filter  [JSON files]
                                         
[Filebeat]  (read JSON files)
    
[Logstash]  Parse/Enrich  [Elasticsearch]
                                
                          [Kibana/Analysis]
```

**Complete configuration example:**

```bash
# /etc/rsyslog.d/ctf-pipeline.conf

# Load modules
module(load="imuxsock")
module(load="imtcp")
module(load="imudp")

# Network inputs
input(type="imtcp" port="514")
input(type="imudp" port="514")

# JSON template
template(name="CTFJson" type="list") {
    constant(value="{")
    constant(value="\"@timestamp\":\"")     property(name="timereported" dateFormat="rfc3339")
    constant(value="\",\"host\":\"")        property(name="hostname")
    constant(value="\",\"source_ip\":\"")   property(name="fromhost-ip")
    constant(value="\",\"facility\":\"")    property(name="syslogfacility-text")
    constant(value="\",\"severity\":\"")    property(name="syslogseverity-text")
    constant(value="\",\"program\":\"")     property(name="programname")
    constant(value="\",\"message\":\"")     property(name="msg" format="json")
    constant(value="\"}\n")
}

# Output to JSON files for Filebeat
if $fromhost-ip != '127.0.0.1' then {
    action(
        type="omfile"
        file="/var/log/ctf/remote.json"
        template="CTFJson"
        queue.type="LinkedList"
        queue.filename="remote"
        queue.maxdiskspace="500m"
    )
}

# Local logs
if $fromhost-ip == '127.0.0.1' then {
    action(
        type="omfile"
        file="/var/log/ctf/local.json"
        template="CTFJson"
    )
}

# Attack patterns to separate file
if $msg contains 'Failed password' or 
   $msg contains 'NOT in sudoers' or
   $msg regex '(union.*select|../../)' then {
    action(
        type="omfile"
        file="/var/log/ctf/attacks.json"
        template="CTFJson"
    )
}
```

### Common rsyslog Issues in CTF

**Permissions:**

```bash
# Fix log file permissions
chown syslog:adm /var/log/custom.log
chmod 640 /var/log/custom.log

# Fix directory permissions
mkdir -p /var/log/ctf
chown -R syslog:adm /var/log/ctf
chmod 755 /var/log/ctf
```

**Firewall blocking remote logs:**

```bash
# Allow incoming syslog
ufw allow 514/tcp
ufw allow 514/udp

# Or iptables
iptables -A INPUT -p tcp --dport 514 -j ACCEPT
iptables -A INPUT -p udp --dport 514 -j ACCEPT
```

**SELinux/AppArmor restrictions:**

```bash
# SELinux - allow rsyslog network
setsebool -P rsyslogd_can_network_connect 1

# Check for denials
ausearch -m avc -c rsyslogd
```

**Restart after configuration changes:**

```bash
# Test first
rsyslogd -N1

# Restart service
systemctl restart rsyslog
systemctl status rsyslog

# Check for errors
journalctl -u rsyslog -n 50
```

### Service Management Commands

```bash
# Start/stop/restart
systemctl start rsyslog
systemctl stop rsyslog
systemctl restart rsyslog
systemctl reload rsyslog        # Reload without dropping connections

# Enable on boot
systemctl enable rsyslog

# Check status
systemctl status rsyslog
journalctl -u rsyslog -f

# Verify listening ports
ss -tulpn | grep rsyslog
netstat -tulpn | grep rsyslog

# Check running processes
ps aux | grep rsyslog
```

### Important Subtopics for Further Study

- **ELK Stack integration**: Complete pipeline from rsyslog  Filebeat  Logstash  Elasticsearch  Kibana for visualization
- **Log parsing optimization**: Advanced Grok patterns and KV parsing for complex log formats
- **High-availability configurations**: Redundant log collectors and failover mechanisms
- **Log retention and rotation**: Implementing logrotate with compressed archives for long-term CTF analysis
- **Real-time alerting**: Integrating Elastalert or similar tools for immediate threat notification based on aggregated log patterns

---

## syslog-ng

syslog-ng (syslog New Generation) is a high-performance log management solution that extends traditional syslog capabilities with advanced filtering, parsing, and routing.

### Architecture and Components

**Core concepts**:

- **Sources**: Input mechanisms (network ports, files, system logs)
- **Destinations**: Output targets (files, databases, remote servers)
- **Filters**: Conditional logic for log processing
- **Parsers**: Extract structured data from unstructured logs
- **Rewrite rules**: Modify log content before routing

### Installation

```bash
# Debian/Ubuntu
sudo apt update
sudo apt install syslog-ng syslog-ng-core

# RHEL/CentOS
sudo yum install syslog-ng

# Start and enable
sudo systemctl start syslog-ng
sudo systemctl enable syslog-ng

# Verify installation
syslog-ng --version
```

### Configuration Files

**Primary configuration**: `/etc/syslog-ng/syslog-ng.conf`

**Configuration structure**:

```conf
@version: 3.35
@include "scl.conf"

# Global options
options {
    chain_hostnames(off);
    flush_lines(0);
    use_dns(no);
    use_fqdn(no);
    owner("root");
    group("adm");
    perm(0640);
    stats_freq(0);
    bad_hostname("^gconfd$");
};

# Source definitions
source s_local {
    system();
    internal();
};

source s_network {
    tcp(ip(0.0.0.0) port(514));
    udp(ip(0.0.0.0) port(514));
};

# Destination definitions
destination d_local {
    file("/var/log/syslog-ng/messages");
};

# Filter definitions
filter f_emergency {
    level(emerg);
};

# Log path (connects source -> filter -> destination)
log {
    source(s_local);
    destination(d_local);
};
```

### Common Source Configurations

**TCP/UDP network sources**:

```conf
# TCP with TLS
source s_secure_network {
    network(
        transport("tls")
        port(6514)
        tls(
            key-file("/etc/syslog-ng/cert.d/serverkey.pem")
            cert-file("/etc/syslog-ng/cert.d/servercert.pem")
            ca-dir("/etc/syslog-ng/ca.d")
        )
    );
};

# UDP with larger buffer
source s_udp_large {
    udp(
        ip(0.0.0.0)
        port(514)
        so_rcvbuf(1048576)
    );
};
```

**File sources**:

```conf
# Monitor specific log file
source s_apache {
    file("/var/log/apache2/access.log"
         follow-freq(1)
         flags(no-parse));
};

# Wildcard pattern
source s_app_logs {
    file("/var/log/apps/*.log"
         recursive(yes)
         follow-freq(1));
};

# Read from specific position
source s_positioned {
    file("/var/log/custom.log"
         follow-freq(1)
         log-fetch-limit(100)
         log-iw-size(1000));
};
```

**System sources**:

```conf
# Standard system logs
source s_system {
    system();
    internal();
};

# Journal (systemd)
source s_journal {
    systemd-journal();
};

# Kernel messages
source s_kernel {
    file("/proc/kmsg" program_override("kernel"));
};
```

### Filter Configurations

**Severity-based filters**:

```conf
# Emergency only
filter f_emerg { level(emerg); };

# Critical and above
filter f_crit { level(crit..emerg); };

# Info and below
filter f_info { level(debug..info); };
```

**Facility-based filters**:

```conf
# Authentication logs
filter f_auth { facility(auth, authpriv); };

# Kernel messages
filter f_kern { facility(kern); };

# Mail system
filter f_mail { facility(mail); };
```

**Content-based filters**:

```conf
# Match specific string
filter f_ssh_login {
    match("sshd.*Accepted" value("MESSAGE"));
};

# Regular expression
filter f_failed_auth {
    match("authentication failure|Failed password" 
          value("MESSAGE") 
          type(pcre) 
          flags(ignore-case));
};

# Multiple conditions (AND)
filter f_complex {
    facility(auth) and 
    level(err..emerg) and 
    match("failed" value("MESSAGE"));
};

# IP address filter
filter f_specific_host {
    host("192.168.1.100");
};

# Exclude pattern
filter f_exclude_noise {
    not match("CRON" value("PROGRAM"));
};
```

**Time-based filters**:

```conf
# Business hours only
filter f_business_hours {
    match("^(0[9-9]|1[0-7])" value("HOUR"));
};
```

### Parser Configurations

**CSV parser**:

```conf
parser p_csv {
    csv-parser(
        columns("timestamp", "severity", "facility", "message")
        delimiters(",")
        flags(escape-double-char)
    );
};
```

**Key-value parser**:

```conf
parser p_keyvalue {
    kv-parser(
        prefix(".kv.")
        pair-separator(" ")
        value-separator("=")
    );
};

# Example log: user=admin action=login result=success
# Creates: .kv.user=admin, .kv.action=login, .kv.result=success
```

**Pattern database (for structured parsing)**:

```conf
parser p_patterndb {
    db-parser(
        file("/etc/syslog-ng/patterndb.xml")
    );
};
```

**JSON parser**:

```conf
parser p_json {
    json-parser(
        prefix(".json.")
    );
};

# Extracts JSON fields as macros
```

**Apache access log parser** [Inference]:

```conf
parser p_apache {
    csv-parser(
        columns("client_ip", "ident", "userid", "timestamp", 
                "request", "status", "bytes", "referer", "useragent")
        delimiters(" ")
        quote-pairs('""[]')
    );
};
```

### Destination Configurations

**File destinations**:

```conf
# Simple file
destination d_file {
    file("/var/log/syslog-ng/all.log");
};

# With rotation
destination d_rotated {
    file("/var/log/syslog-ng/messages"
         create_dirs(yes)
         dir_perm(0755)
         perm(0644));
};

# Dynamic file names (by date)
destination d_daily {
    file("/var/log/syslog-ng/$YEAR$MONTH$DAY/messages.log"
         create_dirs(yes));
};

# Separate by host
destination d_per_host {
    file("/var/log/syslog-ng/hosts/$HOST/$YEAR$MONTH$DAY.log"
         create_dirs(yes));
};

# Separate by facility
destination d_by_facility {
    file("/var/log/syslog-ng/$FACILITY.log");
};
```

**Network destinations**:

```conf
# Forward to remote syslog
destination d_remote {
    tcp("192.168.1.10" port(514));
};

# With TLS
destination d_secure_remote {
    network("192.168.1.10"
            port(6514)
            transport("tls")
            tls(ca-dir("/etc/syslog-ng/ca.d")));
};

# UDP forwarding
destination d_udp_forward {
    udp("192.168.1.10" port(514));
};
```

**Database destinations**:

```conf
# MySQL/MariaDB
destination d_mysql {
    sql(
        type(mysql)
        host("localhost")
        username("syslog")
        password("password")
        database("logs")
        table("messages")
        columns("datetime", "host", "program", "message")
        values("${R_YEAR}-${R_MONTH}-${R_DAY} ${R_HOUR}:${R_MIN}:${R_SEC}",
               "$HOST", "$PROGRAM", "$MESSAGE")
        indexes("datetime", "host", "program")
    );
};

# PostgreSQL
destination d_pgsql {
    sql(
        type(pgsql)
        host("localhost")
        port(5432)
        username("syslog")
        password("password")
        database("logs")
        table("messages")
        columns("datetime", "host", "facility", "priority", "message")
        values("${R_YEAR}-${R_MONTH}-${R_DAY} ${R_HOUR}:${R_MIN}:${R_SEC}",
               "$HOST", "$FACILITY", "$PRIORITY", "$MESSAGE")
    );
};
```

**Program destinations** (pipe to external program):

```conf
destination d_custom_script {
    program("/usr/local/bin/log-processor.sh"
            template("${DATE} ${HOST} ${MESSAGE}\n"));
};
```

### Template Configurations

**Custom output format**:

```conf
template t_custom {
    template("${ISODATE} ${HOST} [${LEVEL}] ${PROGRAM}: ${MESSAGE}\n");
};

# Use in destination
destination d_formatted {
    file("/var/log/formatted.log" template(t_custom));
};
```

**JSON output template**:

```conf
template t_json {
    template("$(format-json --scope rfc5424 --scope nv-pairs)\n");
};

destination d_json_file {
    file("/var/log/syslog-ng/json.log" template(t_json));
};
```

**CEF (Common Event Format) template** [Inference]:

```conf
template t_cef {
    template("CEF:0|${PROGRAM}|${HOST}|1.0|${FACILITY}|${PRIORITY}|${SEVERITY}|msg=${MESSAGE}\n");
};
```

### Complete CTF-Relevant Configuration Example

```conf
@version: 3.35
@include "scl.conf"

options {
    chain_hostnames(off);
    flush_lines(0);
    use_dns(no);
    use_fqdn(no);
    stats_freq(3600);
    mark_freq(3600);
};

# Sources
source s_local {
    system();
    internal();
};

source s_network {
    tcp(ip(0.0.0.0) port(514) max-connections(1000));
    udp(ip(0.0.0.0) port(514));
};

source s_apache {
    file("/var/log/apache2/access.log" follow-freq(1) flags(no-parse));
};

source s_ssh {
    file("/var/log/auth.log" follow-freq(1));
};

# Filters
filter f_auth {
    facility(auth, authpriv);
};

filter f_ssh_success {
    match("sshd.*Accepted" value("MESSAGE"));
};

filter f_ssh_failed {
    match("sshd.*(Failed password|authentication failure)" value("MESSAGE") type(pcre));
};

filter f_sql_injection {
    match("(UNION.*SELECT|' OR '1'='1|admin'--|DROP TABLE)" 
          value("MESSAGE") 
          type(pcre) 
          flags(ignore-case));
};

filter f_critical {
    level(crit..emerg);
};

# Parsers
parser p_apache {
    csv-parser(
        columns("client_ip", "ident", "userid", "timestamp", 
                "request", "status", "bytes")
        delimiters(" ")
        flags(escape-double-char)
    );
};

# Destinations
destination d_all {
    file("/var/log/syslog-ng/all.log"
         create_dirs(yes));
};

destination d_auth {
    file("/var/log/syslog-ng/auth.log"
         create_dirs(yes));
};

destination d_ssh_success {
    file("/var/log/syslog-ng/ssh-success.log"
         create_dirs(yes)
         template("${ISODATE} ${HOST} ${MESSAGE}\n"));
};

destination d_ssh_failed {
    file("/var/log/syslog-ng/ssh-failed.log"
         create_dirs(yes));
};

destination d_sqli_attempts {
    file("/var/log/syslog-ng/sqli-attempts.log"
         create_dirs(yes)
         template("${ISODATE} ${HOST} ${PROGRAM}: ${MESSAGE}\n"));
};

destination d_critical_alerts {
    file("/var/log/syslog-ng/critical.log"
         create_dirs(yes));
    program("/usr/local/bin/alert-critical.sh");
};

destination d_remote_siem {
    tcp("192.168.1.100" port(514));
};

# Log paths
log {
    source(s_local);
    source(s_network);
    destination(d_all);
};

log {
    source(s_local);
    source(s_network);
    filter(f_auth);
    destination(d_auth);
};

log {
    source(s_ssh);
    filter(f_ssh_success);
    destination(d_ssh_success);
};

log {
    source(s_ssh);
    filter(f_ssh_failed);
    destination(d_ssh_failed);
};

log {
    source(s_apache);
    parser(p_apache);
    filter(f_sql_injection);
    destination(d_sqli_attempts);
};

log {
    source(s_local);
    source(s_network);
    filter(f_critical);
    destination(d_critical_alerts);
    destination(d_remote_siem);
};
```

### Testing and Debugging

**Syntax validation**:

```bash
# Check configuration syntax
sudo syslog-ng --syntax-only

# Verbose syntax check
sudo syslog-ng -s -v
```

**Debug mode**:

```bash
# Run in foreground with debug output
sudo syslog-ng -F -d -v

# Test specific configuration file
sudo syslog-ng -f /tmp/test-config.conf -F -d
```

**Generate test messages**:

```bash
# Send to local syslog
logger -p auth.info "Test authentication message"
logger -p kern.crit "Test critical kernel message"

# Send to remote syslog-ng (TCP)
echo "Test message" | nc 192.168.1.10 514

# Send structured syslog message
logger -p local0.info -t myapp "user=admin action=login result=success"
```

**Monitor statistics**:

```bash
# Real-time statistics
syslog-ng-ctl stats

# Query specific source
syslog-ng-ctl query get "*source*"

# Check message processing rate
watch -n 1 'syslog-ng-ctl stats | grep processed'
```

**Reload configuration** (without stopping):

```bash
sudo syslog-ng-ctl reload
```

### Log Analysis Queries

**Query aggregated logs**:

```bash
# Count SSH failures by IP
grep "Failed password" /var/log/syslog-ng/ssh-failed.log | \
  awk '{print $NF}' | sort | uniq -c | sort -rn

# Find authentication attempts in specific timeframe
grep "2025-10-28 14:" /var/log/syslog-ng/auth.log | \
  grep "authentication failure"

# Extract unique source IPs from all logs
grep -oP '\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}' /var/log/syslog-ng/all.log | \
  sort -u

# Identify SQL injection attempts by request
awk '{print $7}' /var/log/syslog-ng/sqli-attempts.log | sort | uniq -c | sort -rn
```

### Performance Tuning

```conf
options {
    # Increase buffer size for high-volume environments
    log_fifo_size(10000);
    
    # Flush more frequently (lower latency)
    flush_lines(100);
    
    # Disable DNS lookups (faster processing)
    use_dns(no);
    use_fqdn(no);
    
    # Thread optimization
    threaded(yes);
};
```

## Fluentd

Fluentd is a unified logging layer designed for high-volume log collection with plugin-based extensibility. It excels at parsing, filtering, and routing logs to multiple destinations simultaneously.

### Architecture and Components

**Core concepts**:

- **Input plugins**: Data sources (tail files, listen on ports, pull from APIs)
- **Parser plugins**: Structure extraction (JSON, regex, CSV)
- **Filter plugins**: Transform/enrich data
- **Output plugins**: Send to destinations (Elasticsearch, S3, databases)
- **Buffer plugins**: Reliability and performance optimization
- **Tag-based routing**: Direct log flow using pattern matching

### Installation

```bash
# Debian/Ubuntu (td-agent package includes Ruby and bundled gems)
curl -fsSL https://toolbelt.treasuredata.com/sh/install-ubuntu-focal-td-agent4.sh | sh

# RHEL/CentOS
curl -fsSL https://toolbelt.treasuredata.com/sh/install-redhat-td-agent4.sh | sh

# Start and enable
sudo systemctl start td-agent
sudo systemctl enable td-agent

# Verify installation
td-agent --version

# Alternative: Install gem directly (requires Ruby)
gem install fluentd
fluentd --setup ./fluent
```

### Configuration Files

**Primary configuration**: `/etc/td-agent/td-agent.conf` (for td-agent) or `/etc/fluent/fluent.conf`

**Configuration structure**:

```conf
# System-wide parameters
<system>
  log_level info
  workers 4
</system>

# Input source
<source>
  @type tail
  path /var/log/nginx/access.log
  tag nginx.access
  <parse>
    @type nginx
  </parse>
</source>

# Filter (transform data)
<filter nginx.access>
  @type record_transformer
  <record>
    hostname ${hostname}
  </record>
</filter>

# Output destination
<match nginx.access>
  @type file
  path /var/log/td-agent/nginx.log
</match>
```

### Input Plugin Configurations

**Tail file input**:

```conf
# Monitor single file
<source>
  @type tail
  path /var/log/apache2/access.log
  pos_file /var/log/td-agent/apache-access.pos
  tag apache.access
  <parse>
    @type apache2
  </parse>
  refresh_interval 5s
  read_from_head true
</source>

# Monitor multiple files with wildcard
<source>
  @type tail
  path /var/log/apps/*.log
  pos_file /var/log/td-agent/apps.pos
  tag app.*
  path_key filename
  <parse>
    @type json
  </parse>
</source>

# Monitor with multiline support (stack traces)
<source>
  @type tail
  path /var/log/application.log
  pos_file /var/log/td-agent/app.pos
  tag app.log
  <parse>
    @type multiline
    format_firstline /^\d{4}-\d{2}-\d{2}/
    format1 /^(?<time>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) \[(?<level>\w+)\] (?<message>.*)/
  </parse>
</source>
```

**Network input (syslog)**:

```conf
# TCP syslog
<source>
  @type syslog
  port 5140
  bind 0.0.0.0
  tag system.syslog
  <parse>
    @type syslog
  </parse>
</source>

# UDP syslog
<source>
  @type syslog
  port 5140
  bind 0.0.0.0
  protocol_type udp
  tag system.syslog
</source>
```

**HTTP input**:

```conf
# Accept JSON over HTTP
<source>
  @type http
  port 8888
  bind 0.0.0.0
  body_size_limit 32m
  keepalive_timeout 10s
</source>

# Send test data:
# curl -X POST -d 'json={"event":"login","user":"admin"}' http://localhost:8888/test.http
```

**Forward input** (from other Fluentd instances):

```conf
<source>
  @type forward
  port 24224
  bind 0.0.0.0
</source>
```

**Exec input** (run command periodically):

```conf
<source>
  @type exec
  command /usr/local/bin/check-status.sh
  tag system.status
  run_interval 60s
  <parse>
    @type json
  </parse>
</source>
```

**System metrics input**:

```conf
<source>
  @type systemd
  path /var/log/journal
  tag systemd
  read_from_head true
  <storage>
    @type local
    path /var/log/td-agent/systemd.pos
  </storage>
  <entry>
    fields_strip_underscores true
  </entry>
</source>
```

### Parser Configurations

**JSON parser**:

```conf
<source>
  @type tail
  path /var/log/app/json.log
  tag app.json
  <parse>
    @type json
    time_key timestamp
    time_format %Y-%m-%dT%H:%M:%S.%NZ
  </parse>
</source>
```

**Regex parser**:

```conf
<source>
  @type tail
  path /var/log/custom.log
  tag custom
  <parse>
    @type regexp
    expression /^(?<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) \[(?<severity>\w+)\] (?<message>.*)$/
    time_key timestamp
    time_format %Y-%m-%d %H:%M:%S
  </parse>
</source>
```

**Multiline parser** (for stack traces):

```conf
<source>
  @type tail
  path /var/log/java/application.log
  tag java.app
  <parse>
    @type multiline
    format_firstline /^\d{4}-\d{2}-\d{2}/
    format1 /^(?<time>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) (?<level>[^\s]+) (?<message>.*)/
    multiline_flush_interval 5s
  </parse>
</source>
```

**Apache/Nginx parsers**:

```conf
# Apache Combined Log Format
<source>
  @type tail
  path /var/log/apache2/access.log
  tag apache.access
  <parse>
    @type apache2
  </parse>
</source>

# Nginx
<source>
  @type tail
  path /var/log/nginx/access.log
  tag nginx.access
  <parse>
    @type nginx
  </parse>
</source>
```

**CSV parser**:

```conf
<source>
  @type tail
  path /var/log/data.csv
  tag data.csv
  <parse>
    @type csv
    keys timestamp,user,action,result
    time_key timestamp
  </parse>
</source>
```

**Grok parser** (requires fluent-plugin-grok-parser):

```conf
<source>
  @type tail
  path /var/log/custom.log
  tag custom
  <parse>
    @type grok
    grok_pattern %{TIMESTAMP_ISO8601:timestamp} \[%{LOGLEVEL:severity}\] %{GREEDYDATA:message}
  </parse>
</source>
```

### Filter Plugin Configurations

**Record transformer** (add/modify fields):

```conf
<filter apache.access>
  @type record_transformer
  <record>
    hostname "#{Socket.gethostname}"
    service_name "web-frontend"
    environment "production"
  </record>
</filter>

# Remove sensitive fields
<filter app.**>
  @type record_transformer
  remove_keys password,token,credit_card
</filter>
```

**Grep filter** (include/exclude records):

```conf
# Include only errors
<filter app.log>
  @type grep
  <regexp>
    key level
    pattern /(ERROR|CRITICAL)/
  </regexp>
</filter>

# Exclude health checks
<filter nginx.access>
  @type grep
  <exclude>
    key path
    pattern /^\/health/
  </exclude>
</filter>

# Multiple conditions
<filter app.**>
  @type grep
  <and>
    <regexp>
      key status
      pattern /^5\d{2}/
    </regexp>
    <exclude>
      key path
      pattern /^\/internal/
    </exclude>
  </and>
</filter>
```

**Parser filter** (parse nested fields):

```conf
<filter nginx.access>
  @type parser
  key_name message
  reserve_data true
  <parse>
    @type json
  </parse>
</filter>
```

**GeoIP filter** (requires fluent-plugin-geoip):

```conf
<filter web.access>
  @type geoip
  geoip_lookup_keys remote_addr
  <record>
    country ${remote_addr["country_code"]}
    city ${remote_addr["city"]}
    latitude ${remote_addr["latitude"]}
    longitude ${remote_addr["longitude"]}
  </record>
</filter>
```

**Detect exceptions filter** (group multiline exceptions):

```conf
<filter app.log>
  @type detect_exceptions
  languages java, python
  multiline_flush_interval 0.1
</filter>
```

**Throttle filter** (rate limiting):

```conf
<filter noisy.logs>
  @type throttle
  group_key user_id
  group_bucket_period_s 60
  group_bucket_limit 100
</filter>
```

### Output Plugin Configurations

**File output**:

```conf
# Simple file output
<match app.**>
  @type file
  path /var/log/td-agent/app.log
  compress gzip
  <format>
    @type json
  </format>
</match>

# Time-sliced files
<match nginx.access>
  @type file
  path /var/log/td-agent/nginx/access.%Y%m%d.log
  <buffer time>
    timekey 86400
    timekey_wait 1m
  </buffer>
</match>

# Separate by tag
<match web.**>
  @type file
  path /var/log/td-agent/${tag}/%Y%m%d.log
  <buffer tag,time>
    timekey 3600
  </buffer>
</match>
```

**Elasticsearch output**:

```conf
<match app.**>
  @type elasticsearch
  host localhost
  port 9200
  logstash_format true
  logstash_prefix fluentd
  logstash_dateformat %Y.%m.%d
  include_tag_key true
  type_name _doc
  <buffer>
    flush_interval 10s
    flush_thread_count 2
  </buffer>
</match>

# With authentication
<match secure.**>
  @type elasticsearch
  host elasticsearch.example.com
  port 9200
  scheme https
  user elastic
  password changeme
  index_name production
</match>
```

**MongoDB output**:

```conf
<match database.logs>
  @type mongo
  host localhost
  port 27017
  database logs
  collection fluentd
  capped
  capped_size 100m
</match>
```

**S3 output**:

```conf
<match archive.**>
  @type s3
  aws_key_id YOUR_AWS_KEY_ID
  aws_sec_key YOUR_AWS_SECRET_KEY
  s3_bucket your-bucket-name
  s3_region us-east-1
  path logs/
  time_slice_format %Y%m%d-%H
  <buffer time>
    timekey 3600
    timekey_wait 10m
  </buffer>
</match>
```

**HTTP output** (webhook/SIEM):

```conf
<match alert.**>
  @type http
  endpoint https://siem.example.com/api/events
  http_method post
  <format>
    @type json
  </format>
  <buffer>
    flush_interval 1s
  </buffer>
</match>
```

**Forward output** (to another Fluentd):

```conf
<match **>
  @type forward
  <server>
    host 192.168.1.100
    port 24224
  </server>
  <buffer>
    flush_interval 5s
  </buffer>
</match>

# With load balancing
<match **>
  @type forward
  <server>
    host fluentd1.example.com
    port 24224
    weight 60
  </server>
  <server>
    host fluentd2.example.com
    port 24224
    weight 40
  </server>
</match>
```

**Copy output** (send to multiple destinations):

```conf
<match important.**>
  @type copy
  <store>
    @type file
    path /var/log/td-agent/important.log
  </store>
  <store>
    @type elasticsearch
    host localhost
    port 9200
  </store>
  <store>
    @type http
    endpoint https://alert.example.com/api/webhook
  </store>
</match>
```

**Stdout output** (debugging):

```conf
<match debug.**>
  @type stdout
  <format>
    @type json
  </format>
</match>
```

### Buffering Configuration

```conf
<match **>
  @type elasticsearch
  host localhost
  port 9200
  
  <buffer>
    # Buffer type (file or memory)
    @type file
    path /var/log/td-agent/buffer/
    
    # Flush settings
    flush_mode interval
    flush_interval 10s
    flush_at_shutdown true
    
    # Retry settings
    retry_type exponential_backoff
    retry_wait 1s
    retry_max_interval 300s
    retry_timeout 72h
    retry_forever false
    
    # Chunk settings
    chunk_limit_size 5M
    total_limit_size 1G
    overflow_action drop_oldest_chunk
    
    # Queue settings
    queue_limit_length 256
  </buffer>
</match>
```

### Complete CTF-Relevant Configuration Example

```conf
<system>
  log_level info
  workers 2
</system>

# Monitor web server logs
<source>
  @type tail
  path /var/log/apache2/access.log
  pos_file /var/log/td-agent/apache-access.pos
  tag web.access
  <parse>
    @type apache2
  </parse>
</source>

# Monitor authentication logs
<source>
  @type tail
  path /var/log/auth.log pos_file /var/log/td-agent/auth.pos tag system.auth <parse> @type syslog </parse> </source>

# Monitor application logs (JSON format)
<source> @type tail path /var/log/app/*.log pos_file /var/log/td-agent/app.pos tag app.logs <parse> @type json time_key timestamp time_format %Y-%m-%dT%H:%M:%S.%NZ </parse> </source>

# Accept remote syslog
<source> @type syslog port 5140 bind 0.0.0.0 tag remote.syslog <parse> @type syslog </parse> </source>

# Monitor database slow queries
<source> @type tail path /var/log/mysql/slow.log pos_file /var/log/td-agent/mysql-slow.pos tag database.slow <parse> @type multiline format_firstline /^# Time:/ format1 /^# Time: (?<time>\d{6}\s+\d{1,2}:\d{2}:\d{2})/ format2 /^# User@Host: (?<user>[^\[]+)\[(?<host>[^\]]+)\]/ format3 /^# Query_time: (?<query_time>[\d.]+)\s+Lock_time: (?<lock_time>[\d.]+)/ format4 /^(?<query>.*)/ </parse> </source>

# Filter: Detect SQL injection attempts in web logs
<filter web.access> @type grep <regexp> key path pattern /(UNION.*SELECT|%27.*OR|admin%27--|DROP%20TABLE|1%3D1)/i </regexp> </filter>

# Filter: Extract failed SSH attempts
<filter system.auth> @type grep <regexp> key message pattern /(Failed password|authentication failure|Invalid user)/ </regexp> </filter>

# Filter: Add hostname to all records
<filter **> @type record_transformer <record> hostname "#{Socket.gethostname}" captured_at ${Time.now.iso8601} </record> </filter>

# Filter: Detect brute force patterns
<filter system.auth> @type record_transformer enable_ruby true <record> alert_level ${record["message"].scan(/Failed password/).count > 5 ? "HIGH" : "NORMAL"} </record> </filter>

# Output: Save SQL injection attempts to dedicated file
<match web.access> @type file path /var/log/td-agent/sqli-attempts/${tag}_%Y%m%d.log <buffer tag,time> timekey 86400 timekey_wait 10m </buffer> <format> @type json </format> </match>

# Output: Save auth failures to file and forward to SIEM
<match system.auth> @type copy <store> @type file path /var/log/td-agent/auth-failures.log <format> @type json </format> </store> <store> @type forward <server> host 192.168.1.100 port 24224 </server> </store> </match>

# Output: Store app logs in Elasticsearch
<match app.logs> @type elasticsearch host localhost port 9200 logstash_format true logstash_prefix app-logs <buffer> flush_interval 10s flush_thread_count 2 </buffer> </match>

# Output: Archive slow queries
<match database.slow> @type file path /var/log/td-agent/db-slow/%Y%m%d.log compress gzip <buffer time> timekey 86400 </buffer> </match>

# Output: Catch-all for unmatched logs
<match **> @type file path /var/log/td-agent/all.log <format> @type json </format> </match>
````

### Testing and Debugging

**Syntax validation**:
```bash
# Test configuration
td-agent --dry-run -c /etc/td-agent/td-agent.conf

# Verbose output
td-agent --dry-run -c /etc/td-agent/td-agent.conf -v
````

**Run in foreground** (debugging):

```bash
# Run with console output
td-agent -c /etc/td-agent/td-agent.conf -v

# Run with specific log level
td-agent -c /etc/td-agent/td-agent.conf -v --log-level debug
```

**Send test events**:

```bash
# Using fluent-cat (included with td-agent)
echo '{"message":"test event","level":"info"}' | fluent-cat test.logs

# Using curl (HTTP input)
curl -X POST -d 'json={"event":"login","user":"admin","result":"success"}' \
  http://localhost:8888/test.logs

# Using logger (syslog input)
logger -p local0.info -t testapp "Test syslog message"

# Generate JSON event
echo '{"timestamp":"2025-10-28T10:00:00Z","severity":"ERROR","message":"Test error"}' | \
  fluent-cat app.logs
```

**Monitor performance**:

```bash
# Check buffer status
ls -lh /var/log/td-agent/buffer/

# Monitor output
tail -f /var/log/td-agent/td-agent.log

# Check process status
systemctl status td-agent

# View metrics (requires monitoring plugin)
curl http://localhost:24220/api/plugins.json
```

**Reload configuration**:

```bash
# Reload without restart (USR2 signal)
sudo systemctl reload td-agent

# Or
sudo kill -USR2 $(cat /var/run/td-agent/td-agent.pid)
```

### Plugin Management

```bash
# List installed plugins
td-agent-gem list

# Install plugin
sudo td-agent-gem install fluent-plugin-geoip

# Install specific version
sudo td-agent-gem install fluent-plugin-elasticsearch -v 5.0.3

# Common useful plugins
sudo td-agent-gem install fluent-plugin-grok-parser
sudo td-agent-gem install fluent-plugin-geoip
sudo td-agent-gem install fluent-plugin-elasticsearch
sudo td-agent-gem install fluent-plugin-s3
sudo td-agent-gem install fluent-plugin-mongo
sudo td-agent-gem install fluent-plugin-kafka
```

### Log Analysis with Fluentd Output

**Query file-based output**:

```bash
# Extract unique IPs from web access logs
jq -r '.remote' /var/log/td-agent/web.access.*.log | sort -u

# Count events by severity
jq -r '.level' /var/log/td-agent/app.logs.*.log | sort | uniq -c

# Find SQL injection attempts in specific timeframe
jq 'select(.captured_at >= "2025-10-28T10:00:00Z" and 
           .captured_at <= "2025-10-28T11:00:00Z")' \
  /var/log/td-agent/sqli-attempts/*.log

# Extract failed login attempts with source IP
jq 'select(.message | contains("Failed password")) | 
    {time: .time, user: .ident, ip: .host}' \
  /var/log/td-agent/auth-failures.log
```

**Analyze buffered data** (troubleshooting):

```bash
# List buffer chunks
ls -lh /var/log/td-agent/buffer/

# Check oldest chunk
ls -lt /var/log/td-agent/buffer/ | tail -1

# Count chunks (indicates backlog)
ls /var/log/td-agent/buffer/ | wc -l

# View chunk metadata (binary file, limited usefulness)
xxd /var/log/td-agent/buffer/buffer.*.log | head
```

### Performance Tuning

```conf
<system>
  # Increase workers for high-volume environments
  workers 4
  
  # Root directory for worker-specific files
  root_dir /var/log/td-agent
  
  # Process name
  process_name td-agent
</system>

# Input optimization
<source>
  @type tail
  path /var/log/nginx/access.log
  
  # Read multiple lines per iteration
  read_lines_limit 1000
  
  # Increase read buffer
  read_bytes_limit_per_second -1
  
  # Position file update frequency
  pos_file_compaction_interval 72h
</source>

# Buffer optimization
<match **>
  @type elasticsearch
  
  <buffer>
    # Use file-based buffer for reliability
    @type file
    path /var/log/td-agent/buffer/
    
    # Larger chunks = better throughput
    chunk_limit_size 10M
    
    # Aggressive flushing
    flush_mode immediate
    flush_thread_count 4
    
    # Increase queue size
    queue_limit_length 512
    
    # Total buffer limit
    total_limit_size 2G
  </buffer>
</match>
```

## Graylog

Graylog is a comprehensive log management platform combining collection, indexing (Elasticsearch/OpenSearch), and web-based analysis. It provides powerful search capabilities, dashboards, and alerting.

### Architecture Components

**Core components**:

- **Graylog Server**: Processing, indexing, web interface
- **Elasticsearch/OpenSearch**: Log storage and search backend
- **MongoDB**: Metadata storage (users, dashboards, configurations)

### Installation

**Docker Compose** (simplest for CTF/lab environments):

```yaml
# docker-compose.yml
version: '3'
services:
  mongodb:
    image: mongo:5.0
    volumes:
      - mongo_data:/data/db
  
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch-oss:7.10.2
    environment:
      - http.host=0.0.0.0
      - transport.host=localhost
      - network.host=0.0.0.0
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    volumes:
      - es_data:/usr/share/elasticsearch/data
    ulimits:
      memlock:
        soft: -1
        hard: -1
    mem_limit: 1g
  
  graylog:
    image: graylog/graylog:5.0
    environment:
      - GRAYLOG_PASSWORD_SECRET=somepasswordpepper
      - GRAYLOG_ROOT_PASSWORD_SHA2=8c6976e5b5410415bde908bd4dee15dfb167a9c873fc4bb8a81f6f2ab448a918
      - GRAYLOG_HTTP_EXTERNAL_URI=http://127.0.0.1:9000/
    entrypoint: /usr/bin/tini -- wait-for-it elasticsearch:9200 --  /docker-entrypoint.sh
    volumes:
      - graylog_data:/usr/share/graylog/data
    depends_on:
      - mongodb
      - elasticsearch
    ports:
      - 9000:9000      # Web interface
      - 1514:1514      # Syslog TCP
      - 1514:1514/udp  # Syslog UDP
      - 12201:12201    # GELF TCP
      - 12201:12201/udp # GELF UDP

volumes:
  mongo_data:
  es_data:
  graylog_data:
```

```bash
# Start Graylog stack
docker-compose up -d

# Check status
docker-compose ps

# View logs
docker-compose logs -f graylog

# Default login: admin / admin (change immediately)
```

**Native installation** (Ubuntu/Debian):

```bash
# Install MongoDB
sudo apt-get install -y mongodb-server

# Install Elasticsearch
wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
echo "deb https://artifacts.elastic.co/packages/oss-7.x/apt stable main" | \
  sudo tee /etc/apt/sources.list.d/elastic-7.x.list
sudo apt-get update && sudo apt-get install elasticsearch-oss

# Configure Elasticsearch
sudo nano /etc/elasticsearch/elasticsearch.yml
# Set: cluster.name: graylog

sudo systemctl daemon-reload
sudo systemctl enable elasticsearch
sudo systemctl start elasticsearch

# Install Graylog
wget https://packages.graylog2.org/repo/packages/graylog-5.0-repository_latest.deb
sudo dpkg -i graylog-5.0-repository_latest.deb
sudo apt-get update && sudo apt-get install graylog-server

# Generate password secret
pwgen -N 1 -s 96

# Generate root password hash
echo -n "yourpassword" | sha256sum

# Configure Graylog
sudo nano /etc/graylog/server/server.conf
# Set:
# password_secret = <generated secret>
# root_password_sha2 = <generated hash>
# http_bind_address = 0.0.0.0:9000

sudo systemctl daemon-reload
sudo systemctl enable graylog-server
sudo systemctl start graylog-server
```

### Input Configuration

Graylog uses "Inputs" to receive logs. Configure via Web UI (System  Inputs) or API.

**Syslog UDP Input** (via Web UI):

1. Navigate to System  Inputs
2. Select "Syslog UDP" from dropdown
3. Click "Launch new input"
4. Configure:
    - Title: "Syslog UDP"
    - Bind address: 0.0.0.0
    - Port: 1514
    - Store full message: checked (for CTF forensics)
5. Save

**Syslog TCP Input**:

- Same process, select "Syslog TCP"
- Recommended for reliability

**GELF (Graylog Extended Log Format) Input**:

- Most feature-rich format
- Supports structured data, compression
- Port: 12201 (UDP or TCP)

**Raw/Plaintext Input**:

- For custom log formats
- Requires extractors to parse

**Beats Input** (Filebeat, etc.):

- Port: 5044
- Select "Beats" input type

**CEF (Common Event Format) Input**:

- For security appliance logs
- Select "CEF" input type

### Sending Logs to Graylog

**From rsyslog**:

```conf
# /etc/rsyslog.d/90-graylog.conf
*.* @192.168.1.100:1514;RSYSLOG_SyslogProtocol23Format
```

**From syslog-ng**:

```conf
destination d_graylog {
    syslog("192.168.1.100" port(1514) transport("udp"));
};

log {
    source(s_local);
    destination(d_graylog);
};
```

**From Fluentd**:

```conf
<match **>
  @type gelf
  host 192.168.1.100
  port 12201
  protocol udp
  <buffer>
    flush_interval 10s
  </buffer>
</match>
```

**Using logger** (testing):

```bash
# Send to syslog input
logger -n 192.168.1.100 -P 1514 "Test message from logger"

# With specific facility/severity
logger -n 192.168.1.100 -P 1514 -p auth.err "Authentication failure test"
```

**Using GELF** (Python example):

```python
import logging
import pygelf

logging.basicConfig(level=logging.INFO)
handler = pygelf.GelfUdpHandler(host='192.168.1.100', port=12201)
logger = logging.getLogger('test')
logger.addHandler(handler)

logger.info('Test GELF message', extra={'user': 'admin', 'action': 'login'})
```

**Using curl** (GELF HTTP):

```bash
curl -X POST http://192.168.1.100:12201/gelf \
  -H 'Content-Type: application/json' \
  -d '{
    "version": "1.1",
    "host": "test-host",
    "short_message": "Test message",
    "level": 1,
    "_user_id": 42,
    "_environment": "production"
  }'
```

### Extractors

Extractors parse fields from raw log messages. Configure via Web UI (System  Inputs  Manage Extractors).

**Regex Extractor** (example: extract IP from message):

1. Find sample message in search
2. Click message  "Create extractor"
3. Select "Regular expression"
4. Regex: `(?<source_ip>\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})`
5. Test against sample
6. Save

**Grok Pattern Extractor**:

```
%{IPORHOST:client_ip} %{USER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] "(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})" %{NUMBER:response} (?:%{NUMBER:bytes}|-)
```

**JSON Extractor**:

- Automatically parses JSON-formatted messages
- Fields become searchable

**Key-Value Extractor**:

- For `key=value` formatted logs
- Specify separator characters

**Copy Input Extractor**:

- Duplicate field for processing
- Useful before destructive transformations

### Search Queries

Graylog uses Elasticsearch query syntax.

**Basic searches**:

```
# Simple text search
failed password

# Field-specific search
source:webserver01

# Exact phrase
"authentication failure"

# Wildcard
user:adm*

# Range
response_time:[500 TO 1000]

# Existence
_exists_:user_id

# Negation
NOT source:localhost
```

**Boolean operators**:

```
# AND (implicit)
failed password

# Explicit AND
failed AND password

# OR
(failed OR error) AND login

# Complex
(status:500 OR status:502) AND source:webserver* NOT path:/health
```

**Field queries**:

```
# Numeric comparison
response_time:>1000
http_status:>=500
bytes:<10000

# IP addresses
source_ip:192.168.1.0/24

# Time-based (relative)
timestamp:[now-1h TO now]
timestamp:[now-1d TO now]

# Date range (absolute)
timestamp:[2025-10-28T10:00:00.000Z TO 2025-10-28T11:00:00.000Z]
```

**Regular expressions**:

```
# Regex search (use /pattern/)
message:/\b\d{3}-\d{2}-\d{4}\b/

# SQL injection patterns
message:/(UNION.*SELECT|' OR |admin'--)/

# IP address extraction
source_ip:/^10\./
```

**CTF-relevant searches**:

```
# Failed SSH attempts
application:sshd AND message:"Failed password"

# SQL injection attempts
message:/(UNION|SELECT.*FROM|' OR '|DROP TABLE)/

# Privilege escalation
message:/(sudo|su -|chmod|chown)/ AND level:>=5

# Web shells
path:/(shell|cmd|exec|system)\.php/

# Brute force detection (high-frequency logins)
# Use aggregation in UI: Count by source_ip, threshold > 100

# Data exfiltration patterns
bytes:>10000000 AND http_method:POST

# Suspicious file access
path:/(etc\/passwd|etc\/shadow|\.ssh\/)/

# Command injection
message:/(;|\||`|&&)/ AND type:web_access
```

### Dashboards and Visualizations

**Create dashboard** (via Web UI):

1. Navigate to Dashboards
2. Create new dashboard
3. Add widgets:
    - **Quick values**: Top N field values
    - **Statistics**: Count, min, max, avg, sum
    - **Charts**: Line, bar, area charts over time
    - **World map**: Geographic distribution (requires GeoIP)
    - **Trend**: Indicator with sparkline

**Common CTF visualizations**:

```
# Failed login attempts by source IP
Widget: Quick values
Field: source_ip
Query: message:"Failed password"
Limit: 20

# Response time distribution
Widget: Histogram
Field: response_time
Query: *
Interval: auto

# HTTP status codes over time
Widget: Line chart
Field: http_status
Query: *
Metric: count()

# Authentication events timeline
Widget: Area chart
Query: application:sshd OR application:sudo
Metric: count()
Interval: 5 minutes

# Geographic attack sources (requires GeoIP)
Widget: World map
Field: source_ip_geolocation
Query: message:"Failed password"
```

### Streams

Streams route messages to specific processing pipelines or storage.

**Create stream** (via Web UI):

1. Navigate to Streams
2. Create stream
3. Define rules (matching conditions):
    
    ```
    Field: applicationType: match exactlyValue: sshd
    ```
    
4. Configure stream to start/pause
5. Connect to outputs or alerts

**Common CTF streams**:

```
# Security events stream
Rules:
- message must match regex (failed|error|denied|unauthorized)
- level is greater than 4

# Web application attacks
Rules:
- type must match exactly "web_access"
- message must match regex (UNION|SELECT|DROP|'--|XSS|javascript:)

# Administrative actions
Rules:
- application is one of [sudo, su, useradd, passwd]
```

### Pipelines and Rules

Pipelines provide advanced message processing with custom logic.

**Create pipeline** (System  Pipelines):

1. Create pipeline: "Security Analysis"
2. Connect to stream
3. Create rule:

**Example rule** (detect SQL injection):

```
rule "detect_sql_injection"
when
  has_field("message") AND
  (
    contains(to_string($message.message), "UNION", true) OR
    contains(to_string($message.message), "' OR '", true) OR
    contains(to_string($message.message), "admin'--", true)
  )
then
  set_field("alert_type", "sql_injection");
  set_field("severity", 8);
  set_field("alert_description", "Potential SQL injection detected");
  route_to_stream("security_alerts");
end
```

**Example rule** (GeoIP enrichment):

```
rule "enrich_with_geoip"
when
  has_field("source_ip")
then
  let geo = geoip(to_string($message.source_ip));
  set_field("src_country", geo.country.iso_code);
  set_field("src_city", geo.city.name);
  set_field("src_coordinates", geo.location);
end
```

**Example rule** (parse user agent):

```
rule "parse_user_agent"
when
  has_field("user_agent")
then
  let parsed = parse_user_agent(to_string($message.user_agent));
  set_field("browser", parsed.browser);
  set_field("os", parsed.os);
  set_field("device", parsed.device);
end
```

**Common pipeline functions**:

```
# String operations
to_string(), concat(), substring(), lowercase(), uppercase()

# Field operations
set_field(), remove_field(), rename_field(), has_field()

# Parsing
grok(), parse_json(), key_value()

# Network
cidr_match("192.168.1.0/24", $message.source_ip)

# Routing
route_to_stream("stream_name")

# Lookups (from lookup tables)
lookup("threat_intel", to_string($message.source_ip))
```

### Alerts

**Create alert** (Alerts  Event Definitions):

1. Title: "Brute Force Detection"
2. Priority: High
3. Conditions:
    - **Filter**: `message:"Failed password"`
    - **Aggregation**: Count
    - **Threshold**: > 10
    - **Time range**: Last 5 minutes
    - **Group by**: source_ip
4. Notifications:
    - Email
    - HTTP callback
    - Slack/PagerDuty (requires plugins)

**Common CTF alerts**:

```
# Excessive failed logins
Condition: Count > 10
Query: message:"Failed password"
Time: 5 minutes
Group by: source_ip

# High error rate
Condition: Count > 100
Query: level:>=5
Time: 1 minute

# SQL injection attempts
Condition: Count > 1
Query: alert_type:sql_injection
Time: Any

# Suspicious file access
Condition: Count > 1
Query: path:/(etc\/passwd|etc\/shadow)/
Time: 1 hour

# Data exfiltration (large POST)
Condition: Sum(bytes) > 100000000
Query: http_method:POST
Time: 10 minutes
Group by: source_ip
```

### API Usage

Graylog provides a comprehensive REST API for automation.

**Authentication**:

```bash
# Get API token (Web UI  System  Users  Create Token)
# Or use Basic Auth with username:password

TOKEN="your_api_token_here"
GRAYLOG_API="http://localhost:9000/api"
```

**Search API**:

```bash
# Absolute time range search
curl -X GET "$GRAYLOG_API/search/universal/absolute" \
  -H "Authorization: Basic $(echo -n 'admin:admin' | base64)" \
  -H "Accept: application/json" \
  -G \
  --data-urlencode "query=source:webserver01" \
  --data-urlencode "from=2025-10-28T10:00:00.000Z" \
  --data-urlencode "to=2025-10-28T11:00:00.000Z" \
  --data-urlencode "limit=100" \
  --data-urlencode "sort=timestamp:desc"

# Relative time range
curl -X GET "$GRAYLOG_API/search/universal/relative" \
  -H "Authorization: Basic $(echo -n 'admin:admin' | base64)" \
  -G \
  --data-urlencode "query=failed password" \
  --data-urlencode "range=3600" \
  --data-urlencode "limit=50"

# Export to CSV
curl -X POST "$GRAYLOG_API/search/universal/absolute/export" \
  -H "Authorization: Basic $(echo -n 'admin:admin' | base64)" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "message:error",
    "from": "2025-10-28T10:00:00.000Z",
    "to": "2025-10-28T11:00:00.000Z",
    "fields": ["timestamp", "source", "message"]
  }' \
  -o results.csv
```

**Statistics API**:

```bash
# Field statistics
curl -X GET "$GRAYLOG_API/search/universal/relative/stats" \
  -H "Authorization: Basic $(echo -n 'admin:admin' | base64)" \
  -G \
  --data-urlencode "field=response_time" \
  --data-urlencode "query=*" \
  --data-urlencode "range=3600"

# Field histogram
curl -X GET "$GRAYLOG_API/search/universal/relative/histogram" \
  -H "Authorization: Basic $(echo -n 'admin:admin' | base64)" \
  -G \
  --data-urlencode "query=*" \
  --data-urlencode "interval=minute" \
  --data-urlencode "range=3600"
```

**Stream API**:

```bash
# List streams
curl -X GET "$GRAYLOG_API/streams" \
  -H "Authorization: Basic $(echo -n 'admin:admin' | base64)"

# Create stream
curl -X POST "$GRAYLOG_API/streams" \
  -H "Authorization: Basic $(echo -n 'admin:admin' | base64)" \
  -H "Content-Type: application/json" \
  -d '{
    "title": "Security Events",
    "description": "Stream for security-related events",
    "matching_type": "AND",
    "rules": [
      {
        "field": "level",
        "type": 5,
        "inverted": false,
        "value": "4"
      }
    ],
    "remove_matches_from_default_stream": false
  }'
```

### Log Analysis Workflow (CTF Scenario)

**1. Initial reconnaissance**:

```
# Overview of message volume
Search: *
Time: Last 24 hours
View: Histogram

# Identify sources
Quick values: source

# Identify applications
Quick values: application
```

**2. Hunt for attacks**:

```
# Authentication attacks
message:"Failed password" OR message:"authentication failure"

# Web attacks
message:/(UNION|SELECT|XSS|javascript:|eval\(|<script)/

# Privilege escalation
application:(sudo OR su) AND message:(COMMAND OR granted)

# File access anomalies
path:/(etc\/passwd|\.ssh|\.bash_history)/
```

**3. Pivot on indicators**:

```
# Find all activity from suspicious IP
source_ip:192.168.1.100

# Timeline of user actions
user:suspicious_user AND _exists_:action

# Related events (same session)
session_id:abc123def456
```

**4. Correlate across logs**:

```
# Match web + auth logs
(type:web_access OR application:sshd) AND source_ip:192.168.1.100

# Cross-reference timestamps
timestamp:[2025-10-28T10:15:00.000Z TO 2025-10-28T10:20:00.000Z] AND 
(message:exploit OR message:"command executed")
```

**5. Export findings**:

```bash
# Via API
curl -X POST "$GRAYLOG_API/search/universal/absolute/export" \
  -H "Authorization: Basic $(echo -n 'admin:admin' | base64)" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "source_ip:192.168.1.100",
    "from": "2025-10-28T00:00:00.000Z",
    "to": "2025-10-28T23:59:59.999Z",
    "fields": ["timestamp", "source", "application", "message"]
  }' \
  -o incident_logs.csv
```

### Performance and Troubleshooting

**Check Graylog health**:

```bash
# System health
curl -X GET "$GRAYLOG_API/system" \
  -H "Authorization: Basic $(echo -n 'admin:admin' | base64)"

# Index statistics
curl -X GET "$GRAYLOG_API/system/indices/ranges" \
  -H "Authorization: Basic $(echo -n 'admin:admin' | base64)"

# Journal status (buffered messages)
curl -X GET "$GRAYLOG_API/system/journal" \
  -H "Authorization: Basic $(echo -n 'admin:admin' | base64)"
```

**Optimize Elasticsearch** (critical for large datasets):

```bash
# Check cluster health
curl -X GET "http://localhost:9200/_cluster/health?pretty"

# Index statistics
curl -X GET "http://localhost:9200/_cat/indices?v"

# Force index optimization (merge segments)
curl -X POST "http://localhost:9200/graylog_*/_forcemerge?max_num_segments=1"

# Delete old indices
curl -X DELETE "http://localhost:9200/graylog_2025-10-01"
````

**Graylog configuration tuning** (`/etc/graylog/server/server.conf`):
```conf
# Increase message processing
processbuffer_processors = 5
outputbuffer_processors = 3

# Message journal settings (reliability vs. performance)
message_journal_enabled = true
message_journal_dir = /var/lib/graylog-server/journal
message_journal_max_size = 5gb
message_journal_flush_interval = 1000ms

# Increase ring buffer sizes (high-volume environments)
ring_size = 65536
inputbuffer_ring_size = 65536
inputbuffer_processors = 2
outputbuffer_ring_size = 65536

# Elasticsearch batch size
output_batch_size = 500
output_flush_interval = 1

# Index optimization
elasticsearch_max_docs_per_index = 20000000
elasticsearch_max_number_of_indices = 20
elasticsearch_shards = 4
elasticsearch_replicas = 0  # No replicas for single-node

# Disable expensive features if not needed
allow_leading_wildcard_searches = false
allow_highlighting = false
````

**Monitor Graylog performance**:

```bash
# Check journal usage
sudo du -sh /var/lib/graylog-server/journal/

# Monitor input throughput (messages/sec)
curl -X GET "$GRAYLOG_API/system/metrics/org.graylog2.throughput.input.1-sec-rate" \
  -H "Authorization: Basic $(echo -n 'admin:admin' | base64)"

# Monitor output throughput
curl -X GET "$GRAYLOG_API/system/metrics/org.graylog2.throughput.output.1-sec-rate" \
  -H "Authorization: Basic $(echo -n 'admin:admin' | base64)"

# Check buffer utilization
curl -X GET "$GRAYLOG_API/system/buffers" \
  -H "Authorization: Basic $(echo -n 'admin:admin' | base64)"
```

**Debug input issues**:

```bash
# Check if input is receiving messages
curl -X GET "$GRAYLOG_API/system/inputs" \
  -H "Authorization: Basic $(echo -n 'admin:admin' | base64)" | \
  jq '.inputs[] | {title: .title, metrics: .metrics}'

# View input errors
docker-compose logs graylog | grep -i "error\|exception"

# Test connectivity
# For syslog:
nc -u 127.0.0.1 1514 <<< "Test message"

# For GELF:
echo -e '{"version":"1.1","host":"test","short_message":"test"}' | \
  nc -u 127.0.0.1 12201
```

**Elasticsearch troubleshooting**:

```bash
# Check index health
curl -X GET "http://localhost:9200/_cat/indices?v&health=yellow"

# View shard allocation
curl -X GET "http://localhost:9200/_cat/shards?v"

# Check unassigned shards
curl -X GET "http://localhost:9200/_cluster/allocation/explain?pretty"

# Clear cache if needed
curl -X POST "http://localhost:9200/_cache/clear"

# Fix read-only indices (disk space issue)
curl -X PUT "http://localhost:9200/_all/_settings" \
  -H 'Content-Type: application/json' \
  -d '{"index.blocks.read_only_allow_delete": null}'
```

### Content Packs

Graylog Content Packs bundle inputs, extractors, streams, dashboards, and alerts for easy deployment.

**Export content pack** (Web UI):

1. System  Content Packs
2. Create content pack
3. Select components to include
4. Export as JSON

**Import content pack**:

1. System  Content Packs
2. Upload JSON file
3. Install content pack

**Example content pack structure** (JSON):

```json
{
  "v": "1",
  "id": "security-monitoring-pack",
  "rev": 1,
  "name": "Security Monitoring Pack",
  "summary": "CTF security event monitoring",
  "description": "Comprehensive security monitoring setup",
  "vendor": "Custom",
  "url": "",
  "parameters": [],
  "entities": [
    {
      "v": "1",
      "type": {
        "name": "input",
        "version": "1"
      },
      "id": "security-syslog-input",
      "data": {
        "title": {
          "@type": "string",
          "@value": "Security Syslog UDP"
        },
        "type": {
          "@type": "string",
          "@value": "org.graylog2.inputs.syslog.udp.SyslogUDPInput"
        },
        "global": {
          "@type": "boolean",
          "@value": true
        },
        "configuration": {
          "bind_address": {
            "@type": "string",
            "@value": "0.0.0.0"
          },
          "port": {
            "@type": "integer",
            "@value": 1514
          }
        }
      }
    }
  ]
}
```

### Lookup Tables

Lookup tables enrich logs with external data (threat intelligence, asset inventory).

**Create lookup table** (System  Lookup Tables):

**Example: IP reputation lookup**

1. Create Data Adapter:
    
    - Type: CSV file
    - File path: `/etc/graylog/threat-intel.csv`
    - CSV format: `ip,reputation,country`
2. Create Cache:
    
    - Type: On-heap cache
    - Expire after access: 60 minutes
    - Maximum entries: 10000
3. Create Lookup Table:
    
    - Name: "threat_intel"
    - Data adapter: CSV threat intel
    - Cache: threat intel cache

**Use in pipeline rule**:

```
rule "enrich_with_threat_intel"
when
  has_field("source_ip")
then
  let threat_info = lookup("threat_intel", to_string($message.source_ip));
  
  if threat_info != null then
    set_field("threat_reputation", threat_info.reputation);
    set_field("threat_country", threat_info.country);
    
    if to_long(threat_info.reputation) > 7 then
      set_field("alert_type", "high_risk_ip");
      route_to_stream("security_alerts");
    end
  end
end
```

**CSV file example** (`/etc/graylog/threat-intel.csv`):

```csv
ip,reputation,country
192.168.1.100,9,CN
10.0.0.50,3,US
172.16.5.10,8,RU
```

### Integration with Other Tools

**Forward to Splunk**:

```conf
# Graylog Output  Syslog Output
Protocol: TCP
Host: splunk.example.com
Port: 514
```

**Forward to ELK Stack**:

```conf
# Use Logstash output in Graylog
# Or export via API and index in Elasticsearch
```

**Integration with SOAR platforms**:

```bash
# Use HTTP callback in alerts
# Webhook URL: https://soar-platform.example.com/api/incidents
# Body template (JSON):
{
  "event_definition_title": "${event_definition_title}",
  "event_definition_description": "${event_definition_description}",
  "event_timestamp": "${event_timestamp}",
  "source_ip": "${source.source_ip}",
  "message": "${source.message}"
}
```

**Export to SIEM** (continuous feed):

```bash
# Create output in Graylog (System  Outputs)
# Type: GELF Output
# Target: siem.example.com:12201

# Attach output to stream for selective forwarding
```

### Advanced Analysis Techniques

**Statistical anomaly detection** [Inference]:

```
# Create alert based on standard deviation
Condition: Field aggregation
Field: response_time
Function: stddev
Threshold: > 2 * normal_stddev
Time: 5 minutes

# Requires baseline calculation
```

**Session reconstruction**:

```
# Search by session ID
session_id:abc123

# Order by timestamp
Sort: timestamp:asc

# Export timeline
Fields: timestamp, source_ip, request, response, user_agent
```

**Attack chain visualization**:

```
# Step 1: Initial compromise
message:"exploit" AND timestamp:[T1 TO T2]

# Step 2: Privilege escalation
source_ip:attacker_ip AND message:"sudo" AND timestamp:[T2 TO T3]

# Step 3: Lateral movement
source_ip:attacker_ip AND destination_ip:internal_network AND timestamp:[T3 TO T4]

# Step 4: Data exfiltration
source_ip:attacker_ip AND bytes:>10000000 AND timestamp:[T4 TO T5]
```

**Baseline deviation detection**:

```
# Establish baseline (7-day average)
Query: *
Aggregation: avg(bytes_sent)
Time: Last 7 days
Group by: source_ip

# Alert on deviation
Condition: avg(bytes_sent) > baseline_avg * 3
Time: Last 1 hour
```

### Backup and Recovery

**Backup Graylog configuration**:

```bash
# Backup MongoDB (configuration data)
mongodump --db graylog --out /backup/graylog-mongo-$(date +%F)

# Backup Graylog server configuration
sudo cp -r /etc/graylog /backup/graylog-config-$(date +%F)

# Backup content packs
curl -X GET "$GRAYLOG_API/system/content_packs" \
  -H "Authorization: Basic $(echo -n 'admin:admin' | base64)" \
  -o /backup/content-packs-$(date +%F).json
```

**Backup Elasticsearch indices**:

```bash
# Create snapshot repository
curl -X PUT "http://localhost:9200/_snapshot/backup_repo" \
  -H 'Content-Type: application/json' \
  -d '{
    "type": "fs",
    "settings": {
      "location": "/backup/elasticsearch"
    }
  }'

# Create snapshot
curl -X PUT "http://localhost:9200/_snapshot/backup_repo/snapshot_$(date +%F)" \
  -H 'Content-Type: application/json' \
  -d '{
    "indices": "graylog_*",
    "ignore_unavailable": true,
    "include_global_state": false
  }'

# List snapshots
curl -X GET "http://localhost:9200/_snapshot/backup_repo/_all?pretty"
```

**Restore from backup**:

```bash
# Restore MongoDB
mongorestore --db graylog /backup/graylog-mongo-2025-10-28/graylog/

# Restore Elasticsearch snapshot
curl -X POST "http://localhost:9200/_snapshot/backup_repo/snapshot_2025-10-28/_restore" \
  -H 'Content-Type: application/json' \
  -d '{
    "indices": "graylog_*",
    "ignore_unavailable": true
  }'
```

### Security Hardening

**Enable TLS for web interface** (`/etc/graylog/server/server.conf`):

```conf
http_enable_tls = true
http_tls_cert_file = /etc/graylog/ssl/cert.pem
http_tls_key_file = /etc/graylog/ssl/key.pem
http_tls_key_password = secret
```

**Restrict API access**:

```conf
# Bind to specific interface
http_bind_address = 127.0.0.1:9000

# Use reverse proxy (nginx) for external access
# Enable authentication on all endpoints
```

**Enable audit logging** [Unverified - Enterprise feature]:

```conf
# May require Graylog Enterprise
audit_event_retention_days = 90
```

**Input authentication** (syslog with TLS):

```
# Configure TLS input
Type: Syslog TCP
Port: 6514
Enable TLS: Yes
Certificate: /path/to/cert.pem
Key: /path/to/key.pem
Client auth: Optional/Required
```

---

## Cross-Tool Integration Workflow

**Centralized logging pipeline**:

```
[Application Servers] 
     (rsyslog/syslog-ng)
[Log Aggregation Layer - syslog-ng]
     (parsing, filtering)
[Processing Layer - Fluentd]
     (enrichment, routing)
[Storage/Analysis - Graylog/Elasticsearch]
     (visualization, alerting)
[SIEM/SOAR Integration]
```

**Example multi-tool configuration**:

**1. Application  syslog-ng** (collect and forward):

```conf
# /etc/syslog-ng/syslog-ng.conf
source s_apps {
    file("/var/log/app/*.log" follow-freq(1));
};

destination d_fluentd {
    syslog("127.0.0.1" port(5140) transport("tcp"));
};

log {
    source(s_apps);
    destination(d_fluentd);
};
```

**2. syslog-ng  Fluentd** (parse and enrich):

```conf
# /etc/td-agent/td-agent.conf
<source>
  @type syslog
  port 5140
  bind 127.0.0.1
  tag syslog.forward
</source>

<filter syslog.forward>
  @type record_transformer
  <record>
    hostname "#{Socket.gethostname}"
    environment "production"
  </record>
</filter>

<match syslog.forward>
  @type gelf
  host graylog.example.com
  port 12201
  protocol udp
</match>
```

**3. Fluentd  Graylog** (analyze and alert):

- Receive via GELF input (port 12201)
- Create streams for different log types
- Configure pipelines for enrichment
- Set up dashboards and alerts

## CTF-Specific Considerations

**Important investigation patterns**:

1. **Timeline reconstruction**: Use Graylog's search with absolute time ranges, export to CSV, analyze with timeline tools
2. **Indicator extraction**: Use Quick Values in Graylog to identify top IPs, users, commands
3. **Correlation**: Cross-reference web server logs (syslog-ng) with database logs (Fluentd) using timestamps and IPs
4. **Persistence detection**: Search for modifications to startup scripts, cron jobs, user accounts across all aggregated logs

**Performance notes for CTF environments**:

- **syslog-ng**: Minimal resource usage, suitable for resource-constrained VMs
- **Fluentd**: Higher memory usage but excellent parsing capabilities
- **Graylog**: Requires significant resources (Elasticsearch), consider reducing retention or using smaller heap sizes for lab environments

**Common misconfigurations to exploit** [Inference]:

- Default credentials (Graylog admin:admin)
- Unauthenticated syslog-ng network inputs accepting from 0.0.0.0
- Fluentd HTTP inputs without authentication
- Exposed Elasticsearch ports (9200) without authentication
- Graylog API accessible without rate limiting

---

## Critical Subtopics for Further Study

**Essential related topics**:

- **Log retention and rotation**: Managing disk space with logrotate, Elasticsearch ILM (Index Lifecycle Management), archive strategies
- **Normalized log formats**: CEF, LEEF, Syslog RFC formats for cross-platform compatibility
- **Performance optimization**: Buffer tuning, batch processing, horizontal scaling
- **Threat hunting queries**: Advanced search patterns for MITRE ATT&CK techniques
- **Compliance requirements**: HIPAA, PCI-DSS, GDPR log retention and protection mandates

---

# Timeline Analysis

Timeline analysis is fundamental to CTF log forensics, enabling reconstruction of attack sequences, identification of lateral movement patterns, and correlation of events across disparate systems. Proper timeline analysis requires precise event sequencing, multi-source correlation, and timezone normalization to establish causality and detect sophisticated attack chains.

## Event Sequencing

Event sequencing involves ordering log entries chronologically to reconstruct the narrative of system interactions, attacks, or data flows. Accurate sequencing is critical for distinguishing cause-effect relationships from coincidental timing.

### Extracting Timestamps from Various Log Formats

**Syslog Format (RFC 3164):**

```bash
# Standard syslog: Oct 28 10:15:23
grep "^[A-Z][a-z][a-z] [0-9 ]\{2\} [0-9]\{2\}:[0-9]\{2\}:[0-9]\{2\}" /var/log/syslog

# Extract and normalize to sortable format
awk '{print $1,$2,$3}' /var/log/syslog | \
while read month day time; do
    date -d "$month $day $(date +%Y) $time" "+%Y-%m-%d %H:%M:%S"
done
```

**ISO 8601 Format:**

```bash
# 2025-10-28T10:15:23.123456Z or 2025-10-28 10:15:23
grep -oP "\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2}(\.\d+)?(Z|[+-]\d{2}:\d{2})?" logfile.log

# Sort ISO timestamps naturally
sort -t'T' -k1,1 -k2,2 logfile.log
```

**Epoch/Unix Timestamps:**

```bash
# Convert epoch to human-readable
awk '{print strftime("%Y-%m-%d %H:%M:%S", $1)}' epoch_times.txt

# Extract and convert embedded epoch timestamps
grep -oP "\d{10}" logfile.log | while read epoch; do
    echo "$epoch => $(date -d @$epoch '+%Y-%m-%d %H:%M:%S')"
done
```

**Windows Event Log Timestamps:**

```powershell
# Extract from EVTX
Get-WinEvent -Path .\Security.evtx | Select-Object TimeCreated,Id,Message | 
Sort-Object TimeCreated | Export-Csv timeline.csv

# Convert to Unix epoch for cross-platform correlation
Get-WinEvent -Path .\Security.evtx | ForEach-Object {
    [Math]::Floor((Get-Date $_.TimeCreated).ToUniversalTime().Subtract((Get-Date "1970-01-01")).TotalSeconds)
}
```

### Multi-Source Event Merging

**Log2timeline/Plaso (Comprehensive Timeline Tool):**

```bash
# Install
apt-get install plaso-tools

# Create super timeline from multiple sources
log2timeline.py --storage-file timeline.plaso \
    --parsers linux,apache,nginx,mysql,ssh \
    /var/log/ /home/*/.*_history /etc/

# Export to CSV
psort.py -o l2tcsv -w timeline.csv timeline.plaso

# Filter by date range
psort.py -o l2tcsv -w filtered.csv timeline.plaso \
    "date >= '2025-10-28 00:00:00' AND date <= '2025-10-28 23:59:59'"
```

**Manual Bash Merging Script:**

```bash
#!/bin/bash
# merge_timelines.sh - Merge multiple log sources with normalized timestamps

OUTPUT="merged_timeline.log"
> "$OUTPUT"

# Apache access logs
awk '{
    cmd="date -d\""$4" "$5"\" +%Y-%m-%dT%H:%M:%S 2>/dev/null"
    cmd | getline timestamp
    close(cmd)
    if(timestamp != "") print timestamp" [Apache] "$0
}' /var/log/apache2/access.log >> "$OUTPUT"

# MySQL query logs
grep -E "^[0-9]{4}-[0-9]{2}-[0-9]{2}T" /var/log/mysql/query.log | \
awk '{print $1" [MySQL] "$0}' >> "$OUTPUT"

# SSH auth logs
grep "sshd" /var/log/auth.log | \
awk '{
    timestamp=$1" "$2" "$3
    cmd="date -d\""timestamp" $(date +%Y)\" +%Y-%m-%dT%H:%M:%S 2>/dev/null"
    cmd | getline iso_time
    close(cmd)
    if(iso_time != "") print iso_time" [SSH] "$0
}' >> "$OUTPUT"

# Sort by timestamp
sort -t'T' -k1,1 -k2,2 "$OUTPUT" -o "$OUTPUT"
echo "Timeline created: $OUTPUT"
```

### Identifying Event Sequences and Patterns

**SQL Injection Attack Sequence Example:**

```bash
# Stage 1: Reconnaissance (error-based enumeration)
grep "syntax error\|mysql_fetch" /var/log/apache2/error.log | \
grep -oP "\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}" | head -1

# Stage 2: Exploitation (union-based injection)
grep -i "union.*select" /var/log/mysql/query.log | \
grep -oP "^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}" | head -1

# Stage 3: Privilege escalation (user enumeration)
grep "SELECT.*FROM.*mysql.user" /var/log/mysql/query.log | \
grep -oP "^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}" | head -1

# Stage 4: Data exfiltration (into outfile)
grep -i "into outfile" /var/log/mysql/query.log | \
grep -oP "^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}" | head -1
```

**Detecting Time-Ordered Anomalies:**

```bash
# Identify rapid-fire requests (potential automation)
awk '{print $1,$2}' access.log | uniq -c | awk '$1 > 100 {print}'

# Find gaps in sequential event IDs (potential log tampering)
awk '{print $NF}' structured.log | sort -n | awk 'NR>1 {gap=$1-prev; if(gap>1) print "Gap:",prev+1,"to",$1-1; prev=$1} NR==1{prev=$1}'
```

### Relative Time Delta Analysis

**Calculate Time Differences Between Events:**

```bash
#!/bin/bash
# time_delta.sh - Calculate seconds between consecutive events

LOGFILE="$1"
PREV_EPOCH=0

grep -oP "\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}" "$LOGFILE" | while read timestamp; do
    CURRENT_EPOCH=$(date -d "$timestamp" +%s)
    
    if [ $PREV_EPOCH -ne 0 ]; then
        DELTA=$((CURRENT_EPOCH - PREV_EPOCH))
        echo "$timestamp | Delta: ${DELTA}s"
        
        # Flag suspiciously fast sequences (< 1 second)
        if [ $DELTA -lt 1 ] && [ $DELTA -gt 0 ]; then
            echo "  [!] Automated activity detected"
        fi
    fi
    
    PREV_EPOCH=$CURRENT_EPOCH
done
```

**Python Alternative for Precise Millisecond Deltas:**

```python
#!/usr/bin/env python3
import re
from datetime import datetime

log_pattern = r'(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}\.?\d*)'
prev_time = None

with open('logfile.log') as f:
    for line in f:
        match = re.search(log_pattern, line)
        if match:
            current_time = datetime.fromisoformat(match.group(1).rstrip('Z'))
            
            if prev_time:
                delta = (current_time - prev_time).total_seconds()
                print(f"{current_time} | Delta: {delta:.3f}s")
                
                if delta < 0:
                    print(f"  [!] TIMESTAMP ANOMALY: Negative delta detected")
            
            prev_time = current_time
```

## Time Correlation

Time correlation links events across multiple systems, services, or log sources to establish causal relationships and reconstruct complete attack chains. This requires handling clock skew, latency, and distributed system timing variations.

### Cross-System Event Correlation

**Correlating Web Access with Database Queries:**

```bash
# Extract user action from Apache access log at specific time
WEB_TIME="2025-10-28T14:32:15"
grep "$WEB_TIME" /var/log/apache2/access.log | \
grep -oP "\d{1,3}(\.\d{1,3}){3}" | head -1 > /tmp/source_ip.txt

SOURCE_IP=$(cat /tmp/source_ip.txt)

# Find corresponding database queries (within 5 second window)
START_TIME=$(date -d "$WEB_TIME -5 seconds" +%Y-%m-%dT%H:%M:%S)
END_TIME=$(date -d "$WEB_TIME +5 seconds" +%Y-%m-%dT%H:%M:%S)

awk -v start="$START_TIME" -v end="$END_TIME" \
    '$1 >= start && $1 <= end' /var/log/mysql/query.log
```

**Network Flow to Application Log Correlation:**

```bash
# Extract connection 5-tuple from network flow
CONN_TIME="2025-10-28 14:32:15"
SRC_IP="192.168.1.100"
DST_PORT="3306"

# Find application logs within 2 second window
START_EPOCH=$(date -d "$CONN_TIME -2 seconds" +%s)
END_EPOCH=$(date -d "$CONN_TIME +2 seconds" +%s)

grep "$SRC_IP" /var/log/mysql/mysql.log | while read line; do
    LINE_TIME=$(echo "$line" | grep -oP "\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}")
    LINE_EPOCH=$(date -d "$LINE_TIME" +%s 2>/dev/null)
    
    if [ $LINE_EPOCH -ge $START_EPOCH ] && [ $LINE_EPOCH -le $END_EPOCH ]; then
        echo "[CORRELATED] $line"
    fi
done
```

### Session Reconstruction

**HTTP Session Timeline from Session IDs:**

```bash
# Extract all requests for specific session
SESSION_ID="a3f8d9e2b1c4567890abcdef12345678"

grep "$SESSION_ID" /var/log/apache2/access.log | \
awk '{print $4,$5,$7,$9}' | \
sed 's/\[//g; s/\]//g' | \
while read timestamp tz path status; do
    echo "$(date -d "$timestamp $tz" +%Y-%m-%dT%H:%M:%S) | $path | HTTP $status"
done | sort
```

**SSH Session Duration Calculation:**

```bash
# Find login/logout pairs
grep "session opened" /var/log/auth.log | \
while read line; do
    USER=$(echo "$line" | grep -oP "for user \K\w+")
    PID=$(echo "$line" | grep -oP "pam_unix\(\w+:session\): session opened for user \w+ by \(uid=\d+\).*" | grep -oP "\(\d+\)" | tr -d "()")
    LOGIN_TIME=$(echo "$line" | awk '{print $1,$2,$3}')
    
    LOGOUT_LINE=$(grep "session closed for user $USER" /var/log/auth.log | grep -F "$PID" | head -1)
    
    if [ ! -z "$LOGOUT_LINE" ]; then
        LOGOUT_TIME=$(echo "$LOGOUT_LINE" | awk '{print $1,$2,$3}')
        
        LOGIN_EPOCH=$(date -d "$LOGIN_TIME $(date +%Y)" +%s)
        LOGOUT_EPOCH=$(date -d "$LOGOUT_TIME $(date +%Y)" +%s)
        DURATION=$((LOGOUT_EPOCH - LOGIN_EPOCH))
        
        echo "$USER | Login: $LOGIN_TIME | Duration: ${DURATION}s"
    fi
done
```

### Statistical Correlation Analysis

**Finding Temporally Clustered Events:**

```bash
# Identify time windows with anomalous event density
awk '{print $1}' access.log | \
cut -d: -f1-2 | \
sort | uniq -c | \
awk '$1 > 1000 {print "High activity at",$2,$3,"("$1" events)"}'
```

**Python Pearson Correlation Between Log Sources:**

```python
#!/usr/bin/env python3
import pandas as pd
from scipy.stats import pearsonr

# Load event counts per minute from two sources
web_events = pd.read_csv('web_events_per_minute.csv', names=['minute', 'count'])
db_events = pd.read_csv('db_events_per_minute.csv', names=['minute', 'count'])

# Merge on timestamp
merged = pd.merge(web_events, db_events, on='minute', suffixes=('_web', '_db'))

# Calculate correlation
correlation, p_value = pearsonr(merged['count_web'], merged['count_db'])
print(f"Correlation coefficient: {correlation:.3f}")
print(f"P-value: {p_value:.6f}")

if correlation > 0.7:
    print("[+] Strong positive correlation detected between web and DB activity")
```

### Handling Clock Skew

**Detecting Clock Drift Between Systems:**

```bash
# Compare timestamps of same event logged on two systems
# System A logged at: 2025-10-28T10:15:23
# System B logged at: 2025-10-28T10:15:29
# Detected skew: 6 seconds

# Calculate average skew from multiple synchronized events
echo "SystemA_Time SystemB_Time" > skew_data.txt
echo "2025-10-28T10:15:23 2025-10-28T10:15:29" >> skew_data.txt
echo "2025-10-28T10:20:45 2025-10-28T10:20:51" >> skew_data.txt

awk 'NR>1 {
    cmd1="date -d "$1" +%s"
    cmd2="date -d "$2" +%s"
    cmd1 | getline t1
    cmd2 | getline t2
    close(cmd1)
    close(cmd2)
    print t2-t1
}' skew_data.txt | awk '{sum+=$1; count++} END {print "Average skew:", sum/count, "seconds"}'
```

**Compensating for Known Skew:**

```bash
# Apply correction to System B timestamps (6 second lag)
SKEW_SECONDS=6

awk -v skew=$SKEW_SECONDS '{
    timestamp=$1
    cmd="date -d\""timestamp" -"skew" seconds\" +%Y-%m-%dT%H:%M:%S"
    cmd | getline corrected
    close(cmd)
    $1=corrected
    print
}' systemB.log > systemB_corrected.log
```

### Network Time Protocol (NTP) Analysis

**Examining NTP Synchronization Logs:**

```bash
# Check NTP sync status
ntpq -p

# Parse ntpd logs for time adjustments
grep "time correction" /var/log/syslog | \
awk '{print $1,$2,$3,"Adjustment:",$NF}'

# Large time jumps may indicate attacks or system issues
grep "step" /var/log/syslog | grep -i "time"
```

## Time Zone Conversion

Time zone handling is critical when correlating logs from geographically distributed systems, cloud services, or international CTF infrastructure. Incorrect timezone interpretation can misalign events by hours, breaking causal analysis.

### Identifying Time Zone Indicators

**Common Timezone Formats:**

```
UTC/Zulu:           2025-10-28T10:15:23Z
                    2025-10-28T10:15:23+00:00

UTC Offset:         2025-10-28T10:15:23+08:00 (UTC+8)
                    2025-10-28T10:15:23-05:00 (UTC-5)

Named Timezone:     2025-10-28 10:15:23 PST
                    2025-10-28 10:15:23 EST

Implicit Local:     2025-10-28 10:15:23 (assumes system timezone)
```

**Detecting Timezone from Log Context:**

```bash
# Check for explicit timezone markers
grep -oP "[+-]\d{2}:\d{2}|Z|UTC|GMT|[A-Z]{3}T" logfile.log | sort -u

# System timezone configuration
timedatectl status
cat /etc/timezone
ls -l /etc/localtime
```

### Converting to Common Reference (UTC)

**Using `date` Command:**

```bash
# Convert local time to UTC
date -u -d "2025-10-28 10:15:23 PST" "+%Y-%m-%dT%H:%M:%SZ"

# Convert from specific timezone
TZ="America/New_York" date -d "2025-10-28 10:15:23" -u "+%Y-%m-%dT%H:%M:%SZ"

# Batch conversion script
while read line; do
    TIMESTAMP=$(echo "$line" | grep -oP "\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}")
    UTC_TIME=$(TZ="Asia/Manila" date -u -d "$TIMESTAMP" "+%Y-%m-%dT%H:%M:%SZ")
    echo "$UTC_TIME | $line"
done < local_time_logs.txt
```

**Using Python for Complex Conversions:**

```python
#!/usr/bin/env python3
from datetime import datetime
import pytz

def convert_to_utc(timestamp_str, source_tz_name):
    """
    Convert timestamp from source timezone to UTC
    
    Args:
        timestamp_str: "2025-10-28 10:15:23"
        source_tz_name: "US/Pacific", "Asia/Manila", etc.
    """
    source_tz = pytz.timezone(source_tz_name)
    
    # Parse timestamp as naive datetime
    naive_dt = datetime.strptime(timestamp_str, "%Y-%m-%d %H:%M:%S")
    
    # Localize to source timezone
    localized_dt = source_tz.localize(naive_dt)
    
    # Convert to UTC
    utc_dt = localized_dt.astimezone(pytz.UTC)
    
    return utc_dt.strftime("%Y-%m-%dT%H:%M:%SZ")

# Example usage
timestamps = [
    ("2025-10-28 10:15:23", "US/Pacific"),
    ("2025-10-28 18:15:23", "Asia/Manila"),
    ("2025-10-28 14:15:23", "US/Eastern")
]

for ts, tz in timestamps:
    print(f"{ts} {tz:15} => {convert_to_utc(ts, tz)}")
```

### Handling Daylight Saving Time (DST)

**DST Transition Detection:**

```bash
# List DST transitions for a timezone
zdump -v /usr/share/zoneinfo/America/New_York | grep 2025

# Example output shows "spring forward" and "fall back" times
# Mar 09 2025 02:00:00 EST -> 03:00:00 EDT (clocks jump forward)
# Nov 02 2025 02:00:00 EDT -> 01:00:00 EST (clocks jump back)
```

**Handling Ambiguous Times During DST:**

```python
#!/usr/bin/env python3
from datetime import datetime
import pytz

# Ambiguous time example: 2025-11-02 01:30:00 occurs twice
# Once at 01:30 EDT, then again at 01:30 EST after clocks "fall back"

eastern = pytz.timezone('US/Eastern')
ambiguous_time = "2025-11-02 01:30:00"

try:
    # Try to localize (will raise exception if ambiguous)
    dt = eastern.localize(datetime.strptime(ambiguous_time, "%Y-%m-%d %H:%M:%S"))
    print(f"Unambiguous: {dt}")
except pytz.exceptions.AmbiguousTimeError:
    # Explicitly choose which occurrence
    dt_first = eastern.localize(
        datetime.strptime(ambiguous_time, "%Y-%m-%d %H:%M:%S"),
        is_dst=True  # First occurrence (EDT)
    )
    dt_second = eastern.localize(
        datetime.strptime(ambiguous_time, "%Y-%m-%d %H:%M:%S"),
        is_dst=False  # Second occurrence (EST)
    )
    
    print(f"[!] Ambiguous timestamp detected")
    print(f"First occurrence:  {dt_first} ({dt_first.tzname()})")
    print(f"Second occurrence: {dt_second} ({dt_second.tzname()})")
```

### Windows Event Log Timezone Handling

**EVTX Timestamps (Always Stored in UTC):**

```powershell
# Extract and display in local time
Get-WinEvent -Path .\Security.evtx | Select-Object @{
    Name='LocalTime'
    Expression={$_.TimeCreated.ToLocalTime()}
},Id,Message

# Convert to specific timezone
$manila = [System.TimeZoneInfo]::FindSystemTimeZoneById("Singapore Standard Time")
Get-WinEvent -Path .\Security.evtx | Select-Object @{
    Name='ManilaTime'
    Expression={[System.TimeZoneInfo]::ConvertTimeFromUtc($_.TimeCreated.ToUniversalTime(), $manila)}
},Id,Message

# Export UTC timestamps for cross-platform correlation
Get-WinEvent -Path .\Security.evtx | Select-Object @{
    Name='UTC'
    Expression={$_.TimeCreated.ToUniversalTime().ToString("yyyy-MM-ddTHH:mm:ssZ")}
},Id | Export-Csv utc_events.csv
```

### Database Timezone Considerations

**MySQL Timezone Handling:**

```sql
-- Check current timezone settings
SELECT @@global.time_zone, @@session.time_zone;

-- Convert stored times to specific timezone
SELECT 
    event_time,
    CONVERT_TZ(event_time, '+00:00', '+08:00') AS manila_time,
    CONVERT_TZ(event_time, '+00:00', '-05:00') AS eastern_time
FROM audit_log;

-- Timestamps stored as UTC epoch (common practice)
SELECT 
    FROM_UNIXTIME(epoch_time) AS utc_time,
    CONVERT_TZ(FROM_UNIXTIME(epoch_time), '+00:00', '+08:00') AS local_time
FROM events;
```

**PostgreSQL Timezone Handling:**

```sql
-- Show timezone setting
SHOW timezone;

-- Timestamps with timezone are automatically converted
SELECT 
    event_time AT TIME ZONE 'UTC' AS utc_time,
    event_time AT TIME ZONE 'Asia/Manila' AS manila_time,
    event_time AT TIME ZONE 'America/New_York' AS ny_time
FROM audit_log;

-- Convert between timezones
SELECT 
    timestamp '2025-10-28 10:15:23' AT TIME ZONE 'America/Los_Angeles' AT TIME ZONE 'UTC';
```

### CTF-Specific Timezone Scenarios

**Geolocation from Timezone Inference:**

```bash
# If attacker's local time leaked in logs, infer timezone
# Example: Application error shows "2025-10-28 18:15:23" when UTC was 10:15:23
# Offset: +8 hours => Likely Asia/Manila, Asia/Singapore, Asia/Shanghai, etc.

UTC_TIME="2025-10-28T10:15:23Z"
LOCAL_TIME="2025-10-28 18:15:23"

UTC_EPOCH=$(date -d "$UTC_TIME" +%s)
LOCAL_EPOCH=$(date -d "$LOCAL_TIME" +%s)
OFFSET=$(( (LOCAL_EPOCH - UTC_EPOCH) / 3600 ))

echo "Detected UTC offset: +${OFFSET} hours"
grep -E "^\+0${OFFSET}00|UTC\+${OFFSET}" /usr/share/zoneinfo/zone.tab
```

**Correlating Events Across Cloud Regions:**

```bash
# AWS CloudWatch Logs (UTC)
# Azure Monitor (UTC)
# On-premises system (Local time)

# Normalize all to UTC for correlation
# Example: Azure event at 10:15:23 UTC, on-prem event at 18:15:23 PHT (UTC+8)

echo "2025-10-28T10:15:23Z [Azure] User login attempt" > normalized_timeline.txt
TZ="Asia/Manila" date -u -d "2025-10-28 18:15:23" "+%Y-%m-%dT%H:%M:%SZ" | \
    awk '{print $1" [OnPrem] Database connection"}' >> normalized_timeline.txt

sort -t'T' normalized_timeline.txt
```

### Automation Tools for Timeline Analysis

**Timesketch (Collaborative Timeline Analysis):**

```bash
# Install Timesketch
docker pull us-docker.pkg.dev/osdfir-registry/timesketch/timesketch:latest
docker run -d -p 5000:5000 --name timesketch timesketch:latest

# Import Plaso timeline
timesketch_importer.py --timeline_name "CTF_Investigation" timeline.plaso

# Features:
# - Automatic timezone normalization
# - Multi-user collaboration
# - Advanced search and filtering
# - Event tagging and annotation
```

**grep Timeline Extraction with Timezone Conversion:**

```bash
#!/bin/bash
# universal_timeline.sh - Extract and normalize timestamps from any log

LOGFILE="$1"
TARGET_TZ="${2:-UTC}"  # Default to UTC if not specified

# Detect and convert various timestamp formats
grep -oP "\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2}([+-]\d{2}:\d{2}|Z)?" "$LOGFILE" | \
while read timestamp; do
    # Remove 'T' separator for date command compatibility
    clean_ts=$(echo "$timestamp" | sed 's/T/ /')
    
    # Convert to target timezone
    if [[ "$clean_ts" =~ Z$ ]] || [[ "$clean_ts" =~ [+-][0-9]{2}:[0-9]{2}$ ]]; then
        # Has timezone info
        TZ="$TARGET_TZ" date -d "$clean_ts" "+%Y-%m-%dT%H:%M:%S %Z" 2>/dev/null
    else
        # Assume local time
        TZ="$TARGET_TZ" date -d "$clean_ts" "+%Y-%m-%dT%H:%M:%S %Z" 2>/dev/null
    fi
done | sort -u
```

**Important Subtopics for Further Study**

- **High-precision timestamp analysis** (nanosecond-level correlation for race condition detection)
- **Log tampering detection** (identifying timestamp manipulation, gaps, or reordering)
- **Distributed tracing** (OpenTelemetry, Jaeger span correlation across microservices)
- **Network latency compensation** (RTT-aware event correlation for WAN scenarios)

---

## Chronological Reconstruction

Chronological reconstruction involves aggregating events from multiple sources into a unified timeline to visualize the sequence of attacker actions.

### Multi-Source Log Aggregation

**Combining Windows Event Logs:**

```bash
# Extract all logs with timestamps to JSON
for log in Security.evtx System.evtx Application.evtx; do
    evtx_dump.py --json "$log" >> raw_timeline.json
done

# Sort by timestamp and create unified timeline
jq -s 'sort_by(.timestamp) | .[]' raw_timeline.json > unified_timeline.json

# Filter to specific timeframe
jq 'select(.timestamp >= "2024-10-01T00:00:00" and .timestamp <= "2024-10-02T23:59:59")' unified_timeline.json > filtered_timeline.json
```

**Using Chainsaw for Timeline Generation:**

```bash
# Generate timeline from multiple logs
./chainsaw dump Security.evtx System.evtx Application.evtx --json --output combined_timeline.json

# Create CSV timeline for spreadsheet analysis
./chainsaw dump *.evtx --csv --output timeline.csv

# Filter timeline by Event IDs
./chainsaw dump *.evtx -e 4624 -e 4625 -e 7045 --json > filtered_events.json
```

**Timeline Formats:**

**Plaso/log2timeline Integration:**

```bash
# Install Plaso tools
apt-get install plaso-tools

# Create super timeline from EVTX files
log2timeline.py --storage-file timeline.plaso /path/to/evidence/

# Parse EVTX specifically
log2timeline.py --parsers winevtx --storage-file evtx_timeline.plaso /path/to/logs/

# Export to human-readable format
psort.py -o l2tcsv -w timeline.csv timeline.plaso

# Filter by date range
psort.py -o l2tcsv -w filtered.csv timeline.plaso "date > '2024-10-01 00:00:00' AND date < '2024-10-02 23:59:59'"
```

### Attack Phase Mapping

**MITRE ATT&CK Timeline Correlation:**

```bash
# Map events to attack phases using Chainsaw with Sigma rules
./chainsaw hunt *.evtx -s sigma_rules/ --mapping mappings/sigma-event-logs-all.yml --json > attack_timeline.json

# Extract by tactic
jq '.[] | select(.tactics[]? | contains("Initial Access"))' attack_timeline.json
jq '.[] | select(.tactics[]? | contains("Privilege Escalation"))' attack_timeline.json
jq '.[] | select(.tactics[]? | contains("Lateral Movement"))' attack_timeline.json
```

**Manual Phase Reconstruction:**

```bash
# Initial Access (Event ID 4624 Type 10 - RDP)
jq '.[] | select(.event_id == 4624 and .event_data.LogonType == "10")' unified_timeline.json

# Execution (Event ID 4688 - Process Creation)
jq '.[] | select(.event_id == 4688)' unified_timeline.json

# Persistence (Event ID 7045 - Service Installation)
jq '.[] | select(.event_id == 7045)' unified_timeline.json

# Credential Access (Event ID 4648 - Explicit Credentials)
jq '.[] | select(.event_id == 4648)' unified_timeline.json

# Lateral Movement (Event ID 4624 Type 3 - Network Logon)
jq '.[] | select(.event_id == 4624 and .event_data.LogonType == "3")' unified_timeline.json

# Exfiltration (Event ID 5145 - Share Access)
jq '.[] | select(.event_id == 5145)' unified_timeline.json
```

### User Activity Timeline

**Track Single User Actions:**

```bash
# Extract all events for specific user
TARGET_USER="administrator"

jq --arg user "$TARGET_USER" '.[] | select(
    .event_data.TargetUserName == $user or 
    .event_data.SubjectUserName == $user or
    .event_data.AccountName == $user
) | {timestamp, event_id, computer, action: .event_data}' unified_timeline.json > user_timeline.json

# Group by hour
jq -r '[.timestamp[0:13], .event_id] | @csv' user_timeline.json | sort | uniq -c
```

**Session Timeline Reconstruction:**

```bash
# Find logon event
LOGON_ID=$(jq -r 'select(.event_id == 4624 and .event_data.TargetUserName == "administrator") | .event_data.TargetLogonId' unified_timeline.json | head -1)

# Extract all events with that Logon ID
jq --arg lid "$LOGON_ID" '.[] | select(.event_data.TargetLogonId == $lid or .event_data.SubjectLogonId == $lid)' unified_timeline.json > session_timeline.json

# Find session end
jq 'select(.event_id == 4634 or .event_id == 4647)' session_timeline.json
```

### Process Execution Timeline

**Command-Line Timeline (requires Event ID 4688 with CommandLine auditing):**

```bash
# Extract process creation with command lines
jq '.[] | select(.event_id == 4688) | {
    time: .timestamp,
    process: .event_data.NewProcessName,
    cmdline: .event_data.CommandLine,
    parent: .event_data.ParentProcessName,
    user: .event_data.SubjectUserName
}' unified_timeline.json > process_timeline.json

# Identify suspicious process trees
jq -r '[.time, .parent, "->", .process, .cmdline] | @tsv' process_timeline.json
```

**PowerShell Execution Timeline:**

```bash
# Extract PowerShell script blocks chronologically
evtx_dump.py --json Microsoft-Windows-PowerShell%4Operational.evtx | \
jq 'select(.event_id == 4104) | {
    time: .timestamp,
    script_block: .event_data.ScriptBlockText,
    script_id: .event_data.ScriptBlockId
}' > powershell_timeline.json

# Order script blocks by ID and time
jq -s 'sort_by(.script_id, .time)' powershell_timeline.json
```

## Gap Analysis

Gap analysis identifies missing events, log tampering, and temporal anomalies that indicate anti-forensics or system issues.

### Detecting Missing Events

**Expected Event Pairs:**

```bash
# Logon without corresponding logoff
jq '.[] | select(.event_id == 4624) | .event_data.TargetLogonId' unified_timeline.json | sort -u > logons.txt
jq '.[] | select(.event_id == 4634) | .event_data.TargetLogonId' unified_timeline.json | sort -u > logoffs.txt

# Find logons without logoffs
comm -23 logons.txt logoffs.txt > missing_logoffs.txt

# Investigate missing logoffs
while read logon_id; do
    jq --arg lid "$logon_id" '.[] | select(.event_data.TargetLogonId == $lid or .event_data.SubjectLogonId == $lid)' unified_timeline.json
done < missing_logoffs.txt
```

**Process Start/Stop Correlation:**

```bash
# Process creation without termination
jq '.[] | select(.event_id == 4688) | .event_data.NewProcessId' unified_timeline.json | sort -u > started.txt
jq '.[] | select(.event_id == 4689) | .event_data.ProcessId' unified_timeline.json | sort -u > terminated.txt

comm -23 started.txt terminated.txt > still_running.txt
```

### Log Service Interruptions

**Detect Event Log Service Stops:**

```bash
# Event ID 1100 - Event log service shutdown
jq '.[] | select(.event_id == 1100)' unified_timeline.json

# Correlate with System events 6006 (Event Log stopped)
jq '.[] | select(.event_id == 6006)' unified_timeline.json

# Check for gaps after service stops
jq -r '.timestamp' unified_timeline.json | sort | awk '{
    if (NR > 1) {
        cmd = "date -d "$0" +%s"
        cmd | getline current
        close(cmd)
        
        gap = current - previous
        if (gap > 300) {  # 5 minute gap
            print "Gap detected: " gap " seconds between " prev_time " and " $0
        }
    }
    prev_time = $0
    cmd = "date -d "$0" +%s"
    cmd | getline previous
    close(cmd)
}'
```

### Temporal Anomaly Detection

**Identify Time Gaps:**

```bash
# Python script for gap detection
cat > gap_detector.py << 'EOF'
import json
from datetime import datetime

timeline = []
with open('unified_timeline.json', 'r') as f:
    for line in f:
        try:
            event = json.loads(line)
            timeline.append(datetime.fromisoformat(event['timestamp'].replace('Z', '+00:00')))
        except:
            continue

timeline.sort()

for i in range(1, len(timeline)):
    gap = (timeline[i] - timeline[i-1]).total_seconds()
    if gap > 300:  # 5 minutes
        print(f"Gap: {gap:.0f}s between {timeline[i-1]} and {timeline[i]}")
EOF

python3 gap_detector.py
```

**Event Density Analysis:**

```bash
# Count events per hour
jq -r '.timestamp[0:13]' unified_timeline.json | sort | uniq -c | awk '{print $2, $1}' > hourly_counts.txt

# Identify low-activity periods
awk '$2 < 10 {print "Low activity: " $1 " - " $2 " events"}' hourly_counts.txt

# Visualize with gnuplot
gnuplot << 'EOF'
set terminal dumb
set xlabel "Hour"
set ylabel "Event Count"
plot "hourly_counts.txt" using 0:2 with lines title "Events per Hour"
EOF
```

### Log Clearing Detection

**Event ID 1102 - Security Log Cleared:**

```bash
# Detect log clearing
jq '.[] | select(.event_id == 1102) | {
    time: .timestamp,
    cleared_by: .event_data.SubjectUserName,
    domain: .event_data.SubjectDomainName
}' unified_timeline.json

# Check for suspicious activity before clearing
LOG_CLEAR_TIME=$(jq -r 'select(.event_id == 1102) | .timestamp' unified_timeline.json | head -1)

# Extract events 1 hour before clearing
jq --arg clear_time "$LOG_CLEAR_TIME" '
    select(.timestamp < $clear_time and 
           (.timestamp | fromdateiso8601) > (($clear_time | fromdateiso8601) - 3600))
' unified_timeline.json
```

**Event ID 104 - System Log Cleared:**

```bash
# System log clearing
jq '.[] | select(.event_id == 104)' unified_timeline.json
```

### Missing Expected Events

[Inference: The absence of certain events may indicate disabled audit policies rather than tampering]

**Audit Policy Verification:**

```bash
# Check for audit policy changes (Event ID 4719)
jq '.[] | select(.event_id == 4719) | {
    time: .timestamp,
    category: .event_data.CategoryId,
    subcategory: .event_data.SubcategoryGuid,
    change: .event_data.AuditPolicyChanges
}' unified_timeline.json

# Identify when process auditing was disabled
jq '.[] | select(.event_id == 4719 and .event_data.SubcategoryGuid == "{0CCE922B-69AE-11D9-BED3-505054503030}")' unified_timeline.json
```

## Timestamp Normalization

Timestamp normalization converts all log timestamps to a common timezone and format for accurate correlation across systems and log sources.

### Timezone Conversion

**Identify Log Timezones:**

```bash
# EVTX timestamps are typically UTC
# Check SystemTime attribute
evtx_dump.py Security.evtx | grep -m 1 "SystemTime" | head -5

# Extract timezone info from events
jq -r '.timestamp' unified_timeline.json | head -1
# Example: 2024-10-28T14:23:45.123456Z (Z indicates UTC)
```

**Convert to Common Timezone:**

```bash
# Convert all timestamps to specific timezone using jq and date
jq -r '.timestamp' unified_timeline.json | while read ts; do
    # Convert to epoch
    epoch=$(date -d "$ts" +%s 2>/dev/null)
    
    # Convert to target timezone (e.g., EST)
    date -d "@$epoch" -u "+%Y-%m-%d %H:%M:%S UTC"
    date -d "@$epoch" "+%Y-%m-%d %H:%M:%S %Z" # Local timezone
done

# Python script for bulk conversion
cat > normalize_timestamps.py << 'EOF'
import json
from datetime import datetime
import pytz

source_tz = pytz.UTC
target_tz = pytz.timezone('US/Eastern')

with open('unified_timeline.json', 'r') as f:
    for line in f:
        try:
            event = json.loads(line)
            dt = datetime.fromisoformat(event['timestamp'].replace('Z', '+00:00'))
            
            # Convert to target timezone
            dt_target = dt.astimezone(target_tz)
            event['timestamp_normalized'] = dt_target.isoformat()
            event['timezone'] = str(target_tz)
            
            print(json.dumps(event))
        except Exception as e:
            continue
EOF

python3 normalize_timestamps.py > normalized_timeline.json
```

### Format Standardization

**ISO 8601 Normalization:**

```bash
# Ensure all timestamps are ISO 8601 compliant
jq '.timestamp |= (. | sub("\\s"; "T") | sub("\\+00:00"; "Z"))' unified_timeline.json > iso_timeline.json

# Convert Windows FileTime to ISO 8601 (if needed)
cat > filetime_converter.py << 'EOF'
from datetime import datetime, timedelta

def filetime_to_iso(filetime_int):
    # Windows FileTime is 100-nanosecond intervals since 1601-01-01
    epoch = datetime(1601, 1, 1)
    delta = timedelta(microseconds=filetime_int / 10)
    return (epoch + delta).isoformat() + 'Z'

# Example
print(filetime_to_iso(132791234567890000))
EOF
```

**Epoch Timestamp Conversion:**

```bash
# Convert Unix epoch to ISO 8601
epoch_to_iso() {
    date -u -d "@$1" +"%Y-%m-%dT%H:%M:%SZ"
}

# Example
epoch_to_iso 1698508800

# Bulk conversion in jq (if timestamps are epoch)
jq '.timestamp |= (. | tonumber | todate)' epoch_timeline.json > iso_timeline.json
```

### Clock Skew Detection and Correction

**Identify Clock Skew:**

```bash
# Event ID 1 in System log - time change events
jq '.[] | select(.event_id == 1) | {
    time: .timestamp,
    old_time: .event_data.OldTime,
    new_time: .event_data.NewTime
}' unified_timeline.json

# Calculate skew
cat > detect_skew.py << 'EOF'
import json
from datetime import datetime

with open('unified_timeline.json', 'r') as f:
    for line in f:
        event = json.loads(line)
        if event.get('event_id') == 1:  # Time change event
            old = datetime.fromisoformat(event['event_data']['OldTime'])
            new = datetime.fromisoformat(event['event_data']['NewTime'])
            skew = (new - old).total_seconds()
            print(f"Time change at {event['timestamp']}: Skew = {skew} seconds")
EOF

python3 detect_skew.py
```

**Correct Timestamps Based on Skew:**

```bash
# Apply skew correction to events before time change
cat > apply_skew_correction.py << 'EOF'
import json
from datetime import datetime, timedelta

skew_events = []
corrected_timeline = []

# First pass: identify skew events
with open('unified_timeline.json', 'r') as f:
    for line in f:
        event = json.loads(line)
        if event.get('event_id') == 1:
            skew_time = datetime.fromisoformat(event['timestamp'].replace('Z', '+00:00'))
            old = datetime.fromisoformat(event['event_data']['OldTime'].replace('Z', '+00:00'))
            new = datetime.fromisoformat(event['event_data']['NewTime'].replace('Z', '+00:00'))
            skew_seconds = (new - old).total_seconds()
            skew_events.append({'time': skew_time, 'skew': skew_seconds})

# Second pass: apply corrections
with open('unified_timeline.json', 'r') as f:
    for line in f:
        event = json.loads(line)
        event_time = datetime.fromisoformat(event['timestamp'].replace('Z', '+00:00'))
        
        # Find applicable skew
        applicable_skew = 0
        for skew in skew_events:
            if event_time < skew['time']:
                applicable_skew = skew['skew']
                break
        
        # Apply correction
        if applicable_skew != 0:
            corrected_time = event_time - timedelta(seconds=applicable_skew)
            event['timestamp_corrected'] = corrected_time.isoformat() + 'Z'
            event['skew_applied'] = applicable_skew
        
        print(json.dumps(event))
EOF

python3 apply_skew_correction.py > corrected_timeline.json
```

### Multi-System Timeline Synchronization

**NTP Synchronization Check:**

```bash
# Event ID 37 - W32Time service synchronized
jq '.[] | select(.event_id == 37) | {
    time: .timestamp,
    source: .event_data.TimeSource
}' unified_timeline.json
```

**Cross-System Correlation:**

```bash
# Merge timelines from multiple systems
for system in DC01 WS01 WS02; do
    jq --arg sys "$system" '. + {system: $sys}' ${system}_timeline.json >> multi_system_timeline.json
done

# Sort by normalized timestamp
jq -s 'sort_by(.timestamp_normalized)' multi_system_timeline.json > synchronized_timeline.json

# Identify events within time window across systems
jq 'select(.event_id == 4624) | {
    system, 
    time: .timestamp_normalized, 
    user: .event_data.TargetUserName,
    source_ip: .event_data.IpAddress
}' synchronized_timeline.json | \
jq -s 'group_by(.time[0:16]) | .[] | select(length > 1)'  # Group by minute, show correlated events
```

### Precision and Resolution

**Microsecond Precision Handling:**

```bash
# EVTX provides microsecond precision
# Example: 2024-10-28T14:23:45.123456Z

# Preserve precision in processing
jq '.timestamp' unified_timeline.json  # Maintains full precision

# Round to seconds if needed
jq '.timestamp |= (. | sub("\\.[0-9]+"; ""))' unified_timeline.json
```

**Handling Simultaneous Events:**

```bash
# Events with identical timestamps (same microsecond)
jq -s 'group_by(.timestamp) | .[] | select(length > 1) | {
    timestamp: .[0].timestamp,
    count: length,
    events: [.[].event_id]
}' unified_timeline.json

# Order by RecordID when timestamps match
jq -s 'sort_by(.timestamp, .event_record_id)' unified_timeline.json
```

## Advanced Timeline Techniques

### Super Timeline Creation with Plaso

```bash
# Create comprehensive super timeline
log2timeline.py \
    --parsers winevtx,prefetch,mft,usnjrnl,registry \
    --storage-file super_timeline.plaso \
    /path/to/evidence/

# Apply timezone
psort.py \
    -z 'US/Eastern' \
    -o l2tcsv \
    -w super_timeline.csv \
    super_timeline.plaso

# Filter to specific dates
psort.py \
    -z 'US/Eastern' \
    -o l2tcsv \
    -w filtered_super_timeline.csv \
    super_timeline.plaso \
    "date > '2024-10-01' and date < '2024-10-02'"
```

### Timeline Visualization

**Using Timesketch (if available):**

```bash
# Import timeline to Timesketch
timesketch-import -t "Investigation Timeline" -n "Windows Events" timeline.csv

# Query specific patterns
# Access via web interface for visual analysis
```

**ASCII Timeline Generation:**

```bash
# Simple ASCII visualization
jq -r '[.timestamp[0:16], .event_id, .computer] | @tsv' unified_timeline.json | \
awk '{print $1 " | Event " $2 " | " $3}' | \
column -t -s '|'

# Hourly heatmap
jq -r '.timestamp[0:13]' unified_timeline.json | sort | uniq -c | \
awk '{print $2, "|", ($1/10)+0; for(i=0;i<$1/10;i++) printf "#"; print ""}' 
```

## CTF-Specific Timeline Analysis

**Common CTF Timeline Patterns:**

1. **Flag in Timestamp:** Flags encoded in event timestamps or time differences

```bash
# Extract unique timestamps
jq -r '.timestamp' unified_timeline.json | sort -u

# Calculate time differences
jq -r '.timestamp' unified_timeline.json | sort | \
awk 'NR>1{print ($0-p)} {p=$0}'
```

2. **Event Sequence Decoding:** Event IDs or data form a sequence

```bash
# Extract event ID sequence
jq -r '.event_id' unified_timeline.json | tr '\n' ',' | sed 's/,$/\n/'
```

3. **Gap-based Encoding:** Gaps between events encode information

```bash
# Analyze gap patterns
python3 gap_detector.py | awk '{print $2}' | tr -d 's' > gaps.txt
# Convert gaps to ASCII/flags
```

**Important Related Topics:**

- **Sysmon Timeline Analysis** - Enhanced process, network, and file system events
- **Registry Timeline Reconstruction** - RegRipper and registry transaction logs
- **Filesystem Timeline Analysis** - $MFT, $UsnJrnl, and INDX parsing
- **Memory Timeline Extraction** - Volatility timeline plugin analysis

---

# Pattern Recognition

## Regex Patterns

### Core Regex Syntax for Log Analysis

**PCRE (Perl-Compatible Regular Expressions) - Standard in grep, jq, Python:**

**Character Classes:**

```regex
[a-z]       Lowercase letters
[A-Z]       Uppercase letters
[0-9]       Digits
[a-zA-Z0-9] Alphanumeric
\d          Digit [0-9]
\D          Non-digit
\w          Word character [a-zA-Z0-9_]
\W          Non-word character
\s          Whitespace [ \t\n\r\f]
\S          Non-whitespace
.           Any character except newline
```

**Quantifiers:**

```regex
*           0 or more
+           1 or more
?           0 or 1 (optional)
{n}         Exactly n times
{n,}        n or more times
{n,m}       Between n and m times
*?  +?  ??  Non-greedy versions
```

**Anchors and Boundaries:**

```regex
^           Start of line
$           End of line
\b          Word boundary
\B          Non-word boundary
\A          Start of string
\Z          End of string
```

**Groups and Alternatives:**

```regex
(pattern)   Capturing group
(?:pattern) Non-capturing group
|           Alternation (OR)
(?=pattern) Positive lookahead
(?!pattern) Negative lookahead
```

### IP Address Pattern Matching

**IPv4 Address (Strict):**

```bash
# Accurate IPv4 validation
grep -oP '(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)' logfile.txt

# Simple extraction (permissive)
grep -oE '([0-9]{1,3}\.){3}[0-9]{1,3}' logfile.txt

# With context
grep -E '\b([0-9]{1,3}\.){3}[0-9]{1,3}\b' access.log
```

**IPv4 with Port:**

```bash
grep -oP '(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?):([0-9]{1,5})' logfile.txt
```

**IPv6 Address:**

```bash
# Full IPv6
grep -oP '(?:[0-9a-fA-F]{1,4}:){7}[0-9a-fA-F]{1,4}' logfile.txt

# IPv6 with compression
grep -oP '(([0-9a-fA-F]{1,4}:){1,7}:|([0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|([0-9a-fA-F]{1,4}:){1,5}(:[0-9a-fA-F]{1,4}){1,2})' logfile.txt
```

**Private IP Ranges:**

```bash
# RFC 1918 private ranges
grep -E '\b(10\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}|172\.(1[6-9]|2[0-9]|3[0-1])\.[0-9]{1,3}\.[0-9]{1,3}|192\.168\.[0-9]{1,3}\.[0-9]{1,3})\b' logfile.txt

# Exclude private IPs
grep -oP '(?!10\.|172\.(1[6-9]|2[0-9]|3[0-1])\.|192\.168\.)(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)' logfile.txt
```

### URL and Domain Pattern Matching

**URL Extraction:**

```bash
# Full URLs
grep -oP 'https?://[^\s<>"{}|\\^`\[\]]+' logfile.txt

# With additional characters allowed
grep -oE 'https?://[a-zA-Z0-9./?=_-]+' logfile.txt

# Extract domains only
grep -oP 'https?://\K[^/]+' logfile.txt
```

**Domain Names:**

```bash
# FQDN
grep -oP '(?:[a-zA-Z0-9](?:[a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?\.)+[a-zA-Z]{2,}' logfile.txt

# Subdomains
grep -oP '\b(?:[a-zA-Z0-9-]+\.)+[a-zA-Z]{2,}\b' logfile.txt
```

**Suspicious TLDs:**

```bash
# High-risk TLDs
grep -iE '\.(tk|ml|ga|cf|gq|xyz|top|click|link|download|zip|loan)\b' logfile.txt
```

### File Path Pattern Matching

**Windows Paths:**

```bash
# Standard paths
grep -oP '[A-Z]:\\(?:[^\\/:*?"<>|\r\n]+\\)*[^\\/:*?"<>|\r\n]*' logfile.txt

# UNC paths
grep -oP '\\\\[a-zA-Z0-9.-]+\\[^\s]+' logfile.txt

# PowerShell PSDrive
grep -oP '[A-Z]+:\\[^\s]+' logfile.txt
```

**Suspicious Windows Paths:**

```bash
# Temp directories
grep -iE 'C:\\(Users\\[^\\]+\\AppData\\(Local|Roaming)\\Temp|Windows\\Temp|Temp)\\' logfile.txt

# Startup locations
grep -iE 'C:\\(Users\\[^\\]+\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Startup|ProgramData\\Microsoft\\Windows\\Start Menu\\Programs\\Startup)' logfile.txt

# System32 abuse
grep -iE 'C:\\Windows\\(System32|SysWOW64)\\[^\\]+\.(bat|cmd|ps1|vbs|js)' logfile.txt
```

**Linux Paths:**

```bash
# Absolute paths
grep -oE '(/[^/ ]*)+/?' logfile.txt

# Hidden files
grep -oE '/[^/]*\.[^/\s]+' logfile.txt | grep '/\.'

# Suspicious writable locations
grep -E '/(tmp|dev/shm|var/tmp)/' logfile.txt
```

### Email Address Pattern Matching

**Standard Email:**

```bash
# RFC 5322 simplified
grep -oP '[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}' logfile.txt

# With validation
grep -oP '\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b' logfile.txt
```

### Hash Pattern Matching

**Cryptographic Hashes:**

```bash
# MD5 (32 hex)
grep -oiE '\b[a-f0-9]{32}\b' logfile.txt

# SHA1 (40 hex)
grep -oiE '\b[a-f0-9]{40}\b' logfile.txt

# SHA256 (64 hex)
grep -oiE '\b[a-f0-9]{64}\b' logfile.txt

# SHA512 (128 hex)
grep -oiE '\b[a-f0-9]{128}\b' logfile.txt

# Combined hash extraction
grep -oiE '\b[a-f0-9]{32,128}\b' logfile.txt
```

### Credential Pattern Matching

**Base64 Encoded Strings:**

```bash
# Standard Base64
grep -oP '[A-Za-z0-9+/]{20,}={0,2}' logfile.txt

# PowerShell encoded commands
grep -oP '(?<=-enc(?:odedcommand)?\s+)[A-Za-z0-9+/]+=*' logfile.txt

# Decode findings
grep -oP '(?<=-enc\s+)[A-Za-z0-9+/]+=*' logfile.txt | while read line; do echo "$line" | base64 -d; echo; done
```

**API Keys and Tokens:**

```bash
# Generic secrets
grep -iE '(api[_-]?key|apikey|api[_-]?secret|access[_-]?token|auth[_-]?token|secret[_-]?key)["\s:=]+[A-Za-z0-9_\-]{20,}' logfile.txt

# AWS keys
grep -oP '(?:AWS|aws|Aws)?_?(?:SECRET_?|ACCESS_?)?KEY[":\s]*=?\s*["'\'']?[A-Za-z0-9/+=]{40}["'\'']?' logfile.txt

# GitHub tokens
grep -oP 'gh[pousr]_[A-Za-z0-9_]{36,}' logfile.txt

# JWT tokens
grep -oP 'eyJ[A-Za-z0-9_-]*\.eyJ[A-Za-z0-9_-]*\.[A-Za-z0-9_-]*' logfile.txt
```

**Password Patterns in Logs:**

```bash
# Common password parameters
grep -iE '(password|passwd|pwd)["\s:=]+[^\s"]+' logfile.txt

# Cleartext credentials
grep -iE '(username|user|login)[:=\s]+[^\s]+\s+(password|passwd|pwd)[:=\s]+[^\s]+' logfile.txt
```

### Command Pattern Matching

**PowerShell Suspicious Patterns:**

```bash
# Download cradles
grep -iE '(IWR|Invoke-WebRequest|Net\.WebClient|DownloadString|DownloadFile|curl|wget)\s+.*http' logfile.txt

# Encoded commands
grep -iP '-e(nc(odedcommand)?)?[\s]+[A-Za-z0-9+/=]{20,}' logfile.txt

# Execution bypass
grep -iE '(bypass|unrestricted|hidden|windowstyle\s+hidden|nop|noninteractive)' logfile.txt

# AMSI bypass
grep -iE '(amsi|AmsiUtils|amsiInitFailed|amsiContext|amsi\.dll)' logfile.txt
```

**CMD/Batch Obfuscation:**

```bash
# Character insertion
grep -E '\^[a-zA-Z]' logfile.txt

# Variable substitution
grep -E '%[a-zA-Z0-9_]+:[~-][0-9,]*%' logfile.txt
```

**Linux Shell Commands:**

```bash
# Reverse shells
grep -iE '(bash|sh|nc|netcat|ncat).*-e\s+(/bin/(ba)?sh|cmd)' logfile.txt
grep -iE '/(dev|tcp|udp)/[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}/[0-9]+' logfile.txt

# SUID exploitation
grep -E 'chmod\s+[0-9]*[4567][0-9]{3}' logfile.txt
grep -E 'find.*-perm.*-u=s' logfile.txt

# Download and execute
grep -iE '(curl|wget|fetch).*\|.*(sh|bash|python|perl|ruby)' logfile.txt
```

### Timestamp Pattern Matching

**Common Log Timestamps:**

```bash
# ISO 8601
grep -oP '\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2}(?:\.\d+)?(?:Z|[+-]\d{2}:?\d{2})?' logfile.txt

# Apache/NGINX Common Log Format
grep -oP '\d{2}/[A-Z][a-z]{2}/\d{4}:\d{2}:\d{2}:\d{2} [+-]\d{4}' access.log

# Syslog format
grep -oP '[A-Z][a-z]{2}\s+\d{1,2}\s+\d{2}:\d{2}:\d{2}' syslog

# Windows Event Log
grep -oP '\d{1,2}/\d{1,2}/\d{4}\s+\d{1,2}:\d{2}:\d{2}\s+(?:AM|PM)' logfile.txt

# Unix epoch
grep -oE '\b[0-9]{10,13}\b' logfile.txt
```

**Timestamp Extraction and Conversion:**

```bash
# Extract Unix timestamps and convert
grep -oE '\b[0-9]{10}\b' logfile.txt | while read ts; do date -d @$ts; done

# Extract millisecond timestamps
grep -oE '\b[0-9]{13}\b' logfile.txt | while read ts; do date -d @$(echo "scale=3; $ts/1000" | bc); done
```

### Advanced Regex Techniques for Log Hunting

**Named Capture Groups (Python):**

```python
import re

pattern = r'(?P<ip>(?:\d{1,3}\.){3}\d{1,3})\s+-\s+-\s+\[(?P<timestamp>[^\]]+)\]\s+"(?P<method>\w+)\s+(?P<path>\S+)\s+HTTP/[^"]+"\s+(?P<status>\d{3})\s+(?P<size>\d+)'

with open('access.log', 'r') as f:
    for line in f:
        match = re.search(pattern, line)
        if match:
            print(f"IP: {match.group('ip')}, Status: {match.group('status')}, Path: {match.group('path')}")
```

**Conditional Patterns:**

```bash
# Match IP if followed by specific port
grep -oP '(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(?=:\d{1,5})' logfile.txt

# Match domain without www
grep -oP '(?!www\.)(?:[a-zA-Z0-9-]+\.)+[a-zA-Z]{2,}' logfile.txt
```

**Backreferences:**

```bash
# Find repeated words
grep -E '\b(\w+)\s+\1\b' logfile.txt

# Find repeated IP addresses
grep -oP '((?:\d{1,3}\.){3}\d{1,3}).*\1' logfile.txt
```

## Anomaly Detection

### Baseline Establishment

**Statistical Baseline Creation:**

**Connection Frequency Baseline:**

```bash
# Extract unique IPs per hour
awk '{print $1}' access.log | sort | uniq -c | awk '{sum+=$1; count++} END {print "Mean:", sum/count, "Total:", count}'

# Hourly connection distribution
awk '{print substr($4,2,11)}' access.log | cut -d: -f1 | sort | uniq -c

# Standard deviation calculation
awk '{print $1}' access.log | sort | uniq -c | awk '{sum+=$1; sumsq+=$1*$1; count++} END {mean=sum/count; print "Mean:", mean, "StdDev:", sqrt((sumsq-sum*sum/count)/count)}'
```

**Process Execution Baseline (Windows Event ID 4688):**

```bash
# Most common processes
jq -r 'select(.Event.System.EventID == 4688) | .Event.EventData.NewProcessName' security.json | sort | uniq -c | sort -rn | head -20

# Uncommon process detection (appear < 5 times)
jq -r 'select(.Event.System.EventID == 4688) | .Event.EventData.NewProcessName' security.json | sort | uniq -c | awk '$1 < 5 {print}'
```

**User Activity Baseline:**

```bash
# Normal logon times per user
jq -r 'select(.Event.System.EventID == 4624) | "\(.Event.EventData.TargetUserName) \(.Event.System.TimeCreated.SystemTime)"' security.json | awk '{print $1, substr($2,12,2)}' | sort | uniq -c

# Users logging in from multiple IPs
jq -r 'select(.Event.System.EventID == 4624 and .Event.EventData.LogonType == "3") | "\(.Event.EventData.TargetUserName) \(.Event.EventData.IpAddress)"' security.json | sort -u | awk '{print $1}' | sort | uniq -c | awk '$1 > 5 {print}'
```

### Time-Based Anomaly Detection

**After-Hours Activity Detection:**

```bash
# Define business hours (09:00-17:00)
jq -r 'select(.Event.System.EventID == 4624) | "\(.Event.System.TimeCreated.SystemTime) \(.Event.EventData.TargetUserName) \(.Event.EventData.LogonType)"' security.json | awk '{hour=substr($1,12,2); if(hour<9 || hour>17) print}'

# Weekend activity
jq -r 'select(.Event.System.EventID == 4624) | "\(.Event.System.TimeCreated.SystemTime) \(.Event.EventData.TargetUserName)"' security.json | while read line; do
    date=$(echo "$line" | awk '{print $1}')
    day=$(date -d "$date" +%u)
    if [ $day -gt 5 ]; then
        echo "$line"
    fi
done
```

**Rapid Succession Events:**

```bash
# Failed logons within 60 seconds
jq -r 'select(.Event.System.EventID == 4625) | "\(.Event.System.TimeCreated.SystemTime) \(.Event.EventData.TargetUserName) \(.Event.EventData.IpAddress)"' security.json | awk '{
    time=$1; gsub(/[-:TZ.]/, " ", time);
    cmd="date -d \""time"\" +%s";
    cmd | getline timestamp;
    close(cmd);
    print timestamp, $2, $3
}' | sort -n | awk '{
    if(prev_time && ($1 - prev_time) < 60 && prev_user == $2) {
        print "Rapid attempt:", $0, "Previous:", prev_time, prev_user
    }
    prev_time=$1; prev_user=$2
}'
```

**Temporal Clustering:**

```bash
# PowerShell executions clustered in time
jq -r 'select(.Event.System.EventID == 4104) | .Event.System.TimeCreated.SystemTime' powershell-operational.json | awk '{
    gsub(/[-:TZ.]/, " ", $0);
    cmd="date -d \""$0"\" +%s";
    cmd | getline timestamp;
    close(cmd);
    print timestamp
}' | sort -n | uniq -c | awk '$1 > 10 {print "Burst detected:", $0}'
```

### Volume-Based Anomaly Detection

**Spike Detection:**

```bash
# Connection count per 5-minute window
awk '{print substr($4,2,16)}' access.log | uniq -c | awk '{
    if(prev_count) {
        change = ($1 - prev_count) / prev_count * 100;
        if(change > 200 || change < -50) {
            print "Spike detected:", $0, "Change:", change"%"
        }
    }
    prev_count=$1
}'

# Process creation spike
jq -r 'select(.Event.System.EventID == 4688) | .Event.System.TimeCreated.SystemTime' security.json | cut -d: -f1-2 | sort | uniq -c | awk 'BEGIN{prev=0} {if($1/prev > 3 && prev > 0) print "Spike:", $0; prev=$1}'
```

**Data Exfiltration Volume Detection:**

```bash
# Large outbound data transfers
awk '$10 > 10000000 {print $1, $7, $10}' access.log | sort -k3 -rn | head -20

# Sum data transfer per IP
awk '{sum[$1]+=$10} END {for(ip in sum) print ip, sum[ip]}' access.log | sort -k2 -rn
```

### Behavioral Anomaly Detection

**User Behavior Analysis:**

```bash
# User accessing unusual resources
# Create baseline
jq -r 'select(.Event.System.EventID == 5140) | "\(.Event.EventData.SubjectUserName) \(.Event.EventData.ShareName)"' security.json | sort | uniq > baseline_shares.txt

# Compare current activity
jq -r 'select(.Event.System.EventID == 5140) | "\(.Event.EventData.SubjectUserName) \(.Event.EventData.ShareName)"' security_new.json | sort | uniq > current_shares.txt

comm -13 baseline_shares.txt current_shares.txt
```

**Process Parentage Anomalies:**

```bash
# Unusual parent-child relationships
jq -r 'select(.Event.System.EventID == 4688) | "\(.Event.EventData.ParentProcessName) -> \(.Event.EventData.NewProcessName)"' security.json | sort | uniq -c | sort -n | awk '$1 < 3 {print}'

# Office apps spawning shells
jq 'select(.Event.System.EventID == 4688 and (.Event.EventData.ParentProcessName | test("(WINWORD|EXCEL|POWERPNT|OUTLOOK)\\.EXE"; "i")) and (.Event.EventData.NewProcessName | test("(cmd|powershell|wscript|cscript)\\.exe"; "i")))' security.json
```

**Network Connection Anomalies:**

```bash
# First-seen domains
cat dns.log | awk '{print $9}' | sort -u > current_domains.txt
comm -13 known_domains.txt current_domains.txt

# Beaconing detection (regular intervals)
# [Inference] Beaconing typically shows consistent interval patterns; this approach identifies potential candidates:
awk '{print $1, $3}' conn.log | sort | awk '{
    if(prev_ip == $1) {
        interval = $2 - prev_time;
        print prev_ip, interval
    }
    prev_ip=$1; prev_time=$2
}' | awk '{sum[$1]+=$2; count[$1]++; sumsq[$1]+=$2*$2} END {
    for(ip in count) {
        mean=sum[ip]/count[ip];
        stddev=sqrt((sumsq[ip]-sum[ip]*sum[ip]/count[ip])/count[ip]);
        if(stddev < mean*0.1 && count[ip] > 10) print ip, "Mean:", mean, "StdDev:", stddev
    }
}'
```

### Outlier Detection Techniques

**Z-Score Based Detection:**

```python
#!/usr/bin/env python3
import json
import statistics

# Failed logon count per user
users = {}
with open('security.json', 'r') as f:
    for line in f:
        try:
            event = json.loads(line)
            if event['Event']['System']['EventID'] == 4625:
                user = event['Event']['EventData']['TargetUserName']
                users[user] = users.get(user, 0) + 1
        except:
            continue

# Calculate z-scores
counts = list(users.values())
mean = statistics.mean(counts)
stdev = statistics.stdev(counts)

print(f"Mean: {mean:.2f}, StdDev: {stdev:.2f}\n")
print("Outliers (Z-score > 3):")
for user, count in users.items():
    zscore = (count - mean) / stdev if stdev > 0 else 0
    if abs(zscore) > 3:
        print(f"{user}: {count} attempts (Z-score: {zscore:.2f})")
```

**Interquartile Range (IQR) Method:**

```bash
# Request size anomalies
awk '{print $10}' access.log | sort -n > sizes.txt

# Calculate quartiles
total=$(wc -l < sizes.txt)
q1_line=$((total / 4))
q3_line=$((total * 3 / 4))

q1=$(sed -n "${q1_line}p" sizes.txt)
q3=$(sed -n "${q3_line}p" sizes.txt)
iqr=$((q3 - q1))

lower=$((q1 - (iqr * 3 / 2)))
upper=$((q3 + (iqr * 3 / 2)))

echo "Q1: $q1, Q3: $q3, IQR: $iqr"
echo "Outlier range: < $lower or > $upper"

awk -v lower=$lower -v upper=$upper '$10 < lower || $10 > upper {print}' access.log
```

### Machine Learning Approaches

**Isolation Forest (Python with scikit-learn):**

```python
#!/usr/bin/env python3
from sklearn.ensemble import IsolationForest
import pandas as pd
import json

# Extract features from process events
features = []
with open('security.json', 'r') as f:
    for line in f:
        try:
            event = json.loads(line)
            if event['Event']['System']['EventID'] == 4688:
                # Extract numerical features
                proc_name_len = len(event['Event']['EventData']['NewProcessName'])
                cmd_line_len = len(event['Event']['EventData'].get('CommandLine', ''))
                features.append([proc_name_len, cmd_line_len])
        except:
            continue

# Train isolation forest
df = pd.DataFrame(features, columns=['ProcessNameLength', 'CommandLineLength'])
iso_forest = IsolationForest(contamination=0.1, random_state=42)
predictions = iso_forest.fit_predict(df)

# Find anomalies (-1 indicates anomaly)
anomalies = df[predictions == -1]
print(f"Detected {len(anomalies)} anomalies:")
print(anomalies)
```

## Frequency Analysis

### Character Frequency Analysis

**Entropy Calculation for Obfuscation Detection:**

```python
#!/usr/bin/env python3
import math
from collections import Counter

def calculate_entropy(data):
    if not data:
        return 0
    entropy = 0
    counter = Counter(data)
    length = len(data)
    for count in counter.values():
        probability = count / length
        entropy -= probability * math.log2(probability)
    return entropy

# Analyze PowerShell commands
import json
with open('powershell-operational.json', 'r') as f:
    for line in f:
        try:
            event = json.loads(line)
            if event['Event']['System']['EventID'] == 4104:
                script = event['Event']['EventData']['ScriptBlockText']
                entropy = calculate_entropy(script)
                if entropy > 4.5:  # High entropy threshold
                    print(f"High entropy ({entropy:.2f}): {script[:100]}")
        except:
            continue
```

**Base64 Detection via Character Distribution:**

```bash
# [Inference] Base64 typically has roughly equal distribution of alphanumeric characters with minimal special chars:
grep -oP '[A-Za-z0-9+/]{20,}={0,2}' logfile.txt | while read str; do
    # Count character types
    alpha=$(echo "$str" | grep -o '[A-Za-z]' | wc -l)
    digit=$(echo "$str" | grep -o '[0-9]' | wc -l)
    special=$(echo "$str" | grep -o '[+/]' | wc -l)
    total=${#str}
    
    # Base64 should have balanced distribution
    alpha_ratio=$(echo "scale=2; $alpha/$total" | bc)
    if (( $(echo "$alpha_ratio > 0.5 && $alpha_ratio < 0.8" | bc -l) )); then
        echo "Likely Base64: $str"
    fi
done
```

### Event Frequency Analysis

**Top-N Analysis:**

```bash
# Top 10 accessed URLs
awk '{print $7}' access.log | sort | uniq -c | sort -rn | head -10

# Top source IPs
awk '{print $1}' access.log | sort | uniq -c | sort -rn | head -20

# Top User-Agents
awk -F'"' '{print $6}' access.log | sort | uniq -c | sort -rn | head -15

# Top processes created
jq -r 'select(.Event.System.EventID == 4688) | .Event.EventData.NewProcessName' security.json | sort | uniq -c | sort -rn | head -20
```

**Rare Event Detection:**

```bash
# Events occurring less than threshold
awk '{print $7}' access.log | sort | uniq -c | sort -n | awk '$1 <= 5 {print}'

# Unique processes (single execution)
jq -r 'select(.Event.System.EventID == 4688) | .Event.EventData.NewProcessName' security.json | sort | uniq -c | awk '$1 == 1 {print $2}'

# Rare error codes
awk '$9 >= 400 {print $9}' access.log | sort | uniq -c | sort -n | awk '$1 <= 3 {print}'
```

### N-gram Frequency Analysis

**Command N-gram Analysis:**

```python
#!/usr/bin/env python3
from collections import Counter
import json

def generate_ngrams(text, n=3):
    words = text.split()
    ngrams = zip(*[words[i:] for i in range(n)])
    return [' '.join(ngram) for ngram in ngrams]

# Analyze PowerShell command patterns
ngram_counter = Counter()
with open('powershell-operational.json', 'r') as f:
    for line in f:
        try:
            event = json.loads(line)
            if event['Event']['System']['EventID'] == 4104:
                script = event['Event']['EventData']['ScriptBlockText']
                ngrams = generate_ngrams(script.lower(), n=3)
                ngram_counter.update(ngrams)
        except:
            continue

print("Top 20 command patterns:")
for ngram, count in ngram_counter.most_common(20):
    print(f"{count:5d}: {ngram}")

print("\nRare patterns (single occurrence):")
for ngram, count in ngram_counter.items():
    if count == 1:
        print(f"  {ngram}")
```

**Byte N-gram for Binary Analysis:**

```python
#!/usr/bin/env python3
from collections import Counter
from math import log2

def byte_ngrams(data, n=2):
    return [data[i:i + n] for i in range(len(data) - n + 1)]

# Analyze suspicious files
with open('suspicious.exe', 'rb') as f:
    data = f.read()
    bigrams = byte_ngrams(data, n=2)
    counter = Counter(bigrams)

print("Top 10 byte bigrams:")
for bigram, count in counter.most_common(10):
    print(f"{bigram.hex()}: {count}")

# Calculate byte entropy
entropy = -sum(
    (count / len(bigrams)) * log2(count / len(bigrams))
    for count in counter.values()
)
print(f"\nByte entropy: {entropy:.2f}")

# High entropy may indicate packing/encryption
if entropy > 7.5:
    print("[ALERT] High entropy - possibly packed/encrypted")
````

### Time-Series Frequency Analysis

**Sliding Window Event Counting:**
```bash
# Count events per 5-minute window
jq -r 'select(.Event.System.EventID == 4625) | .Event.System.TimeCreated.SystemTime' security.json | awk '{
    gsub(/[-:TZ.]/, " ", $0);
    cmd="date -d \""$0"\" +%s";
    cmd | getline timestamp;
    close(cmd);
    window = int(timestamp / 300) * 300;  # 5-minute buckets
    print window
}' | sort | uniq -c | awk '{
    time=$2;
    cmd="date -d @"time" \"+%Y-%m-%d %H:%M:%S\"";
    cmd | getline formatted;
    close(cmd);
    print $1, formatted
}'
````

**Moving Average Calculation:**

```python
#!/usr/bin/env python3
import json
from datetime import datetime
from collections import defaultdict

# Count events per hour
hourly_counts = defaultdict(int)
with open('security.json', 'r') as f:
    for line in f:
        try:
            event = json.loads(line)
            if event['Event']['System']['EventID'] == 4688:
                timestamp = event['Event']['System']['TimeCreated']['SystemTime']
                hour = timestamp[:13]  # YYYY-MM-DDTHH
                hourly_counts[hour] += 1
        except:
            continue

# Calculate moving average (window=3 hours)
sorted_hours = sorted(hourly_counts.keys())
window_size = 3

print("Hour\t\t\tCount\tMovingAvg\tDeviation")
for i in range(len(sorted_hours)):
    hour = sorted_hours[i]
    count = hourly_counts[hour]
    
    if i >= window_size - 1:
        window_values = [hourly_counts[sorted_hours[j]] 
                        for j in range(i - window_size + 1, i + 1)]
        moving_avg = sum(window_values) / window_size
        deviation = ((count - moving_avg) / moving_avg * 100) if moving_avg > 0 else 0
        
        alert = " [ALERT]" if abs(deviation) > 50 else ""
        print(f"{hour}\t{count}\t{moving_avg:.1f}\t\t{deviation:+.1f}%{alert}")
    else:
        print(f"{hour}\t{count}\t-\t\t-")
```

### Protocol and Port Frequency Analysis

**Port Activity Distribution:**

```bash
# Parse connection logs (assuming Zeek/Bro format)
awk '{print $7}' conn.log | sort | uniq -c | sort -rn | head -20

# Detect port scanning (many unique ports from single source)
awk '{print $3, $7}' conn.log | sort -u | awk '{print $1}' | sort | uniq -c | awk '$1 > 50 {print "Possible scan from:", $2, "Ports:", $1}'

# Rare port usage
awk '{print $7}' conn.log | sort | uniq -c | sort -n | awk '$1 <= 3 && $2 !~ /^(80|443|22|3389)$/ {print "Rare port:", $2, "Count:", $1}'
```

**Protocol Distribution:**

```bash
# Zeek conn.log protocol analysis
awk '{print $8}' conn.log | sort | uniq -c | sort -rn

# Unusual protocol combinations
awk '{print $7, $8}' conn.log | sort | uniq -c | sort -n | awk '$1 < 5 {print}'
```

### User Activity Frequency Analysis

**User Session Frequency:**

```bash
# Logon frequency per user
jq -r 'select(.Event.System.EventID == 4624) | .Event.EventData.TargetUserName' security.json | sort | uniq -c | sort -rn

# Interactive vs network logons per user
jq -r 'select(.Event.System.EventID == 4624) | "\(.Event.EventData.TargetUserName) \(.Event.EventData.LogonType)"' security.json | sort | uniq -c | awk '{print $2, $3, $1}' | column -t
```

**Command Execution Frequency per User:**

```bash
# Commands run by each user
jq -r 'select(.Event.System.EventID == 4688) | "\(.Event.EventData.SubjectUserName) \(.Event.EventData.NewProcessName)"' security.json | sort | uniq | awk '{print $1}' | sort | uniq -c | sort -rn

# Users running rare commands
jq -r 'select(.Event.System.EventID == 4688) | "\(.Event.EventData.SubjectUserName) \(.Event.EventData.NewProcessName)"' security.json | sort | uniq -c | awk '$1 == 1 {print $2, $3}'
```

### String Frequency Analysis

**Keyword Frequency in Logs:**

```bash
# Extract and count all words
tr -cs '[:alnum:]' '\n' < logfile.txt | tr '[:upper:]' '[:lower:]' | sort | uniq -c | sort -rn | head -50

# Suspicious keyword frequency
grep -oiE '(admin|password|cred|token|secret|priv|exploit|shell|payload|backdoor)' logfile.txt | sort | uniq -c | sort -rn
```

**Domain Generation Algorithm (DGA) Detection:**

```python
#!/usr/bin/env python3
from collections import Counter
import re

def analyze_domain(domain):
    # Remove TLD
    base = domain.rsplit('.', 1)[0]
    
    # Character distribution
    char_freq = Counter(base)
    
    # Metrics
    length = len(base)
    unique_chars = len(char_freq)
    digit_count = sum(1 for c in base if c.isdigit())
    
    # Entropy
    entropy = 0
    for count in char_freq.values():
        p = count / length
        entropy -= p * (p and __import__('math').log2(p))
    
    # [Inference] DGA domains often have high entropy, many unique chars, unusual length:
    is_suspicious = (
        entropy > 3.5 and 
        unique_chars / length > 0.6 and 
        length > 12 and
        digit_count > 0
    )
    
    return {
        'domain': domain,
        'length': length,
        'entropy': entropy,
        'unique_ratio': unique_chars / length,
        'suspicious': is_suspicious
    }

# Analyze DNS queries
with open('dns.log', 'r') as f:
    for line in f:
        parts = line.split()
        if len(parts) > 8:
            domain = parts[9]
            result = analyze_domain(domain)
            if result['suspicious']:
                print(f"[DGA?] {result['domain']}: entropy={result['entropy']:.2f}, "
                      f"unique_ratio={result['unique_ratio']:.2f}")
```

### File Access Frequency Analysis

**Most Accessed Files:**

```bash
# Windows file access (Event ID 4663)
jq -r 'select(.Event.System.EventID == 4663) | .Event.EventData.ObjectName' security.json | sort | uniq -c | sort -rn | head -30

# Rare file access
jq -r 'select(.Event.System.EventID == 4663) | .Event.EventData.ObjectName' security.json | sort | uniq -c | awk '$1 == 1 {print $2}'

# File access by user frequency
jq -r 'select(.Event.System.EventID == 4663) | "\(.Event.EventData.SubjectUserName) \(.Event.EventData.ObjectName)"' security.json | sort | uniq | awk '{print $1}' | sort | uniq -c | sort -rn
```

**Registry Key Access Patterns:**

```bash
# Registry operations (Event ID 4657)
jq -r 'select(.Event.System.EventID == 4657) | .Event.EventData.ObjectName' security.json | sort | uniq -c | sort -rn | head -20

# Persistence registry keys
jq 'select(.Event.System.EventID == 4657 and (.Event.EventData.ObjectName | test("(Run|RunOnce|Startup|Services|Winlogon)"; "i")))' security.json
```

### HTTP Request Frequency Analysis

**URL Pattern Frequency:**

```bash
# Most requested paths
awk '{print $7}' access.log | cut -d? -f1 | sort | uniq -c | sort -rn | head -20

# Query parameter frequency
awk '{print $7}' access.log | grep -o '?.*' | tr '&' '\n' | cut -d= -f1 | sort | uniq -c | sort -rn

# User-Agent frequency
awk -F'"' '{print $6}' access.log | sort | uniq -c | sort -rn | head -15
```

**SQL Injection Pattern Detection:**

```bash
# Common SQLi patterns frequency
grep -oiE "(union.*select|or.*1.*=.*1|' or|\"or|concat|benchmark|sleep\(|waitfor)" access.log | sort | uniq -c | sort -rn

# Path traversal frequency
grep -o '\.\.\/' access.log | wc -l
grep -o '%2e%2e%2f' access.log | wc -l
```

### Error and Exception Frequency

**HTTP Status Code Distribution:**

```bash
# Status code frequency
awk '{print $9}' access.log | sort | uniq -c | sort -rn

# Error rate over time
awk '{print substr($4,2,11), $9}' access.log | awk '$2 >= 400 {print $1}' | sort | uniq -c

# Client errors vs server errors
awk '{
    if($9 >= 400 && $9 < 500) client++;
    else if($9 >= 500) server++;
} END {
    print "Client errors (4xx):", client;
    print "Server errors (5xx):", server;
    print "Error ratio:", (client+server)/NR*100"%"
}' access.log
```

**Application Exception Frequency:**

```bash
# Exception types from Event ID 1000
jq -r 'select(.Event.System.EventID == 1000) | .Event.EventData.ExceptionCode' application.json | sort | uniq -c | sort -rn

# Crashing applications
jq -r 'select(.Event.System.EventID == 1000) | .Event.EventData.AppName' application.json | sort | uniq -c | sort -rn
```

### Advanced Frequency Correlation

**Co-occurrence Analysis:**

```python
#!/usr/bin/env python3
from collections import defaultdict
import json

# Find events that frequently occur together
event_pairs = defaultdict(int)

events = []
with open('security.json', 'r') as f:
    for line in f:
        try:
            event = json.loads(line)
            timestamp = event['Event']['System']['TimeCreated']['SystemTime']
            event_id = event['Event']['System']['EventID']
            events.append((timestamp[:19], event_id))  # Minute precision
        except:
            continue

# Count co-occurrences within same minute
events.sort()
for i in range(len(events) - 1):
    time1, id1 = events[i]
    for j in range(i + 1, min(i + 10, len(events))):  # Check next 10 events
        time2, id2 = events[j]
        if time1 == time2:  # Same minute
            pair = tuple(sorted([id1, id2]))
            event_pairs[pair] += 1

print("Top event co-occurrences:")
for pair, count in sorted(event_pairs.items(), key=lambda x: x[1], reverse=True)[:15]:
    print(f"Events {pair[0]} & {pair[1]}: {count} times")
```

**Markov Chain Analysis for Sequence Detection:**

```python
#!/usr/bin/env python3
from collections import defaultdict
import json

# Build transition probability matrix for process execution
transitions = defaultdict(lambda: defaultdict(int))

with open('security.json', 'r') as f:
    prev_process = None
    for line in f:
        try:
            event = json.loads(line)
            if event['Event']['System']['EventID'] == 4688:
                current_process = event['Event']['EventData']['NewProcessName'].split('\\')[-1]
                if prev_process:
                    transitions[prev_process][current_process] += 1
                prev_process = current_process
        except:
            continue

# Find unusual transitions (rare sequences)
print("Unusual process execution sequences:")
for source in transitions:
    total = sum(transitions[source].values())
    for target, count in transitions[source].items():
        probability = count / total
        if probability < 0.05 and count > 1:  # Rare but not unique
            print(f"{source} -> {target}: {count} times ({probability*100:.1f}%)")
```

**Benford's Law Analysis:**

```python
#!/usr/bin/env python3
from collections import Counter
import math

# [Inference] Benford's Law states naturally occurring numbers have predictable first-digit distribution:
def benford_expected(digit):
    return math.log10(1 + 1/digit)

# Analyze file sizes or data transfer amounts
sizes = []
with open('access.log', 'r') as f:
    for line in f:
        parts = line.split()
        if len(parts) > 9:
            try:
                size = int(parts[9])
                if size > 0:
                    first_digit = int(str(size)[0])
                    sizes.append(first_digit)
            except:
                continue

# Calculate actual distribution
counter = Counter(sizes)
total = len(sizes)

print("Digit\tActual\tExpected\tDifference")
for digit in range(1, 10):
    actual = counter[digit] / total if total > 0 else 0
    expected = benford_expected(digit)
    difference = abs(actual - expected)
    alert = " [ALERT]" if difference > 0.05 else ""
    print(f"{digit}\t{actual:.3f}\t{expected:.3f}\t\t{difference:.3f}{alert}")

# [Inference] Large deviations may indicate data manipulation or artificial generation
```

**Important related topics:** Consider studying Statistical Process Control (SPC) for anomaly thresholds, Time-Series Forecasting (ARIMA models) for predictive analysis, Clustering algorithms (DBSCAN, K-means) for grouping similar events, and Visualization techniques (heatmaps, timeline graphs) for pattern identification.

---

## Baseline Establishment

Baseline establishment creates a reference model of normal system behavior against which anomalies can be detected. In CTF scenarios, baselines help identify planted backdoors, unusual access patterns, or data exfiltration attempts hidden within legitimate-looking traffic.

### Statistical Baseline Methods

**Time-Series Analysis:**

Establish normal traffic patterns over time intervals:

```bash
# Hourly connection count baseline (FTP)
awk '{print $1, $2, $3}' /var/log/vsftpd.log | cut -d':' -f1 | sort | uniq -c > /tmp/hourly_baseline.txt

# Calculate mean and standard deviation
awk '{sum+=$1; sumsq+=$1*$1} END {
    mean=sum/NR; 
    stddev=sqrt(sumsq/NR - mean*mean); 
    print "Mean:", mean, "StdDev:", stddev
}' /tmp/hourly_baseline.txt
```

**Volume-Based Baseline:**

```bash
# DNS queries per hour baseline
grep "query:" /var/log/named/queries.log | \
  awk '{print $1, $2}' | cut -d':' -f1 | \
  sort | uniq -c | \
  awk '{
    counts[NR]=$1
    sum+=$1
  } END {
    mean=sum/NR
    for(i=1; i<=NR; i++) {
      variance += (counts[i]-mean)^2
    }
    stddev=sqrt(variance/NR)
    print "Baseline - Mean:", mean, "queries/hour"
    print "Standard Deviation:", stddev
    print "Normal range:", mean-2*stddev, "to", mean+2*stddev
  }'
```

**User Behavior Baseline:**

```bash
# Establish normal file transfer sizes per user (FTP)
awk '{
    user=$11
    size=$5
    user_sizes[user]=user_sizes[user]" "size
    user_count[user]++
    user_sum[user]+=size
} END {
    for(u in user_count) {
        avg=user_sum[u]/user_count[u]
        print u, "transfers:", user_count[u], "avg_size:", avg
    }
}' /var/log/xferlog | sort -k3 -n
```

### Python Baseline Framework

```python
import re
from datetime import datetime, timedelta
from collections import defaultdict
import statistics

class BaselineAnalyzer:
    """Establish and monitor baselines for log metrics"""
    
    def __init__(self, window_hours=24):
        self.window_hours = window_hours
        self.metrics = defaultdict(list)
        
    def parse_log_volume(self, logfile, service='generic'):
        """Parse log entries per time interval"""
        hourly_counts = defaultdict(int)
        
        with open(logfile, 'r') as f:
            for line in f:
                # Extract timestamp based on service
                if service == 'dns':
                    match = re.search(r'^(\d{2}-\w{3}-\d{4} \d{2}):', line)
                    if match:
                        timestamp = match.group(1)
                        hourly_counts[timestamp] += 1
                elif service == 'mail':
                    match = re.search(r'^(\w{3}\s+\d{1,2} \d{2}):', line)
                    if match:
                        timestamp = match.group(1)
                        hourly_counts[timestamp] += 1
                elif service == 'ftp':
                    match = re.search(r'^(\w{3} \w{3}\s+\d{1,2} \d{2}):', line)
                    if match:
                        timestamp = match.group(1)
                        hourly_counts[timestamp] += 1
        
        return hourly_counts
    
    def calculate_baseline(self, data_points):
        """Calculate baseline statistics"""
        if len(data_points) < 2:
            return None
            
        return {
            'mean': statistics.mean(data_points),
            'median': statistics.median(data_points),
            'stdev': statistics.stdev(data_points) if len(data_points) > 1 else 0,
            'min': min(data_points),
            'max': max(data_points),
            'count': len(data_points)
        }
    
    def establish_volume_baseline(self, logfile, service='dns'):
        """Establish baseline for log volume"""
        hourly_counts = self.parse_log_volume(logfile, service)
        counts = list(hourly_counts.values())
        
        baseline = self.calculate_baseline(counts)
        
        if baseline:
            # Calculate anomaly thresholds (2 standard deviations)
            baseline['upper_threshold'] = baseline['mean'] + (2 * baseline['stdev'])
            baseline['lower_threshold'] = max(0, baseline['mean'] - (2 * baseline['stdev']))
            
            print(f"\n{service.upper()} Volume Baseline:")
            print(f"  Mean: {baseline['mean']:.2f} events/hour")
            print(f"  Median: {baseline['median']:.2f}")
            print(f"  Std Dev: {baseline['stdev']:.2f}")
            print(f"  Normal Range: {baseline['lower_threshold']:.2f} - {baseline['upper_threshold']:.2f}")
            print(f"  Observed Range: {baseline['min']} - {baseline['max']}")
        
        return baseline, hourly_counts
    
    def establish_user_baseline(self, logfile, service='ftp'):
        """Establish per-user activity baseline"""
        user_metrics = defaultdict(lambda: {
            'logins': 0,
            'transfers': 0,
            'total_bytes': 0,
            'transfer_sizes': []
        })
        
        with open(logfile, 'r') as f:
            for line in f:
                if service == 'ftp':
                    # Parse xferlog format
                    parts = line.split()
                    if len(parts) >= 14:
                        username = parts[12]
                        filesize = int(parts[6])
                        direction = parts[10]
                        
                        user_metrics[username]['transfers'] += 1
                        user_metrics[username]['total_bytes'] += filesize
                        user_metrics[username]['transfer_sizes'].append(filesize)
                
                elif service == 'dns':
                    # Count queries per source IP
                    match = re.search(r'client.*?([0-9.]+)#\d+', line)
                    if match:
                        ip = match.group(1)
                        user_metrics[ip]['logins'] += 1
        
        baselines = {}
        for user, metrics in user_metrics.items():
            if metrics['transfer_sizes']:
                baselines[user] = {
                    'transfer_count': metrics['transfers'],
                    'avg_size': statistics.mean(metrics['transfer_sizes']),
                    'median_size': statistics.median(metrics['transfer_sizes']),
                    'total_bytes': metrics['total_bytes']
                }
                
                if len(metrics['transfer_sizes']) > 1:
                    baselines[user]['stdev_size'] = statistics.stdev(metrics['transfer_sizes'])
        
        print(f"\n{service.upper()} User Baseline:")
        for user, baseline in sorted(baselines.items(), key=lambda x: x[1]['transfer_count'], reverse=True)[:10]:
            print(f"  {user}: {baseline['transfer_count']} transfers, avg {baseline['avg_size']:.0f} bytes")
        
        return baselines
    
    def establish_temporal_baseline(self, logfile, service='dns'):
        """Establish baseline for time-of-day patterns"""
        hourly_activity = defaultdict(int)
        daily_activity = defaultdict(int)
        
        with open(logfile, 'r') as f:
            for line in f:
                # Extract hour and day
                if service == 'dns':
                    match = re.search(r'^(\d{2})-(\w{3})-(\d{4}) (\d{2}):', line)
                    if match:
                        hour = int(match.group(4))
                        day = match.group(2)
                        hourly_activity[hour] += 1
                        daily_activity[day] += 1
        
        print(f"\n{service.upper()} Temporal Baseline:")
        print("  Activity by Hour (UTC):")
        for hour in sorted(hourly_activity.keys()):
            bar = '#' * (hourly_activity[hour] // 100)
            print(f"    {hour:02d}:00 - {hourly_activity[hour]:6d} {bar}")
        
        return {'hourly': hourly_activity, 'daily': daily_activity}
    
    def establish_connection_baseline(self, logfile):
        """Establish baseline for connection patterns"""
        connection_metrics = defaultdict(lambda: {
            'sources': set(),
            'destinations': set(),
            'ports': set(),
            'protocols': set()
        })
        
        # Example for FTP logs
        with open(logfile, 'r') as f:
            for line in f:
                # Parse vsftpd detailed logs
                ip_match = re.search(r'Client "([0-9.]+)"', line)
                if ip_match:
                    ip = ip_match.group(1)
                    connection_metrics['ftp']['sources'].add(ip)
        
        print("\nConnection Baseline:")
        for service, metrics in connection_metrics.items():
            print(f"  {service}: {len(metrics['sources'])} unique sources")
        
        return connection_metrics

# Usage example
analyzer = BaselineAnalyzer()

# Establish baselines
dns_baseline, dns_hourly = analyzer.establish_volume_baseline('/var/log/named/queries.log', 'dns')
ftp_user_baseline = analyzer.establish_user_baseline('/var/log/xferlog', 'ftp')
temporal_baseline = analyzer.establish_temporal_baseline('/var/log/named/queries.log', 'dns')
```

### Service-Specific Baselines

**FTP Baseline Establishment:**

```bash
# Normal transfer size distribution
awk '{print $5}' /var/log/xferlog | sort -n | \
  awk '{
    size=$1
    ranges[int(log(size+1)/log(10))]++
  } END {
    print "File Size Distribution:"
    for(i=0; i<=9; i++) {
      lower=10^i
      upper=10^(i+1)
      printf "  %d - %d bytes: %d files\n", lower, upper, ranges[i]
    }
  }'

# Normal login frequency per user
grep "OK LOGIN" /var/log/vsftpd.log | \
  awk '{print $1, $2, $6}' | cut -d']' -f2 | \
  sort | uniq -c | \
  awk '{print $2, $1}' | \
  awk '{
    user=$1
    logins=$2
    sum[user]+=logins
    count[user]++
  } END {
    for(u in count) {
      print u, "avg_logins_per_day:", sum[u]/count[u]
    }
  }'

# Directory access patterns
awk '{print $8}' /var/log/xferlog | \
  xargs -n1 dirname | \
  sort | uniq -c | sort -rn | head -20
```

**SMTP Baseline Establishment:**

```bash
# Normal email volume per hour
grep "from=<" /var/log/mail.log | \
  awk '{print $1, $2, $3}' | cut -d':' -f1 | \
  sort | uniq -c | \
  awk '{sum+=$1; count++} END {print "Average emails/hour:", sum/count}'

# Typical message sizes
grep "size=" /var/log/mail.log | \
  grep -oP 'size=\K[0-9]+' | \
  awk '{
    sum+=$1
    count++
    sizes[NR]=$1
  } END {
    mean=sum/count
    for(i=1; i<=NR; i++) {
      variance+=(sizes[i]-mean)^2
    }
    stddev=sqrt(variance/NR)
    print "Mean size:", mean, "bytes"
    print "Std Dev:", stddev, "bytes"
  }'

# Common sender domains
grep "from=<" /var/log/mail.log | \
  grep -oP 'from=<[^@]+@\K[^>]+' | \
  sort | uniq -c | sort -rn | head -20
```

**DNS Baseline Establishment:**

```bash
# Normal query types distribution
grep "query:" /var/log/named/queries.log | \
  awk '{print $10}' | \
  sort | uniq -c | sort -rn | \
  awk '{
    type=$2
    count=$1
    total+=count
    types[type]=count
  } END {
    print "Query Type Distribution:"
    for(t in types) {
      percentage=(types[t]/total)*100
      printf "  %s: %.2f%%\n", t, percentage
    }
  }'

# Average domain length by TLD
grep "query:" /var/log/named/queries.log | \
  awk '{print $8}' | \
  awk -F'.' '{
    domain=$0
    tld=$NF
    length=length(domain)
    sum[tld]+=length
    count[tld]++
  } END {
    for(t in count) {
      print t, "avg_length:", sum[t]/count[t]
    }
  }' | sort -k3 -n
```

### Baseline Storage and Comparison

**JSON Baseline Export:**

```python
import json
from datetime import datetime

def export_baseline(baseline_data, filename):
    """Export baseline to JSON for later comparison"""
    baseline_export = {
        'timestamp': datetime.now().isoformat(),
        'baselines': baseline_data,
        'metadata': {
            'window_hours': 24,
            'confidence_interval': 0.95
        }
    }
    
    with open(filename, 'w') as f:
        json.dump(baseline_export, f, indent=2)
    
    print(f"Baseline exported to {filename}")

def load_baseline(filename):
    """Load previously established baseline"""
    with open(filename, 'r') as f:
        return json.load(f)

def compare_to_baseline(current_value, baseline, sigma=2):
    """Compare current value to baseline with z-score"""
    mean = baseline['mean']
    stdev = baseline['stdev']
    
    if stdev == 0:
        return current_value != mean, 0
    
    z_score = (current_value - mean) / stdev
    is_anomaly = abs(z_score) > sigma
    
    return is_anomaly, z_score

# Usage
baseline_data = {
    'dns_volume': dns_baseline,
    'ftp_users': ftp_user_baseline,
    'temporal': temporal_baseline
}

export_baseline(baseline_data, '/tmp/baseline.json')

# Later comparison
loaded_baseline = load_baseline('/tmp/baseline.json')
current_dns_volume = 5000
is_anomaly, z_score = compare_to_baseline(
    current_dns_volume, 
    loaded_baseline['baselines']['dns_volume']
)

if is_anomaly:
    print(f"ANOMALY: Current DNS volume {current_dns_volume} (z-score: {z_score:.2f})")
```

## Outlier Identification

Outlier identification detects deviations from established baselines that may indicate malicious activity, system compromise, or CTF challenge artifacts. This combines statistical methods with domain-specific heuristics.

### Statistical Outlier Detection

**Z-Score Method:**

```python
def detect_outliers_zscore(data_points, threshold=3):
    """Detect outliers using z-score method"""
    mean = statistics.mean(data_points)
    stdev = statistics.stdev(data_points)
    
    outliers = []
    for value in data_points:
        z_score = (value - mean) / stdev if stdev > 0 else 0
        if abs(z_score) > threshold:
            outliers.append({
                'value': value,
                'z_score': z_score,
                'deviation': value - mean
            })
    
    return outliers

# Example: Detect outlier file transfers
with open('/var/log/xferlog', 'r') as f:
    sizes = []
    transfers = []
    for line in f:
        parts = line.split()
        if len(parts) >= 14:
            size = int(parts[6])
            sizes.append(size)
            transfers.append({
                'size': size,
                'filename': parts[7],
                'user': parts[12],
                'timestamp': ' '.join(parts[0:4])
            })

outliers = detect_outliers_zscore(sizes, threshold=3)

print("File Transfer Outliers (>3 sigma):")
for outlier in outliers:
    # Find corresponding transfer
    matching = [t for t in transfers if t['size'] == outlier['value']]
    for transfer in matching:
        print(f"  {transfer['timestamp']} - {transfer['user']}: {transfer['filename']}")
        print(f"    Size: {outlier['value']:,} bytes (z-score: {outlier['z_score']:.2f})")
```

**Interquartile Range (IQR) Method:**

```python
def detect_outliers_iqr(data_points, k=1.5):
    """Detect outliers using IQR method (more robust to extreme outliers)"""
    sorted_data = sorted(data_points)
    n = len(sorted_data)
    
    q1 = sorted_data[n // 4]
    q3 = sorted_data[3 * n // 4]
    iqr = q3 - q1
    
    lower_bound = q1 - (k * iqr)
    upper_bound = q3 + (k * iqr)
    
    outliers = [x for x in data_points if x < lower_bound or x > upper_bound]
    
    return {
        'outliers': outliers,
        'q1': q1,
        'q3': q3,
        'iqr': iqr,
        'lower_bound': lower_bound,
        'upper_bound': upper_bound
    }

# Usage for DNS query volumes
hourly_counts = [150, 160, 155, 158, 2000, 152, 157, 161, 149]  # Note the spike
result = detect_outliers_iqr(hourly_counts)

print(f"IQR Analysis:")
print(f"  Q1: {result['q1']}, Q3: {result['q3']}")
print(f"  Normal range: {result['lower_bound']:.0f} - {result['upper_bound']:.0f}")
print(f"  Outliers: {result['outliers']}")
```

### Behavioral Outlier Detection

**Unusual Authentication Patterns:**

```bash
# Detect unusual login times (off-hours access)
grep "OK LOGIN" /var/log/vsftpd.log | \
  awk '{
    hour=substr($3, 1, 2)
    user=$6
    if(hour < 6 || hour > 22) {
      print $0, "<<<< OFF-HOURS"
    }
  }'

# Failed login bursts (potential brute force)
grep "FAIL LOGIN" /var/log/vsftpd.log | \
  awk '{print $1, $2, $3, $NF}' | \
  awk '{
    timestamp=$1" "$2" "$3
    ip=$4
    key=timestamp"-"ip
    count[key]++
    ips[ip]++
  } END {
    print "Failed Login Bursts:"
    for(k in count) {
      if(count[k] > 5) {
        print "  " k ": " count[k] " attempts"
      }
    }
  }'
```

**Geographic Outliers:**

```bash
# Detect connections from unusual countries (requires GeoIP)
# First, establish baseline of normal countries
grep "connect from" /var/log/vsftpd.log | \
  awk '{print $NF}' | cut -d'[' -f2 | cut -d']' -f1 | sort -u | \
  while read ip; do
    echo "$ip $(geoiplookup $ip 2>/dev/null | cut -d':' -f2 | cut -d',' -f1)"
  done | sort | uniq -c | sort -rn > /tmp/country_baseline.txt

# Then detect outliers (countries with very low connection counts)
awk '$1 == 1 {print "OUTLIER: Single connection from " $0}' /tmp/country_baseline.txt
```

**Velocity-Based Outliers:**

```python
from datetime import datetime
from collections import defaultdict

def detect_velocity_outliers(logfile, service='ftp'):
    """Detect unusual activity velocity (rapid actions)"""
    user_timestamps = defaultdict(list)
    
    with open(logfile, 'r') as f:
        for line in f:
            if service == 'ftp':
                # Parse xferlog
                parts = line.split()
                if len(parts) >= 14:
                    try:
                        timestamp = datetime.strptime(' '.join(parts[0:4]), '%a %b %d %H:%M:%S %Y')
                        user = parts[12]
                        user_timestamps[user].append(timestamp)
                    except:
                        continue
    
    outliers = []
    for user, timestamps in user_timestamps.items():
        if len(timestamps) < 2:
            continue
        
        timestamps.sort()
        
        # Calculate intervals between actions
        for i in range(1, len(timestamps)):
            interval = (timestamps[i] - timestamps[i-1]).total_seconds()
            
            # Flag very rapid actions (< 1 second, potentially automated)
            if interval < 1:
                outliers.append({
                    'user': user,
                    'timestamp': timestamps[i],
                    'interval': interval,
                    'type': 'rapid_action'
                })
    
    print("Velocity Outliers (rapid actions):")
    for outlier in outliers[:20]:
        print(f"  {outlier['user']} at {outlier['timestamp']}: {outlier['interval']:.3f}s interval")
    
    return outliers

# Usage
velocity_outliers = detect_velocity_outliers('/var/log/xferlog', 'ftp')
```

**Data Volume Outliers:**

```bash
# Detect unusually large data transfers per user
awk '{
    user=$11
    size=$5
    user_total[user]+=size
    user_count[user]++
} END {
    for(u in user_total) {
        avg=user_total[u]/user_count[u]
        all_avgs[NR]=avg
        users[NR]=u
        totals[NR]=user_total[u]
    }
    
    # Calculate overall average
    sum=0
    for(i=1; i<=NR; i++) sum+=all_avgs[i]
    global_avg=sum/NR
    
    # Flag users with >5x average
    print "Data Volume Outliers:"
    for(i=1; i<=NR; i++) {
        if(totals[i] > global_avg * 5) {
            printf "  %s: %d bytes (%.1fx average)\n", users[i], totals[i], totals[i]/global_avg
        }
    }
}' /var/log/xferlog
```

### Sequence-Based Outlier Detection

**Unusual Command Sequences:**

```bash
# Detect unusual FTP command sequences (if command logging enabled)
# Example: STOR followed immediately by DELE (upload then delete - suspicious)
grep "FTP command:" /var/log/vsftpd.log | \
  awk '{
    timestamp=$1" "$2" "$3
    user=$5
    command=$7
    
    if(last_user == user && last_command == "STOR" && command == "DELE") {
      print timestamp, user, "SUSPICIOUS: STOR -> DELE sequence"
    }
    
    last_user=user
    last_command=command
  }'
```

**DNS Query Sequence Outliers:**

```python
def detect_query_sequence_outliers(logfile):
    """Detect unusual DNS query sequences"""
    sequences = defaultdict(list)
    
    with open(logfile, 'r') as f:
        last_ip = None
        last_domain = None
        
        for line in f:
            match = re.search(r'client.*?([0-9.]+)#\d+.*query: ([^\s]+)', line)
            if match:
                ip = match.group(1)
                domain = match.group(2)
                
                if ip == last_ip:
                    sequences[ip].append(domain)
                else:
                    sequences[ip] = [domain]
                
                last_ip = ip
                last_domain = domain
    
    outliers = []
    for ip, domain_list in sequences.items():
        # Detect rapid sequential queries to many unique domains
        if len(domain_list) > 100:
            unique_domains = len(set(domain_list))
            if unique_domains > 50:
                outliers.append({
                    'ip': ip,
                    'total_queries': len(domain_list),
                    'unique_domains': unique_domains,
                    'type': 'enumeration_scan'
                })
        
        # Detect queries following suspicious patterns (e.g., sequential subdomains)
        subdomains = [d.split('.')[0] for d in domain_list if '.' in d]
        if len(subdomains) > 20:
            # Check if subdomains are sequential (1.example.com, 2.example.com, etc.)
            numeric_subdomains = [s for s in subdomains if s.isdigit()]
            if len(numeric_subdomains) > 10:
                outliers.append({
                    'ip': ip,
                    'type': 'sequential_enumeration',
                    'count': len(numeric_subdomains)
                })
    
    print("DNS Query Sequence Outliers:")
    for outlier in outliers:
        print(f"  {outlier['ip']}: {outlier['type']}")
        if 'total_queries' in outlier:
            print(f"    {outlier['total_queries']} queries to {outlier['unique_domains']} unique domains")
    
    return outliers

# Usage
dns_outliers = detect_query_sequence_outliers('/var/log/named/queries.log')
```

### Machine Learning-Based Outlier Detection

[Inference] Machine learning approaches can detect complex outliers that rule-based systems miss, though they require training data and may produce false positives:

```python
from sklearn.ensemble import IsolationForest
import numpy as np

def ml_outlier_detection(features, contamination=0.1):
    """
    Use Isolation Forest for outlier detection
    Note: Requires scikit-learn library
    """
    # Features should be 2D array: [[feature1, feature2, ...], ...]
    features_array = np.array(features)
    
    # Train Isolation Forest
    clf = IsolationForest(contamination=contamination, random_state=42)
    predictions = clf.fit_predict(features_array)
    
    # -1 indicates outlier, 1 indicates inlier
    outlier_indices = [i for i, pred in enumerate(predictions) if pred == -1]
    
    return outlier_indices

def extract_features_from_logs(logfile):
    """Extract numerical features for ML analysis"""
    features = []
    metadata = []
    
    with open(logfile, 'r') as f:
        for line in f:
            # Example: Extract FTP transfer features
            parts = line.split()
            if len(parts) >= 14:
                try:
                    duration = int(parts[4])
                    filesize = int(parts[6])
                    
                    # Create feature vector
                    feature_vector = [
                        duration,
                        filesize,
                        filesize / max(duration, 1),  # Transfer rate
                        len(parts[7]),  # Filename length
                        1 if parts[10] == 'o' else 0  # Direction (binary feature)
                    ]
                    
                    features.append(feature_vector)
                    metadata.append({
                        'timestamp': ' '.join(parts[0:4]),
                        'user': parts[12],
                        'filename': parts[7]
                    })
                except:
                    continue
    
    return features, metadata

# Usage
features, metadata = extract_features_from_logs('/var/log/xferlog')
outlier_indices = ml_outlier_detection(features, contamination=0.05)

print(f"ML-Detected Outliers ({len(outlier_indices)} found):")
for idx in outlier_indices[:20]:
    print(f"  {metadata[idx]['timestamp']} - {metadata[idx]['user']}: {metadata[idx]['filename']}")
```

## Signature Matching

Signature matching identifies known attack patterns, malicious indicators, and CTF-specific artifacts by comparing log entries against databases of known threats. This is the most direct form of pattern recognition but requires maintaining up-to-date signature databases.

### Attack Signature Libraries

**Common Attack Patterns:**

```python
# Define attack signatures
ATTACK_SIGNATURES = {
    'sql_injection': {
        'patterns': [
            r"'.*OR.*'.*=.*'",
            r"';.*DROP.*TABLE",
            r"UNION.*SELECT",
            r"1=1--",
            r"admin'--",
            r"' OR '1'='1"
        ],
        'severity': 'critical'
    },
    'path_traversal': {
        'patterns': [
            r'\.\./\.\./\.\.',
            r'\.\.\\\.\.\\',
            r'%2e%2e%2f',
            r'%252e%252e%252f'
        ],
        'severity': 'high'
    },
    'command_injection': {
        'patterns': [
            r';.*cat\s+/etc/passwd',
            r'\|.*nc\s+-',
            r'`.*`',
            r'\$\(.*\)',
            r'&&.*rm\s+-rf'
        ],
        'severity': 'critical'
    },
    'xss': {
        'patterns': [
            r'<script.*>',
            r'javascript:',
            r'onerror\s*=',
            r'onload\s*=',
            r'<iframe'
        ],
        'severity': 'medium'
    },
    'directory_enumeration': {
        'patterns': [
            r'/admin',
            r'/backup',
            r'/.git/',
            r'/\.env',
            r'/config\.php'
        ],
        'severity': 'low'
    }
}

import re

def match_signatures(log_line, signatures):
    """Match log line against attack signatures"""
    matches = []
    
    for attack_type, sig_data in signatures.items():
        for pattern in sig_data['patterns']:
            if re.search(pattern, log_line, re.IGNORECASE):
                matches.append({
                    'attack_type': attack_type,
                    'pattern': pattern,
                    'severity': sig_data['severity'],
                    'line': log_line.strip()
                })
    
    return matches


def scan_logs_for_signatures(logfile, signatures):
    """Scan entire log file for attack signatures"""
    all_matches = []

    with open(logfile, 'r') as f:
        for line_num, line in enumerate(f, 1):
            matches = match_signatures(line, signatures)
            for match in matches:
                match['line_number'] = line_num
                all_matches.append(match)

    # Group by attack type
    by_type = {}
    for match in all_matches:
        attack_type = match['attack_type']
        if attack_type not in by_type:
            by_type[attack_type] = []
        by_type[attack_type].append(match)

    print("Signature Matches Found:")
    for attack_type, matches in sorted(by_type.items(), key=lambda x: len(x[1]), reverse=True):
        print(f"\n  {attack_type.upper()}: {len(matches)} matches")
        for match in matches[:5]:  # Show first 5
            print(f"    Line {match['line_number']}: {match['line'][:100]}")

    return all_matches


# Usage
matches = scan_logs_for_signatures('/var/log/vsftpd.log', ATTACK_SIGNATURES)
````

### Service-Specific Signatures

**FTP Attack Signatures:**

```python
FTP_ATTACK_SIGNATURES = {
    'anonymous_abuse': {
        'patterns': [
            r'OK LOGIN.*anonymous',
            r'ANONYMOUS.*OK DOWNLOAD'
        ],
        'description': 'Anonymous FTP access',
        'severity': 'medium'
    },
    'brute_force': {
        'patterns': [
            r'FAIL LOGIN'
        ],
        'threshold': 10,  # 10+ failures = attack
        'description': 'FTP brute force attempt',
        'severity': 'high'
    },
    'bounce_attack': {
        'patterns': [
            r'PORT\s+\d+,\d+,\d+,\d+,\d+,\d+',
            r'EPRT'
        ],
        'description': 'FTP bounce attack attempt',
        'severity': 'high'
    },
    'suspicious_uploads': {
        'patterns': [
            r'\.php[0-9]?$',
            r'\.jsp$',
            r'\.asp$',
            r'\.exe$',
            r'\.sh$',
            r'\.pl$',
            r'shell\..*'
        ],
        'description': 'Suspicious file upload',
        'severity': 'critical'
    },
    'data_exfiltration': {
        'patterns': [
            r'/etc/passwd',
            r'/etc/shadow',
            r'\.sql$',
            r'\.db$',
            r'backup.*\.tar',
            r'dump.*\.sql'
        ],
        'description': 'Potential data exfiltration',
        'severity': 'critical'
    }
}

def detect_ftp_attacks(logfile):
    """Specialized FTP attack detection"""
    matches = scan_logs_for_signatures(logfile, FTP_ATTACK_SIGNATURES)
    
    # Additional context-aware detection
    # Check for brute force threshold
    failed_logins = {}
    with open(logfile, 'r') as f:
        for line in f:
            if 'FAIL LOGIN' in line:
                ip_match = re.search(r'Client "([0-9.]+)"', line)
                if ip_match:
                    ip = ip_match.group(1)
                    failed_logins[ip] = failed_logins.get(ip, 0) + 1
    
    print("\nBrute Force Analysis:")
    for ip, count in sorted(failed_logins.items(), key=lambda x: x[1], reverse=True):
        if count >= 10:
            print(f"  {ip}: {count} failed attempts - BRUTE FORCE DETECTED")
    
    return matches

# Usage
ftp_attacks = detect_ftp_attacks('/var/log/vsftpd.log')
````

**SMTP Attack Signatures:**

```python
SMTP_ATTACK_SIGNATURES = {
    'spam_relay': {
        'patterns': [
            r'Relay access denied',
            r'sender=<>.*nrcpt=\d+',  # Null sender with multiple recipients
        ],
        'description': 'Spam relay attempt',
        'severity': 'high'
    },
    'email_harvesting': {
        'patterns': [
            r'Recipient address rejected: User unknown',
            r'RCPT TO.*550'
        ],
        'threshold': 20,
        'description': 'Email address harvesting',
        'severity': 'medium'
    },
    'auth_brute_force': {
        'patterns': [
            r'authentication failed',
            r'SASL.*authentication failure',
            r'LOGIN failed'
        ],
        'threshold': 10,
        'description': 'SMTP authentication brute force',
        'severity': 'high'
    },
    'phishing_indicators': {
        'patterns': [
            r'from=<.*@.*>.*to=<[^>]+@(?!origdomain)',  # External sender to multiple internal
            r'message-id=<.*@(?!origdomain)',  # Spoofed message-id
            r'Subject:.*urgent.*verify.*account',
            r'Subject:.*suspended.*confirm'
        ],
        'description': 'Potential phishing email',
        'severity': 'high'
    },
    'malware_attachment': {
        'patterns': [
            r'\.exe\s+attachment',
            r'\.scr\s+attachment',
            r'\.zip.*\.exe',
            r'Content-Type:.*application/x-msdownload'
        ],
        'description': 'Suspicious attachment',
        'severity': 'critical'
    },
    'backscatter': {
        'patterns': [
            r'status=bounced',
            r'Mail Delivery Failed',
            r'Undelivered Mail Returned to Sender'
        ],
        'description': 'Backscatter (possible joe-job)',
        'severity': 'medium'
    }
}

def detect_smtp_attacks(logfile):
    """Specialized SMTP attack detection"""
    matches = scan_logs_for_signatures(logfile, SMTP_ATTACK_SIGNATURES)
    
    # Detect dictionary attacks
    unknown_recipients = {}
    with open(logfile, 'r') as f:
        for line in f:
            if 'User unknown' in line or 'Recipient address rejected' in line:
                ip_match = re.search(r'client=.*\[([0-9.]+)\]', line)
                if ip_match:
                    ip = ip_match.group(1)
                    unknown_recipients[ip] = unknown_recipients.get(ip, 0) + 1
    
    print("\nDictionary Attack Analysis:")
    for ip, count in sorted(unknown_recipients.items(), key=lambda x: x[1], reverse=True):
        if count >= 20:
            print(f"  {ip}: {count} unknown recipients - DICTIONARY ATTACK")
    
    return matches

# Usage
smtp_attacks = detect_smtp_attacks('/var/log/mail.log')
```

**DNS Attack Signatures:**

```python
DNS_ATTACK_SIGNATURES = {
    'dns_tunneling': {
        'patterns': [
            r'query: [a-z0-9]{40,}\..*IN\s+TXT',  # Long subdomain with TXT query
            r'query: [A-Za-z0-9+/=]{20,}\..*IN\s+A',  # Base64-like subdomain
        ],
        'description': 'DNS tunneling detected',
        'severity': 'critical'
    },
    'dga_domain': {
        'patterns': [
            r'query: [bcdfghjklmnpqrstvwxyz]{15,}\.',  # High consonant ratio
            r'query: [a-z0-9]{20,}\.(ru|cn|tk|ml)',  # Long random + suspicious TLD
        ],
        'description': 'DGA-generated domain',
        'severity': 'high'
    },
    'dns_amplification': {
        'patterns': [
            r'query:.*IN\s+ANY',  # ANY queries used in amplification
            r'query:.*IN\s+RRSIG',  # DNSSEC queries for amplification
        ],
        'description': 'DNS amplification attack component',
        'severity': 'medium'
    },
    'zone_transfer_attempt': {
        'patterns': [
            r'query:.*IN\s+AXFR',
            r'query:.*IN\s+IXFR'
        ],
        'description': 'Zone transfer attempt',
        'severity': 'high'
    },
    'subdomain_enumeration': {
        'patterns': [
            r'query: (www|mail|ftp|admin|dev|test|staging)\.',
        ],
        'threshold': 50,  # 50+ common subdomains queried
        'description': 'Subdomain enumeration',
        'severity': 'medium'
    },
    'c2_beaconing': {
        'patterns': [
            r'query: [a-f0-9]{32}\..*',  # MD5-like subdomains
            r'query: \d{10,}\..*',  # Timestamp-like subdomains
        ],
        'description': 'C2 beaconing pattern',
        'severity': 'critical'
    }
}

def detect_dns_attacks(logfile):
    """Specialized DNS attack detection"""
    matches = scan_logs_for_signatures(logfile, DNS_ATTACK_SIGNATURES)
    
    # Detect high-frequency queries (potential DDoS participation)
    query_counts = {}
    with open(logfile, 'r') as f:
        for line in f:
            domain_match = re.search(r'query: ([^\s]+)', line)
            if domain_match:
                domain = domain_match.group(1)
                query_counts[domain] = query_counts.get(domain, 0) + 1
    
    print("\nHigh-Frequency Queries:")
    for domain, count in sorted(query_counts.items(), key=lambda x: x[1], reverse=True)[:10]:
        if count > 1000:
            print(f"  {domain}: {count} queries - POTENTIAL DDoS")
    
    return matches

# Usage
dns_attacks = detect_dns_attacks('/var/log/named/queries.log')
```

### CTF-Specific Signature Matching

**CTF Flag Patterns:**

```python
CTF_FLAG_SIGNATURES = {
    'flag_format': {
        'patterns': [
            r'flag\{[^}]+\}',
            r'CTF\{[^}]+\}',
            r'FLAG\{[^}]+\}',
            r'[A-Z0-9]{32}',  # MD5-like flags
            r'[A-Za-z0-9+/]{40}=*',  # Base64-encoded flags
        ],
        'description': 'CTF flag pattern',
        'severity': 'info'
    },
    'encoded_flags': {
        'patterns': [
            r'ZmxhZ3',  # "flag" in Base64
            r'Q1RG',  # "CTF" in Base64
            r'66 6c 61 67',  # "flag" in hex
            r'CTF.*[A-Za-z0-9+/=]{20,}',  # CTF with Base64
        ],
        'description': 'Encoded flag',
        'severity': 'info'
    },
    'steganography_hints': {
        'patterns': [
            r'\.png.*hidden',
            r'\.jpg.*message',
            r'steghide',
            r'LSB.*image'
        ],
        'description': 'Steganography hint',
        'severity': 'info'
    },
    'crypto_hints': {
        'patterns': [
            r'caesar',
            r'rot13',
            r'vigenere',
            r'decrypt.*key',
            r'cipher.*text'
        ],
        'description': 'Cryptography hint',
        'severity': 'info'
    }
}

def find_ctf_flags(logfile):
    """Search for CTF flags in logs"""
    matches = scan_logs_for_signatures(logfile, CTF_FLAG_SIGNATURES)
    
    # Additional decoding attempts
    potential_flags = []
    with open(logfile, 'r') as f:
        for line in f:
            # Try Base64 decoding on suspicious strings
            base64_candidates = re.findall(r'[A-Za-z0-9+/]{20,}={0,2}', line)
            for candidate in base64_candidates:
                try:
                    import base64
                    decoded = base64.b64decode(candidate).decode('utf-8', errors='ignore')
                    if 'flag' in decoded.lower() or 'CTF' in decoded:
                        potential_flags.append({
                            'original': candidate,
                            'decoded': decoded,
                            'line': line.strip()
                        })
                except:
                    continue
            
            # Try hex decoding
            hex_candidates = re.findall(r'(?:[0-9a-fA-F]{2}\s*){8,}', line)
            for candidate in hex_candidates:
                try:
                    hex_bytes = bytes.fromhex(candidate.replace(' ', ''))
                    decoded = hex_bytes.decode('utf-8', errors='ignore')
                    if 'flag' in decoded.lower() or 'CTF' in decoded:
                        potential_flags.append({
                            'original': candidate,
                            'decoded': decoded,
                            'line': line.strip()
                        })
                except:
                    continue
    
    print("\nPotential Decoded Flags:")
    for flag in potential_flags:
        print(f"  Encoded: {flag['original'][:50]}")
        print(f"  Decoded: {flag['decoded']}")
        print(f"  Source: {flag['line'][:100]}")
        print()
    
    return matches, potential_flags

# Usage
ctf_matches, decoded_flags = find_ctf_flags('/var/log/vsftpd.log')
```

### YARA-Style Rule Engine

**Custom Rule Engine Implementation:**

```python
class SignatureRule:
    """YARA-style rule for log analysis"""
    
    def __init__(self, name, patterns, condition='any', metadata=None):
        self.name = name
        self.patterns = {k: re.compile(v, re.IGNORECASE) for k, v in patterns.items()}
        self.condition = condition
        self.metadata = metadata or {}
    
    def match(self, text):
        """Match rule against text"""
        matches = {}
        for pattern_name, pattern_regex in self.patterns.items():
            match = pattern_regex.search(text)
            if match:
                matches[pattern_name] = match.group(0)
        
        # Evaluate condition
        if self.condition == 'any':
            return len(matches) > 0, matches
        elif self.condition == 'all':
            return len(matches) == len(self.patterns), matches
        elif isinstance(self.condition, int):
            return len(matches) >= self.condition, matches
        
        return False, matches

class RuleEngine:
    """Signature matching engine"""
    
    def __init__(self):
        self.rules = []
    
    def add_rule(self, rule):
        """Add a rule to the engine"""
        self.rules.append(rule)
    
    def scan(self, logfile):
        """Scan log file with all rules"""
        results = []
        
        with open(logfile, 'r') as f:
            for line_num, line in enumerate(f, 1):
                for rule in self.rules:
                    matched, pattern_matches = rule.match(line)
                    if matched:
                        results.append({
                            'rule_name': rule.name,
                            'line_number': line_num,
                            'line': line.strip(),
                            'matches': pattern_matches,
                            'metadata': rule.metadata
                        })
        
        return results
    
    def report(self, results):
        """Generate report from scan results"""
        # Group by rule
        by_rule = {}
        for result in results:
            rule_name = result['rule_name']
            if rule_name not in by_rule:
                by_rule[rule_name] = []
            by_rule[rule_name].append(result)
        
        print("=== Signature Matching Report ===\n")
        for rule_name, matches in sorted(by_rule.items(), key=lambda x: len(x[1]), reverse=True):
            print(f"{rule_name}: {len(matches)} matches")
            if matches[0]['metadata']:
                print(f"  Severity: {matches[0]['metadata'].get('severity', 'unknown')}")
                print(f"  Description: {matches[0]['metadata'].get('description', 'N/A')}")
            
            for match in matches[:3]:  # Show first 3
                print(f"  Line {match['line_number']}: {match['line'][:80]}")
            print()

# Define rules
engine = RuleEngine()

# SQL Injection rule
engine.add_rule(SignatureRule(
    name="sql_injection",
    patterns={
        'union': r'UNION.*SELECT',
        'or_bypass': r"'.*OR.*'.*=.*'",
        'comment': r"'.*--",
    },
    condition='any',
    metadata={
        'severity': 'critical',
        'description': 'SQL injection attempt detected',
        'mitre_attack': 'T1190'
    }
))

# Web shell upload
engine.add_rule(SignatureRule(
    name="web_shell_upload",
    patterns={
        'php_shell': r'(c99|r57|b374k|shell)\.php',
        'jsp_shell': r'cmd\.jsp',
        'asp_shell': r'(cmd|shell)\.asp',
    },
    condition='any',
    metadata={
        'severity': 'critical',
        'description': 'Web shell upload attempt'
    }
))

# Suspicious combo: authentication + large transfer
engine.add_rule(SignatureRule(
    name="auth_and_exfiltration",
    patterns={
        'auth': r'OK LOGIN',
        'large_file': r'size=[0-9]{8,}',
    },
    condition='all',  # Both must match
    metadata={
        'severity': 'high',
        'description': 'Authentication followed by large transfer'
    }
))

# Scan and report
results = engine.scan('/var/log/vsftpd.log')
engine.report(results)
```

### Threat Intelligence Integration

**IOC (Indicator of Compromise) Matching:**

```python
import json
import requests

class ThreatIntelligence:
    """Integrate threat intelligence feeds"""
    
    def __init__(self):
        self.malicious_ips = set()
        self.malicious_domains = set()
        self.malicious_hashes = set()
    
    def load_ioc_feed(self, feed_url=None, local_file=None):
        """Load IOCs from feed or file"""
        if local_file:
            with open(local_file, 'r') as f:
                iocs = json.load(f)
                self.malicious_ips.update(iocs.get('ips', []))
                self.malicious_domains.update(iocs.get('domains', []))
                self.malicious_hashes.update(iocs.get('hashes', []))
        
        # [Unverified] Example of loading from external feed
        # Actual implementation depends on feed format
        if feed_url:
            try:
                response = requests.get(feed_url, timeout=10)
                # Parse feed based on format
                pass
            except:
                pass
    
    def check_ip(self, ip):
        """Check if IP is in threat intel"""
        return ip in self.malicious_ips
    
    def check_domain(self, domain):
        """Check if domain is in threat intel"""
        return domain in self.malicious_domains
    
    def scan_logs_for_iocs(self, logfile, service='dns'):
        """Scan logs for IOC matches"""
        matches = []
        
        with open(logfile, 'r') as f:
            for line_num, line in enumerate(f, 1):
                if service == 'dns':
                    # Extract domain
                    domain_match = re.search(r'query: ([^\s]+)', line)
                    if domain_match:
                        domain = domain_match.group(1)
                        if self.check_domain(domain):
                            matches.append({
                                'type': 'malicious_domain',
                                'value': domain,
                                'line_number': line_num,
                                'line': line.strip()
                            })
                    
                    # Extract client IP
                    ip_match = re.search(r'client.*?([0-9.]+)#', line)
                    if ip_match:
                        ip = ip_match.group(1)
                        if self.check_ip(ip):
                            matches.append({
                                'type': 'malicious_ip',
                                'value': ip,
                                'line_number': line_num,
                                'line': line.strip()
                            })
        
        print(f"Threat Intelligence Matches: {len(matches)}")
        for match in matches[:20]:
            print(f"  {match['type']}: {match['value']} (line {match['line_number']})")
        
        return matches

# Usage
ti = ThreatIntelligence()

# Load IOCs from file
ioc_data = {
    'ips': ['192.0.2.100', '198.51.100.50'],
    'domains': ['malicious-c2.example.com', 'phishing-site.example.org'],
    'hashes': ['d41d8cd98f00b204e9800998ecf8427e']
}

with open('/tmp/iocs.json', 'w') as f:
    json.dump(ioc_data, f)

ti.load_ioc_feed(local_file='/tmp/iocs.json')
ioc_matches = ti.scan_logs_for_iocs('/var/log/named/queries.log', 'dns')
```

### Signature Performance Optimization

**Efficient Pattern Matching:**

```python
import re
from collections import defaultdict

class OptimizedSignatureMatcher:
    """Optimized signature matching using compiled patterns"""
    
    def __init__(self):
        self.compiled_patterns = []
        self.pattern_metadata = {}
    
    def add_signature(self, name, pattern, metadata=None):
        """Add signature with compiled regex"""
        compiled = re.compile(pattern, re.IGNORECASE)
        self.compiled_patterns.append((name, compiled))
        self.pattern_metadata[name] = metadata or {}
    
    def scan_line(self, line):
        """Scan single line against all patterns"""
        matches = []
        for name, pattern in self.compiled_patterns:
            if pattern.search(line):
                matches.append({
                    'signature': name,
                    'metadata': self.pattern_metadata[name]
                })
        return matches
    
    def scan_file(self, logfile, progress_callback=None):
        """Scan entire file with optional progress reporting"""
        results = defaultdict(list)
        line_count = 0
        
        with open(logfile, 'r') as f:
            for line_num, line in enumerate(f, 1):
                line_count += 1
                matches = self.scan_line(line)
                
                for match in matches:
                    results[match['signature']].append({
                        'line_number': line_num,
                        'line': line.strip()
                    })
                
                if progress_callback and line_num % 1000 == 0:
                    progress_callback(line_num)
        
        return dict(results), line_count

# Usage with performance tracking
import time

matcher = OptimizedSignatureMatcher()

# Add multiple signatures
signatures = {
    'sql_injection': r"'.*OR.*'.*=.*'",
    'path_traversal': r'\.\./\.\./\.\.',
    'xss': r'<script.*>',
    'command_injection': r';.*cat\s+/etc/passwd',
}

for name, pattern in signatures.items():
    matcher.add_signature(name, pattern, {'severity': 'high'})

def progress(line_num):
    print(f"Processed {line_num} lines...", end='\r')

start_time = time.time()
results, total_lines = matcher.scan_file('/var/log/vsftpd.log', progress_callback=progress)
elapsed = time.time() - start_time

print(f"\nScanned {total_lines} lines in {elapsed:.2f} seconds")
print(f"Rate: {total_lines/elapsed:.0f} lines/second")
print(f"\nMatches found: {sum(len(v) for v in results.values())}")
```

### Comprehensive Pattern Recognition Workflow

**Integrated Analysis Pipeline:**

```python
class LogAnalysisPipeline:
    """Complete pattern recognition pipeline"""
    
    def __init__(self):
        self.baseline = None
        self.signature_engine = RuleEngine()
        self.threat_intel = ThreatIntelligence()
        self.outliers = []
        self.signature_matches = []
        self.ioc_matches = []
    
    def run_analysis(self, logfile, service='ftp'):
        """Run complete analysis pipeline"""
        print("=== Starting Log Analysis Pipeline ===\n")
        
        # Step 1: Establish baseline
        print("Step 1: Establishing baseline...")
        analyzer = BaselineAnalyzer()
        self.baseline, _ = analyzer.establish_volume_baseline(logfile, service)
        
        # Step 2: Detect outliers
        print("\nStep 2: Detecting outliers...")
        self.outliers = self._detect_outliers(logfile, service)
        
        # Step 3: Signature matching
        print("\nStep 3: Matching attack signatures...")
        self.signature_matches = self.signature_engine.scan(logfile)
        self.signature_engine.report(self.signature_matches)
        
        # Step 4: Threat intelligence
        print("\nStep 4: Checking threat intelligence...")
        self.ioc_matches = self.threat_intel.scan_logs_for_iocs(logfile, service)
        
        # Step 5: Generate final report
        print("\n Step 5: Generating report...")
        self._generate_report()
    
    def _detect_outliers(self, logfile, service):
        """Outlier detection logic"""
        # Implementation depends on service
        return []
    
    def _generate_report(self):
        """Generate comprehensive analysis report"""
        print("\n" + "="*60)
        print("ANALYSIS SUMMARY")
        print("="*60)
        
        if self.baseline:
            print(f"\nBaseline Statistics:")
            print(f"  Mean: {self.baseline['mean']:.2f}")
            print(f"  Normal Range: {self.baseline['lower_threshold']:.2f} - {self.baseline['upper_threshold']:.2f}")
        
        print(f"\nOutliers Detected: {len(self.outliers)}")
        print(f"Signature Matches: {len(self.signature_matches)}")
        print(f"IOC Matches: {len(self.ioc_matches)}")
        
        # Calculate risk score
        risk_score = (
            len(self.outliers) * 1 +
            len([m for m in self.signature_matches if m.get('metadata', {}).get('severity') == 'critical']) * 10 +
            len([m for m in self.signature_matches if m.get('metadata', {}).get('severity') == 'high']) * 5 +
            len(self.ioc_matches) * 15
        )
        
        print(f"\nOverall Risk Score: {risk_score}")
        if risk_score > 100:
            print("  STATUS: CRITICAL - Immediate investigation required")
        elif risk_score > 50:
            print("  STATUS: HIGH - Investigation recommended")
        elif risk_score > 20:
            print("  STATUS: MEDIUM - Monitor closely")
        else:
            print("  STATUS: LOW - Normal activity")

# Usage
pipeline = LogAnalysisPipeline()
# Add rules to pipeline.signature_engine
# Load IOCs into pipeline.threat_intel
# pipeline.run_analysis('/var/log/vsftpd.log', 'ftp')
```

### Important Related Topics

For comprehensive CTF pattern recognition mastery, consider exploring:

1. **Temporal Pattern Analysis** - Time-series anomaly detection, seasonal patterns, periodic behavior
2. **Graph-Based Analysis** - Network relationship mapping, lateral movement detection
3. **Statistical Process Control** - Control charts, CUSUM algorithms for drift detection
4. **Behavioral Analytics** - User and Entity Behavior Analytics (UEBA), peer group analysis
5. **Automated Response** - SOAR integration, automated blocking, alert enrichment

---

# Attack Pattern Identification

Attack pattern identification involves analyzing logs, network traffic, and system artifacts to detect and classify malicious activity. This skill is fundamental in CTF forensics, incident response, and web exploitation challenges.

## Brute Force Attempts

### Detection Mechanisms

**Log Analysis Patterns:**

- Multiple failed authentication attempts from single source IP
- Rapid sequential login attempts (time-based clustering)
- Dictionary-based username enumeration
- Credential stuffing patterns (valid username, multiple passwords)

**Key Log Locations (Linux):**

```bash
/var/log/auth.log          # Debian/Ubuntu authentication logs
/var/log/secure            # RHEL/CentOS authentication logs
/var/log/wtmp              # Successful logins
/var/log/btmp              # Failed login attempts
```

**Analysis Commands:**

```bash
# View failed SSH login attempts
grep "Failed password" /var/log/auth.log | awk '{print $(NF-3)}' | sort | uniq -c | sort -nr

# Extract failed login usernames
grep "Failed password for" /var/log/auth.log | awk '{print $9}' | sort | uniq -c | sort -nr

# Time-based analysis of authentication attempts
grep "Failed password" /var/log/auth.log | awk '{print $1, $2, $3}' | uniq -c

# Analyze failed login attempts from btmp
lastb | head -20

# Extract unique attacking IPs with attempt count
lastb | awk '{print $3}' | sort | uniq -c | sort -nr
```

**Web Application Brute Force Patterns:**

HTTP status code analysis:

```bash
# Analyze Apache/Nginx logs for failed login attempts (401/403)
awk '($9 ~ /401|403/)' /var/log/apache2/access.log | awk '{print $1}' | sort | uniq -c | sort -nr

# Detect rapid POST requests to login endpoints
grep "POST /login" /var/log/nginx/access.log | awk '{print $1, $4}' | uniq -c | awk '$1 > 10'

# Time-window analysis (requests per minute)
awk '{print $4}' /var/log/apache2/access.log | cut -d: -f1-3 | sort | uniq -c | sort -nr
```

**Identifying Brute Force Tools:**

User-Agent signatures indicating automated tools:

- `Hydra`, `Medusa`, `Patator` in User-Agent strings
- Absence of standard browser User-Agent
- Unusual or minimal HTTP headers

```bash
# Extract unique User-Agents from failed auth attempts
grep "401" /var/log/apache2/access.log | awk -F'"' '{print $6}' | sort | uniq -c
```

**Rate-Based Detection:**

[Inference] Thresholds vary by application, but common indicators:

- > 5 failed attempts within 60 seconds from single IP
    
- > 20 attempts within 5 minutes
    
- Sequential attempts with <1 second intervals

---

## SQL Injection Indicators

### Log-Based Detection

**Common SQLi Patterns in Web Logs:**

```bash
# Search for SQL keywords in GET/POST parameters
grep -E "(\%27)|(')|(--)|(\%23)|(#)" /var/log/apache2/access.log

# Detect UNION-based injection attempts
grep -iE "union.*select" /var/log/apache2/access.log

# Boolean-based blind SQLi patterns
grep -E "(\%20AND|\%20OR|'\%20AND|'\%20OR)" /var/log/apache2/access.log

# Time-based blind SQLi
grep -iE "(sleep\(|benchmark\(|waitfor\%20delay)" /var/log/apache2/access.log

# Stacked queries
grep -E ";\%20(SELECT|INSERT|UPDATE|DELETE|DROP)" /var/log/apache2/access.log
```

**URL-Encoded SQL Injection Patterns:**

Common encodings to search for:

- `%27` = single quote (')
- `%22` = double quote (")
- `%23` = hash (#)
- `%2D` = dash (-)
- `%20` = space
- `%3D` = equals (=)
- `%3B` = semicolon (;)

**Advanced SQLi Pattern Detection:**

```bash
# Decode and analyze suspicious URLs
grep "?" /var/log/apache2/access.log | grep -E "(%27|%23|%2D%2D)" | \
  python3 -c "import sys; from urllib.parse import unquote; print(unquote(sys.stdin.read()))"

# Extract potential SQLi payloads with context
awk -F'"' '/(union|select|from|where|and|or)/ {print $2, $6}' /var/log/apache2/access.log
```

**Database Error Leakage in Responses:**

[Inference] Application logs may contain database errors revealing successful injection:

- MySQL: "You have an error in your SQL syntax"
- PostgreSQL: "ERROR: syntax error at or near"
- MSSQL: "Unclosed quotation mark after the character string"
- Oracle: "ORA-" error codes

```bash
# Search application logs for database errors
grep -iE "(sql syntax|mysql_fetch|ora-[0-9]{5}|syntax error at)" /var/log/application.log
```

**SQLi Tool Signatures:**

```bash
# Detect sqlmap usage
grep -i "sqlmap" /var/log/apache2/access.log

# Identify by User-Agent
grep -E "User-Agent.*sqlmap" /var/log/apache2/access.log
```

**Injection Point Analysis:**

Common vulnerable parameters:

- `id=`, `user=`, `product=`, `category=`
- Search functionality: `q=`, `search=`, `query=`
- Sorting/filtering: `sort=`, `order=`, `filter=`

```bash
# Extract parameters from suspicious requests
grep -E "(%27|union|select)" /var/log/apache2/access.log | \
  grep -oP '\?[^"]*' | cut -d'&' -f1 | sort | uniq -c | sort -nr
```

---

## XSS Attack Patterns

### Detection in Logs

**Reflected XSS Indicators:**

```bash
# Basic script tag detection
grep -iE "(<script|%3Cscript)" /var/log/apache2/access.log

# Event handler-based XSS
grep -iE "(onerror|onload|onclick|onmouseover).*=" /var/log/apache2/access.log

# JavaScript protocol handler
grep -iE "(javascript:|data:text/html)" /var/log/apache2/access.log

# URL-encoded XSS payloads
grep -E "(%3C|%3E|%22|%27).*(%3C|%3E)" /var/log/apache2/access.log
```

**Common XSS Payload Patterns:**

```bash
# Standard alert/prompt/confirm patterns
grep -iE "(alert\(|prompt\(|confirm\()" /var/log/apache2/access.log

# IMG tag-based XSS
grep -iE "(<img|%3Cimg).*src" /var/log/apache2/access.log

# SVG-based XSS
grep -iE "(<svg|%3Csvg)" /var/log/apache2/access.log

# Iframe injection
grep -iE "(<iframe|%3Ciframe)" /var/log/apache2/access.log
```

**Obfuscated XSS Detection:**

```bash
# HTML entity encoding
grep -E "(&#x[0-9a-f]{2}|&#[0-9]{2,3})" /var/log/apache2/access.log

# Mixed case evasion
grep -iE "(<ScRiPt|<sCrIpT)" /var/log/apache2/access.log

# Null byte injection
grep -E "%00" /var/log/apache2/access.log

# Unicode/UTF-8 encoding attempts
grep -E "(\xE2\x80\x8B|\\u[0-9a-f]{4})" /var/log/apache2/access.log
```

**XSS in Different Contexts:**

Parameter injection points:

```bash
# Search parameters
grep "search=" /var/log/apache2/access.log | grep -iE "(<|%3C|script)"

# Redirect/callback URLs
grep -E "(redirect|callback|return|url)=" /var/log/apache2/access.log | grep -iE "(javascript:|data:)"

# User input fields
grep -E "(name|comment|message|title)=" /var/log/apache2/access.log | grep -iE "(<script|onerror)"
```

**Stored XSS Identification:**

[Inference] Requires application log correlation:

1. Identify POST requests with XSS payloads
2. Track subsequent GET requests to pages displaying stored content
3. Correlate session IDs or user identifiers

```bash
# Extract POST data containing script tags (if logged)
awk '/POST.*(<script|%3Cscript)/ {print $0}' /var/log/apache2/access.log
```

**DOM-Based XSS Indicators:**

[Unverified] DOM XSS typically leaves minimal server-side traces, but URL patterns may reveal:

- Hash fragments containing JavaScript: `#<script>`
- Client-side routing with injection: `#!/page?param=<payload>`

---

## Command Injection Traces

### Detection Methodology

**Shell Metacharacter Analysis:**

```bash
# Detect command separators
grep -E "(%7C|\||%3B|;|%26|&|%0A|\n|%0D)" /var/log/apache2/access.log

# Backticks and command substitution
grep -E "(%60|`|\$\()" /var/log/apache2/access.log

# Input redirection
grep -E "(%3C|<|%3E|>)" /var/log/apache2/access.log
```

**Common Command Injection Patterns:**

```bash
# System command execution
grep -iE "(whoami|id|uname|cat\%20|ls\%20)" /var/log/apache2/access.log

# Network reconnaissance
grep -iE "(ping|nslookup|dig|wget|curl)" /var/log/apache2/access.log

# File operations
grep -iE "(cat|head|tail|more|less).*(/etc/passwd|/etc/shadow)" /var/log/apache2/access.log

# Reverse shells
grep -iE "(nc|netcat|bash.*-i|/bin/sh|/bin/bash)" /var/log/apache2/access.log
```

**Encoded Command Injection:**

URL encoding patterns:

```bash
# Detect encoded shell commands
grep -E "(%2F|%20)(bin|usr|etc|var)" /var/log/apache2/access.log | \
  python3 -c "import sys; from urllib.parse import unquote; print(unquote(sys.stdin.read()))"

# Base64-encoded payloads (common in command injection)
grep -E "base64|echo.*\|.*base64" /var/log/apache2/access.log
```

**Application-Specific Injection Points:**

Common vulnerable parameters:

- File operations: `file=`, `path=`, `document=`
- System utilities: `ping=`, `host=`, `ip=`
- Administrative functions: `backup=`, `restore=`, `export=`

```bash
# Analyze parameters in requests with shell metacharacters
grep -E "(%7C|\||%3B|;)" /var/log/apache2/access.log | \
  grep -oP '\?[^"]*' | tr '&' '\n' | sort | uniq -c | sort -nr
```

**Post-Exploitation Traces:**

[Inference] Successful command injection may generate:

```bash
# Check system logs for suspicious command execution
grep -E "(www-data|apache|nginx)" /var/log/syslog | grep -iE "(bash|sh|nc|wget|curl)"

# Process execution logs (if auditd enabled)
ausearch -m execve -ts recent | grep -E "(www-data|apache)"

# Network connections from web server process
netstat -antp | grep -E "(apache|nginx|www-data)"
```

**Tool-Specific Signatures:**

Commix (Command Injection Exploitation Tool):

```bash
grep -i "commix" /var/log/apache2/access.log
```

Manual exploitation patterns:

- Time-based detection: `sleep 5`, `ping -c 10 127.0.0.1`
- Output redirection: `> /tmp/output`, `>> /var/www/shell.php`
- Chained commands: `id; uname -a; cat /etc/passwd`

---

## Consolidated Analysis Approach

### Automated Pattern Detection Script

```bash
#!/bin/bash
# attack_pattern_detector.sh
# Usage: ./attack_pattern_detector.sh <logfile>

LOGFILE=$1

echo "[*] Analyzing: $LOGFILE"
echo ""

echo "[+] Brute Force Detection:"
grep -E "(Failed password|401|403)" "$LOGFILE" | awk '{print $1}' | sort | uniq -c | sort -nr | head -10

echo ""
echo "[+] SQL Injection Patterns:"
grep -iE "(union.*select|'.*or.*'|sleep\(|benchmark\()" "$LOGFILE" | wc -l

echo ""
echo "[+] XSS Attempts:"
grep -iE "(<script|onerror=|javascript:)" "$LOGFILE" | wc -l

echo ""
echo "[+] Command Injection:"
grep -E "(\||;|%7C|%3B).*(/bin/|whoami|id)" "$LOGFILE" | wc -l
```

### Timeline Reconstruction

```bash
# Create attack timeline
awk '{print $4, $1, $7}' /var/log/apache2/access.log | \
  grep -E "(<script|union|%7C|Failed)" | \
  sort -k1
```

### Correlation Analysis

[Inference] Combine multiple log sources:

```bash
# Cross-reference web logs with authentication logs
awk '{print $1}' /var/log/apache2/access.log | sort -u > web_ips.txt
awk '{print $(NF-3)}' /var/log/auth.log | sort -u > auth_ips.txt
comm -12 web_ips.txt auth_ips.txt
```

---

## Important CTF Considerations

**Challenge-Specific Patterns:**

- CTF logs often contain intentional patterns for players to discover
- Look for anomalies in timestamp sequences (gaps may indicate removed legitimate traffic)
- Hidden flags may be embedded in User-Agent strings, POST data, or encoded parameters

**Recommended Analysis Tools:**

- `grep`, `awk`, `sed` - basic log parsing
- `jq` - JSON log parsing
- `logstash`, `splunk` (if available) - advanced correlation
- Custom Python/Bash scripts for pattern matching

**Related Topics:**

- Web Application Firewall (WAF) evasion techniques
- Log poisoning and anti-forensics
- Network packet analysis (Wireshark/tcpdump)
- Application-layer protocol analysis (HTTP/HTTPS inspection)

---

## Path Traversal Attempts

### Detection Mechanisms

**Classic Path Traversal Patterns:**

```bash
# Directory traversal sequences
grep -E "(\.\./|\.\.\\|%2e%2e%2f|%2e%2e/|\.\.%2f)" /var/log/apache2/access.log

# URL-encoded traversal
grep -E "(%2e%2e%2f|%2e%2e/|%252e%252e%252f)" /var/log/apache2/access.log

# Double-encoded traversal
grep -E "%252e%252e%252f" /var/log/apache2/access.log

# Unicode/UTF-8 encoded traversal
grep -E "(%c0%ae|%e0%80%ae)" /var/log/apache2/access.log
```

**Platform-Specific Patterns:**

Windows-specific:

```bash
# Backslash traversal
grep -E "(\.\.\\\\|%5c|%255c)" /var/log/apache2/access.log

# Windows absolute paths
grep -E "(C:|%43%3a|D:|%44%3a)" /var/log/apache2/access.log

# UNC path attempts
grep -E "(\\\\\\\\|%5c%5c)" /var/log/apache2/access.log
```

Linux-specific:

```bash
# Absolute path references
grep -E "(/etc/|/proc/|/sys/|/root/|/home/)" /var/log/apache2/access.log

# Null byte injection (legacy PHP vulnerability)
grep -E "%00" /var/log/apache2/access.log
```

**Target File Patterns:**

Sensitive file access attempts:

```bash
# Password files
grep -iE "(passwd|shadow|\.htpasswd)" /var/log/apache2/access.log

# Configuration files
grep -iE "(config|\.conf|\.cfg|\.ini|\.xml)" /var/log/apache2/access.log

# Application files
grep -iE "(\.php|\.asp|\.jsp|\.py|web\.config)" /var/log/apache2/access.log

# SSH keys
grep -iE "(id_rsa|id_dsa|authorized_keys|known_hosts)" /var/log/apache2/access.log

# Database files
grep -iE "(\.db|\.sqlite|\.mdb)" /var/log/apache2/access.log
```

**Common Target Files by Platform:**

Linux:

```bash
# Extract attempts to access common Linux sensitive files
grep -E "/etc/(passwd|shadow|hosts|hostname|issue|group|sudoers)" /var/log/apache2/access.log

# Log files
grep -E "/var/log/(auth\.log|syslog|apache2|nginx)" /var/log/apache2/access.log

# Proc filesystem
grep -E "/proc/(self|[0-9]+)/(cmdline|environ|maps|status)" /var/log/apache2/access.log
```

Windows:

```bash
# Windows system files
grep -iE "(boot\.ini|win\.ini|system\.ini)" /var/log/apache2/access.log

# SAM database
grep -iE "(sam|system|security)$" /var/log/apache2/access.log

# IIS configuration
grep -iE "web\.config" /var/log/apache2/access.log
```

**Bypass Technique Detection:**

```bash
# Redundant traversal sequences
grep -E "(\.\.//|\.\.\\\\\\\\)" /var/log/apache2/access.log

# Mixed encoding
grep -E "(%2e\.\.|\.%2e\.|\.\.%2f)" /var/log/apache2/access.log

# Overlong UTF-8 encoding
grep -E "%c0%af" /var/log/apache2/access.log

# Case variation attempts
grep -iE "(\.\./|\.\.\\)" /var/log/apache2/access.log | grep -v "\.\.\/"
```

**Vulnerable Parameter Analysis:**

Common parameter names:

```bash
# File-related parameters
grep -E "(file=|path=|document=|page=|folder=|root=|pg=)" /var/log/apache2/access.log | \
  grep -E "(\.\./|%2e%2e)"

# Template/view parameters
grep -E "(template=|view=|layout=|style=)" /var/log/apache2/access.log | \
  grep -E "(\.\./|%2e%2e)"

# Download/include parameters
grep -E "(download=|inc=|include=|load=)" /var/log/apache2/access.log | \
  grep -E "(\.\./|%2e%2e)"
```

**Depth Analysis:**

[Inference] Count traversal depth to identify brute-force enumeration:

```bash
# Count traversal sequences per request
grep -E "\.\." /var/log/apache2/access.log | \
  awk '{n=gsub(/\.\./,""); print n, $0}' | sort -rn | head -20

# Extract maximum traversal depth by IP
grep -E "\.\." /var/log/apache2/access.log | \
  awk '{ip=$1; n=gsub(/\.\./,""); if(n>max[ip]) max[ip]=n} END {for(i in max) print max[i], i}' | \
  sort -rn
```

**Success Indicators:**

[Inference] Successful traversal attempts may show:

- HTTP 200 status codes with unusual file parameters
- Response size anomalies (larger than typical responses)
- Time-to-response variations

```bash
# Identify successful traversal attempts (200 status)
grep -E "\.\." /var/log/apache2/access.log | awk '($9 == 200) {print $0}'

# Analyze response sizes for traversal attempts
grep -E "\.\." /var/log/apache2/access.log | awk '{print $1, $7, $9, $10}' | sort -k4 -rn
```

---

## File Inclusion Patterns

### Local File Inclusion (LFI)

**Basic LFI Detection:**

```bash
# Standard LFI patterns with common wrappers
grep -iE "(file://|php://|data://|zip://)" /var/log/apache2/access.log

# Include/require parameter usage
grep -E "(include=|require=|inc=|load=|page=)" /var/log/apache2/access.log

# Direct file references in parameters
grep -E "(\.php|\.inc|\.html|\.txt)" /var/log/apache2/access.log | \
  grep -E "(file=|page=|include=)"
```

**PHP Wrapper Exploitation:**

```bash
# php://filter wrapper (for file disclosure)
grep -i "php://filter" /var/log/apache2/access.log

# Base64 encoding via filter
grep -iE "convert\.base64-encode|convert\.base64-decode" /var/log/apache2/access.log

# php://input wrapper (for code execution)
grep -i "php://input" /var/log/apache2/access.log

# data:// wrapper
grep -iE "data://(text|image)" /var/log/apache2/access.log

# expect:// wrapper
grep -i "expect://" /var/log/apache2/access.log

# zip:// and phar:// wrappers
grep -iE "(zip://|phar://)" /var/log/apache2/access.log
```

**Filter Chain Exploitation:**

```bash
# Detect filter chaining (advanced PHP filter usage)
grep -E "php://filter/[^/]+/[^/]+/" /var/log/apache2/access.log

# Multiple convert filters
grep -E "convert\.[^/]+/convert\." /var/log/apache2/access.log
```

**Log Poisoning Attempts:**

[Inference] Attackers may poison logs to include them as executable code:

```bash
# Detect PHP code in User-Agent
grep -E "User-Agent.*(<\?php|<?=)" /var/log/apache2/access.log

# Detect PHP code in Referer
grep -E "Referer.*(<\?php|<?=)" /var/log/apache2/access.log

# Subsequent access to log files
grep -E "(access\.log|error\.log|auth\.log)" /var/log/apache2/access.log
```

**Session File Inclusion:**

```bash
# Access to session files
grep -iE "/tmp/sess_|/var/lib/php/session" /var/log/apache2/access.log

# PHPSESSID correlation with file inclusion
awk -F'"' '/Cookie.*PHPSESSID/ {print $2}' /var/log/apache2/access.log | \
  grep -E "(include=|file=)"
```

**Null Byte Injection (Legacy):**

```bash
# Null byte to bypass extension checks
grep -E "%00" /var/log/apache2/access.log | grep -E "(include=|file=|page=)"

# Combined with traversal
grep -E "%00.*\.\." /var/log/apache2/access.log
```

### Remote File Inclusion (RFI)

**External URL Detection:**

```bash
# HTTP/HTTPS URLs in parameters
grep -E "(http://|https://|%68%74%74%70)" /var/log/apache2/access.log | \
  grep -E "(include=|require=|file=|page=)"

# FTP URLs
grep -E "(ftp://|%66%74%70)" /var/log/apache2/access.log

# External domain references
grep -E "=(http|https|ftp)://(www\.|[a-z0-9-]+\.)" /var/log/apache2/access.log
```

**RFI Tool Signatures:**

```bash
# Common RFI testing strings
grep -iE "(evil\.txt|shell\.txt|cmd\.txt|backdoor)" /var/log/apache2/access.log

# Attacker-controlled domains
grep -E "=(http|https)://[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}" /var/log/apache2/access.log

# Pastebin/Github raw content
grep -iE "(pastebin\.com|raw\.githubusercontent\.com)" /var/log/apache2/access.log
```

**Encoded URL Detection:**

```bash
# URL-encoded HTTP
grep -E "%68%74%74%70" /var/log/apache2/access.log

# Double-encoded URLs
grep -E "%2568%2574%2574%2570" /var/log/apache2/access.log

# Mixed encoding
grep -E "(%68ttp|h%74tp|ht%74p)" /var/log/apache2/access.log
```

**Bypass Technique Patterns:**

```bash
# Question mark bypass (for extension checks)
grep -E "(include=.*http.*\?)" /var/log/apache2/access.log

# Hash fragment bypass
grep -E "(include=.*http.*#)" /var/log/apache2/access.log

# Slash bypass
grep -E "(include=.*http.*/\.)" /var/log/apache2/access.log
```

### Combined LFI/RFI Analysis

**Correlation Script:**

```bash
#!/bin/bash
# file_inclusion_detector.sh

LOGFILE=$1

echo "[+] Local File Inclusion Attempts:"
grep -iE "(php://|file://|\.\./).*\.(php|inc|txt|log)" "$LOGFILE" | wc -l

echo ""
echo "[+] Remote File Inclusion Attempts:"
grep -E "(include|require|file)=http" "$LOGFILE" | wc -l

echo ""
echo "[+] Wrapper Usage:"
echo "  php://filter: $(grep -i 'php://filter' "$LOGFILE" | wc -l)"
echo "  php://input: $(grep -i 'php://input' "$LOGFILE" | wc -l)"
echo "  data://: $(grep -i 'data://' "$LOGFILE" | wc -l)"

echo ""
echo "[+] Log Poisoning Attempts:"
grep -E "<?php|<?=" "$LOGFILE" | wc -l

echo ""
echo "[+] Top Targeted Files:"
grep -oE "(etc/passwd|etc/shadow|\.\./).*$" "$LOGFILE" | sort | uniq -c | sort -rn | head -10
```

**Success Indicators:**

```bash
# HTTP 200 responses with inclusion parameters
grep -E "(include=|file=|page=)" /var/log/apache2/access.log | awk '($9 == 200)'

# Large response sizes (may indicate file disclosure)
grep -E "(include=|file=)" /var/log/apache2/access.log | awk '($10 > 5000) {print $1, $7, $9, $10}'

# POST requests to included files (potential webshell)
awk '($6 == "POST") && /(include=|file=)/' /var/log/apache2/access.log
```

---

## SSRF Indicators

### Server-Side Request Forgery Detection

**Basic SSRF Patterns:**

```bash
# Internal IP addresses in parameters
grep -E "(url=|uri=|target=|redirect=|proxy=)" /var/log/apache2/access.log | \
  grep -E "(127\.0\.0\.|192\.168\.|10\.|172\.(1[6-9]|2[0-9]|3[01])\.)"

# Localhost references
grep -iE "(localhost|127\.0\.0\.1|0\.0\.0\.0)" /var/log/apache2/access.log | \
  grep -E "(url=|target=|redirect=)"

# IPv6 localhost
grep -E "(::1|0000:0000:0000:0000:0000:0000:0000:0001)" /var/log/apache2/access.log
```

**URL Parameter Analysis:**

Common vulnerable parameters:

```bash
# Standard SSRF-prone parameters
grep -E "(url=|uri=|path=|dest=|redirect=|target=|rurl=|link=)" /var/log/apache2/access.log

# API/webhook parameters
grep -E "(webhook=|callback=|api=|endpoint=)" /var/log/apache2/access.log

# File/resource fetch parameters
grep -E "(fetch=|load=|download=|source=|src=)" /var/log/apache2/access.log

# Proxy parameters
grep -E "(proxy=|http_proxy=|request=)" /var/log/apache2/access.log
```

**Protocol Exploitation:**

```bash
# File protocol access (local file read via SSRF)
grep -iE "(url=|target=).*file://" /var/log/apache2/access.log

# Gopher protocol (advanced SSRF)
grep -iE "gopher://" /var/log/apache2/access.log

# Dict protocol
grep -iE "dict://" /var/log/apache2/access.log

# LDAP protocol
grep -iE "ldap://" /var/log/apache2/access.log

# FTP protocol
grep -E "(url=|target=).*ftp://" /var/log/apache2/access.log
```

**Cloud Metadata Service Targeting:**

```bash
# AWS metadata service
grep -E "169\.254\.169\.254" /var/log/apache2/access.log

# Azure metadata service
grep -E "169\.254\.169\.254.*Metadata:true" /var/log/apache2/access.log

# GCP metadata service
grep -E "metadata\.google\.internal" /var/log/apache2/access.log

# OpenStack metadata
grep -E "169\.254\.169\.254.*openstack" /var/log/apache2/access.log
```

**Internal Service Scanning:**

[Inference] Attackers may use SSRF to scan internal networks:

```bash
# Port scanning patterns (sequential port numbers)
grep -E "192\.168\.[0-9]{1,3}\.[0-9]{1,3}:[0-9]+" /var/log/apache2/access.log | \
  awk -F: '{print $NF}' | sort -n

# Common internal service ports
grep -E "(url=|target=).*:(22|23|25|80|443|3306|5432|6379|8080|9200)" /var/log/apache2/access.log

# Redis targeting
grep -E ":(6379|redis)" /var/log/apache2/access.log
```

**Encoded SSRF Payloads:**

```bash
# URL-encoded internal IPs
grep -E "%31%32%37%2e%30%2e%30%2e%31" /var/log/apache2/access.log  # 127.0.0.1

# Hex encoding
grep -E "0x7f000001" /var/log/apache2/access.log  # 127.0.0.1 in hex

# Decimal encoding
grep -E "2130706433" /var/log/apache2/access.log  # 127.0.0.1 in decimal

# Octal encoding
grep -E "0177\.0000\.0000\.0001" /var/log/apache2/access.log
```

**DNS Rebinding Attempts:**

```bash
# Suspicious domain patterns
grep -E "(url=|target=).*\.(burpcollaborator|ngrok|requestbin|webhook\.site)" /var/log/apache2/access.log

# Short-lived domains
grep -E "url=.*://[a-z0-9]{8,16}\.(com|net|xyz)" /var/log/apache2/access.log
```

**Bypass Techniques:**

```bash
# @ symbol bypass
grep -E "http://.*@(127\.0\.0\.1|localhost)" /var/log/apache2/access.log

# Backslash bypass
grep -E "http://127\.0\.0\.1\\\\@" /var/log/apache2/access.log

# Redirect chains
grep -E "url=http://.*redirect.*http://" /var/log/apache2/access.log

# IP address obfuscation (missing dots)
grep -E "http://1270001" /var/log/apache2/access.log

# Square bracket bypass (IPv6 format)
grep -E "\[127\.0\.0\.1\]" /var/log/apache2/access.log
```

**SSRF via Image Processing:**

```bash
# Image URL parameters
grep -E "(image=|img=|avatar=|thumbnail=)" /var/log/apache2/access.log | \
  grep -E "(127\.0\.0\.1|192\.168\.|file://)"

# SVG file uploads with XXE (related to SSRF)
grep -iE "\.svg" /var/log/apache2/access.log
```

**Cloud-Specific SSRF Patterns:**

AWS-specific:

```bash
# IMDSv1 (vulnerable)
grep -E "169\.254\.169\.254/latest/meta-data" /var/log/apache2/access.log

# IMDSv2 token attempts
grep -E "169\.254\.169\.254/latest/api/token" /var/log/apache2/access.log

# AWS credentials targeting
grep -E "meta-data/(iam|identity-credentials)" /var/log/apache2/access.log
```

Azure-specific:

```bash
# Azure Instance Metadata Service
grep -E "169\.254\.169\.254.*metadata/instance" /var/log/apache2/access.log

# Azure managed identity endpoint
grep -E "169\.254\.169\.254.*metadata/identity" /var/log/apache2/access.log
```

GCP-specific:

```bash
# GCP metadata endpoints
grep -E "metadata\.google\.internal/computeMetadata" /var/log/apache2/access.log

# Service account tokens
grep -E "metadata/v1/instance/service-accounts" /var/log/apache2/access.log
```

### SSRF Response Analysis

**Timing-Based Detection:**

[Inference] Response time variations may indicate successful internal requests:

```bash
# Extract response times (requires custom log format with %D or %T)
grep -E "(url=|target=)" /var/log/apache2/access.log | \
  awk '{print $NF, $7}' | sort -rn | head -20
```

**Application-Level SSRF Detection:**

Check application logs for:

```bash
# Outbound connection attempts
grep -iE "(curl|wget|http_request|file_get_contents)" /var/log/application.log

# DNS resolution of internal addresses
grep -E "(gethostbyname|dns_get_record)" /var/log/application.log | \
  grep -E "(127\.0\.0\.1|192\.168\.|10\.)"
```

**Network-Level Correlation:**

[Unverified] If network logs available:

```bash
# Correlate web server outbound connections
netstat -antp | grep -E "(apache|nginx|www-data)" | grep ESTABLISHED

# Check firewall logs for blocked internal requests
grep -E "DPT=(22|3306|6379|9200)" /var/log/ufw.log
```

---

## Consolidated Detection Strategy

### Multi-Vector Analysis Script

```bash
#!/bin/bash
# advanced_attack_detector.sh
# Detects path traversal, file inclusion, and SSRF patterns

LOGFILE=$1

echo "=== Advanced Attack Pattern Analysis ==="
echo "Analyzing: $LOGFILE"
echo ""

echo "[+] Path Traversal Summary:"
PT_COUNT=$(grep -E "(\.\./|%2e%2e)" "$LOGFILE" | wc -l)
echo "  Total attempts: $PT_COUNT"
echo "  Successful (200): $(grep -E '(\.\./|%2e%2e)' "$LOGFILE" | awk '($9 == 200)' | wc -l)"
echo "  Unique IPs: $(grep -E '(\.\./|%2e%2e)' "$LOGFILE" | awk '{print $1}' | sort -u | wc -l)"

echo ""
echo "[+] File Inclusion Summary:"
LFI_COUNT=$(grep -iE "(php://|file://)" "$LOGFILE" | wc -l)
RFI_COUNT=$(grep -E "(include|file)=http" "$LOGFILE" | wc -l)
echo "  LFI attempts: $LFI_COUNT"
echo "  RFI attempts: $RFI_COUNT"
echo "  Wrapper usage: $(grep -iE "(php://filter|data://)" "$LOGFILE" | wc -l)"

echo ""
echo "[+] SSRF Indicators:"
SSRF_COUNT=$(grep -E "(url=|target=).*(127\.0\.0\.1|192\.168\.|169\.254\.169\.254)" "$LOGFILE" | wc -l)
echo "  Internal IP targeting: $SSRF_COUNT"
echo "  Cloud metadata: $(grep -E '169\.254\.169\.254' "$LOGFILE" | wc -l)"
echo "  Alternative protocols: $(grep -iE '(gopher|dict|ldap)://' "$LOGFILE" | wc -l)"

echo ""
echo "[+] Top Attacking IPs:"
grep -E "(\.\./|php://|url=.*127\.0\.0\.1)" "$LOGFILE" | \
  awk '{print $1}' | sort | uniq -c | sort -rn | head -5

echo ""
echo "[+] Most Targeted Parameters:"
grep -oE "(\?|&)[a-z_]+(=|%3d)" "$LOGFILE" | \
  grep -E "(file|include|url|path|page|target)" | \
  sort | uniq -c | sort -rn | head -5
```

### Timeline Correlation

```bash
# Create comprehensive attack timeline
cat /var/log/apache2/access.log | \
  grep -E "(\.\./|php://|url=.*127|union.*select|<script)" | \
  awk '{print $4, $1, $7, $9}' | \
  sed 's/\[//g' | sort
```

### Parameter Extraction Tool

```bash
# Extract and decode suspicious parameters
grep -E "(\.\./|php://|url=.*127)" /var/log/apache2/access.log | \
  grep -oP '\?[^"]*' | \
  python3 -c "
import sys
from urllib.parse import unquote, parse_qs

for line in sys.stdin:
    params = parse_qs(line.strip('?'))
    for key, values in params.items():
        for value in values:
            if any(x in value.lower() for x in ['../', 'php://', '127.0', 'http://']):
                print(f'{key}={unquote(value)}')
"
```

---

## CTF-Specific Considerations

**Hidden Flags in Traversal Paths:** [Inference] CTF challenges may embed flags in:

- Unusual file paths discovered through traversal
- Base64-encoded responses from php://filter
- Cloud metadata responses in SSRF challenges

**Combined Exploitation Chains:**

- Path traversal  LFI  RCE (via log poisoning)
- SSRF  Internal API  Credential disclosure
- File inclusion  Session file  Authentication bypass

**Recommended Analysis Tools:**

- `grep`/`awk`/`sed` - Pattern matching and extraction
- `python`/`perl` - Complex decoding and parsing
- `burp suite` - Manual request analysis and replay
- `ffuf`/`gobuster` - Parameter fuzzing for discovery
- `ssrf-sheriff`, `gopherus` - SSRF exploitation tools

**Related Topics for Further Study:**

- XXE (XML External Entity) attacks - often combined with SSRF
- Template injection - similar to file inclusion
- Deserialization vulnerabilities - can lead to RCE via file operations
- CRLF injection - can facilitate log poisoning
- AWS/Azure/GCP security misconfigurations

---

# Network Traffic Logs

## tcpdump Log Analysis

tcpdump is a command-line packet analyzer that captures network traffic. In CTF scenarios, tcpdump logs may be provided as text output or PCAP files. Understanding tcpdump syntax and output formats is essential for rapid packet analysis.

### tcpdump Output Formats

**Standard text output format:**

```
timestamp protocol src > dst: flags data-seqno ack window urgent options
```

Example:

```
15:42:33.123456 IP 192.168.1.100.54321 > 10.0.0.5.80: Flags [S], seq 123456789, win 65535, options [mss 1460]
```

**Components:**

- `15:42:33.123456` - Timestamp (hours:minutes:seconds.microseconds)
- `IP` - Protocol (IP, IP6, ARP, etc.)
- `192.168.1.100.54321` - Source IP and port
- `10.0.0.5.80` - Destination IP and port
- `Flags [S]` - TCP flags (see below)
- `seq 123456789` - Sequence number
- `win 65535` - Window size
- `options [mss 1460]` - TCP options

### TCP Flags in tcpdump

- `[S]` - SYN (connection initiation)
- `[.]` - ACK (acknowledgment)
- `[S.]` - SYN-ACK (connection response)
- `[P]` - PSH (push data)
- `[F]` - FIN (connection termination)
- `[R]` - RST (reset connection)
- `[P.]` - PSH-ACK (push with acknowledgment)
- `[F.]` - FIN-ACK (graceful close)
- `[R.]` - RST-ACK (abrupt close)

### Basic tcpdump Analysis Commands

**Reading tcpdump text output:**

```bash
# Basic filtering by IP
grep "192.168.1.100" tcpdump.txt

# Find specific port traffic
grep "\.80:" tcpdump.txt
grep "\.443:" tcpdump.txt

# Extract unique source IPs
grep -oE '([0-9]{1,3}\.){3}[0-9]{1,3}\.[0-9]+' tcpdump.txt | cut -d'.' -f1-4 | sort -u

# Count packets per IP
grep -oE '([0-9]{1,3}\.){3}[0-9]{1,3}' tcpdump.txt | sort | uniq -c | sort -rn

# Find SYN packets (port scans)
grep "Flags \[S\]" tcpdump.txt

# Find SYN-ACK packets (open ports)
grep "Flags \[S\.\]" tcpdump.txt

# Find RST packets (closed ports or connection resets)
grep "Flags \[R\]" tcpdump.txt

# Extract HTTP requests from text dump
grep "GET\|POST\|PUT\|DELETE" tcpdump.txt

# Find DNS queries
grep "A?" tcpdump.txt

# Identify failed connection attempts
grep "Flags \[S\]" tcpdump.txt | cut -d' ' -f3,5 | sort | uniq -c | sort -rn
```

### Protocol-Specific Analysis

**HTTP traffic extraction:**

```bash
# Find HTTP Host headers
grep -i "host:" tcpdump.txt

# Extract User-Agent strings
grep -i "user-agent:" tcpdump.txt | cut -d':' -f3- | sort -u

# Find HTTP response codes
grep "HTTP/1\.[01]" tcpdump.txt | grep -oE "HTTP/1\.[01] [0-9]{3}"

# Identify suspicious HTTP methods
grep -E "TRACE|CONNECT|OPTIONS" tcpdump.txt
```

**DNS traffic analysis:**

```bash
# Extract DNS queries
grep "A?" tcpdump.txt | grep -oE '[a-zA-Z0-9.-]+\?' | tr -d '?'

# Find DNS responses
grep "A " tcpdump.txt | grep -v "A?"

# Identify DNS tunneling (long subdomain names)
grep "A?" tcpdump.txt | awk '{print length, $0}' | sort -rn | head -20

# Find TXT record queries (often used for exfiltration)
grep "TXT?" tcpdump.txt
```

**FTP traffic analysis:**

```bash
# Find FTP commands
grep -E "USER|PASS|RETR|STOR|LIST|PWD|CWD" tcpdump.txt

# Extract FTP credentials
grep "USER\|PASS" tcpdump.txt
```

### Time-Based Analysis

```bash
# Extract timestamps and sort
grep -oE '^[0-9]{2}:[0-9]{2}:[0-9]{2}\.[0-9]+' tcpdump.txt | sort -u

# Find traffic spikes (count packets per second)
awk '{print $1}' tcpdump.txt | cut -d'.' -f1 | uniq -c

# Identify gaps in traffic (potential evasion)
awk '{print $1}' tcpdump.txt | cut -d'.' -f1 | sort -u | awk '{if(NR>1 && $1-prev>60) print "Gap between", prev, "and", $1; prev=$1}'
```

### Port Analysis

```bash
# Extract destination ports
grep -oE '\.([0-9]+):' tcpdump.txt | cut -d'.' -f2 | cut -d':' -f1 | sort -n | uniq -c

# Find uncommon high ports (potential backdoors)
grep -oE '\.([0-9]+):' tcpdump.txt | cut -d'.' -f2 | cut -d':' -f1 | awk '$1 > 49152' | sort -n | uniq -c

# Identify port scanning patterns
grep "Flags \[S\]" tcpdump.txt | awk '{print $3, $5}' | sort | uniq -c | awk '$1 > 10'
```

### Connection State Analysis

```bash
# Find three-way handshakes (successful connections)
awk '/Flags \[S\]/ {syn[$3" "$5]++} /Flags \[S\.\]/ {synack[$5" "$3]++} /Flags \[.\]/ && !/Flags \[S\.\]/ {ack[$3" "$5]++}' tcpdump.txt

# Identify half-open connections (SYN without SYN-ACK)
comm -23 <(grep "Flags \[S\]" tcpdump.txt | awk '{print $3, $5}' | sort) <(grep "Flags \[S\.\]" tcpdump.txt | awk '{print $5, $3}' | sort)

# Find long-lived connections (PSH without FIN)
grep "Flags \[P\]" tcpdump.txt | awk '{print $3, $5}' | sort -u > active.txt
grep "Flags \[F\]" tcpdump.txt | awk '{print $3, $5}' | sort -u > closed.txt
comm -23 active.txt closed.txt
```

### Packet Size Analysis

```bash
# Extract packet lengths (if available in output)
grep "length" tcpdump.txt | grep -oE "length [0-9]+" | awk '{print $2}' | sort -n | uniq -c

# Find large packets (potential data exfiltration)
grep "length" tcpdump.txt | awk '{if ($NF > 1000) print}'

# Identify fixed-size packets (potential covert channel)
grep "length" tcpdump.txt | grep -oE "length [0-9]+" | awk '{print $2}' | sort | uniq -c | sort -rn
```

### ICMP Analysis

```bash
# Find ICMP echo requests (ping)
grep "ICMP echo request" tcpdump.txt

# Find ICMP echo replies
grep "ICMP echo reply" tcpdump.txt

# Identify ICMP tunneling (large ICMP packets)
grep "ICMP" tcpdump.txt | grep "length" | awk '{print $NF}' | sort -rn | head

# Find ICMP types
grep "ICMP" tcpdump.txt | grep -oE "ICMP [a-z ]+" | sort | uniq -c
```

### Advanced Pattern Detection

**Data exfiltration indicators:**

```bash
# Outbound connections with large data volumes
grep "length" tcpdump.txt | awk '{if($5 ~ /^[0-9.]+$/ && $NF > 500) print $3, $5, $NF}' | sort | uniq -c

# Identify beaconing (regular connection intervals)
grep "Flags \[S\]" tcpdump.txt | awk '{print $1, $3, $5}' | sort -k2,3 | awk '{diff=$1-prev; if(diff>1 && diff<5) count++; prev=$1} END {print count}'

# Find repeated identical payloads (C2 communication)
grep "length" tcpdump.txt | awk '{print $NF}' | sort | uniq -c | awk '$1 > 10'
```

**Scanning detection:**

```bash
# Identify horizontal scans (one source, many destinations)
grep "Flags \[S\]" tcpdump.txt | awk '{print $3, $5}' | cut -d'.' -f1-4 | sort | uniq -c | awk '$1 > 20'

# Vertical scans (one destination, many ports)
grep "Flags \[S\]" tcpdump.txt | awk '{print $5}' | cut -d':' -f1 | sort | uniq -c | awk '$1 > 20'

# Stealth scans (unusual flag combinations)
grep -E "Flags \[F\]|Flags \[FPU\]|Flags \[none\]" tcpdump.txt
```

### CTF-Specific tcpdump Analysis

**Flag extraction techniques:**

```bash
# Search for common flag formats
grep -oE 'flag\{[^}]+\}' tcpdump.txt
grep -oE 'CTF\{[^}]+\}' tcpdump.txt
grep -oE '[A-Za-z0-9+/]{20,}={0,2}' tcpdump.txt | while read line; do echo "$line" | base64 -d 2>/dev/null; done

# Extract ASCII strings from hex dumps
grep -oE '0x[0-9a-f]{4}:' tcpdump.txt | sed 's/0x//g' | xxd -r -p

# Find hidden data in TCP sequence numbers
grep "seq" tcpdump.txt | grep -oE "seq [0-9]+" | awk '{print $2}' | while read seq; do printf "%08x\n" $seq; done | xxd -r -p
```

**Protocol anomalies:**

```bash
# Identify non-standard ports for known protocols
grep "\.80:" tcpdump.txt | grep -v "HTTP"
grep "\.443:" tcpdump.txt | grep -v "TLS\|SSL"

# Find packets with unusual TTL values
grep "ttl" tcpdump.txt | grep -oE "ttl [0-9]+" | sort | uniq -c

# Detect IP fragmentation (evasion technique)
grep "frag" tcpdump.txt
```

### Converting tcpdump Text to Structured Data

```bash
# Parse into CSV format
awk '{print $1","$3","$5","$7}' tcpdump.txt > packets.csv

# Create connection summary
awk '{print $3" -> "$5}' tcpdump.txt | sort | uniq -c | sort -rn > connections.txt

# Generate timeline
awk '{print $1, $3, $5}' tcpdump.txt | while read time src dst; do
    echo "$time: Connection from $src to $dst"
done
```

## Wireshark Capture Files

Wireshark is the standard tool for analyzing PCAP (Packet Capture) files. In CTF scenarios, you'll typically work with `.pcap`, `.pcapng`, or `.cap` files containing captured network traffic.

### Opening and Basic Navigation

**Command-line usage:**

```bash
# Open capture file in GUI
wireshark capture.pcap &

# Command-line analysis with tshark
tshark -r capture.pcap

# Display specific number of packets
tshark -r capture.pcap -c 100

# Read multiple capture files
mergecap -w merged.pcap capture1.pcap capture2.pcap capture3.pcap
tshark -r merged.pcap
```

### Display Filters

Wireshark display filters are applied after capture. Syntax differs from capture filters (BPF).

**Basic filter syntax:**

```bash
# IP address filters
ip.addr == 192.168.1.100          # Either source or destination
ip.src == 192.168.1.100           # Source only
ip.dst == 192.168.1.100           # Destination only

# Port filters
tcp.port == 80                    # Either source or destination
tcp.dstport == 443                # Destination port
udp.srcport == 53                 # Source port

# Protocol filters
http                              # HTTP traffic
dns                               # DNS traffic
tls                               # TLS/SSL traffic
icmp                              # ICMP traffic

# TCP flags
tcp.flags.syn == 1                # SYN packets
tcp.flags.ack == 1                # ACK packets
tcp.flags.reset == 1              # RST packets
tcp.flags == 0x002                # SYN only (no other flags)

# Combining filters
ip.src == 192.168.1.100 && tcp.port == 80
http && ip.addr == 10.0.0.5
dns && !ip.addr == 8.8.8.8        # DNS traffic excluding Google DNS
```

**Advanced filters:**

```bash
# String matching in packets
frame contains "password"
http.request.uri contains "admin"
tcp.payload contains "flag"

# Regex matching
http.host matches ".*evil.*"
dns.qry.name matches ".*\.ru$"

# Length filters
frame.len > 1500                  # Large packets
tcp.len == 0                      # Empty TCP packets (ACKs)
http.content_length > 1000000     # Large HTTP responses

# Time-based filters
frame.time >= "2025-10-28 10:00:00"
frame.time_delta > 60             # Gaps larger than 60 seconds

# Protocol-specific
http.request.method == "POST"
http.response.code == 404
dns.qry.type == 1                 # A records
dns.qry.type == 16                # TXT records
tls.handshake.type == 1           # Client Hello
```

### tshark Command-Line Analysis

tshark is Wireshark's command-line interface, essential for scripting and rapid analysis.

**Basic usage:**

```bash
# Display all packets
tshark -r capture.pcap

# Apply display filter
tshark -r capture.pcap -Y "http"

# Show specific fields
tshark -r capture.pcap -T fields -e ip.src -e ip.dst -e tcp.dstport

# Output as JSON
tshark -r capture.pcap -T json > capture.json

# Output as CSV
tshark -r capture.pcap -T fields -E header=y -E separator=, -e ip.src -e ip.dst > capture.csv

# Count packets by type
tshark -r capture.pcap -q -z io,phs
```

**Protocol-specific extraction:**

```bash
# HTTP analysis
# List HTTP requests
tshark -r capture.pcap -Y "http.request" -T fields -e http.request.method -e http.host -e http.request.uri

# Extract HTTP objects
tshark -r capture.pcap --export-objects http,./http_objects/

# Show HTTP response codes
tshark -r capture.pcap -Y "http.response" -T fields -e http.response.code

# DNS analysis
# Extract DNS queries
tshark -r capture.pcap -Y "dns.qry.type == 1" -T fields -e dns.qry.name

# Find DNS responses with IPs
tshark -r capture.pcap -Y "dns.a" -T fields -e dns.qry.name -e dns.a

# TLS/SSL analysis
# Extract SNI (Server Name Indication)
tshark -r capture.pcap -Y "tls.handshake.extensions_server_name" -T fields -e tls.handshake.extensions_server_name

# Show TLS cipher suites
tshark -r capture.pcap -Y "tls.handshake.type == 2" -T fields -e tls.handshake.ciphersuite

# Extract certificates
tshark -r capture.pcap --export-objects x509,./certificates/
```

**Statistics and summaries:**

```bash
# Protocol hierarchy
tshark -r capture.pcap -q -z io,phs

# Conversation statistics
tshark -r capture.pcap -q -z conv,tcp
tshark -r capture.pcap -q -z conv,udp
tshark -r capture.pcap -q -z conv,ip

# Endpoints
tshark -r capture.pcap -q -z endpoints,ip
tshark -r capture.pcap -q -z endpoints,tcp

# HTTP statistics
tshark -r capture.pcap -q -z http,tree
tshark -r capture.pcap -q -z http_req,tree
tshark -r capture.pcap -q -z http_srv,tree
```

### Stream Following and Reconstruction

**TCP stream reconstruction:**

```bash
# List all TCP streams
tshark -r capture.pcap -T fields -e tcp.stream | sort -n | uniq

# Extract specific TCP stream
tshark -r capture.pcap -q -z follow,tcp,ascii,0 > stream_0.txt

# Extract all TCP streams
for stream in $(tshark -r capture.pcap -T fields -e tcp.stream | sort -un); do
    tshark -r capture.pcap -q -z follow,tcp,raw,$stream | tail -n +7 | xxd -r -p > stream_$stream.bin
done

# HTTP stream extraction
tshark -r capture.pcap -q -z follow,http,ascii,0
```

**UDP stream reconstruction:**

```bash
# Extract UDP stream
tshark -r capture.pcap -q -z follow,udp,ascii,0

# DNS stream reconstruction
tshark -r capture.pcap -Y "udp.port == 53" -q -z follow,udp,ascii,0
```

### File Extraction from PCAP

**Exporting objects:**

```bash
# Export HTTP objects
tshark -r capture.pcap --export-objects http,./http_objects/

# Export SMB objects
tshark -r capture.pcap --export-objects smb,./smb_objects/

# Export DICOM objects (medical imaging)
tshark -r capture.pcap --export-objects dicom,./dicom_objects/

# Export IMF (email) objects
tshark -r capture.pcap --export-objects imf,./email_objects/

# List exportable objects without extracting
tshark -r capture.pcap --export-objects http,/dev/null 2>&1 | grep "Saved"
```

**Manual file carving:**

```bash
# Extract specific file types by signature
# Find packet containing file signature
tshark -r capture.pcap -Y "frame contains 50:4b:03:04" -T fields -e frame.number  # ZIP signature
tshark -r capture.pcap -Y "frame contains ff:d8:ff" -T fields -e frame.number      # JPEG signature
tshark -r capture.pcap -Y "frame contains 89:50:4e:47" -T fields -e frame.number   # PNG signature

# Extract payload from specific packet range
tshark -r capture.pcap -Y "frame.number >= 100 && frame.number <= 200" -T fields -e data | tr -d '\n' | xxd -r -p > extracted_file.bin
```

### Credential Extraction

**FTP credentials:**

```bash
# Extract FTP username/password
tshark -r capture.pcap -Y "ftp.request.command == USER || ftp.request.command == PASS" -T fields -e ftp.request.command -e ftp.request.arg
```

**HTTP Basic Authentication:**

```bash
# Extract Authorization headers
tshark -r capture.pcap -Y "http.authorization" -T fields -e http.authorization

# Decode base64 credentials
tshark -r capture.pcap -Y "http.authorization" -T fields -e http.authorization | cut -d' ' -f2 | base64 -d
```

**HTTP POST form data:**

```bash
# Extract POST data containing credentials
tshark -r capture.pcap -Y "http.request.method == POST" -T fields -e http.file_data | grep -i "password\|username"
```

**Telnet credentials:**

```bash
# Reconstruct telnet session (credentials often typed char-by-char)
tshark -r capture.pcap -q -z follow,tcp,ascii,0 | grep -A 20 "login:"
```

### Wireless Traffic Analysis

**802.11 WiFi frames:**

```bash
# Filter for beacon frames
tshark -r wireless.pcap -Y "wlan.fc.type_subtype == 0x08"

# Extract SSIDs
tshark -r wireless.pcap -Y "wlan.ssid" -T fields -e wlan.ssid | sort -u

# Find deauthentication attacks
tshark -r wireless.pcap -Y "wlan.fc.type_subtype == 0x0c" -T fields -e wlan.sa -e wlan.da

# Identify WPA handshakes
tshark -r wireless.pcap -Y "eapol"
```

**Bluetooth traffic:**

```bash
# Filter Bluetooth packets
tshark -r bluetooth.pcap -Y "bthci_acl || bthci_cmd || bthci_evt"

# Extract Bluetooth device addresses
tshark -r bluetooth.pcap -T fields -e bluetooth.addr | sort -u
```

### Malware Traffic Analysis

**C2 (Command and Control) detection:**

```bash
# Find beaconing behavior (regular intervals)
tshark -r capture.pcap -T fields -e frame.time_epoch -e ip.src -e ip.dst -Y "tcp.flags.syn == 1" | \
awk '{print $1, $2, $3}' | sort -k2,3 | \
awk '{if(NR>1 && $2==prev_src && $3==prev_dst) print $1-prev_time; prev_time=$1; prev_src=$2; prev_dst=$3}'

# Identify suspicious TLS certificates
tshark -r capture.pcap -Y "tls.handshake.certificate" -T fields -e x509sat.printableString

# Find long-duration connections (persistent backdoors)
tshark -r capture.pcap -q -z conv,tcp | awk '{print $8}' | sort -rn
```

**Data exfiltration detection:**

```bash
# Find large outbound transfers
tshark -r capture.pcap -Y "tcp" -T fields -e ip.src -e tcp.len | \
awk '{sum[$1]+=$2} END {for(ip in sum) print ip, sum[ip]}' | sort -k2 -rn

# Detect DNS tunneling
tshark -r capture.pcap -Y "dns" -T fields -e dns.qry.name | \
awk '{print length, $0}' | sort -rn | head -20

# Identify ICMP tunneling
tshark -r capture.pcap -Y "icmp && data.len > 64" -T fields -e ip.src -e ip.dst -e data.len
```

### Protocol Anomaly Detection

```bash
# Non-standard ports
tshark -r capture.pcap -Y "tcp.port != 80 && tcp.port != 443 && tcp.port != 22 && tcp.port != 21" -T fields -e tcp.port | sort | uniq -c | sort -rn

# Malformed packets
tshark -r capture.pcap -Y "tcp.analysis.flags" -T fields -e frame.number -e tcp.analysis.flags

# Unusual packet sizes
tshark -r capture.pcap -T fields -e frame.len | sort -n | uniq -c | awk '$1 == 1' | sort -k2 -rn

# Fragmented traffic (evasion)
tshark -r capture.pcap -Y "ip.flags.mf == 1 || ip.frag_offset > 0"
```

### CTF-Specific Wireshark Techniques

**Flag hunting:**

```bash
# Search for common flag formats
tshark -r capture.pcap -Y 'frame contains "flag{" || frame contains "CTF{" || frame contains "FLAG{"'

# Extract data fields from all protocols
tshark -r capture.pcap -T fields -e data.data | grep -v "^$" | xxd -r -p | strings

# Decode base64 in packets
tshark -r capture.pcap -T fields -e data.data | tr -d '\n:' | xxd -r -p | strings | grep -oE '[A-Za-z0-9+/]{20,}={0,2}' | while read b64; do echo "$b64" | base64 -d 2>/dev/null; done

# Check ICMP payload
tshark -r capture.pcap -Y "icmp" -T fields -e data.data | tr -d '\n:' | xxd -r -p

# Extract images (steganography targets)
tshark -r capture.pcap --export-objects http,./images/
file ./images/*
```

**Hidden channels:**

```bash
# IP ID field covert channel
tshark -r capture.pcap -T fields -e ip.id | while read id; do printf "%04x" $id; done | xxd -r -p

# TCP sequence number covert channel
tshark -r capture.pcap -Y "tcp" -T fields -e tcp.seq | while read seq; do printf "%08x" $seq; done | xxd -r -p

# TTL covert channel
tshark -r capture.pcap -T fields -e ip.ttl | while read ttl; do printf "%c" $ttl; done

# TCP initial window size channel
tshark -r capture.pcap -Y "tcp.flags.syn == 1 && tcp.flags.ack == 0" -T fields -e tcp.window_size
```

**Packet timing analysis:**

```bash
# Extract inter-packet delays (timing covert channel)
tshark -r capture.pcap -T fields -e frame.time_epoch | awk 'NR>1 {print $1-prev} {prev=$1}'

# Find suspiciously regular intervals
tshark -r capture.pcap -T fields -e frame.time_epoch | awk 'NR>1 {print int($1-prev)} {prev=$1}' | sort | uniq -c | sort -rn
```

### Wireshark Profiles and Customization

```bash
# Create custom profile for CTF analysis
mkdir -p ~/.config/wireshark/profiles/CTF

# Custom columns configuration (edit preferences)
# Go to Edit -> Preferences -> Appearance -> Columns

# Custom display filter buttons
# View -> Display Filter Expression
```

### Performance Optimization

```bash
# Split large PCAP files
editcap -c 10000 large.pcap split.pcap  # Split into 10000 packet chunks

# Extract time range
editcap -A "2025-10-28 10:00:00" -B "2025-10-28 11:00:00" capture.pcap filtered.pcap

# Remove duplicate packets
editcap -d capture.pcap dedup.pcap

# Compress PCAP
gzip -9 capture.pcap  # Creates capture.pcap.gz
```

## NetFlow Data

NetFlow is a network protocol developed by Cisco for collecting IP traffic metadata. Unlike full packet captures, NetFlow records only flow summaries (source/destination IPs, ports, byte counts, timestamps) making it efficient for long-term network monitoring and analysis.

### NetFlow Record Structure

**NetFlow v5 record fields:**

- `srcaddr` - Source IP address
- `dstaddr` - Destination IP address
- `srcport` - Source port
- `dstport` - Destination port
- `protocol` - IP protocol number (6=TCP, 17=UDP, 1=ICMP)
- `tos` - Type of Service
- `tcp_flags` - TCP flags
- `packets` - Number of packets in flow
- `bytes` - Number of bytes in flow
- `start` - Flow start timestamp
- `end` - Flow end timestamp
- `src_as` - Source AS number
- `dst_as` - Destination AS number
- `nexthop` - Next hop router IP
- `input` - Input interface index
- `output` - Output interface index

**NetFlow v9/IPFIX (extensible format):**

- Template-based: Supports custom fields
- Variable-length records
- Additional fields: Application ID, MPLS labels, IPv6, etc.

### Tools for NetFlow Analysis

**nfdump - NetFlow collector and analyzer:**

```bash
# Install nfdump on Kali
sudo apt-get install nfdump

# Read NetFlow data file
nfdump -r netflow_data.nfcap

# Read all files in directory
nfdump -R /var/netflow/

# Output formats
nfdump -r netflow.nfcap -o extended  # Extended format
nfdump -r netflow.nfcap -o csv       # CSV output
nfdump -r netflow.nfcap -o json      # JSON output
```

**Basic filtering:**

```bash
# Filter by IP address
nfdump -r netflow.nfcap 'src ip 192.168.1.100'
nfdump -r netflow.nfcap 'dst ip 10.0.0.5'
nfdump -r netflow.nfcap 'ip 192.168.1.100'  # Either src or dst

# Filter by port
nfdump -r netflow.nfcap 'port 80'
nfdump -r netflow.nfcap 'src port 443'
nfdump -r netflow.nfcap 'dst port 22'

# Filter by protocol
nfdump -r netflow.nfcap 'proto tcp'
nfdump -r netflow.nfcap 'proto udp'
nfdump -r netflow.nfcap 'proto icmp'

# Combine filters
nfdump -r netflow.nfcap 'src ip 192.168.1.100 and dst port 80'
nfdump -r netflow.nfcap 'proto tcp and port 443'
nfdump -r netflow.nfcap 'net 192.168.0.0/16 and not port 53'
```

**Time-based filtering:**

```bash
# Filter by time range
nfdump -r netflow.nfcap -t 2025/10/28.10:00:00-2025/10/28.11:00:00

# Flows after specific time
nfdump -r netflow.nfcap -t 2025/10/28.10:00:00+

# Flows before specific time
nfdump -r netflow.nfcap -t -2025/10/28.10:00:00

# Last N hours
nfdump -r netflow.nfcap -t -3h  # Last 3 hours
```

### Statistical Analysis

**Top talkers:**

```bash
# Top source IPs by byte count
nfdump -r netflow.nfcap -s srcip/bytes -n 20

# Top destination IPs by packet count
nfdump -r netflow.nfcap -s dstip/packets -n 20

# Top source ports by flow count 
nfdump -r netflow.nfcap -s srcport/flows -n 20

# Top destination ports
nfdump -r netflow.nfcap -s dstport/bytes -n 20

# Top protocols
nfdump -r netflow.nfcap -s proto/bytes -n 10

# Top TCP flags
nfdump -r netflow.nfcap -s flags/flows -n 10
````

**Communication patterns:**

```bash
# Top source-destination pairs
nfdump -r netflow.nfcap -s srcip/dstip/bytes -n 20

# Top conversations by protocol
nfdump -r netflow.nfcap -s 'srcip/dstip/proto/bytes' -n 20

# Top service pairs (src IP + dst port)
nfdump -r netflow.nfcap -s srcip/dstport/bytes -n 20

# Aggregate by /24 network
nfdump -r netflow.nfcap -A srcip24 -s ip/bytes -n 20
nfdump -r netflow.nfcap -A dstip24 -s ip/bytes -n 20
````

**Flow characteristics:**

```bash
# Long-duration flows (potential persistent connections)
nfdump -r netflow.nfcap -o extended | awk '$11 > 300'  # Duration > 300 seconds

# Large flows (potential data transfers)
nfdump -r netflow.nfcap 'bytes > 10000000'  # > 10MB

# High packet count flows
nfdump -r netflow.nfcap 'packets > 10000'

# Small packet flows (potential C2 beaconing)
nfdump -r netflow.nfcap -o extended | awk '$10/$9 < 100'  # Avg bytes/packet < 100
```

### Aggregation and Summarization

```bash
# Aggregate flows by time interval
nfdump -r netflow.nfcap -A proto -s proto/bytes/packets

# Bidirectional flow aggregation
nfdump -r netflow.nfcap -b  # Combine bi-directional flows

# Timeline statistics (5-minute intervals)
nfdump -r netflow.nfcap -A srcip -s ip/bytes -t 2025/10/28.10:00-2025/10/28.12:00

# Export aggregated data
nfdump -r netflow.nfcap -s srcip/bytes -o csv -n 100 > top_sources.csv
```

### Port Scanning Detection

```bash
# Horizontal scan (one source, many destinations, same port)
nfdump -r netflow.nfcap 'flags S and not flags A' -s srcip/dstport -n 50

# Vertical scan (one destination, many ports)
nfdump -r netflow.nfcap 'src ip 192.168.1.100 and flags S and not flags A' -s dstport

# Failed connection attempts (SYN without data transfer)
nfdump -r netflow.nfcap 'flags S and packets < 5'

# Identify scanners by flow count
nfdump -r netflow.nfcap 'flags S and not flags A' -s srcip/flows -n 20

# SYN flood detection
nfdump -r netflow.nfcap 'flags S and not flags A and packets == 1' -s dstip/flows -n 20
```

### Data Exfiltration Detection

```bash
# Unusually large outbound transfers
nfdump -r netflow.nfcap 'src net 192.168.0.0/16' -s srcip/bytes -n 20 | awk '$3 > 100000000'

# Long-duration outbound connections
nfdump -r netflow.nfcap -o extended 'src net 192.168.0.0/16' | awk '$11 > 600'

# High-volume connections to uncommon ports
nfdump -r netflow.nfcap 'dst port > 1024 and bytes > 10000000' -s dstip/dstport/bytes

# DNS tunneling indicators (high DNS traffic volume)
nfdump -r netflow.nfcap 'dst port 53' -s srcip/bytes -n 20

# Identify data transfers to external IPs
nfdump -r netflow.nfcap 'src net 192.168.0.0/16 and not dst net 192.168.0.0/16' -s dstip/bytes -n 50
```

### Beaconing Detection

Beaconing is regular periodic communication typical of malware C2 channels.

```bash
# Find repeated connections to same destination
nfdump -r netflow.nfcap -s srcip/dstip/dstport/flows -n 100 | awk '$5 > 50'

# Export timestamps for interval analysis
nfdump -r netflow.nfcap 'src ip 192.168.1.100 and dst ip 203.0.113.50' -o extended | \
awk '{print $1, $2}' > timestamps.txt

# Calculate inter-connection intervals (requires post-processing)
awk '{if(NR>1) print $1" "$2-prev; prev=$2}' timestamps.txt | sort | uniq -c
```

**Python script for beaconing detection:**

```python
#!/usr/bin/env python3
import subprocess
import statistics

def detect_beaconing(nfcap_file, ip_pair, threshold=0.1):
    """
    [Inference] This approach attempts to detect beaconing by analyzing timing regularity.
    Detection accuracy depends on network conditions and may produce false positives/negatives.
    """
    cmd = f'nfdump -r {nfcap_file} -o csv "src ip {ip_pair[0]} and dst ip {ip_pair[1]}"'
    output = subprocess.check_output(cmd, shell=True).decode()
    
    timestamps = []
    for line in output.split('\n')[1:]:  # Skip header
        if line:
            fields = line.split(',')
            timestamps.append(float(fields[0]))
    
    if len(timestamps) < 3:
        return False
    
    intervals = [timestamps[i+1] - timestamps[i] for i in range(len(timestamps)-1)]
    
    if len(intervals) < 2:
        return False
    
    mean_interval = statistics.mean(intervals)
    stdev_interval = statistics.stdev(intervals)
    
    # Low coefficient of variation suggests regular beaconing
    cv = stdev_interval / mean_interval if mean_interval > 0 else float('inf')
    
    return cv < threshold

# Example usage
# result = detect_beaconing('netflow.nfcap', ('192.168.1.100', '203.0.113.50'))
```

### Protocol Analysis

```bash
# Protocol distribution
nfdump -r netflow.nfcap -s proto/bytes/packets/flows

# Non-standard protocol usage
nfdump -r netflow.nfcap 'proto != 6 and proto != 17 and proto != 1'

# ICMP analysis
nfdump -r netflow.nfcap 'proto icmp' -s srcip/dstip/bytes

# UDP traffic analysis (potential DNS tunneling, VPN)
nfdump -r netflow.nfcap 'proto udp' -s dstport/bytes -n 20

# TCP flag analysis
nfdump -r netflow.nfcap 'proto tcp' -s flags/flows
```

### Geographic and AS Analysis

```bash
# Top source AS numbers
nfdump -r netflow.nfcap -s srcas/bytes -n 20

# Top destination AS numbers
nfdump -r netflow.nfcap -s dstas/bytes -n 20

# Traffic between specific AS pairs
nfdump -r netflow.nfcap 'src as 12345 and dst as 67890'

# Identify connections to uncommon ASNs
nfdump -r netflow.nfcap -s dstas/flows -n 100 | awk '$3 < 10'
```

### NetFlow with nfsen (GUI)

nfsen provides a web-based GUI for NetFlow analysis, though it requires setup beyond basic analysis.

```bash
# Installation (requires configuration)
sudo apt-get install nfsen

# Configuration location
/etc/nfsen/nfsen.conf

# Start nfsen
sudo systemctl start nfsen
```

[Inference] GUI-based analysis is less common in CTF scenarios, which typically provide raw NetFlow files for command-line analysis.

### Converting NetFlow to Other Formats

**Export to CSV for analysis in other tools:**

```bash
# Basic CSV export
nfdump -r netflow.nfcap -o csv > netflow.csv

# Custom field CSV export
nfdump -r netflow.nfcap -o "fmt:%ts,%te,%sa,%da,%sp,%dp,%pr,%byt,%pkt" > custom.csv

# Export to JSON for programmatic analysis
nfdump -r netflow.nfcap -o json > netflow.json
```

**Convert to PCAP (limited reconstruction):**

[Unverified] NetFlow to PCAP conversion tools exist but cannot reconstruct actual packet payloads, only connection metadata. Tools like `nf2pcap` may provide header-only reconstructions.

```bash
# Example concept (tool-dependent)
# nf2pcap netflow.nfcap output.pcap
```

### Anomaly Detection Techniques

**Statistical baselines:**

```bash
# Establish baseline traffic volume per hour
nfdump -R /var/netflow/baseline/ -A srcip -s ip/bytes -n 0 | \
awk '{sum+=$3; count++} END {print "Average:", sum/count}'

# Identify outliers (traffic > 3x baseline)
baseline=1000000  # Example: 1MB average
nfdump -r netflow.nfcap -s srcip/bytes -n 0 | awk -v b=$baseline '$3 > b*3'

# Unusual time-of-day activity
nfdump -r netflow.nfcap -t 2025/10/28.02:00-2025/10/28.06:00 -s srcip/bytes -n 20
```

**Flow entropy analysis:**

```bash
# High entropy in destination ports (potential scanning)
nfdump -r netflow.nfcap 'src ip 192.168.1.100' -s dstport -n 0 | wc -l

# Low entropy in packet sizes (potential C2)
nfdump -r netflow.nfcap -o extended | awk '{print $10}' | sort | uniq -c
```

### NetFlow Performance and Sampling

**Understanding sampling:**

NetFlow sampling (e.g., 1:100) means only 1 in 100 packets is analyzed. This affects byte/packet counts.

```bash
# Check sampling rate (if available in flow records)
nfdump -r netflow.nfcap -o extended | grep -i "sampling"

# Adjust calculations for sampling
# If sampling rate is 1:100, multiply byte/packet counts by 100
```

[Inference] In CTF scenarios, NetFlow data is typically provided without sampling for accuracy, but understanding sampling is important for real-world analysis.

### CTF-Specific NetFlow Analysis

**Flag extraction from flow metadata:**

```bash
# Check for unusual port numbers (flags encoded as ports)
nfdump -r netflow.nfcap -s dstport -n 0 | awk '$1 > 50000'

# Convert port sequences to ASCII
nfdump -r netflow.nfcap 'src ip 192.168.1.100' -o extended | \
awk '{print $8}' | while read port; do printf "%c" $((port % 256)); done

# Analyze byte counts for patterns
nfdump -r netflow.nfcap -o extended | awk '{print $10}' | \
while read bytes; do printf "%02x" $((bytes % 256)); done | xxd -r -p

# Check ToS field for covert data
nfdump -r netflow.nfcap -o extended | awk '{print $7}' | sort -u
```

**Connection sequence analysis:**

```bash
# Extract connection order (timeline reconstruction)
nfdump -r netflow.nfcap 'src ip 192.168.1.100' -o extended | \
sort -k1,2 | awk '{print $1, $2, $5, $8}'

# Identify command-and-control patterns
nfdump -r netflow.nfcap -s srcip/dstip -n 100 | \
awk '{if($5==1) print "Single flow:", $1, "->", $3}'
```

**Hidden channels in NetFlow metadata:**

```bash
# Flow start time covert channel
nfdump -r netflow.nfcap -o extended | awk '{print $2}' | \
while read ts; do echo $ts | rev | cut -c1-2 | rev; done | \
while read val; do printf "%c" $val; done

# TCP flags as data carrier
nfdump -r netflow.nfcap 'proto tcp' -o extended | \
grep -oE '\.....\.\.' | tr -d '.' | \
while read flags; do echo $flags; done
```

### Advanced NetFlow Queries

**Complex multi-condition filters:**

```bash
# Find long-duration small-packet flows (C2 characteristic)
nfdump -r netflow.nfcap 'duration > 300 and packets < 100 and proto tcp'

# Identify one-way communication (data exfiltration)
nfdump -r netflow.nfcap 'packets < 10' -s srcip/dstip/bytes | awk '$5 > 1000000'

# Find flows with unusual flag combinations
nfdump -r netflow.nfcap 'flags F and flags U'  # FIN+URG
nfdump -r netflow.nfcap 'flags F and flags P and flags U'  # Xmas scan

# Flows to multiple destinations on same port (lateral movement)
nfdump -r netflow.nfcap 'src ip 192.168.1.100 and dst port 445' -s dstip -n 0
```

**Time-series analysis:**

```bash
# Generate traffic profile by hour
for hour in {00..23}; do
    count=$(nfdump -r netflow.nfcap -t "2025/10/28.$hour:00-2025/10/28.$hour:59" -q | grep "flows" | awk '{print $4}')
    echo "$hour:00 - $count flows"
done

# Detect sudden traffic spikes
nfdump -r netflow.nfcap -A srcip -s ip/bytes -n 100 | \
awk '{if(NR>1 && $3 > prev*5) print "Spike:", $1, $3, "bytes (was", prev")"; prev=$3}'
```

### NetFlow Log Correlation

Combining NetFlow with other log sources provides comprehensive analysis.

```bash
# Extract IP addresses from NetFlow
nfdump -r netflow.nfcap -s srcip -n 0 | awk '{print $1}' > netflow_ips.txt

# Cross-reference with web server logs
for ip in $(cat netflow_ips.txt); do
    echo "=== $ip ==="
    grep "$ip" /var/log/apache2/access.log
done

# Correlate with system authentication logs
for ip in $(cat netflow_ips.txt); do
    grep "$ip" /var/log/auth.log
done
```

### Scripting NetFlow Analysis

**Bash script for automated analysis:**

```bash
#!/bin/bash
# netflow_analysis.sh - Automated NetFlow analysis for CTF

NFCAP="$1"
REPORT="netflow_report.txt"

echo "NetFlow Analysis Report" > "$REPORT"
echo "======================" >> "$REPORT"
echo "File: $NFCAP" >> "$REPORT"
echo "" >> "$REPORT"

# Top talkers
echo "Top 10 Source IPs by Bytes:" >> "$REPORT"
nfdump -r "$NFCAP" -s srcip/bytes -n 10 -q >> "$REPORT"
echo "" >> "$REPORT"

# Protocol distribution
echo "Protocol Distribution:" >> "$REPORT"
nfdump -r "$NFCAP" -s proto/bytes -q >> "$REPORT"
echo "" >> "$REPORT"

# Scan detection
echo "Potential Port Scans:" >> "$REPORT"
nfdump -r "$NFCAP" 'flags S and not flags A' -s srcip/flows -n 10 -q >> "$REPORT"
echo "" >> "$REPORT"

# Large flows
echo "Large Data Transfers (>10MB):" >> "$REPORT"
nfdump -r "$NFCAP" 'bytes > 10000000' -o extended -q >> "$REPORT"
echo "" >> "$REPORT"

# Uncommon ports
echo "Top Uncommon Destination Ports:" >> "$REPORT"
nfdump -r "$NFCAP" 'dst port > 1024' -s dstport/bytes -n 10 -q >> "$REPORT"

echo "Report saved to $REPORT"
```

**Python script for advanced analysis:**

```python
#!/usr/bin/env python3
import subprocess
import json
from collections import defaultdict

def analyze_netflow(nfcap_file):
    """
    [Inference] This script provides basic NetFlow analysis.
    Results should be validated against other data sources.
    """
    cmd = f'nfdump -r {nfcap_file} -o json'
    output = subprocess.check_output(cmd, shell=True).decode()
    
    flows = []
    for line in output.strip().split('\n'):
        try:
            flows.append(json.loads(line))
        except json.JSONDecodeError:
            continue
    
    # Analyze flows
    src_bytes = defaultdict(int)
    dst_ports = defaultdict(int)
    conversations = defaultdict(int)
    
    for flow in flows:
        src = flow.get('src4_addr') or flow.get('src6_addr', 'unknown')
        dst = flow.get('dst4_addr') or flow.get('dst6_addr', 'unknown')
        dport = flow.get('dst_port', 0)
        bytes_sent = flow.get('bytes', 0)
        
        src_bytes[src] += bytes_sent
        dst_ports[dport] += 1
        conversations[f"{src}->{dst}:{dport}"] += 1
    
    print("Top 5 Sources by Bytes:")
    for src, bytes_val in sorted(src_bytes.items(), key=lambda x: x[1], reverse=True)[:5]:
        print(f"  {src}: {bytes_val:,} bytes")
    
    print("\nTop 5 Destination Ports:")
    for port, count in sorted(dst_ports.items(), key=lambda x: x[1], reverse=True)[:5]:
        print(f"  Port {port}: {count} flows")
    
    print("\nTop 5 Conversations:")
    for conv, count in sorted(conversations.items(), key=lambda x: x[1], reverse=True)[:5]:
        print(f"  {conv}: {count} flows")

# Example usage
# analyze_netflow('netflow.nfcap')
```

### Troubleshooting NetFlow Analysis

```bash
# Verify file format
file netflow.nfcap
nfdump -r netflow.nfcap -V  # Show version info

# Check for corrupted flows
nfdump -r netflow.nfcap -c  # Count flows only

# Convert between NetFlow versions (if needed)
nfcapd -r old_format.fwd -w new_format.nfcap

# Merge multiple NetFlow files
nfcapd -R /path/to/directory/ -w merged.nfcap
```

---

**Important related subtopics for comprehensive network traffic analysis:**

- **Packet timing analysis** - Using inter-arrival times and jitter to detect covert channels and network anomalies
- **Protocol reverse engineering** - Analyzing unknown or proprietary protocols in packet captures
- **Traffic baseline establishment** - Creating normal behavior profiles for anomaly detection in production networks

---

## Zeek (Bro) Logs

Zeek (formerly Bro) is a network security monitoring framework that generates structured logs from network traffic analysis. Zeek logs are tab-separated value (TSV) files with rich metadata about network connections, protocols, and anomalies.

### Log File Structure and Location

**Default Log Directory:**

```bash
/opt/zeek/logs/current/
# or
/var/log/zeek/current/

# Archived logs (rotated daily)
/opt/zeek/logs/YYYY-MM-DD/
```

**Key Log Files:**

- `conn.log` - Connection summaries (all TCP/UDP/ICMP)
- `dns.log` - DNS queries and responses
- `http.log` - HTTP requests and responses
- `ssl.log` - SSL/TLS handshakes and certificates
- `files.log` - File transfers extracted from protocols
- `weird.log` - Protocol anomalies and violations
- `notice.log` - Zeek-generated alerts
- `software.log` - Software version detection
- `x509.log` - X.509 certificate details
- `smtp.log` - SMTP email transactions
- `ssh.log` - SSH connections and versions
- `rdp.log` - RDP connections
- `smb_files.log` / `smb_mapping.log` - SMB file operations

### Log Format Analysis

**Zeek TSV Structure:**

```bash
# Header format
#separator \x09
#set_separator	,
#empty_field	(empty)
#unset_field	-
#path	conn
#open	2024-10-28-12-00-00
#fields	ts	uid	id.orig_h	id.orig_p	id.resp_h	id.resp_p	proto	service	duration	orig_bytes	resp_bytes	conn_state	local_orig	local_resp	missed_bytes	history	orig_pkts	orig_ip_bytes	resp_pkts	resp_ip_bytes	tunnel_parents
#types	time	string	addr	port	addr	port	enum	string	interval	count	count	string	bool	bool	count	string	count	count	count	count	set[string]
```

**Parsing Zeek Logs:**

```bash
# Skip comments and extract data
grep -v '^#' conn.log | head -n 5

# Using zeek-cut (included with Zeek)
zeek-cut ts id.orig_h id.resp_h id.resp_p proto < conn.log

# Count field headers
head -n 20 conn.log | grep '^#fields' | sed 's/#fields\t//' | tr '\t' '\n' | nl
```

### Basic Zeek Log Analysis Commands

**Extract Specific Fields:**

```bash
# Connection 5-tuple
zeek-cut ts id.orig_h id.orig_p id.resp_h id.resp_p proto < conn.log

# HTTP requests with User-Agent
zeek-cut ts id.orig_h method host uri user_agent < http.log

# DNS queries
zeek-cut ts id.orig_h query answers < dns.log

# SSL/TLS certificate subjects
zeek-cut ts id.orig_h id.resp_h server_name < ssl.log
```

**Time-based Filtering:**

```bash
# Convert Zeek epoch timestamp to human-readable
zeek-cut ts < conn.log | head -n 1 | xargs -I {} date -d "@{}" "+%Y-%m-%d %H:%M:%S"

# Filter by time range (epoch timestamps)
awk -F'\t' '$1 >= 1698508800 && $1 <= 1698595200' conn.log

# Using specific date
START=$(date -d "2024-10-28 00:00:00" +%s)
END=$(date -d "2024-10-28 23:59:59" +%s)
awk -F'\t' -v start=$START -v end=$END '$1 >= start && $1 <= end' conn.log
```

**Common Analysis Patterns:**

```bash
# Top talkers (source IPs)
zeek-cut id.orig_h < conn.log | sort | uniq -c | sort -rn | head -20

# Top destinations
zeek-cut id.resp_h id.resp_p < conn.log | sort | uniq -c | sort -rn | head -20

# Bandwidth analysis (bytes transferred)
zeek-cut id.orig_h orig_bytes resp_bytes < conn.log | \
awk '{bytes[$1]+=$2+$3} END {for (ip in bytes) print ip, bytes[ip]}' | \
sort -k2 -rn | head -20

# Connection duration analysis
zeek-cut id.orig_h id.resp_h duration < conn.log | \
awk '$3 > 0' | sort -k3 -rn | head -20
```

### Converting Zeek Logs to JSON

**Using Python:**

```bash
cat > zeek_to_json.py << 'EOF'
#!/usr/bin/env python3
import json
import sys

def parse_zeek_log(filename):
    fields = []
    with open(filename, 'r') as f:
        for line in f:
            if line.startswith('#fields'):
                fields = line.strip().split('\t')[1:]
            elif not line.startswith('#') and fields:
                values = line.strip().split('\t')
                record = dict(zip(fields, values))
                print(json.dumps(record))

if __name__ == '__main__':
    parse_zeek_log(sys.argv[1])
EOF

chmod +x zeek_to_json.py
./zeek_to_json.py conn.log > conn.json
```

**Using jq for Analysis:**

```bash
# Top source IPs
jq -r '.["id.orig_h"]' conn.json | sort | uniq -c | sort -rn | head -20

# Filter by protocol
jq 'select(.proto == "tcp")' conn.json

# Complex filters
jq 'select(.["id.resp_p"] == "443" and .orig_bytes > 1000000)' conn.json
```

## Connection Logs (conn.log)

The `conn.log` file contains summaries of all network connections, providing a comprehensive view of network communication patterns.

### Key Fields in conn.log

|Field|Description|Example|
|---|---|---|
|ts|Timestamp (epoch)|1698508845.123456|
|uid|Unique connection ID|CHhAvVGS1DHFjwGM9|
|id.orig_h|Source IP|192.168.1.100|
|id.orig_p|Source port|49234|
|id.resp_h|Destination IP|10.0.0.50|
|id.resp_p|Destination port|443|
|proto|Protocol|tcp|
|service|Identified service|ssl|
|duration|Connection duration|45.234|
|orig_bytes|Bytes from originator|5432|
|resp_bytes|Bytes from responder|123456|
|conn_state|Connection state|SF|
|missed_bytes|Bytes missed|0|
|history|Connection state history|ShADadFf|
|orig_pkts|Packets from originator|45|
|resp_pkts|Packets from responder|89|

### Connection States (conn_state)

**Common States:**

- **S0** - Connection attempt seen, no reply (SYN sent, no SYN-ACK)
- **S1** - Connection established, not terminated (SYN-ACK received)
- **SF** - Normal SYN/FIN completion
- **REJ** - Connection rejected (RST)
- **S2** - Connection established, originator closed (FIN from originator)
- **S3** - Connection established, responder closed (FIN from responder)
- **RSTO** - Originator sent RST
- **RSTR** - Responder sent RST
- **RSTOS0** - Originator sent SYN, responder sent RST
- **RSTRH** - Responder sent RST after SYN-ACK
- **SH** - Originator sent SYN, responder sent SYN-ACK-data
- **SHR** - Responder sent SYN-ACK, originator sent data
- **OTH** - No SYN seen, midstream traffic

### Connection State Analysis

**Detect Port Scanning:**

```bash
# SYN scans (S0 state - no response)
zeek-cut id.orig_h id.resp_h id.resp_p conn_state < conn.log | \
awk '$4 == "S0"' | \
awk '{print $1, $3}' | \
sort | uniq | \
awk '{count[$1]++} END {for (ip in count) if (count[ip] > 20) print ip, count[ip]}' | \
sort -k2 -rn

# REJ scans (connection rejected)
zeek-cut id.orig_h id.resp_h id.resp_p conn_state < conn.log | \
awk '$4 == "REJ"' | \
awk '{print $1}' | sort | uniq -c | sort -rn
```

**Identify Failed Connections:**

```bash
# All failed connection attempts
zeek-cut ts id.orig_h id.resp_h id.resp_p conn_state service < conn.log | \
awk '$5 ~ /^(S0|REJ|RSTO|RSTR)$/' | \
head -50
```

**Long-Duration Connections (Potential C2 Beaconing):**

```bash
# Connections longer than 1 hour
zeek-cut ts id.orig_h id.resp_h id.resp_p duration service < conn.log | \
awk '$5 > 3600' | \
sort -k5 -rn

# Periodic connections (beaconing detection)
zeek-cut ts id.orig_h id.resp_h id.resp_p < conn.log | \
awk '{key=$2" "$3" "$4; times[key]=times[key]" "$1} 
END {for (k in times) print k, times[k]}' > connection_times.txt

# Analyze intervals
cat > beacon_detector.py << 'EOF'
#!/usr/bin/env python3
import sys
import statistics

threshold_variance = 30  # seconds

with open('connection_times.txt', 'r') as f:
    for line in f:
        parts = line.strip().split()
        if len(parts) < 6:  # Need at least src, dst, port, and 3 timestamps
            continue
        
        src, dst, port = parts[0:3]
        timestamps = [float(t) for t in parts[3:]]
        timestamps.sort()
        
        if len(timestamps) < 3:
            continue
        
        intervals = [timestamps[i+1] - timestamps[i] for i in range(len(timestamps)-1)]
        
        if len(intervals) > 2:
            mean_interval = statistics.mean(intervals)
            stdev_interval = statistics.stdev(intervals)
            
            if stdev_interval < threshold_variance and mean_interval > 10:
                print(f"Beaconing detected: {src} -> {dst}:{port}")
                print(f"  Mean interval: {mean_interval:.2f}s, StdDev: {stdev_interval:.2f}s")
                print(f"  Intervals: {intervals[:5]}")
EOF

python3 beacon_detector.py
```

### Data Transfer Analysis

**Large Data Transfers (Potential Exfiltration):**

```bash
# Outbound transfers > 10MB
zeek-cut ts id.orig_h id.resp_h id.resp_p orig_bytes service < conn.log | \
awk '$5 > 10485760' | \
sort -k5 -rn

# Inbound transfers (potential data staging)
zeek-cut ts id.orig_h id.resp_h id.resp_p resp_bytes service < conn.log | \
awk '$5 > 10485760' | \
sort -k5 -rn

# Calculate total bytes per source IP
zeek-cut id.orig_h orig_bytes resp_bytes < conn.log | \
awk '{total[$1]+=$2+$3} END {for (ip in total) print ip, total[ip]}' | \
sort -k2 -rn | \
numfmt --field=2 --to=iec-i --suffix=B
```

**Unusual Ports:**

```bash
# Non-standard ports for common services
zeek-cut id.orig_h id.resp_h id.resp_p service < conn.log | \
awk '($4 == "http" && $3 != "80") || ($4 == "ssl" && $3 != "443")' | \
head -50

# High ports (ephemeral) used as servers
zeek-cut id.resp_h id.resp_p conn_state < conn.log | \
awk '$2 > 49152 && $3 == "SF"' | \
awk '{print $1, $2}' | sort | uniq -c | sort -rn
```

### Connection History Analysis

The `history` field shows TCP flags and packet flow:

- **S** - SYN sent
- **H** - SYN-ACK sent
- **A** - ACK sent
- **D** - Data sent
- **F** - FIN sent
- **R** - RST sent
- **^** - Packet from originator
- **Lowercase** - Packet from responder

**Analyze Connection Patterns:**

```bash
# Find unusual TCP handshakes
zeek-cut id.orig_h id.resp_h id.resp_p history < conn.log | \
awk '{print $4}' | sort | uniq -c | sort -rn

# Connections with data but no proper handshake
zeek-cut id.orig_h id.resp_h id.resp_p history conn_state < conn.log | \
awk '$4 ~ /D/ && $5 != "SF"'
```

## Protocol Analysis

Zeek provides protocol-specific logs that enable deep inspection of application-layer traffic.

### DNS Analysis (dns.log)

**Key Fields:**

- `query` - Domain queried
- `answers` - Resolved IP addresses
- `qtype_name` - Query type (A, AAAA, PTR, TXT, etc.)
- `rcode_name` - Response code (NOERROR, NXDOMAIN, etc.)

**Basic DNS Analysis:**

```bash
# Most queried domains
zeek-cut query < dns.log | sort | uniq -c | sort -rn | head -20

# Failed DNS queries (NXDOMAIN)
zeek-cut ts id.orig_h query rcode_name < dns.log | \
awk '$4 == "NXDOMAIN"' | head -50

# DNS queries by source IP
zeek-cut id.orig_h query < dns.log | \
awk '{count[$1]++} END {for (ip in count) print ip, count[ip]}' | \
sort -k2 -rn

# Extract unique domains
zeek-cut query < dns.log | sort -u > domains.txt
```

**Detect DNS Tunneling:**

```bash
# Long domain names (potential data exfil)
zeek-cut query < dns.log | awk 'length($1) > 50' | head -20

# High entropy domain names
cat > dns_entropy.py << 'EOF'
#!/usr/bin/env python3
import sys
import math
from collections import Counter

def entropy(s):
    p, lns = Counter(s), float(len(s))
    return -sum(count/lns * math.log(count/lns, 2) for count in p.values())

for line in sys.stdin:
    domain = line.strip()
    if domain and not domain.startswith('#'):
        ent = entropy(domain.split('.')[0])  # Check subdomain entropy
        if ent > 3.5:  # High entropy threshold
            print(f"{ent:.2f}\t{domain}")
EOF

zeek-cut query < dns.log | python3 dns_entropy.py | sort -rn | head -20

# Unusual TXT queries (C2 communication)
zeek-cut ts id.orig_h query qtype_name answers < dns.log | \
awk '$4 == "TXT"' | head -50

# DNS queries to suspicious TLDs
zeek-cut query < dns.log | grep -E '\.(tk|ml|ga|cf|gq)$' | sort -u
```

**DNS-based C2 Detection:**

```bash
# High frequency queries to same domain
zeek-cut ts id.orig_h query < dns.log | \
awk '{count[$2" "$3]++} END {for (k in count) if (count[k] > 50) print k, count[k]}' | \
sort -k3 -rn

# Queries with regular intervals
zeek-cut ts query < dns.log | \
awk '{
    if (last_query[$2]) {
        interval = $1 - last_query[$2]
        intervals[$2] = intervals[$2] " " interval
    }
    last_query[$2] = $1
}
END {
    for (domain in intervals) {
        split(intervals[domain], arr, " ")
        if (length(arr) > 5) print domain, intervals[domain]
    }
}' > dns_intervals.txt
```

### HTTP Analysis (http.log)

**Key Fields:**

- `method` - HTTP method (GET, POST, etc.)
- `host` - Hostname
- `uri` - Request URI
- `referrer` - HTTP Referer header
- `user_agent` - User-Agent string
- `status_code` - HTTP response code
- `response_body_len` - Response size

**Basic HTTP Analysis:**

```bash
# Most accessed hosts
zeek-cut host < http.log | sort | uniq -c | sort -rn | head -20

# HTTP methods distribution
zeek-cut method < http.log | sort | uniq -c | sort -rn

# User-Agent analysis
zeek-cut user_agent < http.log | sort | uniq -c | sort -rn | head -20

# HTTP response codes
zeek-cut status_code < http.log | sort | uniq -c | sort -rn

# Large responses (potential data exfil)
zeek-cut ts id.orig_h host uri response_body_len < http.log | \
awk '$5 > 1048576' | sort -k5 -rn  # > 1MB
```

**Detect Web Attacks:**

```bash
# SQL Injection attempts
zeek-cut ts id.orig_h host uri < http.log | \
grep -iE "(union.*select|'.*or.*1=1|<script|javascript:|onerror=)" | \
head -50

# Directory traversal
zeek-cut ts id.orig_h host uri < http.log | \
grep -E '\.\./|\.\.\\' | head -50

# File upload endpoints
zeek-cut method host uri < http.log | \
awk '$1 == "POST"' | \
grep -iE '(upload|file)' | head -50

# Suspicious user agents
zeek-cut ts id.orig_h user_agent < http.log | \
grep -iE '(sqlmap|nikto|nmap|metasploit|burp|scanner)' | head -50

# Admin panel access
zeek-cut ts id.orig_h host uri status_code < http.log | \
grep -iE '(admin|login|wp-admin|phpmyadmin|manager)' | head -50
```

**Extract Credentials (HTTP Basic Auth):**

```bash
# HTTP Basic Authentication (base64 encoded)
zeek-cut ts id.orig_h host authorization < http.log | \
awk '$4 ~ /^Basic/' | \
awk '{print $1, $2, $3, $4}' | \
while read ts ip host auth; do
    creds=$(echo "$auth" | sed 's/Basic //' | base64 -d 2>/dev/null)
    echo "$ts $ip $host $creds"
done
```

**Command and Control Pattern Detection:**

```bash
# Regular HTTP requests (beaconing)
zeek-cut ts id.orig_h host uri < http.log | \
awk '{key=$2" "$3" "$4; times[key]=times[key]" "$1}
END {for (k in times) print k, times[k]}' > http_beacon_check.txt

# Short URIs with regular access (C2 check-in)
zeek-cut ts id.orig_h host uri < http.log | \
awk 'length($4) < 10' | \
awk '{count[$2" "$3" "$4]++} END {for (k in count) if (count[k] > 10) print k, count[k]}' | \
sort -k4 -rn
```

### SSL/TLS Analysis (ssl.log)

**Key Fields:**

- `server_name` - SNI hostname
- `cert_chain_fuids` - File UIDs of certificate chain
- `subject` - Certificate subject
- `issuer` - Certificate issuer
- `validation_status` - Certificate validation result
- `ja3` - JA3 fingerprint (if enabled)
- `ja3s` - JA3S server fingerprint (if enabled)

**Basic SSL Analysis:**

```bash
# Most common SSL/TLS hosts
zeek-cut server_name < ssl.log | sort | uniq -c | sort -rn | head -20

# Self-signed certificates (potential C2)
zeek-cut ts id.orig_h server_name validation_status < ssl.log | \
awk '$4 != "ok"' | head -50

# Short-lived certificates
zeek-cut ts server_name subject issuer < ssl.log | \
grep -v "Let's Encrypt" | head -50

# Certificate subjects
zeek-cut subject < ssl.log | sort -u | head -50
```

**JA3 Fingerprint Analysis:**

[Inference: JA3 fingerprinting requires Zeek to be configured with the ja3 package]

```bash
# If ja3 logging is enabled
zeek-cut ja3 server_name < ssl.log | \
awk '{count[$1" "$2]++} END {for (k in count) print k, count[k]}' | \
sort -k3 -rn

# Match known malicious JA3 hashes
# Example known bad JA3
BAD_JA3="e7d705a3286e19ea42f587b344ee6865"
zeek-cut ts id.orig_h server_name ja3 < ssl.log | \
awk -v bad="$BAD_JA3" '$4 == bad'
```

**Detect SSL/TLS Anomalies:**

```bash
# Non-standard ports for SSL
zeek-cut id.resp_p server_name < ssl.log | \
awk '$1 != "443"' | sort | uniq -c | sort -rn

# SSL to IP addresses (no SNI)
zeek-cut ts id.orig_h id.resp_h server_name < ssl.log | \
awk '$4 == "-"' | head -50
```

### File Extraction Analysis (files.log)

**Key Fields:**

- `fuid` - File unique ID
- `source` - Protocol source (HTTP, SMTP, FTP)
- `mime_type` - Detected MIME type
- `filename` - Original filename
- `total_bytes` - File size
- `md5` / `sha1` / `sha256` - File hashes (if enabled)

**Basic File Analysis:**

```bash
# Files by MIME type
zeek-cut mime_type < files.log | sort | uniq -c | sort -rn

# Executable downloads
zeek-cut ts source mime_type filename < files.log | \
grep -E '(application/x-dosexec|application/x-executable)' | head -50

# Large file transfers
zeek-cut ts source filename total_bytes < files.log | \
awk '$4 > 10485760' | sort -k4 -rn  # > 10MB

# Files with hash mismatches (if hash checking enabled)
zeek-cut ts filename md5 sha1 < files.log | \
awk 'NF == 4' | head -50
```

**Extract Files from PCAP with Zeek:**

```bash
# Configure Zeek to extract files
cat > local.zeek << 'EOF'
@load policy/frameworks/files/extract-all-files.zeek
redef FileExtract::prefix = "extracted/";
EOF

# Run Zeek with extraction
zeek -r capture.pcap local.zeek

# Check extracted files
ls -lh extracted/
file extracted/*
```

### SSH Analysis (ssh.log)

**Key Fields:**

- `version` - SSH version
- `auth_success` - Authentication result
- `auth_attempts` - Number of attempts
- `client` - SSH client software
- `server` - SSH server software

**Basic SSH Analysis:**

```bash
# Failed SSH authentications
zeek-cut ts id.orig_h id.resp_h auth_success auth_attempts < ssh.log | \
awk '$4 == "F"' | head -50

# Successful SSH logins
zeek-cut ts id.orig_h id.resp_h auth_success < ssh.log | \
awk '$4 == "T"'

# SSH brute force detection
zeek-cut id.orig_h auth_attempts < ssh.log | \
awk '$2 > 5' | sort | uniq

# SSH version enumeration
zeek-cut client server < ssh.log | sort -u
```

### SMB Analysis (smb_*.log)

**Key Files:**

- `smb_files.log` - SMB file operations
- `smb_mapping.log` - Share mappings
- `ntlm.log` - NTLM authentication

**Basic SMB Analysis:**

```bash
# SMB file access
zeek-cut ts id.orig_h id.resp_h path action < smb_files.log | head -50

# Share enumeration
zeek-cut ts id.orig_h id.resp_h path share_type < smb_mapping.log

# NTLM authentication attempts
zeek-cut ts id.orig_h id.resp_h username success < ntlm.log | \
awk '$5 == "F"'  # Failed attempts
```

## Advanced Zeek Analysis Techniques

### Weird.log Analysis (Protocol Violations)

The `weird.log` file contains protocol anomalies and violations detected by Zeek.

```bash
# Most common anomalies
zeek-cut name < weird.log | sort | uniq -c | sort -rn | head -20

# Anomalies by source IP
zeek-cut id.orig_h name < weird.log | \
awk '{print $1, $2}' | sort | uniq -c | sort -rn

# Critical anomalies
zeek-cut ts id.orig_h id.resp_h name notice < weird.log | \
grep -iE '(scan|attack|exploit|overflow)' | head -50
```

### Notice.log Analysis (Zeek Alerts)

```bash
# All notices/alerts
zeek-cut ts src dst note msg < notice.log

# Scan detection
zeek-cut ts src note msg < notice.log | \
grep -i scan

# Specific notice types
zeek-cut note < notice.log | sort | uniq -c | sort -rn
```

### Software Version Detection (software.log)

```bash
# Detected software versions
zeek-cut host software_type name version.major version.minor < software.log | \
column -t | head -50

# Vulnerable software detection (manual lookup required)
zeek-cut name version.major version.minor < software.log | sort -u
```

### Cross-Log Correlation

**Correlate HTTP with DNS:**

```bash
# Extract IPs from HTTP
zeek-cut id.resp_h host < http.log | sort -u > http_ips_hosts.txt

# Extract domains and IPs from DNS
zeek-cut query answers < dns.log | grep -v '^-$' | sort -u > dns_resolution.txt

# Find HTTP connections without prior DNS (potential IP-based C2)
comm -23 <(zeek-cut id.resp_h < http.log | sort -u) \
         <(zeek-cut answers < dns.log | tr ',' '\n' | sort -u)
```

**Correlate Connections with Files:**

```bash
# Find connection UIDs for file transfers
zeek-cut conn_uids source < files.log | tr ',' '\n' | sort -u > file_conn_uids.txt

# Extract those connections
while read uid; do
    zeek-cut uid id.orig_h id.resp_h orig_bytes resp_bytes < conn.log | \
    awk -v u="$uid" '$1 == u'
done < file_conn_uids.txt
```

## Zeek Scripting for Custom Analysis

**Basic Zeek Script Structure:**

```bash
cat > custom_analysis.zeek << 'EOF'
@load base/protocols/http

event http_request(c: connection, method: string, original_URI: string,
                   unescaped_URI: string, version: string)
{
    if (/admin/ in original_URI)
        print fmt("Admin access: %s -> %s%s", c$id$orig_h, c$http$host, original_URI);
}

event zeek_done()
{
    print "Analysis complete";
}
EOF

# Run script on PCAP
zeek -r capture.pcap custom_analysis.zeek
```

## CTF-Specific Zeek Analysis

**Common CTF Patterns:**

1. **Flag in DNS TXT Records:**

```bash
zeek-cut query qtype_name answers < dns.log | \
awk '$2 == "TXT"' | \
sed 's/\\x20/ /g'  # Decode hex-encoded spaces
```

2. **Flag in HTTP Headers:**

```bash
# Extract custom HTTP headers (requires detailed http logging)
zeek-cut ts host uri user_agent < http.log | \
grep -iE 'flag|ctf|x-flag'
```

3. **Flag in Base64 Encoded Data:**

```bash
# Extract HTTP URIs and decode base64
zeek-cut uri < http.log | \
grep -oE '[A-Za-z0-9+/]{20,}={0,2}' | \
while read b64; do
    decoded=$(echo "$b64" | base64 -d 2>/dev/null)
    if [[ $decoded =~ flag|CTF ]]; then
        echo "Potential flag: $decoded"
    fi
done
```

4. **Flag in File Downloads:**

```bash
# Check extracted files
for file in extracted/*; do
    strings "$file" | grep -iE 'flag\{|ctf\{'
done
```

5. **Flag in Connection Timing:**

```bash
# Extract connection timestamps and look for patterns
zeek-cut ts id.resp_p < conn.log | \
awk '{print int($1), $2}' | \
sort | uniq
```

**Important Related Topics:**

- **PCAP Analysis with tcpdump/tshark** - Packet-level inspection and filtering
- **Suricata/Snort IDS Logs** - Signature-based detection and alert correlation
- **NetFlow/IPFIX Analysis** - Flow-based traffic analysis
- **Packet Carving and Reassembly** - Extracting files and sessions from PCAPs

---

# Incident Response Logs

Incident response logs capture system-level events that reveal attacker actions post-compromise. These artifacts are critical for reconstructing attack timelines, identifying persistence mechanisms, and understanding lateral movement. In CTF scenarios, these logs often contain flags, clues about privilege escalation paths, or evidence of backdoor installations.

## Process Execution Logs

Process execution logs track program launches, command-line arguments, parent-child relationships, and execution context. Essential for identifying malicious processes, detecting living-off-the-land (LOtL) attacks, and reconstructing attacker command sequences.

### Linux Process Execution Logging

**auditd (Linux Audit Framework)**

Primary mechanism for detailed process execution tracking on Linux systems.

```bash
# Check if auditd is running
systemctl status auditd

# Verify audit rules
auditctl -l

# Log locations
/var/log/audit/audit.log        # Primary audit log
/var/log/audit/audit.log.1      # Rotated logs
```

**Configure process execution monitoring:**

```bash
# Add rule to monitor all execve syscalls (program execution)
auditctl -a always,exit -F arch=b64 -S execve -k process_execution
auditctl -a always,exit -F arch=b32 -S execve -k process_execution

# Monitor specific directories
auditctl -w /tmp -p x -k tmp_execution
auditctl -w /dev/shm -p x -k shm_execution
auditctl -w /var/www -p x -k web_execution

# Monitor sudo usage
auditctl -a always,exit -F arch=b64 -S execve -F euid=0 -F auid>=1000 -k privileged_execution

# Make rules persistent
# Edit /etc/audit/rules.d/audit.rules or /etc/audit/audit.rules
# Add rules, then: service auditd restart
```

**Parse audit logs for process execution:**

```bash
# View raw execve events
ausearch -sc execve

# Filter by time range
ausearch -sc execve --start today --end now

# Filter by specific user (UID 1000)
ausearch -sc execve -ua 1000

# Search for specific command
ausearch -sc execve -k process_execution | grep -i "/bin/bash"

# Extract command lines with aureport
aureport -x --summary

# Detailed execution report
aureport -x -i --start recent
```

**Extract structured execution data:**

```bash
# Parse audit log for command-line arguments
ausearch -sc execve -i | grep -A 5 "type=EXECVE"

# Example output parsing script
ausearch -sc execve --format csv --start today | \
    awk -F',' '{print $1, $5, $6}' | \
    sort -u

# Extract all executed binaries
ausearch -sc execve -i | \
    grep -oP 'exe="[^"]+' | \
    cut -d'"' -f2 | \
    sort -u > executed_programs.txt
```

**Detailed audit log parsing:**

```bash
#!/bin/bash
# Extract process execution timeline

ausearch -sc execve -i --start today --format text | \
awk '
/^----/ { 
    if (time != "") {
        print time, uid, exe, cmd
    }
    time=""; uid=""; exe=""; cmd=""
}
/^time->/ { time=$2" "$3 }
/auid=/ { match($0, /auid=([^ ]+)/, a); uid=a[1] }
/exe=/ { match($0, /exe="([^"]+)"/, a); exe=a[1] }
/a[0-9]+=/ { 
    match($0, /a[0-9]+="([^"]+)"/, a)
    if (cmd == "") cmd=a[1]
    else cmd=cmd" "a[1]
}
END {
    if (time != "") print time, uid, exe, cmd
}
'
```

**Process accounting (acct/psacct)**

Lightweight alternative to auditd for basic process tracking.

```bash
# Install process accounting
apt-get install acct   # Debian/Ubuntu
dnf install psacct     # Fedora/RHEL

# Enable accounting
systemctl start acct
systemctl enable acct

# Log location
/var/account/pacct
/var/log/account/pacct

# Query commands executed by user
lastcomm root
lastcomm alice

# Show all commands
lastcomm

# Commands executed in last hour
lastcomm --time-range 1h

# Summary by user
sa -u

# Summary by command
sa -m
```

**syslog/journal process execution (limited detail)**

```bash
# Search systemd journal for process spawns
journalctl _COMM=sudo --since today
journalctl _COMM=su --since today

# Cron job executions
journalctl -u cron --since today

# Search for specific executable
journalctl | grep -i "/tmp/suspicious"

# SSH session commands (if configured)
journalctl -u ssh --since today | grep -i "session opened"
```

**bash history (user command history)**

```bash
# View bash history with timestamps (if HISTTIMEFORMAT set)
export HISTTIMEFORMAT="%F %T "
history

# Check all users' bash history
for home in /home/*; do
    user=$(basename "$home")
    if [ -f "$home/.bash_history" ]; then
        echo "=== History for $user ==="
        cat "$home/.bash_history"
    fi
done

# Root history
cat /root/.bash_history

# Check history timestamp file
ls -la /home/*/.bash_history
stat /home/alice/.bash_history

# Search for suspicious commands
grep -iE "(wget|curl|nc|netcat|/dev/tcp|base64|python.*-c)" /home/*/.bash_history /root/.bash_history

# Find cleared/truncated history (suspicious activity)
find /home -name ".bash_history" -size 0
```

**Parse bash history with timestamps:**

```bash
#!/bin/bash
# Extract bash history with reconstructed timestamps

HISTFILE="/home/alice/.bash_history"
STAT_TIME=$(stat -c %Y "$HISTFILE")

# If history has timestamps (format: #1234567890)
if grep -q "^#[0-9]" "$HISTFILE"; then
    awk '
    /^#[0-9]+$/ { 
        timestamp = substr($0, 2)
        getline
        print strftime("%Y-%m-%d %H:%M:%S", timestamp), $0
    }
    ' "$HISTFILE"
else
    # No timestamps; estimate from file modification time
    echo "Warning: No timestamps in history file"
    echo "File last modified: $(date -d @$STAT_TIME)"
    cat "$HISTFILE"
fi
```

### Windows Process Execution Logging

**Windows Event Logs - Process Creation (Event ID 4688)**

Primary source for process execution tracking when auditing is enabled.

```powershell
# Check if process creation auditing is enabled
auditpol /get /subcategory:"Process Creation"

# Enable process creation auditing
auditpol /set /subcategory:"Process Creation" /success:enable /failure:enable

# Enable command-line logging in process creation events
# Registry path: HKLM\SOFTWARE\Microsoft\Windows\CurrentVersion\Policies\System\Audit
reg add "HKLM\SOFTWARE\Microsoft\Windows\CurrentVersion\Policies\System\Audit" /v ProcessCreationIncludeCmdLine_Enabled /t REG_DWORD /d 1 /f

# Query process creation events
Get-WinEvent -FilterHashtable @{LogName='Security'; ID=4688} -MaxEvents 100

# Filter by process name
Get-WinEvent -FilterHashtable @{LogName='Security'; ID=4688} | 
    Where-Object {$_.Message -like "*powershell.exe*"}

# Extract structured data
Get-WinEvent -FilterHashtable @{LogName='Security'; ID=4688} | 
    ForEach-Object {
        $xml = [xml]$_.ToXml()
        [PSCustomObject]@{
            TimeCreated = $_.TimeCreated
            ProcessName = $xml.Event.EventData.Data[5].'#text'
            ProcessId = $xml.Event.EventData.Data[4].'#text'
            CommandLine = $xml.Event.EventData.Data[8].'#text'
            SubjectUserName = $xml.Event.EventData.Data[1].'#text'
            ParentProcessName = $xml.Event.EventData.Data[13].'#text'
        }
    } | Format-Table -AutoSize
```

**Parse Event 4688 from exported EVTX:**

```powershell
# Export Security log
wevtutil epl Security C:\forensics\security.evtx

# On analysis workstation
Get-WinEvent -Path C:\forensics\security.evtx -FilterXPath "*[System[EventID=4688]]" |
    ForEach-Object {
        $xml = [xml]$_.ToXml()
        $eventData = $xml.Event.EventData.Data
        [PSCustomObject]@{
            Time = $_.TimeCreated
            User = ($eventData | Where-Object {$_.Name -eq 'SubjectUserName'}).'#text'
            Process = ($eventData | Where-Object {$_.Name -eq 'NewProcessName'}).'#text'
            CommandLine = ($eventData | Where-Object {$_.Name -eq 'CommandLine'}).'#text'
            ParentProcess = ($eventData | Where-Object {$_.Name -eq 'ParentProcessName'}).'#text'
            ProcessId = ($eventData | Where-Object {$_.Name -eq 'NewProcessId'}).'#text'
        }
    } | Export-Csv -Path process_execution.csv -NoTypeInformation
```

**Sysmon (System Monitor)**

Enhanced process tracking with additional context. Must be installed separately.

```powershell
# Download Sysmon from Microsoft Sysinternals
# Install with configuration
sysmon64.exe -accepteula -i sysmonconfig.xml

# Check if Sysmon is running
Get-Service Sysmon64

# Event ID 1: Process creation (rich detail)
Get-WinEvent -LogName "Microsoft-Windows-Sysmon/Operational" -FilterXPath "*[System[EventID=1]]" -MaxEvents 50

# Extract detailed process info
Get-WinEvent -LogName "Microsoft-Windows-Sysmon/Operational" -FilterXPath "*[System[EventID=1]]" |
    ForEach-Object {
        $xml = [xml]$_.ToXml()
        $eventData = @{}
        $xml.Event.EventData.Data | ForEach-Object { $eventData[$_.Name] = $_.'#text' }
        
        [PSCustomObject]@{
            UtcTime = $eventData.UtcTime
            ProcessGuid = $eventData.ProcessGuid
            ProcessId = $eventData.ProcessId
            Image = $eventData.Image
            CommandLine = $eventData.CommandLine
            User = $eventData.User
            ParentImage = $eventData.ParentImage
            ParentCommandLine = $eventData.ParentCommandLine
            Hashes = $eventData.Hashes
            IntegrityLevel = $eventData.IntegrityLevel
        }
    } | Format-Table -AutoSize
```

**Common Sysmon queries for CTF:**

```powershell
# Find encoded PowerShell commands
Get-WinEvent -LogName "Microsoft-Windows-Sysmon/Operational" -FilterXPath "*[System[EventID=1]]" |
    Where-Object {$_.Message -match "powershell.*(-enc|-e|-encodedcommand)"} |
    ForEach-Object {
        $xml = [xml]$_.ToXml()
        $cmdline = ($xml.Event.EventData.Data | Where-Object {$_.Name -eq 'CommandLine'}).'#text'
        
        # Extract and decode base64
        if ($cmdline -match "-(?:e|enc|encodedcommand)\s+([A-Za-z0-9+/=]+)") {
            $encoded = $matches[1]
            $decoded = [System.Text.Encoding]::Unicode.GetString([System.Convert]::FromBase64String($encoded))
            Write-Output "Time: $($_.TimeCreated)"
            Write-Output "Decoded: $decoded"
            Write-Output "---"
        }
    }

# Find processes launched from suspicious locations
Get-WinEvent -LogName "Microsoft-Windows-Sysmon/Operational" -FilterXPath "*[System[EventID=1]]" |
    Where-Object {$_.Message -match "(\\Temp\\|\\AppData\\Local\\Temp\\|\\Users\\Public\\)"} |
    Select-Object TimeCreated, Message | Format-List

# Identify unusual parent-child relationships
Get-WinEvent -LogName "Microsoft-Windows-Sysmon/Operational" -FilterXPath "*[System[EventID=1]]" |
    ForEach-Object {
        $xml = [xml]$_.ToXml()
        $data = @{}
        $xml.Event.EventData.Data | ForEach-Object { $data[$_.Name] = $_.'#text' }
        
        # Flag suspicious combinations
        $suspicious = $false
        if ($data.ParentImage -match "winword\.exe|excel\.exe" -and $data.Image -match "powershell\.exe|cmd\.exe") {
            $suspicious = $true
        }
        if ($data.ParentImage -match "explorer\.exe" -and $data.Image -match "reg\.exe|sc\.exe|net\.exe") {
            $suspicious = $true
        }
        
        if ($suspicious) {
            [PSCustomObject]@{
                Time = $data.UtcTime
                Parent = $data.ParentImage
                Child = $data.Image
                CommandLine = $data.CommandLine
            }
        }
    } | Format-Table -AutoSize
```

**Prefetch Files (Execution Artifacts)**

Windows prefetch tracks program execution frequency and timestamps.

```powershell
# Prefetch location
cd C:\Windows\Prefetch

# List prefetch files with timestamps
Get-ChildItem C:\Windows\Prefetch\*.pf | 
    Select-Object Name, LastWriteTime, CreationTime |
    Sort-Object LastWriteTime -Descending

# Parse prefetch with external tool (PECmd.exe from Eric Zimmerman)
# Download from https://ericzimmerman.github.io/
.\PECmd.exe -d C:\Windows\Prefetch --csv . --csvf prefetch_analysis.csv

# Parse CSV output
Import-Csv prefetch_analysis.csv | 
    Select-Object ExecutableName, LastRun, RunCount |
    Sort-Object LastRun -Descending |
    Format-Table -AutoSize
```

**Parse prefetch manually (limited info):**

```powershell
# Extract basic info from prefetch filename
# Format: EXECUTABLE-{HASH}.pf
Get-ChildItem C:\Windows\Prefetch\*.pf | ForEach-Object {
    $name = $_.Name -replace '-[A-F0-9]{8}\.pf$', ''
    [PSCustomObject]@{
        Executable = $name
        LastModified = $_.LastWriteTime
        Created = $_.CreationTime
        FullPath = $_.FullName
    }
} | Where-Object {$_.Executable -match "suspicious|malware|backdoor"} |
    Format-Table -AutoSize
```

**AmCache (Application Compatibility Cache)**

Tracks executable execution with file paths and timestamps.

```powershell
# AmCache location
$amcachePath = "C:\Windows\AppCompat\Programs\Amcache.hve"

# Export AmCache for analysis
reg save HKLM\SYSTEM\CurrentControlSet\Control\Session Manager\AppCompatCache C:\forensics\appcompat.reg

# Parse with external tools:
# - AmcacheParser.exe (Eric Zimmerman)
# - RegRipper with amcache plugin

# Example with AmcacheParser
.\AmcacheParser.exe -f C:\Windows\AppCompat\Programs\Amcache.hve --csv . --csvf amcache_output.csv

# Analyze output
Import-Csv amcache_output.csv |
    Where-Object {$_.Path -like "*\Temp\*" -or $_.Path -like "*\AppData\*"} |
    Select-Object Path, FileKeyLastWriteTimestamp, SHA1 |
    Sort-Object FileKeyLastWriteTimestamp -Descending |
    Format-Table -AutoSize
```

**PowerShell Script Block Logging**

Captures PowerShell commands and scripts executed.

```powershell
# Check if script block logging is enabled
Get-ItemProperty "HKLM:\SOFTWARE\Policies\Microsoft\Windows\PowerShell\ScriptBlockLogging" -ErrorAction SilentlyContinue

# Enable script block logging
New-Item -Path "HKLM:\SOFTWARE\Policies\Microsoft\Windows\PowerShell\ScriptBlockLogging" -Force
Set-ItemProperty -Path "HKLM:\SOFTWARE\Policies\Microsoft\Windows\PowerShell\ScriptBlockLogging" -Name "EnableScriptBlockLogging" -Value 1

# Query PowerShell event log (Event ID 4104)
Get-WinEvent -LogName "Microsoft-Windows-PowerShell/Operational" -FilterXPath "*[System[EventID=4104]]" -MaxEvents 100 |
    Select-Object TimeCreated, Message |
    Format-List

# Extract script content
Get-WinEvent -LogName "Microsoft-Windows-PowerShell/Operational" -FilterXPath "*[System[EventID=4104]]" |
    ForEach-Object {
        $xml = [xml]$_.ToXml()
        $scriptBlock = ($xml.Event.EventData.Data | Where-Object {$_.Name -eq 'ScriptBlockText'}).'#text'
        
        [PSCustomObject]@{
            Time = $_.TimeCreated
            ScriptBlock = $scriptBlock
        }
    } | Where-Object {$_.ScriptBlock -match "(Invoke-|Download|IEX|Net\.WebClient)"} |
    Format-List
```

**WMI Event Logs (Suspicious Automation)**

```powershell
# WMI activity logs
Get-WinEvent -LogName "Microsoft-Windows-WMI-Activity/Operational" -MaxEvents 100

# Filter for WMI process execution (Event ID 5857/5858/5859/5860/5861)
Get-WinEvent -LogName "Microsoft-Windows-WMI-Activity/Operational" |
    Where-Object {$_.Id -in @(5857,5858,5859,5860,5861)} |
    Select-Object TimeCreated, Id, Message |
    Format-List
```

### CTF-Specific Process Analysis Patterns

**Detect reverse shells:**

```bash
# Linux: Look for network connections from shells
ausearch -sc execve -i | grep -E "(bash|sh|nc|netcat|socat).*-e.*\/bin"

# Check for /dev/tcp usage (bash reverse shell)
grep -r "/dev/tcp" /home/*/.bash_history /root/.bash_history

# Find processes with ESTABLISHED connections
lsof -i -n | grep ESTABLISHED | grep -E "(bash|sh|python|perl|ruby)"

# Windows: PowerShell reverse shells
Get-WinEvent -LogName "Microsoft-Windows-Sysmon/Operational" -FilterXPath "*[System[EventID=1]]" |
    Where-Object {$_.Message -match "New-Object.*Net\.Sockets\.TCPClient|System\.Net\.Sockets\.TcpClient"}
```

**Identify privilege escalation attempts:**

```bash
# Linux: SUID binary execution
ausearch -sc execve -k process_execution | grep -i "euid=0" | grep -v "auid=0"

# Sudo usage by non-root
ausearch -sc execve -i | grep -i "sudo" | grep -v "auid=0"

# Windows: Process creation with elevation
Get-WinEvent -FilterHashtable @{LogName='Security'; ID=4688} |
    Where-Object {$_.Message -match "Token Elevation Type.*Full"} |
    Select-Object TimeCreated, Message |
    Format-List
```

**Find credential dumping activities:**

```powershell
# Windows: Detect mimikatz, procdump on lsass
Get-WinEvent -LogName "Microsoft-Windows-Sysmon/Operational" -FilterXPath "*[System[EventID=1]]" |
    Where-Object {
        $_.Message -match "mimikatz" -or
        ($_.Message -match "procdump" -and $_.Message -match "lsass") -or
        $_.Message -match "sekurlsa|logonpasswords"
    } | Select-Object TimeCreated, Message | Format-List

# Detect LSASS memory access (Sysmon Event ID 10)
Get-WinEvent -LogName "Microsoft-Windows-Sysmon/Operational" -FilterXPath "*[System[EventID=10]]" |
    Where-Object {$_.Message -match "TargetImage.*lsass\.exe"} |
    Select-Object TimeCreated, Message | Format-List
```

**Timeline reconstruction:**

```bash
#!/bin/bash
# Create unified process execution timeline (Linux)

{
    # Audit logs
    ausearch -sc execve -i --start today --format text | \
    awk '/^time->/{time=$2" "$3} /exe=/{match($0,/exe="([^"]+)"/,a); print time, "AUDIT", a[1]}'
    
    # Bash history (with timestamps if available)
    for hist in /home/*/.bash_history /root/.bash_history; do
        [ -f "$hist" ] && awk '/^#[0-9]+/{ts=$0; gsub("#","",ts); getline; print strftime("%Y-%m-%d %H:%M:%S",ts), "BASH", $0}' "$hist"
    done
    
    # System journal
    journalctl --since today --output=short-iso | grep -i "executed\|started" | \
    awk '{print $1, "JOURNAL", $0}'
    
} | sort | less
```

---

## File Access Logs

File access logs track read, write, modify, and delete operations on files and directories. Critical for identifying data exfiltration, malware deployment, and unauthorized configuration changes.

### Linux File Access Monitoring

**auditd File Access Rules**

```bash
# Monitor specific file for any access
auditctl -w /etc/passwd -p rwa -k passwd_access
auditctl -w /etc/shadow -p rwa -k shadow_access
auditctl -w /etc/sudoers -p rwa -k sudoers_changes

# Monitor directory recursively
auditctl -w /home/ -p rwa -k home_access
auditctl -w /var/www/ -p rwa -k web_files

# Monitor sensitive configuration directories
auditctl -w /etc/ssh/ -p wa -k ssh_config_changes
auditctl -w /root/.ssh/ -p wa -k root_ssh_keys

# Monitor temporary directories (common attack staging)
auditctl -w /tmp -p wa -k tmp_changes
auditctl -w /dev/shm -p wa -k shm_changes
auditctl -w /var/tmp -p wa -k vartmp_changes

# System binary modifications
auditctl -w /bin/ -p wa -k bin_modifications
auditctl -w /sbin/ -p wa -k sbin_modifications
auditctl -w /usr/bin/ -p wa -k usrbin_modifications
```

**Query file access events:**

```bash
# Search by file path
ausearch -f /etc/passwd

# Search by audit key
ausearch -k passwd_access

# File access in time range
ausearch -f /etc/shadow --start 10:00 --end 11:00

# Show file access by specific user
ausearch -f /var/www/html/config.php -ua 1000 -i

# Summary of file accesses
aureport -f -i --summary

# Detailed file access report
aureport -f -i --start today
```

**Parse audit logs for file operations:**

```bash
#!/bin/bash
# Extract file access timeline

ausearch -k passwd_access -i --format text | \
awk '
BEGIN { FS="[ =]+" }
/^----/ {
    if (time != "" && file != "") {
        print time, user, syscall, file, result
    }
    time=""; user=""; syscall=""; file=""; result=""
}
/^time/ { time=$3" "$4 }
/syscall=/ { syscall=$3 }
/name=/ && name=="" { 
    gsub(/"/, "", $3)
    file=$3
}
/auid=/ { user=$3 }
/success=/ { result=$3 }
'
```

**inotify (Real-Time Monitoring)**

```bash
# Install inotify-tools
apt-get install inotify-tools

# Monitor directory for changes
inotifywait -m -r /etc/ -e modify,create,delete,moved_to,moved_from

# Log to file with timestamps
inotifywait -m -r -e modify,create,delete --timefmt '%Y-%m-%d %H:%M:%S' --format '%T %e %w%f' /var/www/ | tee -a file_access.log

# Monitor specific events
inotifywait -m /etc/passwd -e open,access,modify,attrib

# Script to alert on specific file changes
while inotifywait -e modify /etc/sudoers; do
    echo "ALERT: sudoers file modified at $(date)" | tee -a alerts.log
    cp /etc/sudoers /var/log/sudoers.$(date +%s).backup
done
```

**File System Timestamps (Forensic Analysis)**

```bash
# View file timestamps (access, modify, change)
stat /etc/passwd

# Find files modified in last 24 hours
find /etc -type f -mtime -1 -ls

# Files accessed in last hour
find /var/www -type f -amin -60 -ls

# Files with changed metadata (ctime) in last day
find /home -type f -ctime -1 -ls

# Sort by modification time
ls -lt /tmp/ | head -20

# Find recently modified config files
find /etc -name "*.conf" -mtime -7 -exec ls -lh {} \;

# Timeline of /etc changes
find /etc -type f -printf "%T+ %p\n" | sort | tail -50
```

**Detect timestomp (timestamp manipulation):**

```bash
# Look for files where mtime is older than ctime (suspicious)
find /var/www -type f -printf "%T@ %C@ %p\n" | \
awk '{if ($1 < $2) print $3, "mtime:", strftime("%Y-%m-%d %H:%M:%S", $1), "ctime:", strftime("%Y-%m-%d %H:%M:%S", $2)}'

# Files with future timestamps
find / -type f -newermt "$(date)" 2>/dev/null

# Files with timestamps before installation date
# (requires knowing system install date)
find /usr/bin -type f -not -newermt "2023-01-01" -ls
```

**lsof (Open Files)**

```bash
# Show files opened by process
lsof -p <PID>

# Files opened by user
lsof -u alice

# Files open in directory
lsof +D /var/www

# Network connections and files (combined view)
lsof -i -a -u www-data

# Files deleted but still open (data recovery opportunity)
lsof | grep deleted

# Monitor file access in real-time
lsof -r 2 +D /tmp  # Repeat every 2 seconds
```

### Windows File Access Monitoring

**Windows Event Logs - File Auditing**

Requires Object Access auditing to be enabled.

```powershell
# Enable file/folder auditing via Group Policy or:
# 1. Right-click folder > Properties > Security > Advanced
# 2. Auditing tab > Add > Select principal
# 3. Choose events to audit (Read, Write, Delete, etc.)

# Enable via PowerShell (requires admin)
$path = "C:\SensitiveData"
$acl = Get-Acl $path
$auditRule = New-Object System.Security.AccessControl.FileSystemAuditRule(
    "Everyone",
    "Read,Write,Delete",
    "ContainerInherit,ObjectInherit",
    "None",
    "Success,Failure"
)
$acl.AddAuditRule($auditRule)
Set-Acl $path $acl

# Enable object access auditing policy
auditpol /set /subcategory:"File System" /success:enable /failure:enable
```

**Query file access events:**

```powershell
# Event ID 4663: File/Object Access
Get-WinEvent -FilterHashtable @{LogName='Security'; ID=4663} -MaxEvents 100 |
    Select-Object TimeCreated, Message |
    Format-List

# Extract structured data
Get-WinEvent -FilterHashtable @{LogName='Security'; ID=4663} |
    ForEach-Object {
        $xml = [xml]$_.ToXml()
        $eventData = @{}
        $xml.Event.EventData.Data | ForEach-Object { $eventData[$_.Name] = $_.'#text' }
        
        [PSCustomObject]@{
            Time = $_.TimeCreated
            User = $eventData.SubjectUserName
            ObjectName = $eventData.ObjectName
            AccessMask = $eventData.AccessMask
            AccessList = $eventData.AccessList
            ProcessName = $eventData.ProcessName
        }
    } | Format-Table -AutoSize

# Filter by specific file/directory
Get-WinEvent -FilterHashtable @{LogName='Security'; ID=4663} |
    Where-Object {$_.Message -like "*C:\Users\Public\*"} |
    Select-Object TimeCreated, Message
```

**Interpret AccessMask values:**

```powershell
# Common AccessMask hex values:
# 0x1     - ReadData/ListDirectory
# 0x2     - WriteData/AddFile
# 0x4     - AppendData/AddSubdirectory
# 0x10    - ReadEA
# 0x20    - WriteEA
# 0x80    - ReadAttributes
# 0x100   - WriteAttributes
# 0x10000 - Delete
# 0x20000 - ReadControl
# 0x40000 - WriteDac
# 0x80000 - WriteOwner

# Decode AccessMask
function Decode-AccessMask {
    param([string]$mask)
    
    $maskValue = [Convert]::ToInt32($mask.Replace("0x",""), 16)
    $access = @()
    
    if ($maskValue -band 0x1) { $access += "READ_DATA" }
    if ($maskValue -band 0x2) { $access += "WRITE_DATA" }
    if ($maskValue -band 0x4) { $access += "APPEND_DATA" }
    if ($maskValue -band 0x10000) { $access += "DELETE" }
    if ($maskValue -band 0x40000) { $access += "WRITE_DAC" }
    
    return $access -join ", "
}

# Apply to events
Get-WinEvent -FilterHashtable @{LogName='Security'; ID=4663} -MaxEvents 10 |
    ForEach-Object {
        $xml = [xml]$_.ToXml()
        $mask = ($xml.Event.EventData.Data | Where-Object {$_.Name -eq 'AccessMask'}).'#text'
        [PSCustomObject]@{
            Time = $_.TimeCreated
            File = ($xml.Event.EventData.Data | Where-Object {$_.Name -eq 'ObjectName'}).'#text'
            AccessType = Decode-AccessMask $mask
        }
    } | Format-Table -AutoSize
```

**Sysmon File Creation (Event ID 11)**

```
# Query Sysmon file creation events
Get-WinEvent -LogName "Microsoft-Windows-Sysmon/Operational" -FilterXPath "*[System[EventID=11]]" -MaxEvents 100 |
ForEach-Object {
    $xml = [xml]$_.ToXml()
    $eventData = @{}

    $xml.Event.EventData.Data | ForEach-Object {
        $eventData[$_.Name] = $_.'#text'
    }

    [PSCustomObject]@{
        UtcTime         = $eventData.UtcTime
        ProcessGuid     = $eventData.ProcessGuid
        Image           = $eventData.Image
        TargetFilename  = $eventData.TargetFilename
        CreationUtcTime = $eventData.CreationUtcTime
    }
} | Format-Table -AutoSize


# Find files created in suspicious locations
Get-WinEvent -LogName "Microsoft-Windows-Sysmon/Operational" -FilterXPath "*[System[EventID=11]]" |
Where-Object {
    $_.Message -match "(\AppData\Local\Temp\|\Users\Public\|\ProgramData\)" -and
    $_.Message -match ".(exe|dll|ps1|vbs|bat|cmd)$"
} |
Select-Object TimeCreated, Message |
Format-List


# Track file creation by specific process
Get-WinEvent -LogName "Microsoft-Windows-Sysmon/Operational" -FilterXPath "*[System[EventID=11]]" |
Where-Object { $_.Message -like "*powershell.exe*" } |
ForEach-Object {
    $xml = [xml]$_.ToXml()
    ($xml.Event.EventData.Data | Where-Object { $_.Name -eq 'TargetFilename' }).'#text'
} | Sort-Object -Unique
````

**USN Journal (Change Journal)**

The NTFS USN (Update Sequence Number) Journal tracks all file system changes.

```powershell
# Query USN journal with fsutil
fsutil usn queryjournal C:

# Read USN journal records (requires admin)
fsutil usn readjournal C: csv

# [Unverified] - Manual USN journal parsing requires understanding binary format or specialized tools

# Use third-party tools for detailed analysis:
# - MFTECmd.exe (Eric Zimmerman)
# - ExtractUsnJrnl.exe (TZWorks)
# - Analyze-UsnJournal.ps1 (various GitHub repositories)
````

**Parse USN Journal with PowerShell (limited):**

```powershell
# Capture USN journal to file
fsutil usn readjournal C: > usn_journal.txt

# Parse for specific operations
Select-String -Path usn_journal.txt -Pattern "DATA_OVERWRITE|DATA_EXTEND|FILE_DELETE" |
    Select-Object -First 50

# Filter by filename pattern
Select-String -Path usn_journal.txt -Pattern "\.exe|\.dll|\.sys" |
    Where-Object {$_ -match "FILE_CREATE|FILE_DELETE"}
```

**$MFT Analysis (Master File Table)**

The MFT contains metadata for all files, including timestamps.

```powershell
# Extract MFT for analysis (requires external tools)
# Example with RawCopy (must run with SYSTEM privileges)
.\RawCopy64.exe /FileNamePath:C:\$MFT /OutputPath:C:\forensics\

# Parse MFT with MFTECmd (Eric Zimmerman)
.\MFTECmd.exe -f "C:\forensics\$MFT" --csv . --csvf mft_analysis.csv

# Analyze output
Import-Csv mft_analysis.csv |
    Where-Object {$_.Extension -in @('.exe','.dll','.ps1')} |
    Select-Object FileName, ParentPath, Created0x10, Modified0x10 |
    Sort-Object Modified0x10 -Descending |
    Format-Table -AutoSize

# Find files with timeline anomalies
Import-Csv mft_analysis.csv |
    Where-Object {
        $created = [DateTime]::Parse($_.Created0x10)
        $modified = [DateTime]::Parse($_.Modified0x10)
        $modified -lt $created  # Modified before created (timestomping indicator)
    } | Select-Object FileName, ParentPath, Created0x10, Modified0x10
```

**Recent File Activity Artifacts**

```powershell
# Recent files (RecentDocs registry key)
$recentDocs = "HKCU:\Software\Microsoft\Windows\CurrentVersion\Explorer\RecentDocs"
Get-ChildItem $recentDocs -Recurse | ForEach-Object {
    $ext = $_.PSChildName
    $_.GetValueNames() | Where-Object {$_ -match '^\d+$'} | ForEach-Object {
        try {
            $data = (Get-ItemProperty -Path $_.PSPath -Name $_).$_
            [PSCustomObject]@{
                Extension = $ext
                FileName = [System.Text.Encoding]::Unicode.GetString($data) -replace '\x00.*$',''
                LastWrite = (Get-Item $_.PSPath).LastWriteTime
            }
        } catch {}
    }
} | Format-Table -AutoSize

# Office recent files
Get-ChildItem "HKCU:\Software\Microsoft\Office\*\*\User MRU" -Recurse -ErrorAction SilentlyContinue |
    Get-ItemProperty |
    Select-Object PSChildName, PSPath

# Jump lists (recent application file access)
Get-ChildItem "$env:APPDATA\Microsoft\Windows\Recent\AutomaticDestinations\*.automaticDestinations-ms"

# [Inference] - Jump list files require specialized parsing tools like JLECmd.exe
```

**File Timeline Construction**

```powershell
# Create comprehensive file timeline
$targetPath = "C:\Users\Alice\Documents"

Get-ChildItem -Path $targetPath -Recurse -File |
    ForEach-Object {
        [PSCustomObject]@{
            Type = "Creation"
            Time = $_.CreationTime
            Path = $_.FullName
        }
        [PSCustomObject]@{
            Type = "LastWrite"
            Time = $_.LastWriteTime
            Path = $_.FullName
        }
        [PSCustomObject]@{
            Type = "LastAccess"
            Time = $_.LastAccessTime
            Path = $_.FullName
        }
    } | Sort-Object Time -Descending |
    Format-Table -AutoSize

# Export for analysis
Get-ChildItem -Path $targetPath -Recurse -File |
    Select-Object FullName, Length, CreationTime, LastAccessTime, LastWriteTime, Attributes |
    Export-Csv -Path file_timeline.csv -NoTypeInformation
```

### CTF-Specific File Access Patterns

**Detect data staging (exfiltration preparation):**

```bash
# Linux: Find files recently copied to staging directory
find /tmp /dev/shm /var/tmp -type f -mmin -60 -exec ls -lh {} \; | \
    grep -E "\.(zip|tar|gz|7z|rar|sql|csv|txt|pdf)$"

# Check for large archives created recently
find /home -name "*.tar.gz" -o -name "*.zip" -size +10M -mtime -1 -ls

# Windows: Recent archives
Get-ChildItem -Path C:\Users\*\AppData\Local\Temp -Include *.zip,*.7z,*.rar -Recurse -ErrorAction SilentlyContinue |
    Where-Object {$_.LastWriteTime -gt (Get-Date).AddHours(-24)} |
    Select-Object FullName, Length, LastWriteTime |
    Sort-Object Length -Descending
```

**Identify configuration file tampering:**

```bash
# Linux: Check for modified configs
find /etc -name "*.conf" -mtime -1 -exec stat {} \;

# Compare current configs with backups
diff /etc/ssh/sshd_config /etc/ssh/sshd_config.backup

# Track sudoers modifications
ausearch -f /etc/sudoers -i

# Windows: Check critical config files
Get-ChildItem C:\Windows\System32\drivers\etc\hosts, C:\inetpub\wwwroot\web.config |
    Select-Object Name, LastWriteTime, LastAccessTime
```

**Find hidden/suspicious files:**

```bash
# Linux: Hidden files in unusual locations
find /tmp /var/tmp /dev/shm -name ".*" -type f

# Files with suspicious names
find / -type f -name "..." -o -name ".." -o -name ". " 2>/dev/null

# SUID files created recently
find / -perm -4000 -mtime -7 -ls 2>/dev/null

# Windows: Hidden files
Get-ChildItem -Path C:\ -Recurse -Force -ErrorAction SilentlyContinue |
    Where-Object {$_.Attributes -match "Hidden"} |
    Select-Object FullName, LastWriteTime

# Alternate Data Streams (ADS) - hidden data
Get-Item "C:\Users\*\*" -Stream * -ErrorAction SilentlyContinue |
    Where-Object {$_.Stream -ne ':$DATA'}
```

**Malware dropper analysis:**

```powershell
# Track files created by suspicious process
$suspiciousPID = 1234

# Using Sysmon
Get-WinEvent -LogName "Microsoft-Windows-Sysmon/Operational" -FilterXPath "*[System[EventID=11]]" |
    ForEach-Object {
        $xml = [xml]$_.ToXml()
        $processId = ($xml.Event.EventData.Data | Where-Object {$_.Name -eq 'ProcessId'}).'#text'
        if ($processId -eq $suspiciousPID) {
            ($xml.Event.EventData.Data | Where-Object {$_.Name -eq 'TargetFilename'}).'#text'
        }
    } | Sort-Object -Unique

# Check if dropped files still exist
$droppedFiles = @("C:\Temp\malware.exe", "C:\Users\Public\backdoor.dll")
$droppedFiles | ForEach-Object {
    if (Test-Path $_) {
        Get-FileHash $_ -Algorithm SHA256
        Get-ItemProperty $_
    } else {
        Write-Host "File deleted: $_"
    }
}
```

---

## Registry Modifications (Windows)

The Windows Registry stores system and application configuration. Registry modifications reveal persistence mechanisms, malware configuration, privilege escalation, and application exploitation.

### Registry Monitoring and Auditing

**Enable Registry Auditing**

```powershell
# Enable registry auditing via auditpol
auditpol /set /subcategory:"Registry" /success:enable /failure:enable

# Set auditing on specific registry key
$regPath = "HKLM:\Software\Microsoft\Windows\CurrentVersion\Run"
$acl = Get-Acl $regPath
$auditRule = New-Object System.Security.AccessControl.RegistryAuditRule(
    "Everyone",
    "SetValue,CreateSubKey,Delete",
    "ContainerInherit",
    "None",
    "Success"
)
$acl.AddAuditRule($auditRule)
Set-Acl $regPath $acl
```

**Windows Event Logs - Registry Modification**

```powershell
# Event ID 4657: Registry value modification
Get-WinEvent -FilterHashtable @{LogName='Security'; ID=4657} -MaxEvents 100 |
    Select-Object TimeCreated, Message |
    Format-List

# Parse structured data
Get-WinEvent -FilterHashtable @{LogName='Security'; ID=4657} |
    ForEach-Object {
        $xml = [xml]$_.ToXml()
        $eventData = @{}
        $xml.Event.EventData.Data | ForEach-Object { $eventData[$_.Name] = $_.'#text' }
        
        [PSCustomObject]@{
            Time = $_.TimeCreated
            User = $eventData.SubjectUserName
            ProcessName = $eventData.ProcessName
            ObjectName = $eventData.ObjectName
            OperationType = $eventData.OperationType
            OldValue = $eventData.OldValue
            NewValue = $eventData.NewValue
        }
    } | Format-Table -AutoSize

# Filter by specific registry path
Get-WinEvent -FilterHashtable @{LogName='Security'; ID=4657} |
    Where-Object {$_.Message -like "*CurrentVersion\Run*"} |
    Select-Object TimeCreated, Message
```

**Sysmon Registry Events**

```powershell
# Event ID 12: Registry object added or deleted
# Event ID 13: Registry value set
# Event ID 14: Registry object renamed

# Query registry modifications
Get-WinEvent -LogName "Microsoft-Windows-Sysmon/Operational" -FilterXPath "*[System[(EventID=12 or EventID=13 or EventID=14)]]" -MaxEvents 100 |
    ForEach-Object {
        $xml = [xml]$_.ToXml()
        $eventData = @{}
        $xml.Event.EventData.Data | ForEach-Object { $eventData[$_.Name] = $_.'#text' }
        
        [PSCustomObject]@{
            EventID = $_.Id
            EventType = switch($_.Id) {
                12 {"CreateKey/DeleteKey"}
                13 {"SetValue"}
                14 {"RenameKey"}
            }
            UtcTime = $eventData.UtcTime
            ProcessGuid = $eventData.ProcessGuid
            Image = $eventData.Image
            TargetObject = $eventData.TargetObject
            Details = $eventData.Details
        }
    } | Format-Table -AutoSize

# Find registry modifications by suspicious processes
Get-WinEvent -LogName "Microsoft-Windows-Sysmon/Operational" -FilterXPath "*[System[EventID=13]]" |
    Where-Object {
        $_.Message -match "(powershell\.exe|cmd\.exe|wscript\.exe|cscript\.exe)" -and
        $_.Message -match "(\\Run\\|\\RunOnce\\|\\Services\\|\\Winlogon\\)"
    } | Select-Object TimeCreated, Message | Format-List
```

### Critical Registry Keys for Persistence

**Autostart locations:**

```powershell
# Run keys (most common persistence)
$runKeys = @(
    "HKLM:\Software\Microsoft\Windows\CurrentVersion\Run",
    "HKLM:\Software\Microsoft\Windows\CurrentVersion\RunOnce",
    "HKCU:\Software\Microsoft\Windows\CurrentVersion\Run",
    "HKCU:\Software\Microsoft\Windows\CurrentVersion\RunOnce",
    "HKLM:\Software\Wow6432Node\Microsoft\Windows\CurrentVersion\Run"
)

$runKeys | ForEach-Object {
    if (Test-Path $_) {
        Write-Output "`n=== $_ ==="
        Get-ItemProperty $_ | 
            Select-Object * -ExcludeProperty PS* |
            Format-List
    }
}

# Check for modifications in last 7 days
$runKeys | ForEach-Object {
    if (Test-Path $_) {
        $key = Get-Item $_
        if ($key.LastWriteTime -gt (Get-Date).AddDays(-7)) {
            [PSCustomObject]@{
                Path = $_.Path
                LastModified = $key.LastWriteTime
            }
        }
    }
}

# Startup folder (also persistence location)
Get-ChildItem "C:\Users\*\AppData\Roaming\Microsoft\Windows\Start Menu\Programs\Startup" -Recurse -ErrorAction SilentlyContinue
Get-ChildItem "C:\ProgramData\Microsoft\Windows\Start Menu\Programs\Startup" -ErrorAction SilentlyContinue
```

**Service persistence:**

```powershell
# Services registry
Get-ChildItem "HKLM:\System\CurrentControlSet\Services" |
    Get-ItemProperty |
    Where-Object {$_.ImagePath -and $_.ImagePath -notmatch "system32"} |
    Select-Object PSChildName, ImagePath, DisplayName, Start |
    Format-Table -AutoSize

# Find services created recently
Get-ChildItem "HKLM:\System\CurrentControlSet\Services" |
    Where-Object {$_.LastWriteTime -gt (Get-Date).AddDays(-7)} |
    ForEach-Object {
        $props = Get-ItemProperty $_.PSPath
        [PSCustomObject]@{
            ServiceName = $_.PSChildName
            ImagePath = $props.ImagePath
            DisplayName = $props.DisplayName
            Start = $props.Start
            LastModified = $_.LastWriteTime
        }
    } | Format-Table -AutoSize

# Suspicious service paths
Get-ChildItem "HKLM:\System\CurrentControlSet\Services" |
    Get-ItemProperty |
    Where-Object {$_.ImagePath -match "(\\Temp\\|\\AppData\\|\\Users\\Public\\)"} |
    Select-Object PSChildName, ImagePath
```

**WinLogon persistence:**

```powershell
# Winlogon entries
$winlogonPath = "HKLM:\Software\Microsoft\Windows NT\CurrentVersion\Winlogon"
Get-ItemProperty $winlogonPath |
    Select-Object Userinit, Shell, AutoAdminLogon, DefaultUserName |
    Format-List

# Notify key (DLL loading on logon)
Get-ChildItem "HKLM:\Software\Microsoft\Windows NT\CurrentVersion\Winlogon\Notify" -ErrorAction SilentlyContinue

# Check for modifications
$winlogon = Get-Item $winlogonPath
if ($winlogon.LastWriteTime -gt (Get-Date).AddDays(-30)) {
    Write-Output "Winlogon key modified: $($winlogon.LastWriteTime)"
}
```

**Scheduled Tasks (Registry-based):**

```powershell
# Task Scheduler registry location
Get-ChildItem "HKLM:\Software\Microsoft\Windows NT\CurrentVersion\Schedule\TaskCache\Tasks" |
    ForEach-Object {
        $task = Get-ItemProperty $_.PSPath
        [PSCustomObject]@{
            TaskName = $task.PSChildName
            Actions = $task.Actions
            Triggers = $task.Triggers
            Author = $task.Author
        }
    } | Format-Table -AutoSize

# Better approach: Use Get-ScheduledTask cmdlet
Get-ScheduledTask |
    Where-Object {$_.TaskPath -notlike "\Microsoft\*"} |
    ForEach-Object {
        $info = Get-ScheduledTaskInfo $_
        [PSCustomObject]@{
            TaskName = $_.TaskName
            TaskPath = $_.TaskPath
            State = $_.State
            Actions = ($_.Actions | ForEach-Object {$_.Execute})
            LastRunTime = $info.LastRunTime
            NextRunTime = $info.NextRunTime
        }
    } | Format-Table -AutoSize
```

**AppInit_DLLs (DLL injection):**

```powershell
# AppInit_DLLs (loads DLLs into every process)
Get-ItemProperty "HKLM:\Software\Microsoft\Windows NT\CurrentVersion\Windows" |
    Select-Object AppInit_DLLs, LoadAppInit_DLLs |
    Format-List

Get-ItemProperty "HKLM:\Software\Wow6432Node\Microsoft\Windows NT\CurrentVersion\Windows" -ErrorAction SilentlyContinue |
    Select-Object AppInit_DLLs, LoadAppInit_DLLs |
    Format-List
```

**Image File Execution Options (IFEO):**

```powershell
# IFEO - debugger hijacking
Get-ChildItem "HKLM:\Software\Microsoft\Windows NT\CurrentVersion\Image File Execution Options" |
    ForEach-Object {
        $debugger = (Get-ItemProperty $_.PSPath -ErrorAction SilentlyContinue).Debugger
        if ($debugger) {
            [PSCustomObject]@{
                Image = $_.PSChildName
                Debugger = $debugger
            }
        }
    } | Format-Table -AutoSize
```

### Registry Snapshot and Comparison

**Create registry baseline:**

```powershell
# Export critical keys
$criticalKeys = @(
    "HKLM\Software\Microsoft\Windows\CurrentVersion\Run",
    "HKLM\Software\Microsoft\Windows\CurrentVersion\RunOnce",
    "HKCU\Software\Microsoft\Windows\CurrentVersion\Run",
    "HKLM\System\CurrentControlSet\Services"
)

$criticalKeys | ForEach-Object {
    $filename = ($_ -replace '\\','_') + ".reg"
    reg export $_ "C:\baseline\$filename" /y
}

# PowerShell approach
$baseline = @{}
$runKey = "HKLM:\Software\Microsoft\Windows\CurrentVersion\Run"
Get-ItemProperty $runKey | ForEach-Object {
    $_.PSObject.Properties | Where-Object {$_.Name -notlike "PS*"} | ForEach-Object {
        $baseline[$_.Name] = $_.Value
    }
}
$baseline | ConvertTo-Json | Out-File baseline_run.json
```

**Compare against baseline:**

```powershell
# Load baseline
$baseline = Get-Content baseline_run.json | ConvertFrom-Json

# Get current state
$current = @{}
$runKey = "HKLM:\Software\Microsoft\Windows\CurrentVersion\Run"
Get-ItemProperty $runKey | ForEach-Object {
    $_.PSObject.Properties | Where-Object {$_.Name -notlike "PS*"} | ForEach-Object {
        $current[$_.Name] = $_.Value
    }
}

# Find differences
Write-Output "=== New Entries ==="
$current.Keys | Where-Object {$_ -notin $baseline.PSObject.Properties.Name} |
    ForEach-Object {
        [PSCustomObject]@{
            Name = $_
            Value = $current[$_]
        }
    } | Format-Table -AutoSize

Write-Output "`n=== Removed Entries ==="
$baseline.PSObject.Properties.Name | Where-Object {$_ -notin $current.Keys} |
    ForEach-Object { $_ }

Write-Output "`n=== Modified Entries ==="
$current.Keys | Where-Object {
    $_ -in $baseline.PSObject.Properties.Name -and
    $current[$_] -ne $baseline.$_
} | ForEach-Object {
    [PSCustomObject]@{
        Name = $_
        OldValue = $baseline.$_
        NewValue = $current[$_]
    }
} | Format-Table -AutoSize
```

### Registry Forensics Tools

**RegRipper**

```bash
# RegRipper - automated registry analysis
# Download from: https://github.com/keydet89/RegRipper3.0

# Extract registry hives from Windows system
# SAM, SECURITY, SOFTWARE, SYSTEM from C:\Windows\System32\config
# NTUSER.DAT from C:\Users\<username>\

# Run RegRipper
rip.pl -r SOFTWARE -p software_run

# Run all plugins
rip.pl -r SYSTEM -a

# Custom plugin for specific analysis
rip.pl -r NTUSER.DAT -p userassist
```

**Registry Explorer (Eric Zimmerman)**

```powershell
# Load registry hive in Registry Explorer GUI
# Or use command-line tool RECmd

.\RECmd.exe --hive C:\forensics\SOFTWARE --BatchName UserActivity

# Export to CSV for analysis
.\RECmd.exe --hive C:\forensics\SYSTEM --BatchName System --csv C:\output\
```

**Manual registry hive analysis:**

```powershell
# Load offline registry hive
reg load HKLM\OFFLINE C:\forensics\SOFTWARE

# Query loaded hive
reg query HKLM\OFFLINE\Microsoft\Windows\CurrentVersion\Run

# Unload when finished
reg unload HKLM\OFFLINE
```

### CTF-Specific Registry Analysis

**Find credential storage:**

```powershell
# Stored credentials
cmdkey /list

# Check registry for stored passwords (weak encryption)
Get-ItemProperty "HKCU:\Software\*\*" -ErrorAction SilentlyContinue |
    Where-Object {$_.PSObject.Properties.Name -match "password|pwd|pass"} |
    Select-Object PSPath, *password*, *pwd*, *pass* |
    Format-List

# VNC passwords (often stored weakly)
Get-ItemProperty "HKCU:\Software\RealVNC\WinVNC4" -ErrorAction SilentlyContinue
Get-ItemProperty "HKLM:\Software\RealVNC\WinVNC4" -ErrorAction SilentlyContinue

# Putty sessions (may contain credentials)
Get-ChildItem "HKCU:\Software\SimonTatham\PuTTY\Sessions" -ErrorAction SilentlyContinue |
    ForEach-Object {
        Get-ItemProperty $_.PSPath |
            Select-Object PSChildName, HostName, UserName, ProxyPassword
    } | Format-Table -AutoSize
```

**Detect privilege escalation artifacts:**

```powershell
# Check for AlwaysInstallElevated (allows user MSI installs as SYSTEM)
$hklm = Get-ItemProperty "HKLM:\Software\Policies\Microsoft\Windows\Installer" -ErrorAction SilentlyContinue
$hkcu = Get-ItemProperty "HKCU:\Software\Policies\Microsoft\Windows\Installer" -ErrorAction SilentlyContinue

if ($hklm.AlwaysInstallElevated -eq 1 -and $hkcu.AlwaysInstallElevated -eq 1) {
    Write-Output "[!] AlwaysInstallElevated is enabled - privilege escalation vector!"
}

# Check UAC settings
Get-ItemProperty "HKLM:\Software\Microsoft\Windows\CurrentVersion\Policies\System" |
    Select-Object EnableLUA, ConsentPromptBehaviorAdmin, PromptOnSecureDesktop |
    Format-List

# Auto-logon credentials (often in cleartext)
$winlogon = Get-ItemProperty "HKLM:\Software\Microsoft\Windows NT\CurrentVersion\Winlogon"
if ($winlogon.AutoAdminLogon -eq "1") {
    Write-Output "[!] Auto-logon enabled"
    Write-Output "Username: $($winlogon.DefaultUserName)"
    Write-Output "Domain: $($winlogon.DefaultDomainName)"
    Write-Output "Password: $($winlogon.DefaultPassword)"
}
```

**Application-specific artifacts:**

```powershell
# Recent documents
Get-ItemProperty "HKCU:\Software\Microsoft\Windows\CurrentVersion\Explorer\ComDlg32\OpenSavePidlMRU\*" -ErrorAction SilentlyContinue

# Typed URLs (Internet Explorer/Edge)
Get-ItemProperty "HKCU:\Software\Microsoft\Internet Explorer\TypedURLs" -ErrorAction SilentlyContinue |
    Select-Object url* |
    Format-List

# Run MRU (commands from Run dialog)
Get-ItemProperty "HKCU:\Software\Microsoft\Windows\CurrentVersion\Explorer\RunMRU" -ErrorAction SilentlyContinue

# Mounted devices
Get-ItemProperty "HKLM:\System\MountedDevices" -ErrorAction SilentlyContinue |
    Select-Object *Drive* |
    Format-List

# USB device history
Get-ItemProperty "HKLM:\System\CurrentControlSet\Enum\USBSTOR\*\*" -ErrorAction SilentlyContinue |
    Select-Object FriendlyName, Mfg, Service
```

**Timeline reconstruction from registry:**

```powershell
# Get registry key modification times
function Get-RegKeyTimestamp {
    param([string]$Path)
    
    $key = Get-Item $Path -ErrorAction SilentlyContinue
    if ($key) {
        [PSCustomObject]@{
            Path = $Path
            LastWriteTime = $key.LastWriteTime
        }
    }
}

# Check multiple persistence locations
$persistencePaths = @(
    "HKLM:\Software\Microsoft\Windows\CurrentVersion\Run",
    "HKCU:\Software\Microsoft\Windows\CurrentVersion\Run",
    "HKLM:\System\CurrentControlSet\Services\*"
)

$persistencePaths | ForEach-Object {
    if ($_ -like "*\*") {
        Get-ChildItem $_ -ErrorAction SilentlyContinue | ForEach-Object {
            Get-RegKeyTimestamp $_.PSPath
        }
    } else {
        Get-RegKeyTimestamp $_
    }
} | Sort-Object LastWriteTime -Descending | Format-Table -AutoSize
```

**Automated suspicious registry checker:**

```powershell
#!/usr/bin/env powershell
# Automated registry IOC checker

$suspiciousIndicators = @()

# Check Run keys
$runKeys | ForEach-Object {
    if (Test-Path $_) {
        Get-ItemProperty $_ | ForEach-Object {
            $_.PSObject.Properties | Where-Object {$_.Name -notlike "PS*"} | ForEach-Object {
                if ($_.Value -match "(\\Temp\\|\\AppData\\Local\\Temp\\|\\Public\\|powershell.*-enc|cmd.*/c)") {
                    $suspiciousIndicators += [PSCustomObject]@{
                        Type = "Suspicious Run Key"
                        Location = $runKeys
                        Name = $_.Name
                        Value = $_.Value
                    }
                }
            }
        }
    }
}

# Check for debugging hijacks
Get-ChildItem "HKLM:\Software\Microsoft\Windows NT\CurrentVersion\Image File Execution Options" | ForEach-Object {
    $debugger = (Get-ItemProperty $_.PSPath -ErrorAction SilentlyContinue).Debugger
    if ($debugger -and $debugger -notmatch "vsjitdebugger") {
        $suspiciousIndicators += [PSCustomObject]@{
            Type = "IFEO Debugger Hijack"
            Location = $_.PSPath
            Name = $_.PSChildName
            Value = $debugger
        }
    }
}

# Check for AppInit_DLLs
$appInit = Get-ItemProperty "HKLM:\Software\Microsoft\Windows NT\CurrentVersion\Windows" -ErrorAction SilentlyContinue
if ($appInit.AppInit_DLLs) {
    $suspiciousIndicators += [PSCustomObject]@{
        Type = "AppInit_DLLs"
        Location = "HKLM:\Software\Microsoft\Windows NT\CurrentVersion\Windows"
        Name = "AppInit_DLLs"
        Value = $appInit.AppInit_DLLs
    }
}

# Display findings
if ($suspiciousIndicators.Count -gt 0) {
    Write-Output "=== SUSPICIOUS REGISTRY ARTIFACTS FOUND ==="
    $suspiciousIndicators | Format-Table -AutoSize
} else {
    Write-Output "No suspicious registry artifacts detected"
}
```

---

## Important Related Topics

For comprehensive incident response log analysis in CTF environments, consider exploring these interconnected areas:

- **Memory Forensics (Volatility Framework)** - Extract process memory, registry hives, and credentials from RAM dumps
- **Event Log Analysis (Windows Event Viewer, PowerShell)** - Correlate process execution, file access, and registry changes with authentication and security events
- **Timeline Analysis (Plaso/log2timeline)** - Create super-timelines combining multiple log sources for comprehensive incident reconstruction
- **YARA Rules and IOC Scanning** - Detect malware and suspicious patterns in files created during incident response
- **Network Traffic Analysis** - Correlate process execution with network connections to identify C2 communication
- **Windows Artifacts (Prefetch, Shimcache, AmCache)** - Additional execution artifacts that complement process logs for complete activity reconstruction
- **Linux System Logs (syslog, journald, dmesg)** - Core system event logging that provides context for process and file operations
- **Container and Virtualization Logs (Docker, Kubernetes)** - Modern application deployment platforms with distinct logging architectures

---

## Comprehensive CTF Incident Response Workflow

### Unified Log Analysis Approach

When analyzing incident response logs in CTF scenarios, use a systematic approach that combines all three log types:

**Phase 1: Initial Triage**

```bash
#!/bin/bash
# Linux triage script

echo "=== SYSTEM TRIAGE $(date) ==="

# Check currently running suspicious processes
echo -e "\n[+] Suspicious Processes:"
ps aux | grep -E "(nc|ncat|netcat|/tmp/|/dev/shm/)" | grep -v grep

# Recent process executions
echo -e "\n[+] Recent Process Executions (last hour):"
ausearch -sc execve --start recent -i 2>/dev/null | grep -A 5 "type=EXECVE"

# Recent file modifications
echo -e "\n[+] Recent File Changes (last 24h):"
find /etc /var/www /home -type f -mtime -1 -ls 2>/dev/null | head -20

# Check for persistence
echo -e "\n[+] Crontab Entries:"
cat /etc/crontab
ls -la /etc/cron.*/ 2>/dev/null

echo -e "\n[+] Systemd Services (non-standard):"
systemctl list-unit-files --type=service --state=enabled | grep -v "@"

# Network connections
echo -e "\n[+] Established Network Connections:"
netstat -tunap | grep ESTABLISHED

echo -e "\n[+] Listening Ports:"
netstat -tulpn
```

```powershell
# Windows triage script
Write-Output "=== SYSTEM TRIAGE $(Get-Date) ==="

# Suspicious processes
Write-Output "`n[+] Suspicious Processes:"
Get-Process | Where-Object {
    $_.Path -match "(\\Temp\\|\\AppData\\Local\\Temp\\|\\Public\\)" -or
    $_.ProcessName -match "(powershell|cmd|wscript|cscript)"
} | Select-Object Id, ProcessName, Path, StartTime | Format-Table -AutoSize

# Recent process creation
Write-Output "`n[+] Recent Process Creation (last hour):"
Get-WinEvent -FilterHashtable @{
    LogName='Security'
    Id=4688
    StartTime=(Get-Date).AddHours(-1)
} -ErrorAction SilentlyContinue | Select-Object -First 10 TimeCreated, Message

# Recent file creation
Write-Output "`n[+] Recent File Creation:"
Get-ChildItem C:\Users\*\AppData\Local\Temp, C:\Windows\Temp -Recurse -File -ErrorAction SilentlyContinue |
    Where-Object {$_.LastWriteTime -gt (Get-Date).AddHours(-1)} |
    Select-Object FullName, Length, LastWriteTime |
    Sort-Object LastWriteTime -Descending |
    Select-Object -First 20 | Format-Table -AutoSize

# Persistence mechanisms
Write-Output "`n[+] Run Key Entries:"
Get-ItemProperty "HKLM:\Software\Microsoft\Windows\CurrentVersion\Run" -ErrorAction SilentlyContinue |
    Select-Object * -ExcludeProperty PS* | Format-List

Write-Output "`n[+] Scheduled Tasks (non-Microsoft):"
Get-ScheduledTask | Where-Object {$_.TaskPath -notlike "\Microsoft\*"} |
    Select-Object TaskName, State, TaskPath | Format-Table -AutoSize

# Network connections
Write-Output "`n[+] Active Network Connections:"
Get-NetTCPConnection -State Established |
    Select-Object LocalAddress, LocalPort, RemoteAddress, RemotePort, OwningProcess |
    ForEach-Object {
        $_ | Add-Member -NotePropertyName ProcessName -NotePropertyValue (Get-Process -Id $_.OwningProcess).Name -PassThru
    } | Format-Table -AutoSize
```

**Phase 2: Deep Dive Timeline Analysis**

```bash
#!/bin/bash
# Create unified timeline (Linux)

OUTPUT="incident_timeline.csv"
echo "Timestamp,Source,Event_Type,Details" > $OUTPUT

# Audit logs - process execution
ausearch -sc execve --start today -i 2>/dev/null | \
awk '/^time->/{time=$2" "$3} /exe=/{match($0,/exe="([^"]+)"/,a); if(time && a[1]) print time",AUDIT,EXECVE," a[1]}' >> $OUTPUT

# File access logs
ausearch -k passwd_access --start today -i 2>/dev/null | \
awk '/^time->/{time=$2" "$3} /name=/{match($0,/name="([^"]+)"/,a); if(time && a[1]) print time",AUDIT,FILE_ACCESS," a[1]}' >> $OUTPUT

# File modifications
find /etc /var/www /home -type f -mtime -1 -printf "%T+,FILESYSTEM,FILE_MODIFY,%p\n" 2>/dev/null >> $OUTPUT

# Auth logs
grep -h "authentication failure\|Failed password\|Accepted" /var/log/auth.log* 2>/dev/null | \
awk '{print $1" "$2" "$3",AUTH,"$0}' >> $OUTPUT

# Sort timeline
sort -t',' -k1 $OUTPUT > ${OUTPUT}.sorted
mv ${OUTPUT}.sorted $OUTPUT

echo "Timeline created: $OUTPUT"
echo "Total events: $(wc -l < $OUTPUT)"
```

```powershell
# Create unified timeline (Windows)
$timeline = @()

# Process creation events
Get-WinEvent -FilterHashtable @{LogName='Security'; Id=4688} -MaxEvents 500 -ErrorAction SilentlyContinue |
    ForEach-Object {
        $xml = [xml]$_.ToXml()
        $timeline += [PSCustomObject]@{
            Timestamp = $_.TimeCreated
            Source = "Security"
            EventType = "ProcessCreation"
            Details = ($xml.Event.EventData.Data | Where-Object {$_.Name -eq 'NewProcessName'}).'#text'
        }
    }

# Sysmon process creation (if available)
Get-WinEvent -LogName "Microsoft-Windows-Sysmon/Operational" -FilterXPath "*[System[EventID=1]]" -MaxEvents 500 -ErrorAction SilentlyContinue |
    ForEach-Object {
        $xml = [xml]$_.ToXml()
        $cmdline = ($xml.Event.EventData.Data | Where-Object {$_.Name -eq 'CommandLine'}).'#text'
        $timeline += [PSCustomObject]@{
            Timestamp = $_.TimeCreated
            Source = "Sysmon"
            EventType = "ProcessCreation"
            Details = $cmdline
        }
    }

# File creation events
Get-WinEvent -LogName "Microsoft-Windows-Sysmon/Operational" -FilterXPath "*[System[EventID=11]]" -MaxEvents 500 -ErrorAction SilentlyContinue |
    ForEach-Object {
        $xml = [xml]$_.ToXml()
        $filename = ($xml.Event.EventData.Data | Where-Object {$_.Name -eq 'TargetFilename'}).'#text'
        $timeline += [PSCustomObject]@{
            Timestamp = $_.TimeCreated
            Source = "Sysmon"
            EventType = "FileCreation"
            Details = $filename
        }
    }

# Registry modifications
Get-WinEvent -LogName "Microsoft-Windows-Sysmon/Operational" -FilterXPath "*[System[EventID=13]]" -MaxEvents 500 -ErrorAction SilentlyContinue |
    ForEach-Object {
        $xml = [xml]$_.ToXml()
        $regkey = ($xml.Event.EventData.Data | Where-Object {$_.Name -eq 'TargetObject'}).'#text'
        $timeline += [PSCustomObject]@{
            Timestamp = $_.TimeCreated
            Source = "Sysmon"
            EventType = "RegistrySet"
            Details = $regkey
        }
    }

# Sort and export
$timeline | Sort-Object Timestamp | Export-Csv -Path incident_timeline.csv -NoTypeInformation
Write-Output "Timeline created: incident_timeline.csv"
Write-Output "Total events: $($timeline.Count)"
```

**Phase 3: Correlation and Pivot Analysis**

```python
#!/usr/bin/env python3
# Correlate events across log sources

import csv
from datetime import datetime, timedelta
from collections import defaultdict

def load_timeline(filename):
    """Load timeline CSV"""
    events = []
    with open(filename, 'r') as f:
        reader = csv.DictReader(f)
        for row in reader:
            try:
                row['timestamp'] = datetime.fromisoformat(row['Timestamp'].replace('Z', '+00:00'))
                events.append(row)
            except:
                continue
    return sorted(events, key=lambda x: x['timestamp'])

def find_process_tree(events, target_process):
    """Find parent-child process relationships"""
    # [Inference] - This requires parent PID data from logs
    print(f"[+] Process tree for: {target_process}")
    for event in events:
        if target_process in event.get('Details', ''):
            print(f"  {event['timestamp']} - {event['EventType']}: {event['Details']}")

def find_temporal_correlation(events, event_of_interest, time_window_seconds=300):
    """Find events within time window of interesting event"""
    target_time = event_of_interest['timestamp']
    window_start = target_time - timedelta(seconds=time_window_seconds)
    window_end = target_time + timedelta(seconds=time_window_seconds)
    
    print(f"\n[+] Events within {time_window_seconds}s of {event_of_interest['Details']}:")
    for event in events:
        if window_start <= event['timestamp'] <= window_end and event != event_of_interest:
            print(f"  {event['timestamp']} - {event['Source']} - {event['EventType']}: {event['Details']}")

def identify_attack_patterns(events):
    """Detect common attack patterns"""
    patterns = {
        'credential_dumping': ['mimikatz', 'procdump', 'lsass'],
        'lateral_movement': ['psexec', 'wmiexec', 'smb', 'rdp'],
        'persistence': ['Run', 'RunOnce', 'Schedule', 'cron', 'systemd'],
        'privilege_escalation': ['sudo', 'runas', 'UAC'],
        'data_staging': ['.zip', '.tar', '.rar', '.7z', 'archive']
    }
    
    findings = defaultdict(list)
    for event in events:
        details = event.get('Details', '').lower()
        for pattern_name, keywords in patterns.items():
            if any(keyword.lower() in details for keyword in keywords):
                findings[pattern_name].append(event)
    
    print("\n[+] Attack Pattern Detection:")
    for pattern, matched_events in findings.items():
        if matched_events:
            print(f"\n  {pattern.upper()} ({len(matched_events)} events):")
            for event in matched_events[:5]:  # Show first 5
                print(f"    {event['timestamp']} - {event['Details'][:100]}")

def main():
    timeline = load_timeline('incident_timeline.csv')
    print(f"[*] Loaded {len(timeline)} events")
    
    # Identify attack patterns
    identify_attack_patterns(timeline)
    
    # Find specific process activity
    suspicious_processes = ['powershell.exe', 'cmd.exe', 'nc.exe', 'mimikatz']
    for proc in suspicious_processes:
        matching = [e for e in timeline if proc in e.get('Details', '')]
        if matching:
            print(f"\n[+] Found {len(matching)} events for {proc}")
            find_process_tree(timeline, proc)
            if matching:
                find_temporal_correlation(timeline, matching[0], 300)
            break

if __name__ == "__main__":
    main()
```

**Phase 4: IOC Extraction**

```powershell
# Extract Indicators of Compromise
$iocs = @{
    FileHashes = @()
    FilePaths = @()
    RegistryKeys = @()
    CommandLines = @()
    IPAddresses = @()
}

# Extract from Sysmon logs
Get-WinEvent -LogName "Microsoft-Windows-Sysmon/Operational" -FilterXPath "*[System[EventID=1]]" -MaxEvents 1000 -ErrorAction SilentlyContinue |
    ForEach-Object {
        $xml = [xml]$_.ToXml()
        
        # Extract hashes
        $hashes = ($xml.Event.EventData.Data | Where-Object {$_.Name -eq 'Hashes'}).'#text'
        if ($hashes -match 'SHA256=([A-F0-9]{64})') {
            $iocs.FileHashes += $matches[1]
        }
        
        # Extract suspicious file paths
        $image = ($xml.Event.EventData.Data | Where-Object {$_.Name -eq 'Image'}).'#text'
        if ($image -match '(\\Temp\\|\\AppData\\Local\\Temp\\|\\Public\\)') {
            $iocs.FilePaths += $image
        }
        
        # Extract command lines
        $cmdline = ($xml.Event.EventData.Data | Where-Object {$_.Name -eq 'CommandLine'}).'#text'
        if ($cmdline -match '(-enc|-e|IEX|Invoke-|DownloadString)') {
            $iocs.CommandLines += $cmdline
        }
    }

# Extract registry IOCs
Get-WinEvent -LogName "Microsoft-Windows-Sysmon/Operational" -FilterXPath "*[System[EventID=13]]" -MaxEvents 1000 -ErrorAction SilentlyContinue |
    ForEach-Object {
        $xml = [xml]$_.ToXml()
        $target = ($xml.Event.EventData.Data | Where-Object {$_.Name -eq 'TargetObject'}).'#text'
        if ($target -match '(\\Run\\|\\RunOnce\\|\\Services\\)') {
            $iocs.RegistryKeys += $target
        }
    }

# Extract network IOCs
Get-WinEvent -LogName "Microsoft-Windows-Sysmon/Operational" -FilterXPath "*[System[EventID=3]]" -MaxEvents 1000 -ErrorAction SilentlyContinue |
    ForEach-Object {
        $xml = [xml]$_.ToXml()
        $destIP = ($xml.Event.EventData.Data | Where-Object {$_.Name -eq 'DestinationIp'}).'#text'
        # Filter out local/common IPs
        if ($destIP -notmatch '^(10\.|172\.(1[6-9]|2[0-9]|3[01])\.|192\.168\.|127\.)') {
            $iocs.IPAddresses += $destIP
        }
    }

# Deduplicate and export
Write-Output "`n=== EXTRACTED IOCs ==="
Write-Output "`n[+] File Hashes (SHA256):"
$iocs.FileHashes | Select-Object -Unique | ForEach-Object { Write-Output "  $_" }

Write-Output "`n[+] Suspicious File Paths:"
$iocs.FilePaths | Select-Object -Unique | ForEach-Object { Write-Output "  $_" }

Write-Output "`n[+] Malicious Command Lines:"
$iocs.CommandLines | Select-Object -Unique | ForEach-Object { Write-Output "  $_" }

Write-Output "`n[+] Persistence Registry Keys:"
$iocs.RegistryKeys | Select-Object -Unique | ForEach-Object { Write-Output "  $_" }

Write-Output "`n[+] External IP Addresses:"
$iocs.IPAddresses | Select-Object -Unique | ForEach-Object { Write-Output "  $_" }

# Export to STIX format (simplified)
$stixIocs = @{
    spec_version = "2.1"
    type = "bundle"
    objects = @()
}

$iocs.FileHashes | Select-Object -Unique | ForEach-Object {
    $stixIocs.objects += @{
        type = "indicator"
        pattern = "[file:hashes.SHA256 = '$_']"
        pattern_type = "stix"
    }
}

$stixIocs | ConvertTo-Json -Depth 5 | Out-File iocs_stix.json
Write-Output "`n[*] IOCs exported to iocs_stix.json"
```

**Phase 5: Report Generation**

```python
#!/usr/bin/env python3
# Generate incident report from analysis

from datetime import datetime
import json

def generate_incident_report(timeline_file, iocs_file):
    """Generate comprehensive incident report"""
    
    report = f"""
# INCIDENT RESPONSE REPORT
Generated: {datetime.now().isoformat()}

## EXECUTIVE SUMMARY
[Automatically populated based on findings]

## TIMELINE OF EVENTS
"""
    
    # Load timeline
    with open(timeline_file, 'r') as f:
        import csv
        reader = csv.DictReader(f)
        events = list(reader)
    
    # Identify key events
    key_events = []
    for event in events:
        details = event.get('Details', '').lower()
        if any(keyword in details for keyword in ['mimikatz', 'powershell -enc', 'nc.exe', 'reverse']):
            key_events.append(event)
    
    report += f"\nTotal Events Analyzed: {len(events)}\n"
    report += f"Suspicious Events Identified: {len(key_events)}\n\n"
    
    report += "### Key Suspicious Events:\n"
    for event in key_events[:10]:
        report += f"- {event['Timestamp']} | {event['EventType']} | {event['Details'][:100]}...\n"
    
    # Load IOCs
    report += "\n## INDICATORS OF COMPROMISE\n\n"
    try:
        with open(iocs_file, 'r') as f:
            iocs = json.load(f)
            report += f"Total IOCs Extracted: {len(iocs.get('objects', []))}\n"
    except:
        report += "IOC file not found\n"
    
    # Attack chain reconstruction
    report += "\n## ATTACK CHAIN RECONSTRUCTION\n\n"
    report += "### Phase 1: Initial Access\n"
    report += "[Analyze earliest suspicious events]\n\n"
    report += "### Phase 2: Execution\n"
    report += "[Identify malicious process execution]\n\n"
    report += "### Phase 3: Persistence\n"
    report += "[Document persistence mechanisms]\n\n"
    report += "### Phase 4: Privilege Escalation\n"
    report += "[Track elevation attempts]\n\n"
    report += "### Phase 5: Lateral Movement\n"
    report += "[Map network propagation]\n\n"
    report += "### Phase 6: Exfiltration\n"
    report += "[Identify data staging and transfer]\n\n"
    
    # Recommendations
    report += "\n## RECOMMENDATIONS\n\n"
    report += "1. Isolate affected systems\n"
    report += "2. Reset credentials for compromised accounts\n"
    report += "3. Remove identified persistence mechanisms\n"
    report += "4. Apply patches for exploited vulnerabilities\n"
    report += "5. Update IOC detection rules\n"
    
    # Write report
    with open('incident_report.md', 'w') as f:
        f.write(report)
    
    print("[+] Report generated: incident_report.md")

if __name__ == "__main__":
    generate_incident_report('incident_timeline.csv', 'iocs_stix.json')
```

---

## Advanced CTF Techniques

### Memory-Resident Artifacts

**Extract command history from memory (Linux):**

```bash
# Dump process memory
gcore -o bash_dump $(pgrep bash | head -1)

# Search for commands in dump
strings bash_dump.* | grep -E "(wget|curl|nc|chmod|sudo)" | sort -u

# Extract environment variables
strings bash_dump.* | grep -E "^[A-Z_]+=.*" | head -20
```

**Extract PowerShell history from memory (Windows):**

```powershell
# PowerShell command history location
Get-Content (Get-PSReadlineOption).HistorySavePath

# Search for encoded commands in recent history
Get-Content (Get-PSReadlineOption).HistorySavePath | 
    Select-String "-enc|-e |-encodedcommand" |
    ForEach-Object {
        if ($_ -match "(?:-enc|-e|-encodedcommand)\s+([A-Za-z0-9+/=]+)") {
            $encoded = $matches[1]
            try {
                $decoded = [System.Text.Encoding]::Unicode.GetString([System.Convert]::FromBase64String($encoded))
                Write-Output "Original: $_"
                Write-Output "Decoded: $decoded`n"
            } catch {}
        }
    }
```

### Anti-Forensics Detection

**Detect log tampering:**

```bash
#!/bin/bash
# Check for log manipulation

# Missing log entries (gaps in sequence)
echo "[+] Checking for log gaps..."
last_timestamp=0
while read line; do
    timestamp=$(echo "$line" | grep -oP '\d{10}')
    if [ ! -z "$timestamp" ] && [ $last_timestamp -ne 0 ]; then
        gap=$((timestamp - last_timestamp))
        if [ $gap -gt 3600 ]; then  # Gap > 1 hour
            echo "  [!] Suspicious gap detected: $gap seconds"
            echo "      Last: $(date -d @$last_timestamp)"
            echo "      Next: $(date -d @$timestamp)"
        fi
    fi
    last_timestamp=$timestamp
done < /var/log/audit/audit.log

# Check for log file truncation
find /var/log -name "*.log" -size 0 -ls

# Check log service status
systemctl status rsyslog auditd

# Verify log file permissions (should not be writable by non-root)
find /var/log -type f -perm /022 -ls
```

```powershell
# Windows log tampering detection

# Check for cleared event logs (Event ID 1102)
Get-WinEvent -FilterHashtable @{LogName='Security'; Id=1102} -ErrorAction SilentlyContinue |
    Select-Object TimeCreated, Message |
    Format-List

# Find gaps in event log sequence
$events = Get-WinEvent -LogName Security -MaxEvents 1000 |
    Sort-Object RecordId

for ($i = 1; $i < $events.Count; $i++) {
    $gap = $events[$i-1].RecordId - $events[$i].RecordId
    if ($gap -gt 10) {
        Write-Output "[!] Gap detected: $gap missing events between $($events[$i].RecordId) and $($events[$i-1].RecordId)"
    }
}

# Check event log service status
Get-Service EventLog | Select-Object Name, Status, StartType
```

### Steganography in Logs

**Hidden data in ADS (Alternate Data Streams):**

```powershell
# Find files with ADS
Get-ChildItem -Path C:\ -Recurse -ErrorAction SilentlyContinue |
    ForEach-Object {
        Get-Item $_.FullName -Stream * -ErrorAction SilentlyContinue |
            Where-Object {$_.Stream -ne ':$DATA'}
    } | Select-Object FileName, Stream, Length

# Extract ADS content
Get-Content -Path "suspicious_file.txt" -Stream hidden_data

# Check for data in registry binary values
Get-ItemProperty "HKLM:\Software\CustomKey" |
    Select-Object * -ExcludeProperty PS* |
    ForEach-Object {
        $_.PSObject.Properties | Where-Object {$_.Value -is [byte[]]} |
            ForEach-Object {
                $text = [System.Text.Encoding]::ASCII.GetString($_.Value)
                if ($text -match '[a-zA-Z0-9]{20,}') {
                    Write-Output "Potential hidden data in $($_.Name): $text"
                }
            }
    }
```

---

## Final Integration Example

```bash
#!/bin/bash
# Complete CTF incident response script (Linux)

echo "========================================="
echo " CTF INCIDENT RESPONSE ANALYZER"
echo "========================================="
echo ""

OUTDIR="ir_analysis_$(date +%Y%m%d_%H%M%S)"
mkdir -p "$OUTDIR"

echo "[*] Output directory: $OUTDIR"
echo ""

# 1. Process Execution Analysis
echo "[+] Analyzing process execution logs..."
ausearch -sc execve --start boot -i > "$OUTDIR/process_executions.txt" 2>/dev/null
grep -E "(nc|ncat|netcat|python|perl|ruby|bash|sh)" "$OUTDIR/process_executions.txt" > "$OUTDIR/suspicious_processes.txt"
echo "    Found $(wc -l < "$OUTDIR/suspicious_processes.txt") suspicious process executions"

# 2. File Access Analysis
echo "[+] Analyzing file access logs..."
ausearch -k passwd_access -k shadow_access --start boot -i > "$OUTDIR/file_access.txt" 2>/dev/null
find /tmp /dev/shm /var/tmp -type f -mtime -1 -ls > "$OUTDIR/temp_files.txt" 2>/dev/null
echo "    Found $(wc -l < "$OUTDIR/temp_files.txt") recent temporary files"

# 3. Persistence Check
echo "[+] Checking persistence mechanisms..."
{
    echo "=== Crontabs ==="
    cat /etc/crontab
    ls -la /etc/cron.*/ 2>/dev/null
    
    echo -e "\n=== Systemd Services ==="
    systemctl list-unit-files --type=service --state=enabled
    
    echo -e "\n=== User Bash Profiles ==="
    grep -r "^[^#]" /home/*/.bashrc /home/*/.bash_profile /root/.bashrc 2>/dev/null
} > "$OUTDIR/persistence.txt"

# 4. Timeline Creation
echo "[+] Creating unified timeline..."
{
    ausearch --start boot -i 2>/dev/null | grep -E "type=(EXECVE|PATH|SYSCALL)" | \
        awk '{print $0}' | head -100
    find /etc /var/www -type f -mtime -1 -printf "%T+ FILESYSTEM %p\n" 2>/dev/null
} | sort > "$OUTDIR/timeline.txt"
echo "    Timeline contains $(wc -l < "$OUTDIR/timeline.txt") events"

# 5. Network Analysis
echo "[+] Analyzing network activity..."
{
    echo "=== Active Connections ==="
    netstat -tunap
    echo -e "\n=== Listening Services ==="
    netstat -tulpn
} > "$OUTDIR/network.txt"

# 6. Generate Summary
echo "[+] Generating summary report..."
{
    echo "INCIDENT RESPONSE SUMMARY"
    echo "========================="
    echo "Analysis Date: $(date)"
    echo ""
    echo "Suspicious Process Executions: $(wc -l < "$OUTDIR/suspicious_processes.txt")"
    echo "Recent Temporary Files: $(wc -l < "$OUTDIR/temp_files.txt")"
    echo "Timeline Events: $(wc -l < "$OUTDIR/timeline.txt")"
    echo ""
    echo "TOP SUSPICIOUS PROCESSES:"
    head -10 "$OUTDIR/suspicious_processes.txt"
    echo ""
    echo "RECENT TEMP FILES:"
    head -10 "$OUTDIR/temp_files.txt"
} > "$OUTDIR/summary.txt"

cat "$OUTDIR/summary.txt"
echo ""
echo "[*] Analysis complete. Results in: $OUTDIR/"
```

This comprehensive guide covers the essential techniques for analyzing incident response logs in CTF environments. Practice these methods systematically to build proficiency in rapid threat detection and forensic analysis.

---

## Memory Dump Analysis Logs

Memory dumps contain volatile system state including running processes, network connections, loaded drivers, encryption keys, and unpacked malware. Analysis logs document findings from memory forensics tools and preserve investigative chains of evidence.

### Memory Acquisition

**Linux Memory Acquisition:**

```bash
# LiME (Linux Memory Extractor) - kernel module approach
apt-get install lime-forensics-dkms

# Build LiME module for current kernel
cd /usr/src/lime-forensics
make

# Acquire memory to file
insmod lime.ko "path=/tmp/memory.lime format=lime"

# Alternative: using /proc/kcore (requires root)
dd if=/proc/kcore of=/tmp/memory.raw bs=1M
# Note: /proc/kcore may have gaps; LiME is preferred

# AVML (Azure open-source tool, static binary)
wget https://github.com/microsoft/avml/releases/download/v0.13.0/avml
chmod +x avml
./avml memory.lime

# Log acquisition metadata
echo "Acquisition Date: $(date -u +%Y-%m-%dT%H:%M:%SZ)" > acquisition.log
echo "Hostname: $(hostname)" >> acquisition.log
echo "Kernel: $(uname -r)" >> acquisition.log
echo "MD5: $(md5sum memory.lime | awk '{print $1}')" >> acquisition.log
```

**Windows Memory Acquisition:**

```powershell
# DumpIt (free GUI tool) - creates MEMORY.DMP in current directory
# https://www.comae.com/

# FTK Imager (GUI with CLI)
FTKImager.exe --memory-dump C:\forensics\memory.mem --verify

# WinPmem (Rekall/Velocidex)
winpmem_mini_x64.exe memory.aff4

# Log acquisition
$acqTime = Get-Date -Format "yyyy-MM-ddTHH:mm:ssZ"
$hash = Get-FileHash -Algorithm SHA256 memory.mem
@"
Acquisition Time: $acqTime
Computer Name: $env:COMPUTERNAME
OS Version: $(Get-WmiObject Win32_OperatingSystem | Select-Object -Expand Caption)
SHA256: $($hash.Hash)
"@ | Out-File acquisition_log.txt
```

### Volatility Framework Analysis

**Volatility 3 (Current Version):**

```bash
# Install
pip3 install volatility3

# Identify image profile (automatic in Vol3)
vol -f memory.lime banners.Banners

# List running processes with timestamps
vol -f memory.lime linux.pslist.PsList > pslist.log
vol -f memory.lime linux.pstree.PsTree > pstree.log

# Extract command history
vol -f memory.lime linux.bash.Bash > bash_history.log

# Network connections
vol -f memory.lime linux.netstat.Netstat > network_connections.log

# Loaded kernel modules (potential rootkits)
vol -f memory.lime linux.lsmod.Lsmod > kernel_modules.log

# Check for hidden processes (rootkit detection)
vol -f memory.lime linux.check_syscall.Check_syscall > syscall_check.log

# Dump suspicious process memory
vol -f memory.lime -o /tmp/dumps linux.procdump.ProcDump --pid 1337
```

**Volatility 2 (Still widely used for Windows analysis):**

```bash
# Identify Windows profile
volatility -f memory.mem imageinfo > imageinfo.log

# Use identified profile (example: Win10x64_19041)
PROFILE="Win10x64_19041"

# Process listing with parent relationships
volatility -f memory.mem --profile=$PROFILE pslist > pslist.log
volatility -f memory.mem --profile=$PROFILE pstree > pstree.log
volatility -f memory.mem --profile=$PROFILE psscan > psscan.log  # Find hidden processes

# Command line arguments (critical for detecting malicious execution)
volatility -f memory.mem --profile=$PROFILE cmdline > cmdline.log

# Network connections
volatility -f memory.mem --profile=$PROFILE netscan > netscan.log

# DLL injection detection
volatility -f memory.mem --profile=$PROFILE dlllist -p 1337 > dlllist_1337.log
volatility -f memory.mem --profile=$PROFILE malfind > malfind.log  # Find injected code

# Registry analysis
volatility -f memory.mem --profile=$PROFILE hivelist > hivelist.log
volatility -f memory.mem --profile=$PROFILE printkey -K "Software\Microsoft\Windows\CurrentVersion\Run" > autoruns.log
```

### Analyzing Process Memory Logs

**Detecting Injected Code:**

```bash
# Malfind output analysis - suspicious memory regions
grep -A10 "Process: " malfind.log | grep -E "VadTag|Protection|Flags"

# Example suspicious indicators:
# - PAGE_EXECUTE_READWRITE protection (unusual for legitimate code)
# - Private memory without mapped file
# - MZ header (PE) in non-image memory

# Extract suspicious regions for further analysis
awk '/Process: /{proc=$2} /0x[0-9a-f]+/{addr=$1; getline; if(/PAGE_EXECUTE_READWRITE/) print proc, addr}' malfind.log
```

**Command Line Archaeology:**

```bash
# Parse cmdline.log for suspicious patterns
grep -iE "(powershell|cmd\.exe|wscript|cscript|rundll32|regsvr32)" cmdline.log > suspicious_cmdlines.log

# Base64-encoded PowerShell (common obfuscation)
grep -i "powershell.*-enc" cmdline.log | while read line; do
    encoded=$(echo "$line" | grep -oP '\-enc[^\s]*\s+\K[A-Za-z0-9+/=]+')
    echo "$line"
    echo "Decoded: $(echo "$encoded" | base64 -d 2>/dev/null)"
    echo "---"
done > decoded_powershell.log

# Living-off-the-land binaries (LOLBins)
grep -iE "(mshta|regsvr32|rundll32|certutil|bitsadmin)" cmdline.log | \
awk '{print "[LOLBin Detected]", $0}' > lolbins.log
```

**Network Connection Timeline:**

```bash
# Parse netscan.log for external connections
awk '$4 == "TCPv4" || $4 == "TCPv6" {
    split($3, local, ":")
    split($5, remote, ":")
    if(remote[1] !~ /^(127\.|10\.|192\.168\.|172\.(1[6-9]|2[0-9]|3[01]))/) {
        print $0
    }
}' netscan.log > external_connections.log

# Correlate with process info
while read line; do
    pid=$(echo "$line" | awk '{print $6}')
    process=$(grep "^$pid " pslist.log | awk '{print $2}')
    echo "$line | Process: $process"
done < external_connections.log > correlated_connections.log
```

### Extracting Credentials from Memory

**Mimikatz-style Extraction (Post-Mortem):**

```bash
# Volatility mimikatz plugin
volatility -f memory.mem --profile=$PROFILE mimikatz > mimikatz_output.log

# Extract NTLM hashes
grep "NTLM" mimikatz_output.log | awk '{print $NF}' | sort -u > ntlm_hashes.txt

# Extract clear-text passwords (if WDigest enabled)
grep -A2 "Password" mimikatz_output.log | grep -v "^--$"
```

**Linux Credential Extraction:**

```bash
# Search for SSH keys in memory
vol -f memory.lime -o /tmp linux.procdump.ProcDump --pid <ssh-agent-pid>
strings /tmp/pid.<ssh-agent-pid>.dmp | grep -A20 "BEGIN.*PRIVATE KEY"

# Extract environment variables (may contain tokens/secrets)
vol -f memory.lime linux.envars.Envars | grep -iE "(password|token|secret|api_key)"
```

### Malware Analysis Logging

**Extracting Suspicious Executables:**

```bash
# Dump process executable
volatility -f memory.mem --profile=$PROFILE procdump -p 1337 -D /tmp/dumps/

# Calculate hash and check reputation
sha256sum /tmp/dumps/executable.1337.exe > hashes.txt

# VirusTotal lookup (requires API key)
VT_API_KEY="your_api_key"
HASH=$(sha256sum /tmp/dumps/executable.1337.exe | awk '{print $1}')
curl -H "x-apikey: $VT_API_KEY" \
  "https://www.virustotal.com/api/v3/files/$HASH" > vt_report.json

# Log malware characteristics
echo "=== Malware Sample Analysis ===" > malware_report.txt
echo "Extraction Time: $(date -u +%Y-%m-%dT%H:%M:%SZ)" >> malware_report.txt
echo "SHA256: $HASH" >> malware_report.txt
echo "Parent Process: $(grep "^1337 " pslist.log | awk '{print $3}')" >> malware_report.txt
jq -r '.data.attributes.last_analysis_stats' vt_report.json >> malware_report.txt
```

**Unpacking Packed Malware from Memory:**

```bash
# Packed malware often unpacks itself in memory
# Compare disk version vs memory version

# Dump from disk
volatility -f memory.mem --profile=$PROFILE dumpfiles -Q 0x000000001a2b3c4d -D /tmp/disk/

# Dump from memory
volatility -f memory.mem --profile=$PROFILE memdump -p 1337 -D /tmp/memory/

# Compare entropy (packed files have higher entropy)
# [Inference] High entropy (>7.0) often indicates packing/encryption
python3 << 'EOF'
import math
from collections import Counter

def calculate_entropy(data):
    if not data:
        return 0
    entropy = 0
    counter = Counter(data)
    for count in counter.values():
        p_x = count / len(data)
        entropy -= p_x * math.log2(p_x)
    return entropy

with open('/tmp/disk/file.exe', 'rb') as f:
    disk_data = f.read()
print(f"Disk entropy: {calculate_entropy(disk_data):.2f}")

with open('/tmp/memory/1337.dmp', 'rb') as f:
    mem_data = f.read()
print(f"Memory entropy: {calculate_entropy(mem_data):.2f}")
EOF
```

## Persistence Mechanisms

Persistence mechanisms allow attackers to maintain access across system reboots and user logouts. Detection requires analyzing system configuration, scheduled tasks, startup locations, and registry keys.

### Windows Persistence Detection

**Registry Run Keys:**

```powershell
# Query all common autorun registry locations
$runKeys = @(
    "HKLM:\SOFTWARE\Microsoft\Windows\CurrentVersion\Run",
    "HKLM:\SOFTWARE\Microsoft\Windows\CurrentVersion\RunOnce",
    "HKCU:\SOFTWARE\Microsoft\Windows\CurrentVersion\Run",
    "HKCU:\SOFTWARE\Microsoft\Windows\CurrentVersion\RunOnce",
    "HKLM:\SOFTWARE\WOW6432Node\Microsoft\Windows\CurrentVersion\Run",
    "HKLM:\SOFTWARE\Microsoft\Windows\CurrentVersion\RunServices",
    "HKLM:\SOFTWARE\Microsoft\Windows\CurrentVersion\RunServicesOnce"
)

foreach ($key in $runKeys) {
    if (Test-Path $key) {
        Write-Output "`n=== $key ==="
        Get-ItemProperty $key | Format-List
    }
}

# Export to log
Get-ItemProperty "HKLM:\SOFTWARE\Microsoft\Windows\CurrentVersion\Run" | 
Out-File -Append registry_autoruns.log

# Detect suspicious patterns
Get-ItemProperty "HKLM:\SOFTWARE\Microsoft\Windows\CurrentVersion\Run" | 
Select-Object * -ExcludeProperty PS* | 
Where-Object { 
    $_.PSObject.Properties.Value -match "(powershell|cmd|wscript|temp|appdata)" 
} | Format-List | Out-File suspicious_autoruns.log
```

**Scheduled Tasks Analysis:**

```powershell
# Export all scheduled tasks
Get-ScheduledTask | Select-Object TaskName,TaskPath,State,Author | 
Export-Csv scheduled_tasks.csv

# Detailed XML export for suspicious tasks
Get-ScheduledTask | Where-Object {
    $_.TaskName -match "update" -or 
    $_.Author -eq "" -or
    $_.Author -notmatch "Microsoft"
} | ForEach-Object {
    $xml = Export-ScheduledTask -TaskName $_.TaskName -TaskPath $_.TaskPath
    $xml | Out-File "tasks\$($_.TaskName).xml"
}

# Find tasks with suspicious triggers or actions
Get-ScheduledTask | ForEach-Object {
    $task = $_
    $info = Get-ScheduledTaskInfo -TaskName $task.TaskName -TaskPath $task.TaskPath -ErrorAction SilentlyContinue
    
    if ($task.Actions.Execute -match "(powershell|cmd|wscript|mshta)") {
        [PSCustomObject]@{
            TaskName = $task.TaskName
            Executable = $task.Actions.Execute
            Arguments = $task.Actions.Arguments
            LastRunTime = $info.LastRunTime
            NextRunTime = $info.NextRunTime
        }
    }
} | Export-Csv suspicious_scheduled_tasks.csv
```

**Linux Scheduled Tasks (Cron):**

```bash
# System-wide cron jobs
cat /etc/crontab > cron_analysis.log
ls -laR /etc/cron.* >> cron_analysis.log

# User cron jobs
for user in $(cut -f1 -d: /etc/passwd); do
    echo "=== Cron for $user ===" >> cron_analysis.log
    crontab -l -u $user 2>/dev/null >> cron_analysis.log
done

# Detect suspicious patterns
grep -rE "(curl|wget|/tmp|/dev/shm|nc |bash -i)" /etc/cron* /var/spool/cron/ 2>/dev/null > suspicious_crons.log

# Systemd timers (modern alternative to cron)
systemctl list-timers --all > systemd_timers.log
systemctl list-unit-files --type=timer >> systemd_timers.log
```

**Service Persistence:**

```powershell
# Windows Services
Get-Service | Where-Object {
    $_.StartType -eq "Automatic" -and 
    $_.Status -eq "Running"
} | Select-Object Name,DisplayName,StartType,Status | 
Export-Csv running_services.csv

# Service binary paths
Get-WmiObject Win32_Service | Select-Object Name,PathName,StartMode | 
Where-Object { $_.PathName -match "(temp|appdata|users)" } | 
Export-Csv suspicious_services.csv

# Service DLL hijacking check
Get-ItemProperty "HKLM:\SYSTEM\CurrentControlSet\Services\*\Parameters" -Name ServiceDll -ErrorAction SilentlyContinue | 
Where-Object { $_.ServiceDll -match "(temp|appdata)" }
```

```bash
# Linux systemd services
systemctl list-unit-files --type=service --state=enabled > enabled_services.log

# Inspect suspicious service files
find /etc/systemd/system /usr/lib/systemd/system -name "*.service" -type f -exec grep -H "ExecStart" {} \; | 
grep -E "(tmp|curl|wget|nc)" > suspicious_services.log

# Recently modified service files
find /etc/systemd/system /usr/lib/systemd/system -name "*.service" -type f -mtime -7 -ls
```

**Shell Profile Persistence:**

```bash
# Bash profiles
for file in /etc/profile /etc/bash.bashrc ~/.bashrc ~/.bash_profile ~/.profile; do
    if [ -f "$file" ]; then
        echo "=== $file ===" >> shell_profiles.log
        cat "$file" >> shell_profiles.log
        echo "" >> shell_profiles.log
    fi
done

# Detect suspicious commands in profiles
grep -E "(curl|wget|nc |/tmp|base64|eval)" ~/.bashrc ~/.bash_profile /etc/profile 2>/dev/null > suspicious_profiles.log

# Check for SSH key manipulation
cat ~/.ssh/authorized_keys > ssh_keys_snapshot.log
stat ~/.ssh/authorized_keys >> ssh_keys_snapshot.log
```

**Boot/Logon Scripts:**

```powershell
# Windows Logon Scripts (Group Policy)
Get-ItemProperty "HKLM:\SOFTWARE\Microsoft\Windows\CurrentVersion\Group Policy\Scripts\Startup" -ErrorAction SilentlyContinue
Get-ItemProperty "HKLM:\SOFTWARE\Microsoft\Windows\CurrentVersion\Group Policy\Scripts\Logon" -ErrorAction SilentlyContinue

# User logon scripts
Get-ItemProperty "HKCU:\Environment" -Name UserInitMprLogonScript -ErrorAction SilentlyContinue
```

```bash
# Linux init scripts
ls -la /etc/rc.local /etc/init.d/ > init_scripts.log

# Check rc.local for modifications
if [ -f /etc/rc.local ]; then
    echo "=== /etc/rc.local ===" >> persistence_check.log
    cat /etc/rc.local >> persistence_check.log
    stat /etc/rc.local >> persistence_check.log
fi
```

**WMI Event Subscriptions (Advanced Windows Persistence):**

```powershell
# List WMI event filters
Get-WmiObject -Namespace root\subscription -Class __EventFilter | 
Select-Object Name,Query | Format-List | Out-File wmi_filters.log

# List WMI event consumers
Get-WmiObject -Namespace root\subscription -Class __EventConsumer | 
Select-Object Name | Format-List | Out-File wmi_consumers.log

# List filter-to-consumer bindings
Get-WmiObject -Namespace root\subscription -Class __FilterToConsumerBinding | 
Select-Object Filter,Consumer | Format-List | Out-File wmi_bindings.log

# Detailed analysis of suspicious bindings
Get-WmiObject -Namespace root\subscription -Class __FilterToConsumerBinding | ForEach-Object {
    $filter = [wmi]$_.Filter
    $consumer = [wmi]$_.Consumer
    
    [PSCustomObject]@{
        FilterName = $filter.Name
        FilterQuery = $filter.Query
        ConsumerName = $consumer.Name
        ConsumerScript = $consumer.ScriptText
        ConsumerCommand = $consumer.CommandLineTemplate
    }
} | Export-Csv wmi_persistence.csv
```

### Linux Persistence Mechanisms

**Backdoored Binaries:**

```bash
# Check for modified system binaries
debsums -c 2>&1 | tee modified_binaries.log  # Debian/Ubuntu
rpm -Va 2>&1 | tee modified_binaries.log     # RHEL/CentOS

# Compare hashes against known-good
md5sum /bin/* /sbin/* /usr/bin/* /usr/sbin/* > current_hashes.txt
# Compare against baseline_hashes.txt

# Find SUID binaries (potential privilege escalation)
find / -perm -4000 -type f 2>/dev/null > suid_binaries.log

# Recently modified executables
find /bin /sbin /usr/bin /usr/sbin -type f -mtime -7 -ls > recent_binary_mods.log
```

**Kernel Module Persistence:**

```bash
# List loaded kernel modules
lsmod > loaded_modules.log

# Check for unsigned modules (if UEFI Secure Boot enabled)
for mod in $(lsmod | tail -n +2 | awk '{print $1}'); do
    modinfo $mod | grep -E "^(filename|signature)"
done > module_signatures.log

# Detect hidden modules (rootkit technique)
diff <(lsmod | awk '{print $1}' | tail -n +2 | sort) \
     <(cat /proc/modules | awk '{print $1}' | sort) > hidden_modules.log

# Check for module loading at boot
cat /etc/modules >> module_persistence.log
ls -la /etc/modules-load.d/ >> module_persistence.log
```

**LD_PRELOAD Hijacking:**

```bash
# Check for LD_PRELOAD in environment
env | grep LD_PRELOAD > ld_preload_check.log

# Check system-wide ld.so.preload
if [ -f /etc/ld.so.preload ]; then
    echo "=== /etc/ld.so.preload FOUND ===" >> ld_preload_check.log
    cat /etc/ld.so.preload >> ld_preload_check.log
    stat /etc/ld.so.preload >> ld_preload_check.log
fi

# Detect preloaded libraries
ldd /bin/bash | grep -v "^[[:space:]]*/lib" > unusual_libraries.log
```

## Lateral Movement Traces

Lateral movement involves attackers pivoting from compromised systems to additional network hosts. Detection requires correlating authentication logs, network activity, and remote execution artifacts.

### Windows Lateral Movement Detection

**Event Log Analysis:**

```powershell
# Logon events (Event ID 4624)
Get-WinEvent -FilterHashtable @{
    LogName='Security'
    ID=4624
} | Where-Object {
    $_.Properties[8].Value -eq 3  # Network logon type
} | Select-Object TimeCreated,
    @{Name='User';Expression={$_.Properties[5].Value}},
    @{Name='SourceIP';Expression={$_.Properties[18].Value}},
    @{Name='LogonType';Expression={$_.Properties[8].Value}} | 
Export-Csv network_logons.csv

# Failed logons (Event ID 4625 - brute force indicator)
Get-WinEvent -FilterHashtable @{LogName='Security';ID=4625} | 
Group-Object {$_.Properties[5].Value} | 
Where-Object {$_.Count -gt 10} | 
Select-Object Name,Count | 
Export-Csv failed_logon_attempts.csv

# Explicit credential use (Event ID 4648 - PsExec, runas)
Get-WinEvent -FilterHashtable @{LogName='Security';ID=4648} | 
Select-Object TimeCreated,
    @{Name='User';Expression={$_.Properties[5].Value}},
    @{Name='TargetUser';Expression={$_.Properties[9].Value}},
    @{Name='TargetServer';Expression={$_.Properties[11].Value}} | 
Export-Csv explicit_credentials.csv
```

**PsExec Detection:**

```powershell
# Service installation (PsExec creates PSEXESVC service)
Get-WinEvent -FilterHashtable @{
    LogName='System'
    ProviderName='Service Control Manager'
    ID=7045
} | Where-Object {
    $_.Message -match "PSEXESVC|ADMIN\$|C\$"
} | Select-Object TimeCreated,Message | 
Export-Csv psexec_indicators.csv

# Named pipe creation (PsExec communication channel)
# Requires Sysmon or advanced auditing
Get-WinEvent -FilterHashtable @{
    LogName='Microsoft-Windows-Sysmon/Operational'
    ID=17,18  # Pipe Created, Pipe Connected
} | Where-Object {
    $_.Message -match "\\psexec|\\paexec|\\remcom"
} | Export-Csv named_pipes.csv
```

**RDP Connection Artifacts:**

```powershell
# RDP logons (Type 10)
Get-WinEvent -FilterHashtable @{LogName='Security';ID=4624} | 
Where-Object {$_.Properties[8].Value -eq 10} | 
Select-Object TimeCreated,
    @{Name='User';Expression={$_.Properties[5].Value}},
    @{Name='SourceIP';Expression={$_.Properties[18].Value}} | 
Export-Csv rdp_logons.csv

# RDP session reconnection (Event ID 4778, 4779)
Get-WinEvent -FilterHashtable @{LogName='Security';ID=4778} | 
Select-Object TimeCreated,
    @{Name='User';Expression={$_.Properties[0].Value}},
    @{Name='SourceIP';Expression={$_.Properties[2].Value}} | 
Export-Csv rdp_sessions.csv

# Bitmap cache analysis (recovers screenshots)
# [Inference] Located in user profile, requires specialized tools
$rdpCache = "$env:USERPROFILE\AppData\Local\Microsoft\Terminal Server Client\Cache"
if (Test-Path $rdpCache) {
    Get-ChildItem $rdpCache -Recurse | Select-Object FullName,Length,LastWriteTime | 
    Export-Csv rdp_cache_files.csv
}
```

**WinRM/PowerShell Remoting:**

```powershell
# PowerShell remoting events (Event ID 4103, 4104)
Get-WinEvent -FilterHashtable @{
    LogName='Microsoft-Windows-PowerShell/Operational'
    ID=4103,4104
} | Where-Object {
    $_.Message -match "(Enter-PSSession|Invoke-Command|New-PSSession)"
} | Select-Object TimeCreated,Message | 
Export-Csv powershell_remoting.csv

# WinRM connections
Get-WinEvent -FilterHashtable @{
    LogName='Microsoft-Windows-WinRM/Operational'
    ID=91,168
} | Export-Csv winrm_connections.csv
```

**SMB Session Analysis:**

```powershell
# Network share access (Event ID 5140)
Get-WinEvent -FilterHashtable @{LogName='Security';ID=5140} | 
Where-Object {
    $_.Properties[4].Value -match "ADMIN\$|C\$|IPC\$"
} | Select-Object TimeCreated,
    @{Name='User';Expression={$_.Properties[1].Value}},
    @{Name='Share';Expression={$_.Properties[4].Value}},
    @{Name='SourceIP';Expression={$_.Properties[7].Value}} | 
Export-Csv smb_access.csv

# Current SMB sessions
Get-SmbSession | Select-Object ClientComputerName,ClientUserName,SessionId | 
Export-Csv current_smb_sessions.csv

# Open files over SMB
Get-SmbOpenFile | Select-Object ClientComputerName,ClientUserName,Path | 
Export-Csv smb_open_files.csv
```

**Pass-the-Hash Detection:**

```powershell
# Logons with NTLM (Event ID 4624, LogonType 3, AuthenticationPackage = NTLM)
Get-WinEvent -FilterHashtable @{LogName='Security';ID=4624} | 
Where-Object {
    $_.Properties[8].Value -eq 3 -and 
    $_.Properties[10].Value -eq "NTLM"
} | Select-Object TimeCreated,
    @{Name='User';Expression={$_.Properties[5].Value}},
    @{Name='SourceIP';Expression={$_.Properties[18].Value}},
    @{Name='LogonGuid';Expression={$_.Properties[17].Value}} | 
Group-Object LogonGuid | 
Where-Object {$_.Count -gt 1} |  # Same credential used multiple places
Export-Csv potential_pth.csv
```

### Linux Lateral Movement Detection

**SSH Authentication Logs:**

```bash
# Successful SSH logins
grep "Accepted" /var/log/auth.log | \
awk '{print $1,$2,$3,$9,"from",$11}' > ssh_successful.log

# Failed SSH attempts
grep "Failed password" /var/log/auth.log | \
awk '{print $1,$2,$3,$9,$11}' | sort | uniq -c | sort -rn > ssh_failed.log

# SSH key-based authentication
grep "Accepted publickey" /var/log/auth.log > ssh_key_auth.log

# Detect credential stuffing (many users from single IP)
grep "Failed password" /var/log/auth.log | \
grep -oP "from \K\d{1,3}(\.\d{1,3}){3}" | \
sort | uniq -c | sort -rn | \
awk '$1 > 50 {print "[ALERT] "$2" attempted "$1" logins"}' > brute_force_ips.log
```

**SSH Session Artifacts:**

```bash
# Currently logged-in users
w > current_users.log
who -a >> current_users.log

# Last login information
lastlog > lastlog.log

# Login history
last -f /var/log/wtmp > login_history.log
last -f /var/log/btmp > failed_logins.log  # Failed login attempts

# Extract unusual login times (outside business hours)
last | awk '{
    hour = substr($7, 1, 2)
    if((hour < 7 || hour > 19) && $1 != "reboot" && $1 != "wtmp") {
        print $0
    }
}' > off_hours_logins.log
```

**SSH Tunneling Detection:**

```bash
# Active SSH tunnels
netstat -tnlp | grep sshd | grep -v ":22" > ssh_tunnels.log

# SSH forwarding in configuration
grep -E "PermitTunnel|AllowTcpForwarding|X11Forwarding" /etc/ssh/sshd_config

# Commands executed over SSH (requires detailed logging)
# Enable: echo "Subsystem sftp /usr/lib/openssh/sftp-server -l INFO" >> /etc/ssh/sshd_config
grep "session opened" /var/log/auth.log | \
while read line; do
    pid=$(echo "$line" | grep -oP "pam_unix\(sshd:session\): session opened for user \w+ by \(uid=\K\d+")
    if [ ! -z "$pid" ]; then
        ps -fp $pid 2>/dev/null
    fi
done > ssh_session_commands.log
```

**Remote Code Execution via SSH:**

```bash
# Audit SSH command execution
# Requires snoopy or process accounting

# Install snoopy logger
apt-get install snoopy

# View logged commands with context
cat /var/log/auth.log | grep snoopy > ssh_commands.log

# Alternative: process accounting
apt-get install acct
sa -u > user_command_summary.log
lastcomm | grep sshd > ssh_executed_commands.log
```

**Horizontal Scanning Detection:**

```bash
# Netstat connections to multiple internal hosts
netstat -tn | grep ESTABLISHED | \
awk '{split($5,a,":"); print a[1]}' | \
grep -E "^192\.168\.|^10\.|^172\.(1[6-9]|2[0-9]|3[01])\." | \
sort | uniq -c | sort -rn > internal_connections.log

# Firewall logs showing connection attempts
grep "SRC=.*DST=" /var/log/kern.log | \
grep -oP "SRC=\K\d{1,3}(\.\d{1,3}){3}" |  
sort | uniq -c | sort -rn |  
awk '$1 > 100 {print "[ALERT] "$2" initiated "$1" connections"}' > scanning_activity.log

# Port scanning detection from connection logs
awk '{print $1,$3,$5}' /var/log/firewall.log |  
grep "SYN" |  
awk '{print $2}' |  
sort | uniq -c |  
awk '$1 > 50 {print "Potential scanner: "$2}' > port_scanners.log
````

### Remote Execution Mechanisms

**Windows Management Instrumentation (WMI):**
```powershell
# WMI process creation (Event ID 4688 with wmic.exe parent)
Get-WinEvent -FilterHashtable @{LogName='Security';ID=4688} | 
Where-Object {
    $_.Properties[5].Value -match "wmic|wmiexec"
} | Select-Object TimeCreated,
    @{Name='Process';Expression={$_.Properties[5].Value}},
    @{Name='CommandLine';Expression={$_.Properties[8].Value}},
    @{Name='User';Expression={$_.Properties[1].Value}} | 
Export-Csv wmi_execution.csv

# WMI Event Consumer executions (requires WMI-Activity logs)
Get-WinEvent -LogName "Microsoft-Windows-WMI-Activity/Operational" | 
Where-Object {$_.Id -eq 5861} | 
Select-Object TimeCreated,Message | 
Export-Csv wmi_consumer_activity.csv

# Query WMI process creation events from CIM
Get-WmiObject -Class Win32_ProcessStartTrace -Filter "ProcessName='cmd.exe' OR ProcessName='powershell.exe'" | 
Select-Object ProcessName,ProcessID,ParentProcessID,SessionID,@{Name='Time';Expression={$_.ConvertToDateTime($_.TIME_CREATED)}}
````

**DCOM Lateral Movement:**

```powershell
# DCOM process activations (Event ID 10016 - DistributedCOM)
Get-WinEvent -FilterHashtable @{
    LogName='System'
    ProviderName='Microsoft-Windows-DistributedCOM'
    ID=10016
} | Select-Object TimeCreated,Message | 
Export-Csv dcom_activations.csv

# MMC20.Application abuse (common DCOM lateral movement)
Get-WinEvent -FilterHashtable @{LogName='Security';ID=4688} | 
Where-Object {
    $_.Properties[5].Value -match "mmc\.exe" -and
    $_.Properties[8].Value -match "-Embedding"
} | Export-Csv mmc_dcom_abuse.csv
```

**Scheduled Task Remote Creation:**

```powershell
# Task Scheduler events (Event ID 4698 - task created)
Get-WinEvent -FilterHashtable @{LogName='Security';ID=4698} | 
Where-Object {
    $_.Properties[1].Value -notmatch "Microsoft|Windows"
} | ForEach-Object {
    $xmlData = [xml]$_.Properties[0].Value
    [PSCustomObject]@{
        TimeCreated = $_.TimeCreated
        TaskName = $xmlData.Task.RegistrationInfo.URI
        Command = $xmlData.Task.Actions.Exec.Command
        Arguments = $xmlData.Task.Actions.Exec.Arguments
        Creator = $_.Properties[1].Value
    }
} | Export-Csv remote_scheduled_tasks.csv

# Network logon followed by task creation (correlation)
$logons = Get-WinEvent -FilterHashtable @{LogName='Security';ID=4624} | 
    Where-Object {$_.Properties[8].Value -eq 3}
$tasks = Get-WinEvent -FilterHashtable @{LogName='Security';ID=4698}

foreach ($task in $tasks) {
    $taskTime = $task.TimeCreated
    $correlatedLogon = $logons | Where-Object {
        ($taskTime - $_.TimeCreated).TotalSeconds -lt 300 -and
        ($taskTime - $_.TimeCreated).TotalSeconds -gt 0
    } | Select-Object -First 1
    
    if ($correlatedLogon) {
        [PSCustomObject]@{
            TaskCreated = $taskTime
            LogonTime = $correlatedLogon.TimeCreated
            SourceIP = $correlatedLogon.Properties[18].Value
            User = $correlatedLogon.Properties[5].Value
            TaskName = $task.Properties[0].Value
        }
    }
} | Export-Csv correlated_task_logons.csv
```

### Network-Based Lateral Movement Artifacts

**Kerberos Ticket Analysis:**

```powershell
# Golden Ticket detection (unusual Kerberos TGT requests)
Get-WinEvent -FilterHashtable @{LogName='Security';ID=4768} | 
Where-Object {
    # Encryption type 0x17 (RC4) with unusual characteristics
    $_.Properties[8].Value -eq "0x17" -and
    $_.Properties[0].Value -notmatch '@'  # Missing realm
} | Select-Object TimeCreated,
    @{Name='User';Expression={$_.Properties[0].Value}},
    @{Name='ClientIP';Expression={$_.Properties[9].Value}},
    @{Name='EncryptionType';Expression={$_.Properties[8].Value}} | 
Export-Csv potential_golden_tickets.csv

# Silver Ticket detection (Event ID 4769 - TGS requests)
Get-WinEvent -FilterHashtable @{LogName='Security';ID=4769} | 
Where-Object {
    $_.Properties[8].Value -eq "0x17" -and  # RC4 encryption
    $_.Properties[5].Value -notmatch '\$'    # Non-machine account
} | Export-Csv potential_silver_tickets.csv

# Kerberoasting detection (RC4 service tickets requested)
Get-WinEvent -FilterHashtable @{LogName='Security';ID=4769} | 
Where-Object {
    $_.Properties[8].Value -eq "0x17" -and
    $_.Properties[0].Value -match "SPN"
} | Group-Object {$_.Properties[0].Value} | 
Where-Object {$_.Count -gt 10} |  # Multiple SPN requests
Select-Object Name,Count | 
Export-Csv kerberoasting_activity.csv
```

**NTLM Relay Detection:**

```bash
# Linux - Responder/ntlmrelayx indicators
grep -rE "NTLMSSP|NEGOTIATE_MESSAGE" /var/log/ > ntlm_traffic.log

# Check for SMB signing disabled (vulnerable to relay)
smbclient -L //target --option='client signing'=no 2>&1 | tee smb_signing_check.log
```

```powershell
# Windows - NTLM authentication to non-standard targets
Get-WinEvent -FilterHashtable @{LogName='Security';ID=4624} | 
Where-Object {
    $_.Properties[10].Value -eq "NTLM" -and
    $_.Properties[18].Value -notmatch "^(192\.168\.|10\.|172\.(1[6-9]|2[0-9]|3[01]))"
} | Select-Object TimeCreated,
    @{Name='User';Expression={$_.Properties[5].Value}},
    @{Name='SourceIP';Expression={$_.Properties[18].Value}} | 
Export-Csv external_ntlm_auth.log
```

### Credential Dumping Artifacts

**LSASS Memory Access:**

```powershell
# LSASS process access (Sysmon Event ID 10)
Get-WinEvent -FilterHashtable @{
    LogName='Microsoft-Windows-Sysmon/Operational'
    ID=10  # Process Access
} | Where-Object {
    $_.Properties[7].Value -eq "lsass.exe"
} | Select-Object TimeCreated,
    @{Name='SourceImage';Expression={$_.Properties[2].Value}},
    @{Name='TargetImage';Expression={$_.Properties[7].Value}},
    @{Name='GrantedAccess';Expression={$_.Properties[9].Value}} | 
Export-Csv lsass_access.csv

# Suspicious access rights (0x1010 = PROCESS_QUERY_INFORMATION + PROCESS_VM_READ)
Get-WinEvent -FilterHashtable @{
    LogName='Microsoft-Windows-Sysmon/Operational'
    ID=10
} | Where-Object {
    $_.Properties[7].Value -eq "lsass.exe" -and
    $_.Properties[9].Value -match "(0x1410|0x1010|0x1438)"
} | Export-Csv suspicious_lsass_access.csv

# Comsvcs.dll MiniDump technique
Get-WinEvent -FilterHashtable @{LogName='Security';ID=4688} | 
Where-Object {
    $_.Properties[8].Value -match "comsvcs\.dll.*MiniDump"
} | Export-Csv comsvcs_dump_attempts.csv
```

**SAM/NTDS Database Access:**

```powershell
# Registry hive access (Event ID 4663)
Get-WinEvent -FilterHashtable @{LogName='Security';ID=4663} | 
Where-Object {
    $_.Properties[6].Value -match "SAM|SECURITY|NTDS\.dit"
} | Select-Object TimeCreated,
    @{Name='Process';Expression={$_.Properties[11].Value}},
    @{Name='Object';Expression={$_.Properties[6].Value}},
    @{Name='Access';Expression={$_.Properties[8].Value}} | 
Export-Csv sensitive_registry_access.csv

# Volume Shadow Copy creation (for NTDS.dit extraction)
Get-WinEvent -FilterHashtable @{
    LogName='System'
    ProviderName='Microsoft-Windows-VSSAudit'
    ID=8194,8195
} | Select-Object TimeCreated,Message | 
Export-Csv vss_activity.csv

# Vssadmin usage
Get-WinEvent -FilterHashtable @{LogName='Security';ID=4688} | 
Where-Object {
    $_.Properties[5].Value -match "vssadmin\.exe"
} | Export-Csv vssadmin_usage.csv
```

**Linux Credential Harvesting:**

```bash
# /etc/shadow access attempts
ausearch -k shadow_access 2>/dev/null > shadow_access.log

# SSH key access
find /home/*/.ssh /root/.ssh -type f -name "*id_*" -o -name "authorized_keys" | \
while read keyfile; do
    echo "=== $keyfile ===" >> ssh_key_access.log
    stat "$keyfile" >> ssh_key_access.log
    # Check for recent reads
    ausearch -f "$keyfile" 2>/dev/null >> ssh_key_access.log
done

# Browser credential files accessed
find /home/*/.mozilla /home/*/.config/google-chrome -name "*login*" -o -name "*cookies*" | \
xargs -I{} stat {} > browser_credential_access.log
```

### Tool-Specific Lateral Movement Indicators

**Cobalt Strike Beacons:**

```powershell
# Named pipe indicators (Cobalt Strike default pipes)
Get-WinEvent -FilterHashtable @{
    LogName='Microsoft-Windows-Sysmon/Operational'
    ID=17,18
} | Where-Object {
    $_.Message -match "(MSSE-|postex_|status_|msagent_)"
} | Export-Csv cobaltstrike_pipes.csv

# Service creation with encoded payloads
Get-WinEvent -FilterHashtable @{LogName='System';ID=7045} | 
Where-Object {
    $_.Message -match "(rundll32|powershell).*-enc"
} | Export-Csv encoded_service_payloads.csv

# HTTP C2 traffic indicators (requires network logs)
# [Unverified] Cobalt Strike default user agents, check network proxy logs
grep -E "(Mozilla.*compatible|Internet Explorer)" proxy.log | \
grep -v "Chrome|Firefox|Edge" > potential_cs_traffic.log
```

**Mimikatz Execution Evidence:**

```powershell
# Mimikatz command patterns in PowerShell logs
Get-WinEvent -FilterHashtable @{
    LogName='Microsoft-Windows-PowerShell/Operational'
    ID=4104
} | Where-Object {
    $_.Message -match "(sekurlsa|lsadump|kerberos::golden|kerberos::ptt)"
} | Export-Csv mimikatz_powershell.csv

# Mimikatz module loads
Get-WinEvent -FilterHashtable @{
    LogName='Microsoft-Windows-Sysmon/Operational'
    ID=7  # Image Loaded
} | Where-Object {
    $_.Properties[4].Value -match "(sekurlsa|kiwi|mimidrv)"
} | Export-Csv mimikatz_modules.csv
```

**Impacket Tool Artifacts:**

```bash
# Linux - Impacket execution traces
find / -name "*impacket*" 2>/dev/null > impacket_files.log

# Check command history for impacket tools
grep -rE "(psexec\.py|smbexec\.py|wmiexec\.py|secretsdump\.py|GetNPUsers\.py)" \
    /home/*/.bash_history /root/.bash_history 2>/dev/null > impacket_usage.log

# Network connections to Windows admin shares (Impacket pattern)
grep -E "445.*ESTABLISHED" /var/log/syslog | \
awk '{print $NF}' | sort -u > smb_connections.log
```

```powershell
# Windows - Impacket connection characteristics
# Python-based tools leave distinctive patterns
Get-WinEvent -FilterHashtable @{LogName='Security';ID=5140} | 
Where-Object {
    # Impacket often uses specific share patterns
    $_.Properties[4].Value -match "ADMIN\$" -and
    $_.Properties[1].Value -notmatch "\$"  # Non-machine account
} | Select-Object TimeCreated,
    @{Name='User';Expression={$_.Properties[1].Value}},
    @{Name='Share';Expression={$_.Properties[4].Value}},
    @{Name='SourceIP';Expression={$_.Properties[7].Value}} | 
Export-Csv impacket_indicators.csv
```

### Timeline Correlation for Lateral Movement

**Multi-Event Correlation Script:**

```powershell
# Correlate network logon -> process creation -> network connection
$startTime = (Get-Date).AddHours(-24)

# 1. Get network logons
$logons = Get-WinEvent -FilterHashtable @{
    LogName='Security'
    ID=4624
    StartTime=$startTime
} | Where-Object {$_.Properties[8].Value -eq 3} | 
Select-Object @{Name='LogonID';Expression={$_.Properties[7].Value}},
              @{Name='User';Expression={$_.Properties[5].Value}},
              @{Name='SourceIP';Expression={$_.Properties[18].Value}},
              TimeCreated

# 2. Get process creations linked to those logon IDs
$processes = Get-WinEvent -FilterHashtable @{
    LogName='Security'
    ID=4688
    StartTime=$startTime
} | Select-Object @{Name='LogonID';Expression={$_.Properties[3].Value}},
                  @{Name='Process';Expression={$_.Properties[5].Value}},
                  @{Name='CommandLine';Expression={$_.Properties[8].Value}},
                  @{Name='ProcessID';Expression={$_.Properties[4].Value}},
                  TimeCreated

# 3. Correlate
foreach ($logon in $logons) {
    $relatedProcesses = $processes | Where-Object {
        $_.LogonID -eq $logon.LogonID -and
        ($_.TimeCreated - $logon.TimeCreated).TotalSeconds -lt 300
    }
    
    if ($relatedProcesses) {
        [PSCustomObject]@{
            LogonTime = $logon.TimeCreated
            User = $logon.User
            SourceIP = $logon.SourceIP
            ProcessCount = $relatedProcesses.Count
            Processes = ($relatedProcesses.Process -join "; ")
            CommandLines = ($relatedProcesses.CommandLine -join "; ")
        }
    }
} | Export-Csv lateral_movement_timeline.csv
```

**Linux Cross-Log Correlation:**

```bash
#!/bin/bash
# correlate_lateral_movement.sh

OUTPUT="lateral_movement_timeline.log"
> "$OUTPUT"

# Extract SSH logins with timestamps
echo "=== SSH Authentication Events ===" >> "$OUTPUT"
grep "Accepted" /var/log/auth.log | \
awk '{print $1" "$2" "$3" [SSH] User:"$9" From:"$11}' >> "$OUTPUT"

# Extract sudo usage (privilege escalation)
echo -e "\n=== Sudo Usage ===" >> "$OUTPUT"
grep "sudo" /var/log/auth.log | \
awk '{print $1" "$2" "$3" [SUDO] "$0}' >> "$OUTPUT"

# Extract network connections
echo -e "\n=== Outbound Connections ===" >> "$OUTPUT"
grep "SYN" /var/log/firewall.log | \
awk '{print $1" "$2" [NETWORK] "$0}' >> "$OUTPUT"

# Sort chronologically
sort -M -k1 -k2 -k3 "$OUTPUT" -o "$OUTPUT"

echo "Timeline created: $OUTPUT"
```

### Advanced Persistence via Lateral Movement

**Golden Ticket Usage Patterns:**

```powershell
# Identify authentication patterns consistent with Golden Tickets
# [Inference] Golden Tickets often show: unusual encryption, missing pre-auth, excessive lifetime

Get-WinEvent -FilterHashtable @{LogName='Security';ID=4768} | 
Group-Object {$_.Properties[0].Value} | 
Where-Object {
    # Multiple TGT requests in short time
    $_.Count -gt 50 -and
    ($_.Group[-1].TimeCreated - $_.Group[0].TimeCreated).TotalHours -lt 1
} | Select-Object Name,Count | 
Export-Csv potential_golden_ticket_usage.csv
```

**Distributed Backdoor Detection:**

```bash
# Find identical files across multiple systems (potential mass deployment)
# Run on central log server with mounted evidence

find /evidence/host* -type f -name "*.sh" -o -name "*.py" | \
xargs -I{} md5sum {} | \
awk '{print $1}' | sort | uniq -c | \
awk '$1 > 5 {print "Hash "$2" found on "$1" systems"}' > distributed_files.log
```

### CTF-Specific Lateral Movement Scenarios

**Pivoting Through Compromised Hosts:**

```bash
# Detect SSH tunneling chains
netstat -tnp | grep "ssh" | \
awk '{print $4,$5,$7}' | \
while read local remote prog; do
    echo "Local:$local Remote:$remote Process:$prog"
done > ssh_tunnel_chains.log

# Correlate with authentication logs
for ip in $(netstat -tn | grep ESTABLISHED | awk '{print $5}' | cut -d: -f1 | sort -u); do
    echo "=== Connections from $ip ===" >> pivot_analysis.log
    grep "$ip" /var/log/auth.log >> pivot_analysis.log
done
```

**Flag Exfiltration via Lateral Movement:**

```bash
# Search for flag patterns in network traffic logs
grep -rE "flag\{[a-f0-9]{32}\}|CTF\{.*\}" /var/log/apache2/ /var/log/nginx/ > flag_exfiltration.log

# Correlate with authentication and file access
while read line; do
    timestamp=$(echo "$line" | grep -oP "^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}")
    echo "Flag access at: $timestamp" >> flag_timeline.log
    grep "$timestamp" /var/log/auth.log >> flag_timeline.log
done < flag_exfiltration.log
```

## Important Subtopics for Further Study

- **EDR telemetry analysis** (Endpoint Detection and Response logs from Carbon Black, CrowdStrike, SentinelOne)
- **Container escape detection** (Docker/Kubernetes breakout artifacts in audit logs)
- **Cloud lateral movement** (AWS CloudTrail, Azure Activity Logs for cross-account/subscription pivoting)
- **Advanced memory forensics** (YARA rules for memory scanning, custom volatility plugin development)
- **Anti-forensics detection** (log deletion, timestamp manipulation, rootkit presence in memory dumps)

---

# Container & Virtualization Logs

## Docker Container Logs

### Docker Logging Architecture

**Log Drivers:** Docker supports multiple logging drivers that determine how container logs are stored and accessed:

```bash
# Check current logging driver
docker info --format '{{.LoggingDriver}}'

# Available drivers:
# - json-file (default): JSON formatted files
# - syslog: Send to syslog daemon
# - journald: Send to systemd journal
# - gelf: Graylog Extended Log Format
# - fluentd: Forward to Fluentd
# - awslogs: AWS CloudWatch
# - splunk: Splunk Enterprise
# - none: Disable logging
```

**Default Log Locations:**

```bash
# Container logs (json-file driver)
/var/lib/docker/containers/<container-id>/<container-id>-json.log

# Docker daemon logs
# SystemD: journalctl -u docker.service
# SysVinit: /var/log/docker.log or /var/log/messages
```

### Accessing Docker Container Logs

**Basic Log Retrieval:**

```bash
# View logs from running container
docker logs <container_name_or_id>

# Follow logs (live tail)
docker logs -f <container_name_or_id>

# Show timestamps
docker logs -t <container_name_or_id>

# Limit output (last N lines)
docker logs --tail 100 <container_name_or_id>

# Logs since specific time
docker logs --since 2025-10-28T10:00:00 <container_name_or_id>
docker logs --since 30m <container_name_or_id>

# Logs until specific time
docker logs --until 2025-10-28T12:00:00 <container_name_or_id>

# Combined filters
docker logs --since 1h --tail 500 -t <container_name_or_id>
```

**Extracting Logs from Host:**

```bash
# List all container IDs
docker ps -a --format '{{.ID}}'

# Find container log file
CONTAINER_ID=$(docker ps -aqf "name=myapp")
LOG_FILE="/var/lib/docker/containers/${CONTAINER_ID}/${CONTAINER_ID}-json.log"

# Parse JSON logs
cat $LOG_FILE | jq -r '.log'

# Extract with timestamps
cat $LOG_FILE | jq -r '"\(.time) \(.log)"'

# Filter by stream (stdout/stderr)
cat $LOG_FILE | jq -r 'select(.stream=="stderr") | "\(.time) \(.log)"'
```

**Docker Log File Structure (json-file driver):**

```json
{
  "log": "Application started successfully\n",
  "stream": "stdout",
  "time": "2025-10-28T14:32:11.123456789Z"
}
```

### Docker Events and Audit Logs

**Docker Events API:**

```bash
# Real-time event stream
docker events

# Filtered events
docker events --filter 'type=container'
docker events --filter 'event=start'
docker events --filter 'event=die'
docker events --filter 'container=myapp'

# Events in time range
docker events --since '2025-10-28T10:00:00' --until '2025-10-28T12:00:00'

# Format output as JSON
docker events --format '{{json .}}'

# Monitor security-relevant events
docker events --filter 'type=container' --filter 'event=exec_create' --format '{{json .}}'
```

**Key Docker Event Types:**

```bash
# Container lifecycle events
attach, commit, copy, create, destroy, detach, die, exec_create, exec_detach, exec_start, export, kill, oom, pause, rename, resize, restart, start, stop, top, unpause, update

# Image events
delete, import, load, pull, push, save, tag, untag

# Volume events
create, destroy, mount, unmount

# Network events
connect, disconnect, create, destroy, remove

# Daemon events
reload
```

**Security-Relevant Event Monitoring:**

```bash
# Monitor privileged container creation
docker events --filter 'type=container' --filter 'event=create' --format '{{json .}}' | jq 'select(.Actor.Attributes.privileged=="true")'

# Monitor exec into containers
docker events --filter 'event=exec_create' --format '{{json .}}' | jq -r '"\(.time) Container: \(.Actor.Attributes.container) Command: \(.Actor.Attributes.execID)"'

# Monitor container escapes (die with non-zero exit)
docker events --filter 'event=die' --format '{{json .}}' | jq -r 'select(.Actor.Attributes.exitCode!="0") | "\(.time) \(.Actor.Attributes.name) Exit: \(.Actor.Attributes.exitCode)"'
```

### Docker Daemon Audit Logging

**SystemD Journal Analysis:**

```bash
# View Docker daemon logs
journalctl -u docker.service

# Follow daemon logs
journalctl -u docker.service -f

# Logs since time
journalctl -u docker.service --since "1 hour ago"

# Filter by priority
journalctl -u docker.service -p err

# Export to JSON
journalctl -u docker.service -o json > docker-daemon.json

# Parse specific fields
journalctl -u docker.service -o json | jq -r 'select(.MESSAGE | contains("error")) | "\(.MESSAGE)"'
```

**Docker Daemon Configuration for Enhanced Logging:**

```json
{
  "log-level": "debug",
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "10m",
    "max-file": "5",
    "labels": "production_status",
    "env": "os,customer"
  }
}
```

### Docker Inspect for Forensics

**Container Inspection:**

```bash
# Full container configuration
docker inspect <container_id>

# Extract specific fields
docker inspect --format='{{.State.Running}}' <container_id>
docker inspect --format='{{.State.Pid}}' <container_id>
docker inspect --format='{{.LogPath}}' <container_id>

# Network configuration
docker inspect --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' <container_id>

# Mounted volumes
docker inspect --format='{{json .Mounts}}' <container_id> | jq

# Environment variables
docker inspect --format='{{json .Config.Env}}' <container_id> | jq

# Command and entrypoint
docker inspect --format='{{.Config.Cmd}}' <container_id>
docker inspect --format='{{.Config.Entrypoint}}' <container_id>

# Capabilities
docker inspect --format='{{json .HostConfig.CapAdd}}' <container_id> | jq
docker inspect --format='{{json .HostConfig.CapDrop}}' <container_id> | jq

# Security options
docker inspect --format='{{json .HostConfig.SecurityOpt}}' <container_id> | jq

# Check for privileged mode
docker inspect --format='{{.HostConfig.Privileged}}' <container_id>
```

**Batch Container Analysis:**

```bash
# Find all privileged containers
docker ps -q | xargs docker inspect --format='{{.Id}} {{.HostConfig.Privileged}}' | grep true

# Find containers with host network
docker ps -q | xargs docker inspect --format='{{.Id}} {{.HostConfig.NetworkMode}}' | grep host

# Find containers with sensitive mounts
docker ps -q | xargs docker inspect --format='{{.Id}} {{json .Mounts}}' | jq -r 'select(.[1] | contains("/var/run/docker.sock")) | .[0]'
```

### Docker Log Analysis Techniques

**Suspicious Activity Detection:**

```bash
# Find shell spawning inside containers
docker events --filter 'event=exec_create' --format '{{json .}}' | jq -r 'select(.Actor.Attributes.execID) | "\(.time) \(.Actor.Attributes.name)"'

# Monitor failed authentication attempts
for container in $(docker ps -q); do
    docker logs $container 2>&1 | grep -iE '(failed|authentication|unauthorized|403|401)'
done

# Detect outbound network connections
for container in $(docker ps -q); do
    docker logs $container 2>&1 | grep -oE '([0-9]{1,3}\.){3}[0-9]{1,3}' | sort -u
done

# Find error patterns
docker logs <container_id> 2>&1 | grep -iE '(error|exception|fatal|critical|panic)'
```

**Container Process Monitoring:**

```bash
# List processes in container
docker top <container_id>

# Detailed process info
docker top <container_id> aux

# Monitor resource usage
docker stats <container_id>

# Get all process details
docker exec <container_id> ps auxww
```

**Extracting Application Logs from Volume:**

```bash
# Find volume mount points
docker inspect --format='{{json .Mounts}}' <container_id> | jq

# Access logs from host
docker inspect --format='{{range .Mounts}}{{.Source}}{{end}}' <container_id>

# Copy logs from container
docker cp <container_id>:/var/log/app/app.log ./app.log

# Execute commands in container for log extraction
docker exec <container_id> cat /var/log/nginx/access.log
docker exec <container_id> tail -f /var/log/app.log
```

### Docker Compose Log Analysis

**Docker Compose Logging:**

```bash
# View all service logs
docker-compose logs

# Specific service logs
docker-compose logs web

# Follow logs
docker-compose logs -f

# Tail logs
docker-compose logs --tail=100

# Logs with timestamps
docker-compose logs -t

# Multiple services
docker-compose logs web db redis
```

**Compose Event Monitoring:**

```bash
# Monitor compose project events
docker events --filter "label=com.docker.compose.project=myproject"

# Service-specific events
docker events --filter "label=com.docker.compose.service=web"
```

## Kubernetes Logs

### Kubernetes Logging Architecture

**Log Sources:**

```
1. Container logs (stdout/stderr)
2. Pod logs
3. Node logs (kubelet, kube-proxy)
4. Control plane logs (kube-apiserver, kube-scheduler, kube-controller-manager)
5. Audit logs
6. Event logs
```

**Default Log Locations:**

```bash
# Container logs (managed by container runtime)
/var/log/containers/*.log  symlink to /var/log/pods/
/var/log/pods/<namespace>_<pod-name>_<pod-uid>/<container-name>/*.log

# Kubelet logs
# SystemD: journalctl -u kubelet
/var/log/kubelet.log

# Control plane logs (if running as pods)
/var/log/kube-apiserver.log
/var/log/kube-scheduler.log
/var/log/kube-controller-manager.log
/var/log/kube-proxy.log
```

### Accessing Pod and Container Logs

**Basic Log Retrieval:**

```bash
# View pod logs (single container)
kubectl logs <pod-name>

# Specific container in multi-container pod
kubectl logs <pod-name> -c <container-name>

# All containers in pod
kubectl logs <pod-name> --all-containers=true

# Follow logs
kubectl logs -f <pod-name>

# Previous instance (after restart/crash)
kubectl logs <pod-name> --previous

# Logs with timestamps
kubectl logs <pod-name> --timestamps=true

# Since specific time
kubectl logs <pod-name> --since=1h
kubectl logs <pod-name> --since-time=2025-10-28T10:00:00Z

# Tail logs
kubectl logs <pod-name> --tail=100

# Logs from specific namespace
kubectl logs <pod-name> -n <namespace>
```

**Label-Based Log Retrieval:**

```bash
# Logs from pods matching label selector
kubectl logs -l app=nginx

# Logs from deployment
kubectl logs deployment/<deployment-name>

# Logs from all pods in namespace
kubectl logs -n production --all-containers=true -l app=web
```

**Streaming Logs from Multiple Pods:**

```bash
# Using stern (third-party tool)
stern <pod-query>

# All pods with label
stern -l app=nginx

# Specific namespace
stern -n production web

# Multiple namespaces
stern --all-namespaces -l app=nginx

# With context
stern --context production-cluster web

# Exclude containers
stern web --exclude-container istio-proxy

# Color-coded output by pod
stern --color always web
```

### Kubernetes Audit Logs

**Audit Log Configuration:**

```yaml
# kube-apiserver configuration
apiVersion: v1
kind: Pod
metadata:
  name: kube-apiserver
spec:
  containers:
  - command:
    - kube-apiserver
    - --audit-policy-file=/etc/kubernetes/audit-policy.yaml
    - --audit-log-path=/var/log/kubernetes/audit.log
    - --audit-log-maxage=30
    - --audit-log-maxbackup=10
    - --audit-log-maxsize=100
```

**Audit Policy Example:**

```yaml
apiVersion: audit.k8s.io/v1
kind: Policy
rules:
  # Log secrets changes at RequestResponse level
  - level: RequestResponse
    resources:
    - group: ""
      resources: ["secrets"]
  
  # Log pod exec/attach at Metadata level
  - level: Metadata
    resources:
    - group: ""
      resources: ["pods/exec", "pods/attach", "pods/portforward"]
  
  # Log authentication failures
  - level: Metadata
    omitStages:
    - RequestReceived
    users: ["system:anonymous"]
  
  # Don't log requests to certain non-resource URLs
  - level: None
    nonResourceURLs:
    - /healthz*
    - /version
    - /swagger*
```

**Parsing Audit Logs:**

```bash
# Audit log location (if file backend)
/var/log/kubernetes/audit.log

# Parse JSON audit logs
cat audit.log | jq

# Extract authentication failures
cat audit.log | jq -r 'select(.responseStatus.code >= 400) | "\(.timestamp) \(.user.username) \(.verb) \(.objectRef.resource) \(.responseStatus.code)"'

# Find secret access
cat audit.log | jq 'select(.objectRef.resource=="secrets")'

# Exec into pods
cat audit.log | jq 'select(.objectRef.subresource=="exec") | {time: .timestamp, user: .user.username, pod: .objectRef.name, namespace: .objectRef.namespace}'

# Unauthorized attempts
cat audit.log | jq 'select(.responseStatus.code==403) | {time: .timestamp, user: .user.username, verb: .verb, resource: .objectRef.resource}'

# Service account token usage
cat audit.log | jq 'select(.user.username | startswith("system:serviceaccount:")) | {time: .timestamp, sa: .user.username, verb: .verb, resource: .objectRef.resource}'
```

**Audit Log Levels:**

```
None: Don't log events
Metadata: Log request metadata (user, timestamp, resource, verb) but not request/response bodies
Request: Log event metadata and request body
RequestResponse: Log event metadata, request and response bodies (may contain secrets)
```

### Kubernetes Event Logs

**Viewing Events:**

```bash
# All events in namespace
kubectl get events

# All events cluster-wide
kubectl get events --all-namespaces

# Sort by timestamp
kubectl get events --sort-by='.lastTimestamp'

# Watch events in real-time
kubectl get events -w

# Events for specific resource
kubectl describe pod <pod-name> | grep -A 10 Events

# Filter by type
kubectl get events --field-selector type=Warning

# Filter by reason
kubectl get events --field-selector reason=Failed

# Events in last hour
kubectl get events --field-selector involvedObject.kind=Pod --sort-by='.lastTimestamp' | awk 'BEGIN {now=systime()} {gsub(/[TZ]/," ",$1); cmd="date -d \""$1"\" +%s"; cmd | getline ts; close(cmd); if(now-ts<3600) print}'
```

**Critical Event Monitoring:**

```bash
# OOM kills
kubectl get events --field-selector reason=OOMKilling

# Image pull failures
kubectl get events --field-selector reason=Failed,involvedObject.fieldPath=spec.containers{*}

# Pod evictions
kubectl get events --field-selector reason=Evicted

# Failed scheduling
kubectl get events --field-selector reason=FailedScheduling

# Liveness/Readiness probe failures
kubectl get events --field-selector reason=Unhealthy

# Back-off restarting
kubectl get events --field-selector reason=BackOff
```

**Event Log Export:**

```bash
# Export to JSON
kubectl get events -o json > events.json

# Parse with jq
kubectl get events -o json | jq '.items[] | {time: .lastTimestamp, type: .type, reason: .reason, message: .message, pod: .involvedObject.name}'

# Filter warning events
kubectl get events -o json | jq '.items[] | select(.type=="Warning") | {time: .lastTimestamp, reason: .reason, pod: .involvedObject.name, message: .message}'
```

### Node-Level Log Analysis

**Kubelet Logs:**

```bash
# SystemD-based systems
journalctl -u kubelet

# Follow kubelet logs
journalctl -u kubelet -f

# Since time
journalctl -u kubelet --since "2025-10-28 10:00:00"

# Export to JSON
journalctl -u kubelet -o json > kubelet.json

# Filter errors
journalctl -u kubelet -p err

# Kubelet log analysis
journalctl -u kubelet | grep -iE '(error|failed|warning)'
```

**Kube-Proxy Logs:**

```bash
# If running as pod
kubectl logs -n kube-system -l k8s-app=kube-proxy

# If running as systemd service
journalctl -u kube-proxy
```

**Container Runtime Logs:**

```bash
# Docker
journalctl -u docker

# containerd
journalctl -u containerd

# CRI-O
journalctl -u crio
```

### Control Plane Log Analysis

**API Server Logs:**

```bash
# If running as pod
kubectl logs -n kube-system kube-apiserver-<node-name>

# If running as systemd service
journalctl -u kube-apiserver

# API request analysis
kubectl logs -n kube-system kube-apiserver-<node> | grep -E 'verb=(GET|POST|PUT|DELETE|PATCH)'

# Failed authentication
kubectl logs -n kube-system kube-apiserver-<node> | grep -i 'unable to authenticate'

# Unauthorized access attempts
kubectl logs -n kube-system kube-apiserver-<node> | grep -i 'forbidden'
```

**Controller Manager Logs:**

```bash
# Pod-based
kubectl logs -n kube-system kube-controller-manager-<node-name>

# Service-based
journalctl -u kube-controller-manager

# Controller errors
kubectl logs -n kube-system kube-controller-manager-<node> | grep -iE '(error|failed)'
```

**Scheduler Logs:**

```bash
# Pod-based
kubectl logs -n kube-system kube-scheduler-<node-name>

# Failed scheduling decisions
kubectl logs -n kube-system kube-scheduler-<node> | grep -i 'failed'

# Node predicates
kubectl logs -n kube-system kube-scheduler-<node> | grep -i 'predicate'
```

### Advanced Kubernetes Log Analysis

**Detecting Suspicious Activity:**

```bash
# Exec into pods (from audit logs)
cat audit.log | jq -r 'select(.verb=="create" and .objectRef.subresource=="exec") | "\(.timestamp) User: \(.user.username) Pod: \(.objectRef.name) Namespace: \(.objectRef.namespace) Command: \(.requestObject.command)"'

# Privilege escalation attempts
kubectl get pods --all-namespaces -o json | jq -r '.items[] | select(.spec.containers[]?.securityContext?.privileged==true) | "\(.metadata.namespace) \(.metadata.name)"'

# Host network usage
kubectl get pods --all-namespaces -o json | jq -r '.items[] | select(.spec.hostNetwork==true) | "\(.metadata.namespace) \(.metadata.name)"'

# Host path mounts
kubectl get pods --all-namespaces -o json | jq -r '.items[] | select(.spec.volumes[]?.hostPath) | "\(.metadata.namespace) \(.metadata.name) \(.spec.volumes[] | select(.hostPath) | .hostPath.path)"'

# Containers running as root
kubectl get pods --all-namespaces -o json | jq -r '.items[] | select(.spec.containers[]?.securityContext?.runAsUser==0 or (.spec.containers[]?.securityContext?.runAsUser==null and .spec.securityContext?.runAsUser==null)) | "\(.metadata.namespace) \(.metadata.name)"'
```

**Resource Usage Anomalies:**

```bash
# Pods with high restart counts
kubectl get pods --all-namespaces --field-selector status.phase=Running -o json | jq -r '.items[] | select(.status.containerStatuses[]?.restartCount > 5) | "\(.metadata.namespace) \(.metadata.name) Restarts: \(.status.containerStatuses[].restartCount)"'

# CrashLoopBackOff pods
kubectl get pods --all-namespaces --field-selector status.phase!=Running,status.phase!=Succeeded -o json | jq -r '.items[] | select(.status.containerStatuses[]?.state.waiting?.reason=="CrashLoopBackOff") | "\(.metadata.namespace) \(.metadata.name)"'

# Evicted pods
kubectl get pods --all-namespaces --field-selector status.phase=Failed -o json | jq -r '.items[] | select(.status.reason=="Evicted") | "\(.metadata.namespace) \(.metadata.name) \(.status.message)"'
```

**Network Policy Violations (from logs):**

```bash
# Parse CNI logs for dropped packets (example with Calico)
kubectl logs -n kube-system -l k8s-app=calico-node | grep -i 'drop'

# Denied connections
kubectl logs -n kube-system -l k8s-app=calico-node | grep -i 'denied'
```

### Kubernetes Log Aggregation

**Centralized Logging Architecture:**

[Inference] Common patterns involve shipping logs to centralized systems; exact implementation varies:

**Using Fluentd/Fluent Bit:**

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
data:
  fluent.conf: |
    <source>
      @type tail
      path /var/log/containers/*.log
      pos_file /var/log/fluentd-containers.log.pos
      tag kubernetes.*
      read_from_head true
      <parse>
        @type json
        time_format %Y-%m-%dT%H:%M:%S.%NZ
      </parse>
    </source>
    
    <filter kubernetes.**>
      @type kubernetes_metadata
    </filter>
    
    <match **>
      @type elasticsearch
      host elasticsearch.logging.svc.cluster.local
      port 9200
      logstash_format true
    </match>
```

**Querying Aggregated Logs (example patterns):**

```bash
# Elasticsearch query for errors
curl -X GET "localhost:9200/logstash-*/_search?pretty" -H 'Content-Type: application/json' -d '
{
  "query": {
    "bool": {
      "must": [
        {"match": {"kubernetes.namespace_name": "production"}},
        {"match": {"log": "error"}}
      ],
      "filter": {
        "range": {
          "@timestamp": {
            "gte": "now-1h"
          }
        }
      }
    }
  }
}'
```

## LXC/LXD Logs

### LXC Logging Architecture

**Log Locations:**

```bash
# LXC container logs
/var/log/lxc/<container-name>.log

# LXD daemon logs
# SystemD: journalctl -u lxd
# Snap: journalctl -u snap.lxd.daemon

# LXD database
/var/lib/lxd/database/

# Container-specific logs
/var/lib/lxd/logs/<container-name>/
```

### LXC Container Log Access

**Basic Log Retrieval:**

```bash
# View container console log
lxc console <container-name> --show-log

# Container boot log
cat /var/log/lxc/<container-name>.log

# Follow container log
tail -f /var/log/lxc/<container-name>.log
```

**LXC Log Content:**

```
lxc <container-name> 20251028143211.123 INFO     conf - conf.c:run_buffer:310 - Script exited with status 0
lxc <container-name> 20251028143211.456 INFO     conf - conf.c:lxc_setup:4326 - Mounted proc filesystem
lxc <container-name> 20251028143211.789 INFO     start - start.c:start:2098 - Container started with PID 12345
lxc <container-name> 20251028143212.012 WARN     cgfsng - cgroups/cgfsng.c:cgfsng_attach:2134 - Failed to attach to cgroup
lxc <container-name> 20251028143212.345 ERROR    lxccontainer - lxccontainer.c:wait_on_daemonized_start:877 - Received container state "ABORTING" instead of "RUNNING"
```

**Parsing LXC Logs:**

```bash
# Extract errors
grep ERROR /var/log/lxc/<container-name>.log

# Extract warnings and errors
grep -E '(WARN|ERROR)' /var/log/lxc/<container-name>.log

# Container start/stop events
grep -E '(started|stopped)' /var/log/lxc/<container-name>.log

# Failed operations
grep -i 'failed' /var/log/lxc/<container-name>.log

# Timeline of events with timestamps
awk '{print $2, $4, $6}' /var/log/lxc/<container-name>.log | sort
```

### LXD Daemon Logs

**Accessing LXD Daemon Logs:**

```bash
# SystemD installation
journalctl -u lxd

# Snap installation
journalctl -u snap.lxd.daemon

# Follow daemon logs
journalctl -u lxd -f

# Logs since time
journalctl -u lxd --since "1 hour ago"

# Export to JSON
journalctl -u lxd -o json > lxd-daemon.json

# Filter by priority
journalctl -u lxd -p err
```

**LXD Log Analysis:**

```bash
# API requests
journalctl -u lxd | grep -E 'method=(GET|POST|PUT|DELETE|PATCH)'

# Authentication failures
journalctl -u lxd | grep -i 'authentication failed'

# Container lifecycle events
journalctl -u lxd | grep -E 'Starting|Stopping|container'

# Database operations
journalctl -u lxd | grep -i 'database'

# Network operations
journalctl -u lxd | grep -i 'network'
```

### LXD Info and Monitoring Commands

**Container Information:**

```bash
# List all containers with status
lxc list

# Detailed container info
lxc info <container-name>

# Container configuration
lxc config show <container-name>

# Container state
lxc info <container-name> | grep Status

# Resource usage
lxc info <container-name> | grep -A 20 "Resources:"
```

**Container Events:**

```bash
# Monitor LXD events (real-time)
lxc monitor

# Events for specific container
lxc monitor --type=lifecycle <container-name>

# All event types
lxc monitor --type=logging,lifecycle,operation
```

**Event Types:**

```
lifecycle: Container start, stop, restart, create, delete
logging: Log messages from containers
operation: Long-running operations (create, copy, move)
```

### Container Exec and Command Logs

**Executing Commands with Logging:**

```bash
# Execute command in container
lxc exec <container-name> -- <command>

# Interactive shell
lxc exec <container-name> -- /bin/bash

# Run command as specific user
lxc exec <container-name> -- su - username -c "command"

# Execute with environment variables
lxc exec <container-name> --env KEY=value -- command
```

**[Unverified]** LXD does not maintain a built-in audit log of exec commands by default. To track exec activity, you would need to:

1. Enable comprehensive daemon logging
2. Monitor journald for exec operations
3. Implement custom audit logging

**Monitoring Exec Activity:**

```bash
# Monitor for exec operations in daemon logs
journalctl -u lxd -f | grep exec

# Extract exec commands from logs (if verbose logging enabled)
journalctl -u lxd | grep -i "executing command"
```

### LXD Storage and Network Logs

**Storage Pool Operations:**

```bash
# List storage pools
lxc storage list

# Storage pool info
lxc storage info <pool-name>

# Storage volume operations from logs
journalctl -u lxd | grep -i 'storage'

# Volume mount/unmount events
journalctl -u lxd | grep -E '(mount|unmount)'
```

**Network Operations:**

```bash
# List networks
lxc network list

# Network info
lxc network show <network-name>

# Network operations from logs
journalctl -u lxd | grep -i 'network'

# Bridge operations
journalctl -u lxd | grep -i 'bridge'

# DHCP operations
journalctl -u lxd | grep -i 'dhcp'
```

### LXC/LXD Security Analysis

**Privileged Container Detection:**

```bash
# List all containers and check security settings
for container in $(lxc list -c n --format csv); do
    echo "=== $container ==="
    lxc config get $container security.privileged
    lxc config get $container security.nesting
done

# Find privileged containers
lxc list -c ns4 --format json | jq -r '.[] | select(.config["security.privileged"]=="true") | .name'

# Find containers with nesting enabled
lxc list -c ns4 --format json | jq -r '.[] | select(.config["security.nesting"]=="true") | .name'
```

**Container Escape Indicators:**

```bash
# Check for suspicious mounts
for container in $(lxc list -c n --format csv); do
    echo "=== $container Devices ==="
    lxc config device list $container
done

# Check for host filesystem mounts
lxc list -c ns4 --format json | jq -r '.[] | select(.devices | to_entries[] | .value.type=="disk" and .value.source=="/") | .name'

# Check for /proc or /sys mounts
lxc list -c ns4 --format json | jq -r '.[] | select(.devices | to_entries[] | .value.source | test("/proc|/sys")) | .name'
```

**Capability Analysis:**

```bash
# Check raw.lxc configuration for capability modifications
for container in $(lxc list -c n --format csv); do
    echo "=== $container Capabilities ==="
    lxc config show $container | grep -A 5 'raw.lxc'
done

# Find containers with added capabilities
lxc list --format json | jq -r '.[] | select(.config | to_entries[] | .key | contains("raw.lxc") and .value |
```

---

## VMware Logs

### Log File Locations

**ESXi Host Logs:**

```bash
/var/log/vmkernel.log          # Core VMkernel events
/var/log/vmware/hostd.log       # Host management service
/var/log/vmware/vpxa.log        # vCenter agent logs
/var/log/auth.log               # Authentication events
/var/log/shell.log              # ESXi Shell activity
/var/log/vmksummary.log         # System state summaries
/var/log/vobd.log               # VMware ESXi Observation
```

**Guest VM Logs (Linux):**

```bash
/var/log/vmware-*.log           # VMware Tools logs
/var/log/vmware-vmsvc.log       # Service daemon logs
/var/log/vmware-vmusr.log       # User-level daemon logs
```

**Guest VM Logs (Windows):**

```
C:\ProgramData\VMware\VMware Tools\logs\vmware-*.log
C:\ProgramData\VMware\VMware Tools\logs\vmsvc.log
C:\ProgramData\VMware\VMware Tools\logs\vmtoolsd.log
```

**VM-Specific Logs (Workstation/Fusion):**

```bash
# Location varies by VM
<VM_directory>/vmware.log       # Main VM log
<VM_directory>/vmware-*.log     # Additional activity logs
```

### Critical Event Detection

**VM Power State Changes:**

```bash
# ESXi - Track VM power operations
grep -E "(poweredOn|poweredOff|suspended|reset)" /var/log/vmkernel.log

# Extract VM power events with timestamps
awk '/Powering|Power state/ {print $1, $2, $3, $0}' /var/log/vmkernel.log

# Identify unexpected reboots
grep -i "panic\|crash\|fault" /var/log/vmkernel.log
```

**Guest-Host Communication:**

```bash
# VMware Tools activity
grep "vmtoolsd" /var/log/vmware-vmsvc.log

# Clipboard operations (potential data exfiltration)
grep -i "clipboard" /var/log/vmware-vmusr.log

# Shared folder access
grep -i "hgfs\|shared folder" /var/log/vmware-vmsvc.log

# Drag-and-drop operations
grep -i "dnd\|drag" /var/log/vmware-vmusr.log
```

**Network Activity Monitoring:**

```bash
# Virtual network adapter changes
grep -E "vnic|vmnic" /var/log/vmkernel.log

# Port group modifications
grep -i "portgroup" /var/log/hostd.log

# MAC address changes
grep -i "mac.*change" /var/log/vmkernel.log | grep -v "unchanged"

# Promiscuous mode detection
grep -i "promiscuous" /var/log/vmkernel.log
```

**Snapshot Operations:**

```bash
# Snapshot creation/deletion
grep -i "snapshot" /var/log/hostd.log

# Extract snapshot timeline
awk '/snapshot.*create|snapshot.*delete/ {print $1, $2, $0}' /var/log/hostd.log

# Identify snapshot consolidation
grep -i "consolidate" /var/log/hostd.log
```

**Authentication and Access:**

```bash
# ESXi shell access
grep -E "login|logout|session" /var/log/shell.log

# Failed authentication attempts
grep -i "failed\|incorrect" /var/log/auth.log

# Root access events
grep "root" /var/log/auth.log | grep -v "cron"

# SSH connections
grep "sshd" /var/log/auth.log
```

**VM Configuration Changes:**

```bash
# Hardware modifications
grep -E "AddDevice|RemoveDevice|ReconfigVM" /var/log/hostd.log

# Memory/CPU changes
grep -E "memSize|numvcpus" /var/log/hostd.log

# Disk operations
grep -E "diskadditions|diskremoval" /var/log/hostd.log

# USB device passthrough
grep -i "usb.*attach\|usb.*detach" /var/log/vmware.log
```

### Escape Attempt Detection

**Guest-to-Host Breakout Indicators:**

```bash
# VMware backdoor command attempts
grep -i "backdoor" /var/log/vmware.log

# VMCI (Virtual Machine Communication Interface) anomalies
grep -i "vmci" /var/log/vmkernel.log | grep -iE "error|fail|denied"

# VMX process crashes (potential exploit attempts)
grep -E "vmx.*crash|vmx.*fault" /var/log/hostd.log

# Abnormal VMCI socket creation
grep "vmci.*socket.*create" /var/log/vmkernel.log | \
  awk '{print $NF}' | sort | uniq -c | sort -rn
```

**Privilege Escalation Patterns:**

```bash
# Unauthorized vSphere API calls
grep -E "privilege.*denied|permission.*denied" /var/log/hostd.log

# Suspicious VM operations from low-privilege accounts
grep -E "ReconfigVM|PowerOn|PowerOff" /var/log/hostd.log | \
  grep -v "root@"

# Guest operation execution
grep "GuestOperations" /var/log/hostd.log
```

**Resource Abuse Detection:**

```bash
# CPU/Memory limits exceeded
grep -E "limit.*exceed|resource.*constraint" /var/log/vmkernel.log

# Excessive disk I/O
grep -E "latency.*high|scsi.*timeout" /var/log/vmkernel.log

# Network bandwidth anomalies
grep -E "pkt.*drop|queue.*full" /var/log/vmkernel.log
```

### Analysis Scripts

**VM Activity Timeline:**

```bash
#!/bin/bash
# vmware_timeline.sh - Create VM activity timeline

LOGDIR="/var/log"

echo "=== VMware Activity Timeline ==="
echo ""

# Power events
echo "[+] Power State Changes:"
grep -h "Power state" "$LOGDIR/vmkernel.log" 2>/dev/null | tail -20

echo ""
echo "[+] Configuration Changes:"
grep -h "ReconfigVM" "$LOGDIR/vmware/hostd.log" 2>/dev/null | tail -10

echo ""
echo "[+] Authentication Events:"
grep -h "Accepted\|Failed" "$LOGDIR/auth.log" 2>/dev/null | tail -10

echo ""
echo "[+] Snapshot Operations:"
grep -h -i "snapshot" "$LOGDIR/vmware/hostd.log" 2>/dev/null | tail -10
```

**Security Event Detector:**

```bash
#!/bin/bash
# vmware_security_audit.sh

HOSTD_LOG="/var/log/vmware/hostd.log"
VMKERNEL_LOG="/var/log/vmkernel.log"
AUTH_LOG="/var/log/auth.log"

echo "=== VMware Security Audit ==="
echo ""

echo "[!] Failed Authentication Attempts:"
grep -i "failed\|incorrect" "$AUTH_LOG" 2>/dev/null | tail -5

echo ""
echo "[!] Privilege Violations:"
grep -E "privilege.*denied|permission.*denied" "$HOSTD_LOG" 2>/dev/null | tail -5

echo ""
echo "[!] Promiscuous Mode Enabled:"
grep -i "promiscuous.*enable" "$VMKERNEL_LOG" 2>/dev/null

echo ""
echo "[!] VM Crashes/Faults:"
grep -E "panic|crash|fault" "$VMKERNEL_LOG" 2>/dev/null | tail -5

echo ""
echo "[!] Unusual Guest Operations:"
grep "GuestOperations" "$HOSTD_LOG" 2>/dev/null | \
  grep -v "VMware Tools" | tail -5
```

---

## VirtualBox Logs

### Log File Locations

**Host System Logs:**

Linux:

```bash
~/.VirtualBox/VBoxSVC.log       # VirtualBox service log
~/.VirtualBox/VBoxXPCOMC.log    # XPCOM component log
/var/log/vbox-install.log       # Installation log
```

Windows:

```
%USERPROFILE%\.VirtualBox\VBoxSVC.log
C:\Windows\system32\LogFiles\VBoxHardening.log
```

**Per-VM Logs:**

```bash
~/VirtualBox VMs/<VM_Name>/Logs/VBox.log         # Current session
~/VirtualBox VMs/<VM_Name>/Logs/VBox.log.1       # Previous session
~/VirtualBox VMs/<VM_Name>/Logs/VBox.log.2       # Older sessions
~/VirtualBox VMs/<VM_Name>/Logs/VBoxHardening.log # Security events
```

**Guest Additions Logs (Guest OS):**

Linux:

```bash
/var/log/vboxadd-install.log
/var/log/vboxadd-setup.log
/var/log/VBoxClient-all.log
```

Windows:

```
C:\Windows\Temp\VBoxGuestAdditions.log
C:\Windows\Temp\vbox-*.log
```

### Critical Event Analysis

**VM Lifecycle Events:**

```bash
# Power state changes
grep -E "Changing the VM state|Power" ~/VirtualBox\ VMs/*/Logs/VBox.log

# VM start events
grep "VirtualBox VM" ~/VirtualBox\ VMs/*/Logs/VBox.log | grep "started"

# Abnormal terminations
grep -i "abort\|crash\|fatal\|panic" ~/VirtualBox\ VMs/*/Logs/VBox.log

# Saved state operations
grep -i "save.*state\|restore.*state" ~/VirtualBox\ VMs/*/Logs/VBox.log
```

**Hardware Configuration Tracking:**

```bash
# CPU configuration
grep -i "cpu\|processor" ~/VirtualBox\ VMs/*/Logs/VBox.log | grep -i "enable\|config"

# Memory allocation
grep -i "ram\|memory" ~/VirtualBox\ VMs/*/Logs/VBox.log | grep -iE "[0-9]+.*MB"

# Storage attachments
grep -i "storage.*attach\|disk.*attach" ~/VirtualBox\ VMs/*/Logs/VBox.log

# Network adapter configuration
grep -E "Network.*Adapter.*[0-9]" ~/VirtualBox\ VMs/*/Logs/VBox.log

# USB device passthrough
grep -i "usb" ~/VirtualBox\ VMs/*/Logs/VBox.log | grep -iE "attach|detach"
```

**Guest Additions Activity:**

```bash
# Guest Additions version
grep "Guest Additions" ~/VirtualBox\ VMs/*/Logs/VBox.log | grep "version"

# Shared folders
grep -i "shared.*folder" ~/VirtualBox\ VMs/*/Logs/VBox.log

# Clipboard operations
grep -i "clipboard" ~/VirtualBox\ VMs/*/Logs/VBox.log

# Drag-and-drop activity
grep -i "drag.*drop\|dnd" ~/VirtualBox\ VMs/*/Logs/VBox.log

# Seamless mode transitions
grep -i "seamless" ~/VirtualBox\ VMs/*/Logs/VBox.log
```

**Network Monitoring:**

```bash
# NAT port forwarding rules
grep -i "nat.*rule\|port.*forward" ~/VirtualBox\ VMs/*/Logs/VBox.log

# Network mode changes
grep -E "NAT|Bridged|Host-only|Internal" ~/VirtualBox\ VMs/*/Logs/VBox.log

# Packet capture
grep -i "pcap\|capture" ~/VirtualBox\ VMs/*/Logs/VBox.log

# Network adapter MAC addresses
grep -i "mac.*address" ~/VirtualBox\ VMs/*/Logs/VBox.log
```

**Snapshot Operations:**

```bash
# Snapshot creation
grep -i "taking.*snapshot\|snapshot.*taken" ~/VirtualBox\ VMs/*/Logs/VBox.log

# Snapshot restoration
grep -i "restor.*snapshot" ~/VirtualBox\ VMs/*/Logs/VBox.log

# Snapshot deletion
grep -i "delet.*snapshot" ~/VirtualBox\ VMs/*/Logs/VBox.log
```

### Security Event Detection

**Hardening and Security Violations:**

```bash
# Hardening log analysis
grep -i "denied\|violation\|refused" ~/VirtualBox\ VMs/*/Logs/VBoxHardening.log

# Privilege escalation attempts
grep -i "privilege\|elevation" ~/VirtualBox\ VMs/*/Logs/VBoxHardening.log

# Suspicious process injection
grep -i "inject\|hook" ~/VirtualBox\ VMs/*/Logs/VBoxHardening.log
```

**Guest-to-Host Communication Anomalies:**

```bash
# HGCM (Host-Guest Communication Manager) errors
grep -i "hgcm.*error\|hgcm.*fail" ~/VirtualBox\ VMs/*/Logs/VBox.log

# Shared buffer overflows
grep -i "buffer.*overflow\|buffer.*exceed" ~/VirtualBox\ VMs/*/Logs/VBox.log

# Invalid guest requests
grep -i "invalid.*request\|malformed" ~/VirtualBox\ VMs/*/Logs/VBox.log
```

**Escape Attempt Indicators:**

```bash
# Guest additions exploit attempts
grep -iE "exploit|overflow|corruption" ~/VirtualBox\ VMs/*/Logs/VBox.log

# Unexpected HGCM service calls
grep "HGCM.*call" ~/VirtualBox\ VMs/*/Logs/VBox.log | \
  awk '{print $NF}' | sort | uniq -c | sort -rn

# VMM (Virtual Machine Monitor) errors
grep -i "vmm.*error\|vmm.*fatal" ~/VirtualBox\ VMs/*/Logs/VBox.log
```

**Resource Abuse:**

```bash
# CPU usage warnings
grep -i "cpu.*100%\|processor.*overload" ~/VirtualBox\ VMs/*/Logs/VBox.log

# Memory pressure
grep -i "memory.*low\|out.*of.*memory" ~/VirtualBox\ VMs/*/Logs/VBox.log

# Disk I/O errors
grep -i "disk.*error\|i/o.*error" ~/VirtualBox\ VMs/*/Logs/VBox.log
```

### VirtualBox Log Parser

```bash
#!/bin/bash
# vbox_log_analyzer.sh - Analyze VirtualBox VM logs

VM_NAME="$1"
LOG_DIR="$HOME/VirtualBox VMs/$VM_NAME/Logs"

if [ ! -d "$LOG_DIR" ]; then
    echo "[-] VM logs not found: $LOG_DIR"
    exit 1
fi

echo "=== VirtualBox Log Analysis: $VM_NAME ==="
echo ""

echo "[+] VM Sessions:"
ls -lh "$LOG_DIR"/VBox.log* | awk '{print $6, $7, $8, $9}'

echo ""
echo "[+] Power State History:"
grep -h "Changing the VM state" "$LOG_DIR"/VBox.log* | \
  awk '{print $1, $2, $0}' | tail -10

echo ""
echo "[+] Hardware Configuration:"
echo "  CPU: $(grep -h "CPU#" "$LOG_DIR"/VBox.log | head -1)"
echo "  RAM: $(grep -h "RAM" "$LOG_DIR"/VBox.log | grep -i "size" | head -1)"
echo "  Storage: $(grep -h "StorageController" "$LOG_DIR"/VBox.log | wc -l) controllers"

echo ""
echo "[+] Network Configuration:"
grep -h "Network.*Adapter" "$LOG_DIR"/VBox.log | head -4

echo ""
echo "[+] Guest Additions:"
grep -h "Guest Additions" "$LOG_DIR"/VBox.log | tail -1

echo ""
echo "[+] Warnings/Errors:"
grep -h -iE "warning|error" "$LOG_DIR"/VBox.log | tail -10

echo ""
echo "[+] Security Events:"
if [ -f "$LOG_DIR/VBoxHardening.log" ]; then
    grep -h -i "denied\|violation" "$LOG_DIR"/VBoxHardening.log | tail -5
else
    echo "  No hardening log found"
fi
```

---

## Container Runtime Logs

### Docker Logs

**Docker Daemon Logs:**

Linux (systemd):

```bash
journalctl -u docker.service           # Docker daemon logs
journalctl -u docker.service -f        # Follow daemon logs
journalctl -u docker.service --since "1 hour ago"

# Alternative locations
/var/log/docker.log                    # Some distributions
/var/log/syslog | grep docker          # Syslog integration
```

**Container-Specific Logs:**

```bash
# View container logs
docker logs <container_id>
docker logs <container_name>

# Follow logs in real-time
docker logs -f <container_id>

# Show timestamps
docker logs -t <container_id>

# Tail last N lines
docker logs --tail 100 <container_id>

# Time-range filtering
docker logs --since 2024-01-01T00:00:00 <container_id>
docker logs --since 1h <container_id>
```

**Log File Locations:**

```bash
# JSON log driver (default)
/var/lib/docker/containers/<container_id>/<container_id>-json.log

# Direct log file inspection
cat /var/lib/docker/containers/*/*-json.log | jq '.'
```

### Docker Event Monitoring

**Real-Time Event Stream:**

```bash
# Monitor all Docker events
docker events

# Filter by event type
docker events --filter event=start
docker events --filter event=stop
docker events --filter event=die
docker events --filter event=kill

# Filter by container
docker events --filter container=<container_name>

# Filter by image
docker events --filter image=<image_name>

# Time-range events
docker events --since 1h
docker events --since '2024-01-01T00:00:00' --until '2024-01-02T00:00:00'
```

**Event Types to Monitor:**

```bash
# Container lifecycle
docker events --filter event=create
docker events --filter event=destroy
docker events --filter event=start
docker events --filter event=stop
docker events --filter event=restart
docker events --filter event=pause
docker events --filter event=unpause

# Network events
docker events --filter type=network
docker events --filter event=connect
docker events --filter event=disconnect

# Volume events
docker events --filter type=volume
docker events --filter event=mount
docker events --filter event=unmount

# Image events
docker events --filter type=image
docker events --filter event=pull
docker events --filter event=push
docker events --filter event=delete
```

### Critical Container Activity Detection

**Container Breakout Attempts:**

```bash
# Privileged container creation
docker ps -a --filter "label=privileged=true" --format "table {{.ID}}\t{{.Image}}\t{{.Command}}"

# Identify privileged containers
docker inspect <container_id> | jq '.[0].HostConfig.Privileged'

# Capability additions (potential escape vector)
docker inspect <container_id> | jq '.[0].HostConfig.CapAdd'

# Host network mode (bypasses network isolation)
docker ps --filter "network=host"

# Host PID namespace sharing
docker inspect <container_id> | jq '.[0].HostConfig.PidMode'

# Volume mounts to sensitive host paths
docker inspect <container_id> | jq '.[0].Mounts[] | select(.Source | startswith("/"))'
```

**Suspicious Mount Points:**

```bash
# Root filesystem mounts
docker ps -a --format "{{.ID}}" | xargs -I {} docker inspect {} | \
  jq '.[] | select(.Mounts[].Source == "/" or .Mounts[].Source == "/etc" or .Mounts[].Source == "/var")'

# Docker socket mounting (dangerous)
docker inspect <container_id> | jq '.[] | select(.Mounts[].Source | contains("docker.sock"))'

# Detect all socket mounts
find /var/lib/docker/containers/ -name "config.v2.json" -exec grep -l "docker.sock" {} \;
```

**Resource Abuse Detection:**

```bash
# High CPU usage containers
docker stats --no-stream --format "table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}" | \
  sort -k2 -rn | head -10

# Memory consumption
docker stats --no-stream --format "table {{.Container}}\t{{.MemPerc}}\t{{.MemUsage}}" | \
  sort -k2 -rn

# Network I/O analysis
docker stats --no-stream --format "table {{.Container}}\t{{.NetIO}}"

# Block I/O analysis
docker stats --no-stream --format "table {{.Container}}\t{{.BlockIO}}"
```

**Image Analysis:**

```bash
# Identify unsigned/unverified images
docker images --format "{{.Repository}}:{{.Tag}}" | \
  xargs -I {} sh -c 'docker trust inspect {} 2>/dev/null || echo "Unsigned: {}"'

# List all image layers
docker history <image_name>

# Inspect image for suspicious commands
docker history --no-trunc <image_name> | grep -iE "curl|wget|nc|netcat|/bin/bash"

# Check for secrets in image layers
docker save <image_name> | tar -xO | grep -a -E "(password|secret|api_key|token)"
```

### Container Runtime Command Execution

**Exec Events Detection:**

```bash
# Monitor exec commands (requires audit logging)
journalctl -u docker.service | grep "exec"

# Audit exec into containers
docker events --filter event=exec_create
docker events --filter event=exec_start

# List recent exec sessions (if audit enabled)
journalctl -u docker.service | grep "exec" | tail -20
```

[Unverified] Docker daemon logs may not always capture exec commands depending on logging configuration.

**Shell Access Monitoring:**

```bash
# Detect interactive shell spawns
journalctl -u docker.service | grep -E "/bin/bash|/bin/sh" | tail -20

# Container restarts (may indicate compromise)
docker events --filter event=restart | head -20
```

### Kubernetes/Container Orchestration Logs

**Kubernetes Pod Logs:**

```bash
# View pod logs
kubectl logs <pod_name>
kubectl logs <pod_name> -c <container_name>

# Follow logs
kubectl logs -f <pod_name>

# Previous container instance logs
kubectl logs <pod_name> --previous

# All containers in pod
kubectl logs <pod_name> --all-containers=true

# Timestamp inclusion
kubectl logs <pod_name> --timestamps=true

# Tail logs
kubectl logs <pod_name> --tail=100
```

**Kubernetes Events:**

```bash
# View cluster events
kubectl get events --all-namespaces --sort-by='.lastTimestamp'

# Filter by namespace
kubectl get events -n <namespace>

# Warning events only
kubectl get events --field-selector type=Warning

# Events for specific resource
kubectl get events --field-selector involvedObject.name=<pod_name>
```

**Critical Kubernetes Security Events:**

```bash
# Failed pod scheduling
kubectl get events | grep -i "failedscheduling"

# Failed image pulls
kubectl get events | grep -i "failed.*pull\|imagepullbackoff"

# Container restarts
kubectl get events | grep -i "backoff.*restarting"

# Privilege escalation
kubectl get events | grep -i "privilege"

# Security policy violations
kubectl get events | grep -i "security.*violation\|policy.*denied"
```

**Pod Security Context Analysis:**

```bash
# Identify privileged pods
kubectl get pods -o json | jq '.items[] | select(.spec.containers[].securityContext.privileged == true) | .metadata.name'

# Host network pods
kubectl get pods -o json | jq '.items[] | select(.spec.hostNetwork == true) | .metadata.name'

# Host PID namespace
kubectl get pods -o json | jq '.items[] | select(.spec.hostPID == true) | .metadata.name'

# Capability additions
kubectl get pods -o json | jq '.items[] | select(.spec.containers[].securityContext.capabilities.add != null) | {name: .metadata.name, caps: .spec.containers[].securityContext.capabilities.add}'
```

### Containerd Logs

**Containerd Daemon Logs:**

```bash
# Systemd-based systems
journalctl -u containerd.service
journalctl -u containerd.service -f

# Log file locations
/var/log/containerd/containerd.log
```

**Container Operations:**

```bash
# List containers
ctr containers list

# Container task logs
ctr task ls

# Namespace inspection
ctr namespace ls

# Image operations
ctr image ls
```

### Podman Logs

**Podman Container Logs:**

```bash
# View container logs
podman logs <container_id>

# Follow logs
podman logs -f <container_id>

# System events
podman events

# Systemd journal integration
journalctl -u podman.service
```

**Rootless Container Detection:**

```bash
# User-specific containers
podman ps --all

# Rootless container locations
~/.local/share/containers/storage/
```

---

## Consolidated Analysis Strategies

### Multi-Platform Container Security Audit

```bash
#!/bin/bash
# container_security_audit.sh
# Comprehensive container security analysis

echo "=== Container Security Audit ==="
echo ""

# Docker Analysis
if command -v docker &> /dev/null; then
    echo "[+] Docker Containers:"
    echo "  Total: $(docker ps -a | wc -l)"
    echo "  Running: $(docker ps | wc -l)"
    
    echo ""
    echo "[!] Privileged Containers:"
    docker ps --quiet | xargs docker inspect | \
      jq -r '.[] | select(.HostConfig.Privileged == true) | .Name'
    
    echo ""
    echo "[!] Docker Socket Mounts:"
    docker ps --quiet | xargs docker inspect | \
      jq -r '.[] | select(.Mounts[].Source | contains("docker.sock")) | .Name'
    
    echo ""
    echo "[!] Host Network Mode:"
    docker ps --filter "network=host" --format "{{.Names}}"
fi

# Kubernetes Analysis
if command -v kubectl &> /dev/null; then
    echo ""
    echo "[+] Kubernetes Pods:"
    echo "  Total: $(kubectl get pods --all-namespaces | wc -l)"
    
    echo ""
    echo "[!] Privileged Pods:"
    kubectl get pods -A -o json | \
      jq -r '.items[] | select(.spec.containers[].securityContext.privileged == true) | .metadata.namespace + "/" + .metadata.name'
    
    echo ""
    echo "[!] Recent Warning Events:"
    kubectl get events -A --field-selector type=Warning --sort-by='.lastTimestamp' | tail -10
fi

echo ""
echo "[+] Recent Container Events (Last Hour):"
if command -v docker &> /dev/null; then
    docker events --since 1h --until 0s | tail -20
fi
```

### Cross-Platform Log Correlation

```bash
#!/bin/bash
# virtualization_timeline.sh
# Create unified timeline across virtualization platforms

echo "=== Virtualization Event Timeline ==="
echo ""

# VMware events
if [ -f /var/log/vmkernel.log ]; then
    echo "[VMware]" > /tmp/virt_timeline.txt
    grep -h "Power state" /var/log/vmkernel.log | \
      awk '{print $1, $2, "[VMware]", $0}' >> /tmp/virt_timeline.txt
fi

# VirtualBox events
if [ -d "$HOME/VirtualBox VMs" ]; then
    echo "[VirtualBox]" >> /tmp/virt_timeline.txt
    find "$HOME/VirtualBox VMs" -name "VBox.log" -exec \
      grep -h "Changing the VM state" {} \; | \
      awk '{print $1, $2, "[VBox]", $0}' >> /tmp/virt_timeline.txt
fi

# Docker events
if command -v docker &> /dev/null; then
    echo "[Docker]" >> /tmp/virt_timeline.txt
    docker events --since 24h --until 0s --format '{{.Time}} [Docker] {{.Action}} {{.Actor.Attributes.name}}' \
      >> /tmp/virt_timeline.txt 2>/dev/null
fi

# Sort and display
sort /tmp/virt_timeline.txt | tail -50
rm /tmp/virt_timeline.txt
```

### Container Escape Detection

```bash
#!/bin/bash
# detect_container_escape.sh
# Detect potential container escape attempts

echo "=== Container Escape Detection ==="
echo ""

echo "[!] Checking for common escape vectors..."
echo ""

# Check for privileged containers
echo "[+] Privileged Containers:"
docker ps -q | xargs docker inspect | \
  jq -r '.[] | select(.HostConfig.Privileged == true) | "ALERT: " + .Name + " is privileged"'

# Check for dangerous capabilities
echo ""
echo "[+] Dangerous Capability Additions:"
docker ps -q | xargs docker inspect | \
  jq -r '.[] | select(.HostConfig.CapAdd != null) | "WARN: " + .Name + " has caps: " + (.HostConfig.CapAdd | join(", "))'

# Check for host filesystem mounts
echo ""
echo "[+] Root Filesystem Mounts:"
docker ps -q | xargs docker inspect | \
  jq -r '.[] | select(.Mounts[].Source == "/" or .Mounts[].Source | startswith("/etc") or .Mounts[].Source | startswith("/var")) | "CRITICAL: " + .Name + " mounts: " + (.Mounts[].Source | tostring)'

# Check for recent exec commands
echo ""
echo "[+] Recent Exec Commands:"
journalctl -u docker.service --since "1 hour ago" | grep "exec" | tail -10

# Check for container restarts (possible crash after exploit)
echo ""
echo "[+] Containers with Recent Restarts:"
docker ps --format "{{.Names}}" | while read container; do
    restarts=$(docker inspect "$container" | jq -r '.[0].RestartCount')
    if [ "$restarts" -gt 0 ]; then
        echo "WARN: $container has $restarts restarts"
    fi
done
```

---

## CTF-Specific Considerations

**Hidden Data in Container Layers:** [Inference] Flags or sensitive information may be embedded in:

- Image layer history (deleted files still in layers)
- Environment variables in container config
- Volume mounts revealing host paths
- Container metadata and labels

**Common CTF Container Scenarios:**

- Docker socket mounted containers (Docker-in-Docker escape)
- Privileged containers with `/proc` access
- Kubernetes secrets mounted as volumes
- Shared memory segments between containers
- Network policy misconfigurations

**Analysis Techniques:**

```bash
# Extract all environment variables from running containers
docker ps -q | xargs -I {} docker inspect {} | jq -r '.[].Config.Env[]'

# Search for flags in container filesystems
docker export <container_id> | tar -xO | grep -a "flag{" 2>/dev/null

# Analyze image layers for secrets
docker save <image> -o image.tar
tar -xf image.tar
find . -name "layer.tar" -exec tar -xOf {} \; | strings | grep -i "flag\|password\|secret"
```

## Advanced Container Forensics

### Docker Image Layer Analysis

**Layer Extraction and Inspection:**

```bash
# Export image to tarball
docker save <image_name> -o image.tar

# Extract image layers
mkdir image_analysis && cd image_analysis
tar -xf ../image.tar

# List all layer directories
ls -la */

# Extract each layer
for layer in */layer.tar; do
    mkdir -p "extracted/$(dirname $layer)"
    tar -xf "$layer" -C "extracted/$(dirname $layer)"
done

# Search for sensitive data across all layers
find extracted/ -type f -exec grep -l "password\|secret\|token\|key\|flag" {} \;

# Find deleted files still present in layers
find extracted/ -name ".wh.*"  # Whiteout files indicate deletions
```

**Manifest Analysis:**

```bash
# View image manifest
cat manifest.json | jq '.'

# Extract layer order
cat manifest.json | jq '.[0].Layers[]'

# View image configuration
cat */json | jq '.'

# Extract command history
cat */json | jq '.config.Cmd'
cat */json | jq '.container_config.Cmd'

# Environment variables
cat */json | jq '.config.Env[]'
```

**Dockerfile Reconstruction:**

```bash
# Reconstruct Dockerfile from history
docker history --no-trunc <image_name> | \
  tac | \
  awk '{if ($1 != "<missing>") print $NF}' | \
  sed 's/^\/bin\/sh -c #(nop) //' | \
  sed 's/^\/bin\/sh -c /RUN /'

# Extract detailed layer commands
docker history --no-trunc --format "{{.CreatedBy}}" <image_name>
```

**File System Diff Analysis:**

```bash
# Compare container filesystem with image
docker diff <container_id>

# Export changes only
docker export <container_id> > container_fs.tar
docker save <image_name> > image_fs.tar

# Compare extracted filesystems
# (requires manual extraction and diff)
```

### Container Runtime Forensics

**Live Container Analysis:**

```bash
# Inspect running processes
docker top <container_id>
docker top <container_id> aux

# Extended process information
docker exec <container_id> ps auxf

# Network connections
docker exec <container_id> netstat -tulpn
docker exec <container_id> ss -tulpn

# Open files
docker exec <container_id> lsof

# Environment variables
docker exec <container_id> env

# Loaded libraries
docker exec <container_id> ldconfig -p
```

**Memory Analysis:**

```bash
# Dump container memory (requires appropriate permissions)
docker inspect <container_id> | jq -r '.[0].State.Pid' | \
  xargs -I {} gdb -p {} -batch -ex 'generate-core-file /tmp/container_core.dump' -ex 'quit'

# Alternative: use gcore
docker inspect <container_id> | jq -r '.[0].State.Pid' | \
  xargs -I {} gcore -o /tmp/container_mem {}

# Search memory dump for strings
strings /tmp/container_mem.* | grep -i "flag\|password\|secret"
```

**Filesystem Snapshot:**

```bash
# Create container snapshot
docker commit <container_id> evidence:snapshot

# Export for offline analysis
docker save evidence:snapshot -o evidence_snapshot.tar

# Mount container filesystem (if using overlay2)
CONTAINER_ID="<container_id>"
OVERLAY_DIR="/var/lib/docker/overlay2"

# Find container's overlay directory
docker inspect $CONTAINER_ID | jq -r '.[0].GraphDriver.Data.MergedDir'

# Create read-only bind mount
MERGED_DIR=$(docker inspect $CONTAINER_ID | jq -r '.[0].GraphDriver.Data.MergedDir')
mkdir -p /mnt/container_forensics
mount --bind -o ro "$MERGED_DIR" /mnt/container_forensics
```

**Network Traffic Capture:**

```bash
# Capture traffic from container namespace
CONTAINER_PID=$(docker inspect -f '{{.State.Pid}}' <container_id>)
nsenter -t $CONTAINER_PID -n tcpdump -i eth0 -w /tmp/container_traffic.pcap

# Alternative: capture on Docker bridge
docker network inspect bridge | jq -r '.[0].IPAM.Config[0].Subnet'
tcpdump -i docker0 -w /tmp/docker_traffic.pcap

# Analyze captured traffic
tcpdump -r /tmp/container_traffic.pcap -nn
wireshark /tmp/container_traffic.pcap
```

### Docker Registry and Distribution

**Registry Access Logs:**

```bash
# Docker Hub pulls (from daemon logs)
journalctl -u docker.service | grep "pull"

# Local registry logs (if running)
docker logs <registry_container_id>

# Extract image pull history
grep "pull" /var/log/docker.log | awk '{print $NF}' | sort | uniq -c
```

**Image Provenance Tracking:**

```bash
# View image labels
docker inspect <image_name> | jq '.[0].Config.Labels'

# Check image signatures
docker trust inspect <image_name>

# View image digest
docker images --digests <image_name>

# Image creation date
docker inspect <image_name> | jq '.[0].Created'

# Image author/maintainer
docker inspect <image_name> | jq '.[0].Author'
```

**Registry API Analysis:**

```bash
# Query local registry (if accessible)
curl http://localhost:5000/v2/_catalog

# List image tags
curl http://localhost:5000/v2/<image_name>/tags/list

# Get image manifest
curl http://localhost:5000/v2/<image_name>/manifests/<tag>
```

---

## Container Orchestration Deep Dive

### Kubernetes Audit Logs

**Audit Log Configuration:**

```bash
# Check if audit logging is enabled
kubectl get pods -n kube-system | grep kube-apiserver

# View API server configuration
kubectl describe pod <kube-apiserver-pod> -n kube-system | grep audit

# Audit log location (on master node)
/var/log/kubernetes/audit/audit.log
```

**Audit Log Analysis:**

```bash
# View recent audit entries
tail -100 /var/log/kubernetes/audit/audit.log | jq '.'

# Filter by user
cat /var/log/kubernetes/audit/audit.log | jq 'select(.user.username == "admin")'

# Filter by verb (action)
cat /var/log/kubernetes/audit/audit.log | jq 'select(.verb == "delete")'

# Filter by resource
cat /var/log/kubernetes/audit/audit.log | jq 'select(.objectRef.resource == "secrets")'

# Failed authentication attempts
cat /var/log/kubernetes/audit/audit.log | jq 'select(.responseStatus.code >= 400)'

# Privileged operations
cat /var/log/kubernetes/audit/audit.log | \
  jq 'select(.objectRef.resource == "pods" and (.requestObject.spec.hostPID == true or .requestObject.spec.hostNetwork == true))'
```

**Critical Kubernetes Events:**

```bash
# Unauthorized access attempts
cat /var/log/kubernetes/audit/audit.log | \
  jq 'select(.responseStatus.code == 403)' | \
  jq -r '[.timestamp, .user.username, .objectRef.resource, .verb] | @tsv'

# Secret access
cat /var/log/kubernetes/audit/audit.log | \
  jq 'select(.objectRef.resource == "secrets" and .verb == "get")'

# ConfigMap modifications
cat /var/log/kubernetes/audit/audit.log | \
  jq 'select(.objectRef.resource == "configmaps" and (.verb == "create" or .verb == "update" or .verb == "delete"))'

# Exec into pods
cat /var/log/kubernetes/audit/audit.log | \
  jq 'select(.verb == "create" and .objectRef.subresource == "exec")'

# Service account token access
cat /var/log/kubernetes/audit/audit.log | \
  jq 'select(.objectRef.resource == "serviceaccounts/token")'
```

### Kubernetes Pod Security

**Security Context Analysis:**

```bash
# All pods with privileged containers
kubectl get pods -A -o json | \
  jq -r '.items[] | 
    select(.spec.containers[].securityContext.privileged == true) | 
    [.metadata.namespace, .metadata.name] | @tsv'

# Pods with root user
kubectl get pods -A -o json | \
  jq -r '.items[] | 
    select(.spec.containers[].securityContext.runAsUser == 0 or 
           (.spec.containers[].securityContext.runAsUser == null and 
            .spec.securityContext.runAsUser == null)) | 
    [.metadata.namespace, .metadata.name] | @tsv'

# Pods without read-only root filesystem
kubectl get pods -A -o json | \
  jq -r '.items[] | 
    select(.spec.containers[].securityContext.readOnlyRootFilesystem != true) | 
    [.metadata.namespace, .metadata.name] | @tsv'

# Capability analysis
kubectl get pods -A -o json | \
  jq -r '.items[] | 
    select(.spec.containers[].securityContext.capabilities.add != null) | 
    [.metadata.namespace, .metadata.name, .spec.containers[].securityContext.capabilities.add] | @tsv'
```

**Network Policy Analysis:**

```bash
# List all network policies
kubectl get networkpolicies -A

# Pods without network policies
kubectl get pods -A -o json | \
  jq -r '.items[] | [.metadata.namespace, .metadata.name] | @tsv' | \
  while read ns pod; do
    if ! kubectl get networkpolicy -n "$ns" -o json | \
         jq -e ".items[] | select(.spec.podSelector.matchLabels // {} | 
                to_entries | map(.key + \"=\" + .value) | 
                inside([$(kubectl get pod "$pod" -n "$ns" -o json | \
                jq -r '.metadata.labels | to_entries | map(.key + \"=\" + .value) | .[]' | 
                sed 's/^/\"/' | sed 's/$/\"/' | tr '\n' ',')]))" > /dev/null 2>&1; then
        echo "No NetworkPolicy: $ns/$pod"
    fi
  done

# Egress rules allowing all traffic
kubectl get networkpolicies -A -o json | \
  jq -r '.items[] | 
    select(.spec.egress == null or .spec.egress == [{}]) | 
    [.metadata.namespace, .metadata.name] | @tsv'
```

**RBAC Analysis:**

```bash
# List all ClusterRoleBindings
kubectl get clusterrolebindings -o json | \
  jq -r '.items[] | [.metadata.name, .subjects[].kind, .subjects[].name, .roleRef.name] | @tsv'

# Find privileged roles
kubectl get clusterroles -o json | \
  jq -r '.items[] | 
    select(.rules[] | 
           (.verbs // [] | contains(["*"])) or 
           (.resources // [] | contains(["*"]))) | 
    .metadata.name'

# Service accounts with cluster-admin
kubectl get clusterrolebindings -o json | \
  jq -r '.items[] | 
    select(.roleRef.name == "cluster-admin") | 
    [.metadata.name, .subjects[].namespace, .subjects[].name] | @tsv'

# Who can execute into pods
kubectl get rolebindings,clusterrolebindings -A -o json | \
  jq -r '.items[] | 
    select(.roleRef.name | contains("edit") or contains("admin")) | 
    [.metadata.namespace // "cluster", .metadata.name, .subjects[].name] | @tsv'
```

### Kubernetes Secret Analysis

**Secret Enumeration:**

```bash
# List all secrets
kubectl get secrets -A

# Secrets mounted in pods
kubectl get pods -A -o json | \
  jq -r '.items[] | 
    [.metadata.namespace, .metadata.name, .spec.volumes[]?.secret.secretName // empty] | @tsv'

# Extract secret values (requires permissions)
kubectl get secret <secret_name> -o json | \
  jq -r '.data | to_entries[] | "\(.key): \(.value | @base64d)"'

# Find secrets with sensitive keywords
kubectl get secrets -A -o json | \
  jq -r '.items[] | 
    select(.metadata.name | test("password|token|key|credential"; "i")) | 
    [.metadata.namespace, .metadata.name] | @tsv'
```

**ConfigMap Analysis:**

```bash
# List all ConfigMaps
kubectl get configmaps -A

# Search ConfigMaps for sensitive data
kubectl get configmaps -A -o json | \
  jq -r '.items[] | 
    select(.data | to_entries[] | .value | test("password|secret|token|key"; "i")) | 
    [.metadata.namespace, .metadata.name] | @tsv'

# ConfigMaps mounted as volumes
kubectl get pods -A -o json | \
  jq -r '.items[] | 
    [.metadata.namespace, .metadata.name, .spec.volumes[]?.configMap.name // empty] | @tsv'
```

### Container Persistence Mechanisms

**Volume Mounts Analysis:**

```bash
# List all PersistentVolumes
kubectl get pv

# PersistentVolumeClaims
kubectl get pvc -A

# Pods with hostPath volumes
kubectl get pods -A -o json | \
  jq -r '.items[] | 
    select(.spec.volumes[]?.hostPath != null) | 
    [.metadata.namespace, .metadata.name, .spec.volumes[]?.hostPath.path] | @tsv'

# Pods with emptyDir volumes
kubectl get pods -A -o json | \
  jq -r '.items[] | 
    select(.spec.volumes[]?.emptyDir != null) | 
    [.metadata.namespace, .metadata.name] | @tsv'
```

**Docker Volume Forensics:**

```bash
# List all Docker volumes
docker volume ls

# Inspect volume
docker volume inspect <volume_name>

# Access volume data (requires privileges)
VOLUME_PATH=$(docker volume inspect <volume_name> | jq -r '.[0].Mountpoint')
ls -la "$VOLUME_PATH"

# Search volume for sensitive data
find "$VOLUME_PATH" -type f -exec grep -l "flag\|password\|secret" {} \;

# Volume mount points for running containers
docker ps -q | xargs docker inspect | \
  jq -r '.[] | [.Name, .Mounts[].Source, .Mounts[].Destination] | @tsv'
```

---

## Advanced Detection Techniques

### Rootkit and Malware Detection in Containers

**Container Rootkit Indicators:**

```bash
# Check for hidden processes
docker exec <container_id> ps aux
docker top <container_id>  # Compare outputs

# Check for LD_PRELOAD hooking
docker exec <container_id> env | grep LD_PRELOAD

# Inspect loaded libraries
docker exec <container_id> cat /proc/self/maps

# Check for modified binaries
docker exec <container_id> debsums -c  # Debian/Ubuntu
docker exec <container_id> rpm -Va      # RHEL/CentOS

# Find SUID binaries
docker exec <container_id> find / -perm -4000 -type f 2>/dev/null

# Detect reverse shells
docker exec <container_id> netstat -antp | grep ESTABLISHED | \
  awk '{print $5}' | cut -d: -f1 | sort -u
```

**Cryptominer Detection:**

```bash
# High CPU usage containers
docker stats --no-stream | awk '$3 > 80 {print $1, $2, $3}'

# Known mining ports
docker exec <container_id> netstat -an | \
  grep -E ":(3333|4444|5555|7777|8888|14433|14444|45560)"

# Mining pool domains in DNS queries
journalctl -u docker.service | grep -iE "(pool\.|mining\.|crypto)"

# Suspicious process names
docker exec <container_id> ps aux | \
  grep -iE "(xmrig|minergate|cpuminer|crypto|mine)"

# Outbound connection analysis
docker exec <container_id> lsof -i | \
  awk '{print $9}' | grep -E "^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+" | sort -u
```

### Container Network Traffic Analysis

**Inter-Container Communication:**

```bash
# Map container network topology
docker network ls
docker network inspect bridge

# Container IP addresses
docker ps -q | xargs docker inspect | \
  jq -r '.[] | [.Name, .NetworkSettings.IPAddress] | @tsv'

# DNS resolution logs (if using custom DNS)
docker logs <container_id> | grep -i "dns\|resolve"

# Connection tracking
docker exec <container_id> conntrack -L 2>/dev/null

# Iptables rules affecting containers
iptables -L -n -v | grep docker
```

**Traffic Capture and Analysis:**

```bash
# Capture on Docker network interface
tcpdump -i docker0 -w /tmp/docker_capture.pcap

# Capture specific container traffic
CONTAINER_IP=$(docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' <container_id>)
tcpdump -i docker0 host $CONTAINER_IP -w /tmp/container_capture.pcap

# Analyze HTTP traffic
tcpdump -i docker0 -A | grep -E "GET |POST |HTTP/"

# Extract URLs from traffic
tcpdump -r /tmp/container_capture.pcap -A | \
  grep -oE "Host: [a-zA-Z0-9\.\-]+" | cut -d' ' -f2 | sort -u
```

**TLS/SSL Traffic Analysis:**

```bash
# Identify TLS connections
docker exec <container_id> ss -antp | grep ":443"

# Certificate inspection (if accessible)
docker exec <container_id> openssl s_client -connect <host>:443 -showcerts

# SNI monitoring
tcpdump -i docker0 -s 0 -A 'tcp[((tcp[12:1] & 0xf0) >> 2):4] = 0x16030100' | \
  grep -oE "Host: [a-zA-Z0-9\.\-]+"
```

### Container Escape Techniques and Detection

**Common Escape Vectors:**

```bash
# Check for Docker socket access
docker exec <container_id> ls -la /var/run/docker.sock 2>/dev/null

# Check for privileged mode
docker inspect <container_id> | jq '.[0].HostConfig.Privileged'

# Check capabilities
docker inspect <container_id> | jq '.[0].HostConfig.CapAdd'

# Cgroup escape detection (check for cgroup manipulation)
docker exec <container_id> cat /proc/self/cgroup

# Namespace escape attempts
docker exec <container_id> ls -la /proc/1/ns/

# Host filesystem access
docker exec <container_id> df -h | grep -E "/$|/host"
```

**Runtime Detection:**

```bash
# Monitor for suspicious syscalls (requires auditd)
auditctl -l | grep container

# Kernel module loading from containers
journalctl -k | grep -i "module.*docker"

# Privilege escalation via SUID
find /var/lib/docker/overlay2 -perm -4000 2>/dev/null

# Container breakout via /proc/self/exe
journalctl | grep "proc.*exe" | grep docker
```

**Post-Escape Indicators:**

```bash
# New processes on host from container UID range
ps aux | awk '$1 >= 100000 && $1 <= 200000'

# Files created in host filesystem from container
find / -uid 100000 -uid 200000 2>/dev/null

# Unusual network connections from Docker process
netstat -antp | grep docker | grep -v ":0\|:80\|:443"
```

---

## Forensic Analysis Workflows

### Complete Container Incident Response

```bash
#!/bin/bash
# container_incident_response.sh
# Comprehensive container forensics collection

CONTAINER_ID="$1"
OUTPUT_DIR="forensics_$(date +%Y%m%d_%H%M%S)"

if [ -z "$CONTAINER_ID" ]; then
    echo "Usage: $0 <container_id>"
    exit 1
fi

mkdir -p "$OUTPUT_DIR"
echo "[*] Collecting forensic data for container: $CONTAINER_ID"

# Basic information
echo "[+] Collecting container metadata..."
docker inspect "$CONTAINER_ID" > "$OUTPUT_DIR/inspect.json"
docker logs "$CONTAINER_ID" > "$OUTPUT_DIR/logs.txt" 2>&1
docker stats --no-stream "$CONTAINER_ID" > "$OUTPUT_DIR/stats.txt"

# Process information
echo "[+] Collecting process data..."
docker top "$CONTAINER_ID" aux > "$OUTPUT_DIR/processes.txt"
docker exec "$CONTAINER_ID" ps auxf > "$OUTPUT_DIR/process_tree.txt" 2>/dev/null

# Network information
echo "[+] Collecting network data..."
docker exec "$CONTAINER_ID" netstat -tulpn > "$OUTPUT_DIR/netstat.txt" 2>/dev/null
docker exec "$CONTAINER_ID" ss -tulpn > "$OUTPUT_DIR/sockets.txt" 2>/dev/null
docker exec "$CONTAINER_ID" ip addr > "$OUTPUT_DIR/ip_addr.txt" 2>/dev/null
docker exec "$CONTAINER_ID" ip route > "$OUTPUT_DIR/ip_route.txt" 2>/dev/null

# Filesystem information
echo "[+] Collecting filesystem data..."
docker exec "$CONTAINER_ID" df -h > "$OUTPUT_DIR/disk_usage.txt" 2>/dev/null
docker exec "$CONTAINER_ID" mount > "$OUTPUT_DIR/mounts.txt" 2>/dev/null
docker diff "$CONTAINER_ID" > "$OUTPUT_DIR/filesystem_changes.txt"

# Environment and configuration
echo "[+] Collecting environment data..."
docker exec "$CONTAINER_ID" env > "$OUTPUT_DIR/environment.txt" 2>/dev/null
docker exec "$CONTAINER_ID" cat /etc/hosts > "$OUTPUT_DIR/hosts.txt" 2>/dev/null
docker exec "$CONTAINER_ID" cat /etc/resolv.conf > "$OUTPUT_DIR/resolv.conf" 2>/dev/null

# Security context
echo "[+] Collecting security data..."
docker exec "$CONTAINER_ID" cat /proc/self/status > "$OUTPUT_DIR/proc_status.txt" 2>/dev/null
docker exec "$CONTAINER_ID" capsh --print > "$OUTPUT_DIR/capabilities.txt" 2>/dev/null

# Create filesystem snapshot
echo "[+] Creating filesystem snapshot..."
docker export "$CONTAINER_ID" -o "$OUTPUT_DIR/filesystem_export.tar"

# Network capture (5 seconds)
echo "[+] Capturing network traffic (5 seconds)..."
CONTAINER_PID=$(docker inspect -f '{{.State.Pid}}' "$CONTAINER_ID")
timeout 5 nsenter -t "$CONTAINER_PID" -n tcpdump -i eth0 -w "$OUTPUT_DIR/traffic.pcap" 2>/dev/null &

# Collect image information
echo "[+] Collecting image data..."
IMAGE=$(docker inspect "$CONTAINER_ID" | jq -r '.[0].Config.Image')
docker history --no-trunc "$IMAGE" > "$OUTPUT_DIR/image_history.txt" 2>/dev/null
docker inspect "$IMAGE" > "$OUTPUT_DIR/image_inspect.json" 2>/dev/null

# Wait for network capture
wait

# Create archive
echo "[+] Creating evidence archive..."
tar -czf "${OUTPUT_DIR}.tar.gz" "$OUTPUT_DIR"

echo "[*] Forensic collection complete: ${OUTPUT_DIR}.tar.gz"
echo "[*] Evidence hash: $(sha256sum "${OUTPUT_DIR}.tar.gz" | cut -d' ' -f1)"
```

### Automated Suspicious Activity Detection

```bash
#!/bin/bash
# container_threat_hunter.sh
# Automated threat hunting in containers

echo "=== Container Threat Hunting ==="
echo ""

# Privileged containers
echo "[!] CRITICAL: Privileged Containers"
docker ps -q | while read cid; do
    if docker inspect "$cid" | jq -e '.[0].HostConfig.Privileged == true' >/dev/null 2>&1; then
        NAME=$(docker inspect "$cid" | jq -r '.[0].Name')
        echo "  - $NAME ($cid)"
    fi
done

# Docker socket mounts
echo ""
echo "[!] CRITICAL: Docker Socket Access"
docker ps -q | while read cid; do
    if docker inspect "$cid" | jq -e '.[0].Mounts[] | select(.Source | contains("docker.sock"))' >/dev/null 2>&1; then
        NAME=$(docker inspect "$cid" | jq -r '.[0].Name')
        echo "  - $NAME ($cid)"
    fi
done

# Suspicious network connections
echo ""
echo "[!] WARNING: Suspicious Outbound Connections"
docker ps -q | while read cid; do
    CONNS=$(docker exec "$cid" netstat -an 2>/dev/null | \
            grep ESTABLISHED | \
            awk '{print $5}' | \
            cut -d: -f1 | \
            grep -vE "^(10\.|172\.(1[6-9]|2[0-9]|3[01])\.|192\.168\.|127\.)" | \
            sort -u)
    if [ -n "$CONNS" ]; then
        NAME=$(docker inspect "$cid" | jq -r '.[0].Name')
        echo "  - $NAME ($cid):"
        echo "$CONNS" | sed 's/^/      /'
    fi
done

# High CPU usage (possible cryptomining)
echo ""
echo "[!] WARNING: High CPU Usage"
docker stats --no-stream --format "table {{.Container}}\t{{.CPUPerc}}" | \
    awk 'NR>1 && $2+0 > 80'

# Recently created containers
echo ""
echo "[*] INFO: Recently Created Containers (last hour)"
docker ps -a --filter "since=$(date -d '1 hour ago' +%s)" --format "table {{.Names}}\t{{.Image}}\t{{.CreatedAt}}"

# Containers with restarts
echo ""
echo "[!] WARNING: Containers with Restart History"
docker ps --format "{{.Names}}" | while read name; do
    RESTARTS=$(docker inspect "$name" | jq -r '.[0].RestartCount')
    if [ "$RESTARTS" -gt 0 ]; then
        echo "  - $name: $RESTARTS restarts"
    fi
done

# Search for reverse shells
echo ""
echo "[!] CRITICAL: Potential Reverse Shells"
docker ps -q | while read cid; do
    SHELLS=$(docker exec "$cid" ps aux 2>/dev/null | \
             grep -E "nc.*-e|bash.*-i|sh.*-i|python.*pty\.spawn" | \
             grep -v grep)
    if [ -n "$SHELLS" ]; then
        NAME=$(docker inspect "$cid" | jq -r '.[0].Name')
        echo "  - $NAME ($cid):"
        echo "$SHELLS" | sed 's/^/      /'
    fi
done
```

---

## CTF-Specific Analysis Techniques

**Common CTF Container Challenges:**

```bash
# Check for flags in environment variables
docker ps -q | xargs -I {} docker inspect {} | \
  jq -r '.[].Config.Env[]' | grep -iE "flag|ctf"

# Search running containers for flags
docker ps -q | while read cid; do
    echo "Checking $cid..."
    docker exec "$cid" find / -name "*flag*" 2>/dev/null
    docker exec "$cid" grep -r "flag{" / 2>/dev/null
done

# Check Docker events for hidden activity
docker events --since 24h --until 0s | \
  grep -E "exec_create|exec_start" | \
  jq -r '[.time, .Action, .Actor.Attributes.name, .Actor.Attributes.execID] | @tsv'

# Analyze image labels for hints
docker images -q | xargs docker inspect | \
  jq -r '.[].Config.Labels | to_entries[] | "\(.key): \(.value)"'

# Check for hidden layers
docker history --no-trunc <image> | grep -v "<missing>"
```

**Kubernetes CTF Enumeration:**

```bash
# Service account tokens
kubectl get pods -A -o json | \
  jq -r '.items[] | [.metadata.namespace, .metadata.name, .spec.serviceAccountName] | @tsv'

# Secrets with suspicious names
kubectl get secrets -A -o json | \
  jq -r '.items[] | select(.metadata.name | test("flag|ctf|key")) | [.metadata.namespace, .metadata.name] | @tsv'

# Pod annotations (may contain hints)
kubectl get pods -A -o json | \
  jq -r '.items[] | [.metadata.namespace, .metadata.name, .metadata.annotations] | @tsv'
```

---

## Related Topics for Further Study

- **Container Escape Exploits**: CVE-specific breakout techniques (runC, Docker)
- **Kubernetes Security**: Pod Security Standards, OPA/Gatekeeper
- **Service Mesh Logs**: Istio, Linkerd traffic analysis
- **Container Image Scanning**: Trivy, Clair, Anchore vulnerability analysis
- **Runtime Security**: Falco, Sysdig behavioral detection
- **Docker Bench Security**: Automated security auditing
- **Linux Namespaces & Cgroups**: Low-level container isolation mechanisms

---

# Cloud Service Logs

Cloud service logs are critical artifacts in CTF scenarios involving cloud infrastructure compromise, insider threats, or misconfiguration exploitation. These logs record API calls, authentication events, resource modifications, and administrative actions across cloud platforms. Understanding their structure, acquisition methods, and analysis techniques is essential for investigating cloud-based attack chains.

## AWS CloudTrail

AWS CloudTrail records AWS API calls and related events for your AWS account. These logs capture who made requests, from which source IP, at what time, and what the results were.

### Log Structure and Location

CloudTrail logs are stored as JSON files in S3 buckets. Each log entry contains:

- **Event metadata**: eventVersion, eventID, eventTime, eventType
- **Identity information**: userIdentity (type, principalId, arn, accountId, accessKeyId)
- **Request parameters**: specific to each AWS service
- **Response elements**: what AWS returned
- **Network data**: sourceIPAddress, userAgent
- **Geographic data**: awsRegion

Default log location: `s3://<bucket-name>/AWSLogs/<account-id>/CloudTrail/<region>/<year>/<month>/<day>/`

### Acquisition Methods

**Direct S3 Download** (requires AWS CLI configured with appropriate credentials):

```bash
# List available CloudTrail logs
aws s3 ls s3://bucket-name/AWSLogs/123456789012/CloudTrail/us-east-1/ --recursive

# Download specific log files
aws s3 cp s3://bucket-name/AWSLogs/123456789012/CloudTrail/us-east-1/2024/10/29/ . --recursive

# Download with date filtering
aws s3 sync s3://bucket-name/AWSLogs/123456789012/CloudTrail/us-east-1/2024/10/ ./cloudtrail-logs/
```

**Using CloudTrail API**:

```bash
# Lookup events within timeframe
aws cloudtrail lookup-events \
    --lookup-attributes AttributeKey=EventName,AttributeValue=CreateUser \
    --start-time 2024-10-01T00:00:00Z \
    --end-time 2024-10-29T23:59:59Z \
    --max-results 50

# Get specific trail information
aws cloudtrail describe-trails --trail-name-list my-trail

# Check trail status
aws cloudtrail get-trail-status --name my-trail
```

**Extracting compressed logs**:

```bash
# CloudTrail logs are gzip compressed JSON
gunzip *.json.gz

# Batch extraction
for file in *.gz; do gunzip "$file"; done
```

### Analysis Techniques

**Parsing with jq** (essential for JSON manipulation):

```bash
# Extract all event names
cat cloudtrail-log.json | jq -r '.Records[].eventName' | sort | uniq

# Filter by specific user
cat cloudtrail-log.json | jq '.Records[] | select(.userIdentity.principalId | contains("SPECIFIC-USER-ID"))'

# Find failed authentication attempts
cat cloudtrail-log.json | jq '.Records[] | select(.errorCode == "UnauthorizedOperation" or .errorCode == "AccessDenied")'

# Extract source IPs for specific event
cat cloudtrail-log.json | jq -r '.Records[] | select(.eventName == "ConsoleLogin") | .sourceIPAddress'

# Find all actions by specific IP
cat cloudtrail-log.json | jq '.Records[] | select(.sourceIPAddress == "192.168.1.100")'

# Extract IAM privilege escalation events
cat cloudtrail-log.json | jq '.Records[] | select(.eventName | test("PutUser|PutRole|Attach.*Policy|CreateAccessKey"))'

# Timeline of events with key fields
cat cloudtrail-log.json | jq -r '.Records[] | "\(.eventTime) | \(.eventName) | \(.userIdentity.principalId) | \(.sourceIPAddress)"'
```

**Analyzing multiple log files**:

```bash
# Combine all logs and analyze
cat *.json | jq -s 'map(.Records[]) | group_by(.eventName) | map({eventName: .[0].eventName, count: length})'

# Find anomalous access patterns
cat *.json | jq -r '.Records[] | "\(.sourceIPAddress) \(.userIdentity.principalId)"' | sort | uniq -c | sort -rn
```

**Using specialized tools**:

**CloudTrail Visualizer** [Inference - based on common CTF tool usage]:

```bash
# Install
pip3 install cloudtrail-visualizer

# Generate timeline
cloudtrail-visualizer --input ./logs/ --output timeline.html
```

**DeepBlueCLI** (supports CloudTrail):

```bash
git clone https://github.com/sans-blue-team/DeepBlueCLI.git
cd DeepBlueCLI
./DeepBlue.ps1 cloudtrail-log.json
```

**AWS Security Analytics Bootstrap**:

```bash
# Using Amazon Athena for large-scale log analysis
# Create table in Athena (execute in AWS Console or CLI)
aws athena start-query-execution \
    --query-string "CREATE EXTERNAL TABLE cloudtrail_logs (...)" \
    --result-configuration OutputLocation=s3://query-results-bucket/
```

### Key Events to Investigate in CTF Scenarios

**Authentication & Access**:

- `ConsoleLogin` - AWS Management Console logins
- `AssumeRole` - IAM role assumption (lateral movement)
- `GetSessionToken` - Temporary credential requests
- `ConsoleLoginFailure` - Failed login attempts

**Privilege Escalation**:

- `CreateAccessKey` - New access key creation
- `PutUserPolicy` - Direct policy attachment to users
- `AttachUserPolicy` - Managed policy attachment
- `AttachRolePolicy` - Policy attachment to roles
- `PutRolePolicy` - Inline policy creation for roles
- `CreateUser` - New IAM user creation
- `UpdateAssumeRolePolicy` - Trust relationship modification

**Data Exfiltration**:

- `GetObject` - S3 object downloads
- `CopyObject` - S3 object copying
- `CreateSnapshot` - EBS volume snapshots
- `ModifySnapshotAttribute` - Snapshot sharing changes
- `CreateDBSnapshot` - RDS database snapshots

**Persistence Mechanisms**:

- `CreateLoginProfile` - Console password creation
- `UpdateLoginProfile` - Password changes
- `CreateVirtualMFADevice` - MFA device creation (can disable legitimate MFA)
- `RunInstances` - EC2 instance launches (backdoor instances)
- `CreateFunction` - Lambda function creation

**Defense Evasion**:

- `StopLogging` - CloudTrail disabling
- `DeleteTrail` - Trail deletion
- `PutEventSelectors` - Logging configuration changes
- `DeleteFlowLogs` - VPC flow log deletion

### Common Attack Patterns in Logs

**Pattern 1: Stolen Credentials Usage**

```bash
# Look for unusual source IPs for known users
cat *.json | jq -r '.Records[] | select(.userIdentity.type == "IAMUser") | "\(.userIdentity.userName) \(.sourceIPAddress)"' | sort | uniq

# Check for geographically impossible access
cat *.json | jq '.Records[] | select(.userIdentity.userName == "target-user") | {time: .eventTime, ip: .sourceIPAddress, region: .awsRegion}'
```

**Pattern 2: Privilege Escalation Chain**

```bash
# Extract escalation-related events in chronological order
cat *.json | jq -r '.Records[] | select(.eventName | test("PutUserPolicy|AttachUserPolicy|CreateAccessKey|UpdateAssumeRolePolicy")) | "\(.eventTime) | \(.eventName) | \(.userIdentity.principalId) | \(.requestParameters)"'
```

**Pattern 3: Data Exfiltration via Snapshot Sharing**

```bash
# Find snapshot creation and sharing events
cat *.json | jq '.Records[] | select(.eventName == "CreateSnapshot" or .eventName == "ModifySnapshotAttribute")'
```

### CloudTrail-Specific Challenges in CTFs

[Inference] CloudTrail logs are commonly used in forensics-focused CTF challenges where:

1. **Log gaps** indicate deleted or disabled trails
2. **Timestamp analysis** reveals the attack timeline
3. **Request parameters** contain encoded payloads or sensitive data
4. **Error codes** expose reconnaissance attempts
5. **User-agent strings** reveal automated tools or custom scripts

## Azure Activity Logs

Azure Activity Logs (formerly Audit Logs) record subscription-level events in Azure, including administrative operations, service health events, and resource changes.

### Log Structure and Location

Activity logs contain:

- **Authorization**: caller identity, subscription, resource group
- **Claims**: JWT token claims from Azure AD
- **Operation metadata**: operationName, status, timestamp
- **Resource information**: resourceId, resourceGroupName, resourceProviderName
- **HTTP details**: httpRequest (clientRequestId, clientIpAddress)

Logs are retained for **90 days** in Azure portal; longer retention requires export to Storage Account, Event Hub, or Log Analytics.

### Acquisition Methods

**Azure CLI**:

```bash
# Install Azure CLI
curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash

# Login
az login

# List activity logs (last 7 days by default)
az monitor activity-log list --output table

# Filter by timeframe
az monitor activity-log list \
    --start-time 2024-10-01T00:00:00Z \
    --end-time 2024-10-29T23:59:59Z \
    --output json > activity-logs.json

# Filter by resource group
az monitor activity-log list \
    --resource-group MyResourceGroup \
    --max-events 100

# Filter by specific caller
az monitor activity-log list \
    --caller "user@domain.com" \
    --output json

# Export to JSON for analysis
az monitor activity-log list \
    --start-time 2024-10-01 \
    --offset 30d \
    --query "[].{Time:eventTimestamp, Operation:operationName.value, Status:status.value, Caller:caller}" \
    --output json > azure-activity.json
```

**Azure PowerShell**:

```powershell
# Install Azure PowerShell module
Install-Module -Name Az -AllowClobber -Scope CurrentUser

# Connect
Connect-AzAccount

# Get activity logs
Get-AzActivityLog -StartTime (Get-Date).AddDays(-7) | Export-Csv activity-logs.csv

# Filter by status
Get-AzActivityLog -Status Failed | Select-Object EventTimestamp, Caller, OperationName, ResourceId

# Get detailed log entry
Get-AzActivityLog -MaxRecord 1 | Select-Object -Property *
```

**From Storage Account** (if exported):

```bash
# List exported logs
az storage blob list \
    --account-name mystorageaccount \
    --container-name insights-activity-logs \
    --output table

# Download exported logs
az storage blob download-batch \
    --account-name mystorageaccount \
    --source insights-activity-logs \
    --destination ./azure-logs/

# Logs are in JSON format within nested folder structure:
# resourceId=/SUBSCRIPTIONS/{subscription-id}/y={year}/m={month}/d={day}/h={hour}/m={minute}/PT1H.json
```

**Using Log Analytics (KQL)**:

```
# Connect to Log Analytics workspace
AzureActivity
| where TimeGenerated > ago(7d)
| where OperationNameValue == "MICROSOFT.COMPUTE/VIRTUALMACHINES/WRITE"
| project TimeGenerated, Caller, OperationNameValue, ActivityStatusValue, ResourceId

# Find failed operations
AzureActivity
| where ActivityStatusValue == "Failed"
| summarize count() by OperationNameValue, Caller

# Track specific user activity
AzureActivity
| where Caller == "user@domain.com"
| order by TimeGenerated desc
```

### Analysis Techniques

**Parsing with jq**:

```bash
# Extract unique operations
cat activity-logs.json | jq -r '.[].operationName.value' | sort | uniq

# Find administrative actions
cat activity-logs.json | jq '.[] | select(.authorization.action | test("write|delete|action"))'

# Extract failed operations
cat activity-logs.json | jq '.[] | select(.status.value == "Failed")'

# Find operations from specific IP
cat activity-logs.json | jq '.[] | select(.httpRequest.clientIpAddress == "192.168.1.100")'

# Timeline of events
cat activity-logs.json | jq -r '.[] | "\(.eventTimestamp) | \(.operationName.value) | \(.caller) | \(.status.value)"' | sort

# Extract permission changes
cat activity-logs.json | jq '.[] | select(.operationName.value | contains("RoleAssignment"))'
```

**Python analysis script**:

```python
#!/usr/bin/env python3
import json
from collections import Counter
from datetime import datetime

with open('activity-logs.json', 'r') as f:
    logs = json.load(f)

# Count operations per user
user_operations = Counter()
for log in logs:
    user_operations[log.get('caller', 'Unknown')] += 1

print("Top 10 Active Users:")
for user, count in user_operations.most_common(10):
    print(f"{user}: {count}")

# Find suspicious time-based patterns
for log in logs:
    timestamp = datetime.fromisoformat(log['eventTimestamp'].replace('Z', '+00:00'))
    if timestamp.hour < 6 or timestamp.hour > 22:  # Outside business hours
        print(f"Off-hours activity: {timestamp} | {log['caller']} | {log['operationName']['value']}")
```

### Key Events to Investigate

**Authentication & Identity**:

- `Microsoft.Authorization/roleAssignments/write` - Role assignments
- `Microsoft.Authorization/roleDefinitions/write` - Custom role creation
- `Microsoft.AAD/Users/Write` - User creation/modification
- `Microsoft.AAD/Groups/Write` - Group membership changes
- `Microsoft.AAD/servicePrincipals/write` - Service principal creation

**Resource Manipulation**:

- `Microsoft.Compute/virtualMachines/write` - VM creation/modification
- `Microsoft.Compute/virtualMachines/delete` - VM deletion
- `Microsoft.Storage/storageAccounts/write` - Storage account changes
- `Microsoft.Network/networkSecurityGroups/write` - NSG rule changes
- `Microsoft.KeyVault/vaults/write` - Key Vault modifications

**Data Access**:

- `Microsoft.Storage/storageAccounts/listKeys/action` - Storage key access
- `Microsoft.KeyVault/vaults/secrets/read` - Secret access
- `Microsoft.Sql/servers/databases/read` - Database access

**Defense Evasion**:

- `Microsoft.Insights/ActivityLogAlerts/Delete` - Alert deletion
- `Microsoft.Insights/DiagnosticSettings/Delete` - Logging disabling
- `Microsoft.Security/securityContacts/delete` - Security contact removal

### Common Attack Patterns

**Pattern 1: Privilege Escalation via Role Assignment**

```bash
# Find all role assignments
cat activity-logs.json | jq '.[] | select(.operationName.value == "Microsoft.Authorization/roleAssignments/write") | {time: .eventTimestamp, caller: .caller, resource: .resourceId, status: .status.value}'
```

**Pattern 2: Resource Tampering**

```bash
# Track changes to specific resource
cat activity-logs.json | jq '.[] | select(.resourceId | contains("TARGET-RESOURCE-ID"))'
```

**Pattern 3: Unauthorized Data Access**

```bash
# Find storage key access attempts
cat activity-logs.json | jq '.[] | select(.operationName.value | contains("listKeys"))'
```

## Google Cloud Audit Logs

Google Cloud Platform (GCP) provides four types of audit logs: Admin Activity, Data Access, System Event, and Policy Denied. These logs record all GCP API calls and administrative actions.

### Log Types and Structure

**Admin Activity Logs**:

- Administrative actions that modify resource configuration
- Always enabled, free, retained for 400 days
- Examples: VM instance creation, firewall rule changes, IAM policy modifications

**Data Access Logs**:

- API calls that read resource configuration or user-provided data
- Disabled by default (except BigQuery)
- Generate high volume, subject to quotas
- Examples: GCS object reads, Cloud SQL queries

**System Event Logs**:

- GCP administrative actions (non-user-initiated)
- Examples: VM live migrations, key rotations

**Policy Denied Logs**:

- Operations denied due to security policy violations
- Includes VPC Service Controls denials

Log entry structure:

- **protoPayload**: Core audit information
    - serviceName, methodName, resourceName
    - authenticationInfo (principalEmail)
    - authorizationInfo (permission, granted)
    - requestMetadata (callerIp, userAgent)
- **timestamp**, **severity**, **logName**
- **resource**: Resource type and labels

### Acquisition Methods

**gcloud CLI**:

```bash
# Install gcloud SDK
curl https://sdk.cloud.google.com | bash
exec -l $SHELL
gcloud init

# Read Admin Activity logs
gcloud logging read "logName=projects/PROJECT_ID/logs/cloudaudit.googleapis.com%2Factivity" \
    --limit 100 \
    --format json > admin-activity.json

# Filter by timeframe
gcloud logging read "logName=projects/PROJECT_ID/logs/cloudaudit.googleapis.com%2Factivity AND timestamp>=\"2024-10-01T00:00:00Z\"" \
    --format json > filtered-logs.json

# Filter by specific user
gcloud logging read "protoPayload.authenticationInfo.principalEmail=\"user@domain.com\"" \
    --limit 50 \
    --format json

# Filter by service
gcloud logging read "protoPayload.serviceName=\"compute.googleapis.com\"" \
    --limit 100

# Data Access logs (if enabled)
gcloud logging read "logName=projects/PROJECT_ID/logs/cloudaudit.googleapis.com%2Fdata_access" \
    --limit 50 \
    --format json > data-access.json

# Export logs to Cloud Storage (for large datasets)
gcloud logging sinks create my-sink \
    storage.googleapis.com/my-export-bucket \
    --log-filter='logName="projects/PROJECT_ID/logs/cloudaudit.googleapis.com%2Factivity"'

# List existing sinks
gcloud logging sinks list
```

**Using Cloud Logging API**:

```bash
# Advanced filtering with API
curl -H "Authorization: Bearer $(gcloud auth print-access-token)" \
    "https://logging.googleapis.com/v2/entries:list" \
    -d '{
      "resourceNames": ["projects/PROJECT_ID"],
      "filter": "logName=\"projects/PROJECT_ID/logs/cloudaudit.googleapis.com%2Factivity\" AND timestamp>=\"2024-10-01T00:00:00Z\"",
      "pageSize": 100
    }'
```

**Downloading from Cloud Storage** (if exported):

```bash
# List exported logs
gsutil ls gs://my-export-bucket/

# Download recursively
gsutil -m cp -r gs://my-export-bucket/cloudaudit.googleapis.com_activity/ ./gcp-logs/

# Logs are exported as newline-delimited JSON (.jsonl)
```

### Analysis Techniques

**Parsing with jq**:

```bash
# Extract method names (operations)
cat admin-activity.json | jq -r '.[] | .protoPayload.methodName' | sort | uniq

# Find operations by specific user
cat admin-activity.json | jq '.[] | select(.protoPayload.authenticationInfo.principalEmail == "user@domain.com")'

# Extract unauthorized attempts (Policy Denied)
cat admin-activity.json | jq '.[] | select(.severity == "ERROR" and (.protoPayload.status.message | contains("denied")))'

# Timeline of events
cat admin-activity.json | jq -r '.[] | "\(.timestamp) | \(.protoPayload.methodName) | \(.protoPayload.authenticationInfo.principalEmail) | \(.protoPayload.status.code // "SUCCESS")"' | sort

# Find IAM policy changes
cat admin-activity.json | jq '.[] | select(.protoPayload.methodName | contains("SetIamPolicy"))'

# Extract source IPs
cat admin-activity.json | jq -r '.[] | .protoPayload.requestMetadata.callerIp' | sort | uniq

# Find permission grants
cat admin-activity.json | jq '.[] | select(.protoPayload.authorizationInfo[]?.granted == true)'

# Extract service account usage
cat admin-activity.json | jq '.[] | select(.protoPayload.authenticationInfo.principalEmail | contains("gserviceaccount.com"))'
```

**Analyzing newline-delimited JSON** (exported logs):

```bash
# Each line is a separate JSON object
cat cloudaudit.googleapis.com_activity_*.jsonl | jq -r '.protoPayload.methodName' | sort | uniq -c | sort -rn

# Filter and extract
cat *.jsonl | jq '. | select(.protoPayload.methodName == "v1.compute.instances.insert")'
```

**Python analysis for complex patterns**:

```python
#!/usr/bin/env python3
import json
from datetime import datetime
from collections import defaultdict

with open('admin-activity.json', 'r') as f:
    logs = json.load(f)

# Detect rapid successive operations (potential automation/compromise)
user_timeline = defaultdict(list)
for log in logs:
    email = log.get('protoPayload', {}).get('authenticationInfo', {}).get('principalEmail')
    timestamp = log.get('timestamp')
    method = log.get('protoPayload', {}).get('methodName')
    
    if email and timestamp:
        user_timeline[email].append((datetime.fromisoformat(timestamp.replace('Z', '+00:00')), method))

# Find users with > 10 operations in 60 seconds
for user, events in user_timeline.items():
    events.sort()
    for i in range(len(events) - 9):
        window_duration = (events[i+9][0] - events[i][0]).total_seconds()
        if window_duration < 60:
            print(f"Rapid activity detected: {user} performed 10+ operations in {window_duration}s")
            break
```

### Key Events to Investigate

**IAM & Authentication**:

- `google.iam.admin.v1.CreateServiceAccountKey` - Service account key creation
- `google.iam.admin.v1.SetIamPolicy` - IAM policy changes
- `google.iam.admin.v1.CreateRole` - Custom role creation
- `SetIamPolicy` (any service) - Permission modifications

**Compute & Resources**:

- `v1.compute.instances.insert` - VM instance creation
- `v1.compute.instances.delete` - VM deletion
- `v1.compute.firewalls.insert` - Firewall rule creation
- `v1.compute.firewalls.patch` - Firewall modifications
- `v1.compute.instanceTemplates.insert` - Instance template creation

**Storage & Data**:

- `storage.buckets.update` - Bucket configuration changes
- `storage.buckets.setIamPolicy` - Bucket permission changes
- `storage.objects.create` - Object uploads
- `storage.objects.get` - Object downloads (Data Access logs)

**Persistence & Backdoors**:

- `v1.compute.instances.setMetadata` - Startup script injection
- `cloudfunctions.functions.create` - Cloud Function creation
- `google.cloud.scheduler.v1.CloudScheduler.CreateJob` - Scheduled job creation

**Defense Evasion**:

- `google.logging.v2.ConfigServiceV2.DeleteSink` - Log export deletion
- `google.logging.v2.ConfigServiceV2.DeleteLogMetric` - Metric deletion

### Common Attack Patterns

**Pattern 1: Service Account Key Compromise**

```bash
# Find all service account key operations
cat admin-activity.json | jq '.[] | select(.protoPayload.methodName | contains("ServiceAccountKey"))'

# Track subsequent actions by that service account
cat admin-activity.json | jq '.[] | select(.protoPayload.authenticationInfo.principalEmail == "suspicious-sa@project.iam.gserviceaccount.com")'
```

**Pattern 2: Privilege Escalation via IAM**

```bash
# Extract all SetIamPolicy events
cat admin-activity.json | jq '.[] | select(.protoPayload.methodName | endswith("SetIamPolicy")) | {time: .timestamp, caller: .protoPayload.authenticationInfo.principalEmail, resource: .protoPayload.resourceName, bindings: .protoPayload.request.policy.bindings}'
```

**Pattern 3: Data Exfiltration via Storage**

```bash
# Find bucket permission changes (making public)
cat admin-activity.json | jq '.[] | select(.protoPayload.methodName == "storage.setIamPermissions") | select(.protoPayload.request.policy.bindings[].members[] | contains("allUsers"))'
```

### GCP-Specific Challenges in CTFs

[Inference] GCP audit logs in CTF scenarios often involve:

1. **Service account abuse** - compromised SA keys used for lateral movement
2. **IAM policy analysis** - extracting permissions from policy bindings
3. **Multi-project pivoting** - tracking activity across linked projects
4. **API method correlation** - understanding which API calls indicate compromise
5. **Resource hierarchy traversal** - organization  folder  project  resource tracking

---

## Cross-Platform Analysis Considerations

When analyzing cloud logs across multiple platforms:

1. **Timestamp normalization**: AWS uses ISO 8601, Azure uses UTC, GCP uses RFC 3339
2. **Identity mapping**: AWS ARNs vs Azure Object IDs vs GCP emails
3. **Event correlation**: Different platforms use different operation naming conventions
4. **Log retention**: AWS (90 days default), Azure (90 days), GCP (400 days for Admin Activity)
5. **Cost implications**: Data Access logs can be expensive; verify they're enabled for CTF targets

### Recommended Tool Stack for CTF Log Analysis

Essential tools:

- **jq** - JSON parsing and filtering (required)
- **grep/egrep** - Pattern matching
- **Python 3** - Custom analysis scripts
- **Cloud provider CLIs** - aws-cli, azure-cli, gcloud
- **Timeline tools** - Plaso/log2timeline for event correlation
- **Visualization** - ELK stack for large datasets [Inference - commonly used in professional analysis]

### Important Subtopics for Further Study

- **CloudTrail log integrity validation** - verifying logs haven't been tampered with using digest files
- **Azure AD Sign-in Logs** - complement to Activity Logs for authentication analysis
- **GCP VPC Flow Logs** - network-level traffic analysis
- **Cross-cloud federated identity tracking** - following identities across AWS, Azure, GCP
- **Log tampering detection** - identifying gaps, modifications, or disabled logging

---

## S3 Access Logs

S3 (Simple Storage Service) access logs record detailed information about requests made to S3 buckets. These logs are stored as log files in a target bucket you specify.

### Log Format and Location

S3 access logs use a space-delimited format with the following key fields:

- Bucket Owner
- Bucket Name
- Request DateTime
- Remote IP
- Requester (IAM identity or anonymous)
- Request ID
- Operation (GET, PUT, DELETE, etc.)
- Key (object path)
- HTTP status
- Error Code
- Bytes Sent
- Object Size
- Total Time
- Turn-Around Time
- Referrer
- User-Agent
- Version ID

### Extraction and Analysis

**Downloading S3 logs (authenticated):**

```bash
aws s3 sync s3://target-bucket/logs/ ./s3-logs/ --profile ctf-profile
```

**Downloading from public/misconfigured buckets:**

```bash
aws s3 sync s3://target-bucket/logs/ ./s3-logs/ --no-sign-request
```

**Parsing S3 logs with awk:**

```bash
# Extract all GET requests
awk '$8 == "REST.GET.OBJECT"' access.log

# Find requests from specific IP
awk '$4 == "192.168.1.100"' access.log

# Extract accessed object keys
awk '{print $8, $9}' access.log | grep REST.GET

# Filter by date/time range
awk '$3 >= "[01/Jan/2024:00:00:00" && $3 <= "[31/Jan/2024:23:59:59"' access.log
```

**Using s3-log-parser (Python tool):**

```bash
# Install
pip install s3-log-parser

# Parse to JSON
s3-log-parser -i access.log -o output.json

# Filter and analyze
s3-log-parser -i access.log --filter-ip 10.0.0.1
```

**Analyzing with CloudTrail logs for S3 (more detailed):**

```bash
# Download CloudTrail logs
aws s3 sync s3://cloudtrail-bucket/AWSLogs/ACCOUNT-ID/CloudTrail/REGION/ ./cloudtrail/

# Decompress
gunzip cloudtrail/*.json.gz

# Search for S3 events
jq '.Records[] | select(.eventSource == "s3.amazonaws.com")' *.json

# Find specific bucket access
jq '.Records[] | select(.requestParameters.bucketName == "target-bucket")' *.json
```

### CTF-Relevant Patterns

**Indicators of misconfiguration:**

- Anonymous requests succeeding (look for `-` in requester field)
- 403 errors followed by 200s (permission enumeration)
- Sequential GET requests on predictable filenames (brute forcing)
- Unusual User-Agent strings

**Extracting credentials/secrets from logs:**

```bash
# Search for common patterns in object keys
grep -E "(key|secret|password|token|credential|config|\.env)" access.log

# Find recently uploaded files
awk '$8 == "REST.PUT.OBJECT" {print $3, $9}' access.log | sort
```

## Lambda Function Logs

Lambda function logs are stored in CloudWatch Logs, organized into log groups (per function) and log streams (per execution instance).

### Log Group Structure

Format: `/aws/lambda/FUNCTION_NAME`

Each invocation creates entries with:

- START RequestId
- Log output from function code
- END RequestId
- REPORT line (duration, memory usage, billed duration)

### Accessing Lambda Logs

**Using AWS CLI:**

```bash
# List log groups
aws logs describe-log-groups --log-group-name-prefix /aws/lambda/

# List log streams for a function
aws logs describe-log-streams \
  --log-group-name /aws/lambda/ctf-function \
  --order-by LastEventTime \
  --descending

# Get recent log events
aws logs tail /aws/lambda/ctf-function --follow

# Filter log events by pattern
aws logs filter-log-events \
  --log-group-name /aws/lambda/ctf-function \
  --filter-pattern "ERROR" \
  --start-time $(date -d '1 hour ago' +%s)000

# Download entire log stream
aws logs get-log-events \
  --log-group-name /aws/lambda/ctf-function \
  --log-stream-name '2024/01/15/[$LATEST]abc123' \
  --output json > lambda-logs.json
```

**Using awslogs tool (enhanced filtering):**

```bash
# Install
pip install awslogs

# Stream logs in real-time
awslogs get /aws/lambda/ctf-function --watch

# Filter by time range
awslogs get /aws/lambda/ctf-function -s '2h ago' -e '1h ago'

# Search for specific patterns
awslogs get /aws/lambda/ctf-function --filter-pattern 'Exception'

# Multiple log groups
awslogs get /aws/lambda/ ALL --filter-pattern 'flag{'
```

### Analyzing Lambda Execution Logs

**Extracting function invocation details:**

```bash
# Parse START/END/REPORT lines
jq -r '.events[] | select(.message | contains("REPORT"))' lambda-logs.json

# Find cold starts (initialization time)
grep "Init Duration" lambda-logs.json

# Extract custom application logs between START and END
awk '/START RequestId/,/END RequestId/' raw-logs.txt
```

**Common analysis patterns:**

```bash
# Find errors and exceptions
jq -r '.events[] | select(.message | contains("ERROR") or contains("Exception")) | .message' lambda-logs.json

# Extract environment variables leaked in logs
grep -oP 'AWS_[A-Z_]+|SECRET|KEY|TOKEN' lambda-logs.json

# Identify invocation sources (from context logging)
jq '.events[].message | select(contains("event"))' lambda-logs.json | grep -oP '"eventSource":"[^"]+","eventName":"[^"]+"'

# Memory and performance analysis
awk '/REPORT/ {print $5, $8, $11}' raw-logs.txt | sort -nk3
```

### CTF Exploitation Vectors

**Information disclosure:**

- Debug logs containing sensitive data
- Stack traces revealing file paths/code structure
- Printed environment variables
- Exposed AWS credentials in error messages

**Timing attacks via logs:**

```bash
# Analyze execution duration patterns
awk '/Duration:/ {print $2}' raw-logs.txt | sort -n | uniq -c
```

## API Gateway Logs

API Gateway provides two types of logs:

1. **Execution logs**: Detailed information about request/response processing
2. **Access logs**: Customizable logs similar to Apache/nginx access logs

### Enabling and Configuring Logs

[Inference] In CTF scenarios, these logs are typically pre-configured. In real assessments, you may need to enable them if you have sufficient permissions.

**CloudWatch execution logs location:** Format: `API-Gateway-Execution-Logs_REST-API-ID/STAGE-NAME`

**Access logs:** Can be configured to write to CloudWatch Logs or Kinesis Data Firehose with custom format.

### Accessing API Gateway Logs

**Using AWS CLI:**

```bash
# List API Gateway REST APIs
aws apigateway get-rest-apis

# Get stage information (includes logging configuration)
aws apigateway get-stage \
  --rest-api-id abc123xyz \
  --stage-name prod

# Download execution logs
aws logs tail API-Gateway-Execution-Logs_abc123xyz/prod --follow

# Filter by specific resource/method
aws logs filter-log-events \
  --log-group-name API-Gateway-Execution-Logs_abc123xyz/prod \
  --filter-pattern '"GET /api/users"'
```

### Execution Log Format

Execution logs contain structured entries with:

- Request ID
- Extended Request ID
- Request time
- Method and resource path
- Status code
- Protocol
- Response latency
- Integration latency
- Authorizer details (if applicable)
- WAF information
- Error messages

**Example execution log entry:**

```
(request-id) Method request body before transformations: [request body]
(request-id) Endpoint request URI: https://lambda.region.amazonaws.com/...
(request-id) Endpoint response headers: {header-name=header-value}
(request-id) Method response body after transformations: [response body]
(request-id) Method completed with status: 200
```

### Access Log Format Variables

Common variables for custom access log formats:

- `$context.requestId`
- `$context.requestTime`
- `$context.identity.sourceIp`
- `$context.identity.userAgent`
- `$context.httpMethod`
- `$context.resourcePath`
- `$context.status`
- `$context.error.message`
- `$context.authorizer.claims.sub` (JWT claims)
- `$context.identity.cognitoIdentityId`

### Analysis Techniques

**Parsing execution logs:**

```bash
# Extract request/response bodies
grep "request body before transformations" api-logs.txt | sed 's/.*transformations: //'

# Find authorization failures
grep -i "unauthorized\|forbidden\|denied" api-logs.txt

# Track request flow through integration
awk '/Request ID/{id=$NF} /Endpoint request/{print id, $0} /response headers/{print id, $0}' api-logs.txt
```

**Analyzing access logs (JSON format):**

```bash
# Filter by status code
jq 'select(.status >= 400)' access-logs.json

# Identify high-latency requests
jq 'select(.responseLatency > 1000)' access-logs.json

# Extract unique user agents
jq -r '.userAgent' access-logs.json | sort -u

# Find authentication bypasses
jq 'select(.authorizer.status == null and .status == 200)' access-logs.json

# Enumerate accessed endpoints
jq -r '"\(.httpMethod) \(.resourcePath)"' access-logs.json | sort | uniq -c | sort -rn
```

**Correlating with CloudTrail:**

```bash
# Find API Gateway configuration changes
jq '.Records[] | select(.eventSource == "apigateway.amazonaws.com" and .eventName | contains("Update"))' cloudtrail.json

# Identify who deployed APIs
jq '.Records[] | select(.eventName == "CreateDeployment") | {time: .eventTime, user: .userIdentity.principalId, api: .requestParameters.restApiId}' cloudtrail.json
```

### CTF Exploitation Indicators

**Common vulnerabilities revealed in logs:**

- IDOR patterns: Sequential IDs in successful requests from different IPs
- Broken authentication: 401s followed by 200s on same endpoint
- Mass assignment: Unusual request body parameters
- Rate limit bypasses: High request volumes from single source
- SSRF attempts: Backend integration errors with unusual hostnames

**Extracting sensitive data:**

```bash
# Search for common secret patterns in request/response bodies
grep -rE "(flag\{|CTF\{|[A-Za-z0-9]{32,})" api-logs/

# Find API keys in query parameters
grep -oP '\?[^"]*api[_-]?key[^&"]*' api-logs.txt

# Extract JWT tokens
grep -oP 'eyJ[A-Za-z0-9_-]+\.eyJ[A-Za-z0-9_-]+\.[A-Za-z0-9_-]+' api-logs.txt
```

**Identifying vulnerable endpoints:**

```bash
# Find endpoints with error spikes
jq -r '"\(.resourcePath) \(.status)"' access-logs.json | grep -E " (500|502|503)" | sort | uniq -c | sort -rn

# Detect injection attempts
grep -iE "(union select|<script|javascript:|\.\.\/|%2e%2e)" api-logs.txt
```

### Cross-Service Log Correlation

**Linking API Gateway  Lambda  S3:**

```bash
# Extract request ID from API Gateway
api_request_id=$(jq -r '.requestId' api-log.json | head -1)

# Find corresponding Lambda execution
grep -r "$api_request_id" /aws/lambda/*/

# Trace to S3 access if Lambda accesses buckets
# Match timestamp ranges and IAM role
```

### Tools for Comprehensive Analysis

**CloudWatch Insights queries:**

```sql
# Query syntax for CloudWatch Logs Insights
fields @timestamp, @message
| filter @message like /ERROR/
| stats count() by bin(5m)

# API Gateway specific
fields @timestamp, status, ip, httpMethod, resourcePath
| filter status >= 400
| sort @timestamp desc
```

**Using aws-cloudwatch-logs-insights CLI:**

```bash
aws logs start-query \
  --log-group-name API-Gateway-Execution-Logs_abc123/prod \
  --start-time $(date -d '1 day ago' +%s) \
  --end-time $(date +%s) \
  --query-string 'fields @message | filter @message like /flag/'
```

---

**Important subtopics for deeper cloud log analysis:**

- CloudTrail log analysis (AWS API audit trail)
- VPC Flow Logs (network-level traffic analysis)
- CloudWatch Metrics and Anomaly Detection
- Cognito authentication logs
- DynamoDB table streaming and audit logs
- IAM credential usage patterns in CloudTrail

---

# Log Encoding & Obfuscation

Log encoding and obfuscation techniques are critical for both attack detection and evasion analysis in CTF scenarios. Attackers encode payloads to bypass filters, evade signatures, and hide malicious activity within seemingly legitimate traffic. As a defender analyzing logs, you must recognize and decode these patterns.

## Base64 Encoded Data

Base64 encoding converts binary data into ASCII text using a 64-character alphabet (A-Z, a-z, 0-9, +, /). In logs, it appears as strings ending with `=` or `==` padding, though padding may be omitted.

**Recognition patterns:**

- Character set limited to `[A-Za-z0-9+/=]`
- String length typically divisible by 4
- Common in HTTP headers (Authorization, cookies), email attachments, XML/JSON payloads
- Look for patterns like `Authorization: Basic [base64]` or `data:image/png;base64,[data]`

**Decoding in Kali Linux:**

```bash
# Standard decoding
echo "SGVsbG8gV29ybGQ=" | base64 -d

# Decode from file
base64 -d encoded.txt > decoded.txt

# Decode specific field from logs (using awk)
cat access.log | awk '{print $7}' | base64 -d

# Handle URL-safe base64 (- and _ instead of + and /)
echo "SGVsbG8gV29ybGQ" | base64 -d 2>/dev/null || \
  echo "SGVsbG8gV29ybGQ" | tr '_-' '/+' | base64 -d
```

**Multi-layer encoding detection:**

```bash
# Recursive base64 decoding (common obfuscation)
decode_recursive() {
    local input="$1"
    local decoded
    decoded=$(echo "$input" | base64 -d 2>/dev/null)
    if [ $? -eq 0 ] && [ "$decoded" != "$input" ]; then
        echo "Layer: $decoded"
        decode_recursive "$decoded"
    fi
}
```

**Common CTF scenarios:**

- SQL injection payloads: `' OR '1'='1`  `JyBPUiAnMSc9JzE=`
- Command injection: `; cat /etc/passwd`  `OyBjYXQgL2V0Yy9wYXNzd2Q=`
- Encoded web shells in POST data or User-Agent headers
- Credentials in HTTP Basic Authentication headers

**Python scripting for log analysis:**

```python
import base64
import re

def find_base64_in_logs(log_line):
    # Match base64-like patterns (minimum 8 chars)
    pattern = r'[A-Za-z0-9+/]{8,}={0,2}'
    matches = re.findall(pattern, log_line)
    
    for match in matches:
        try:
            decoded = base64.b64decode(match).decode('utf-8', errors='ignore')
            if decoded.isprintable():
                print(f"Found: {match[:20]}... -> {decoded}")
        except:
            pass
```

## URL Encoding

URL encoding (percent-encoding) represents characters as `%` followed by two hexadecimal digits (e.g., space = `%20`). Critical for analyzing web application logs, as both legitimate browsers and attackers use it.

**Recognition patterns:**

- Percent signs followed by hex pairs: `%20`, `%3C`, `%2F`
- Multiple encoding layers: `%2527` (double-encoded single quote)
- Mixed encoding: `%2e%2e%2f` (../) for path traversal
- Non-standard characters encoded unnecessarily (obfuscation indicator)

**Decoding in Kali Linux:**

```bash
# Using Python's urllib (built into most systems)
python3 -c "import urllib.parse; print(urllib.parse.unquote('%2e%2e%2f%2e%2e%2f'))"

# Using printf for simple cases
printf '%b\n' "$(echo '%2e%2e%2f' | sed 's/%/\\x/g')"

# Decode entire log file URLs
cat access.log | grep -oP 'GET \K[^ ]+' | python3 -c "
import sys, urllib.parse
for line in sys.stdin:
    print(urllib.parse.unquote(line.strip()))"

# Using curl for complex cases
curl "http://example.com/" --get --data-urlencode "param=%2e%2e%2f" --trace-ascii /dev/stdout
```

**Double/Triple encoding detection:**

```bash
# Recursive URL decoding
url_decode_recursive() {
    local input="$1"
    local decoded=$(python3 -c "import urllib.parse; print(urllib.parse.unquote('$input'))")
    
    if [ "$decoded" != "$input" ]; then
        echo "Decoded: $decoded"
        url_decode_recursive "$decoded"
    else
        echo "Final: $decoded"
    fi
}

# Usage: url_decode_recursive "%252e%252e%252f"
```

**Attack pattern examples:**

```bash
# Path traversal
%2e%2e%2f%2e%2e%2f%65%74%63%2f%70%61%73%73%77%64
# Decodes to: ../../etc/passwd

# XSS payload
%3Cscript%3Ealert%28%27XSS%27%29%3C%2Fscript%3E
# Decodes to: <script>alert('XSS')</script>

# SQL injection
%27%20OR%20%271%27%3D%271
# Decodes to: ' OR '1'='1

# Null byte injection (historic)
%00
# Decodes to: null byte
```

**CyberChef for quick analysis:**

```bash
# CyberChef is GUI-based but useful for CTFs
# Can chain: URL Decode  Base64 Decode  Regex search
# Available online or installable locally
```

## Unicode Encoding

Unicode encoding represents characters using various schemes (UTF-8, UTF-16, HTML entities, Unicode escapes). Attackers abuse this for filter bypass and WAF evasion.

**Common Unicode representations:**

**HTML entities:**

```
Decimal: &#60; (less than)
Hex: &#x3C; (less than)
Named: &lt; (less than)
```

**Unicode escapes:**

```
JavaScript: \u003c (less than)
CSS: \3c (less than)
JSON: \u003c (less than)
URL: %u003c (less than, non-standard but sometimes parsed)
```

**UTF-8 encoding:**

```
Overlong encoding: %C0%AE (. encoded as 2 bytes instead of 1)
Standard: %C2%A9 ( copyright symbol)
```

**Decoding in Kali Linux:**

```bash
# HTML entities decoding
echo "&#60;script&#62;alert&#40;1&#41;&#60;/script&#62;" | \
  python3 -c "import html, sys; print(html.unescape(sys.stdin.read()))"

# Unicode escape sequences (JavaScript style)
echo '\u003cscript\u003e' | \
  python3 -c "import sys; print(sys.stdin.read().encode().decode('unicode-escape'))"

# Mixed Unicode decoding script
python3 << 'EOF'
import re
import html

text = "&#x3C;script&#x3E;alert\u0028\u0031\u0029"

# Decode HTML entities
text = html.unescape(text)

# Decode Unicode escapes
text = text.encode('utf-8').decode('unicode-escape', errors='ignore')

print(text)
EOF
```

**Detecting overlong UTF-8 encoding:**

```python
# Overlong encodings are invalid UTF-8 but sometimes accepted by parsers
# Example: / can be encoded as %2F (valid) or %C0%AF (overlong, invalid)

def detect_overlong_utf8(url_encoded_str):
    import urllib.parse
    decoded = urllib.parse.unquote(url_encoded_str, encoding='latin1')
    
    # Check for overlong sequences
    overlong_patterns = {
        r'\xC0[\x80-\xBF]': '2-byte overlong',
        r'\xE0[\x80-\x9F]': '3-byte overlong',
        r'\xF0[\x80-\x8F]': '4-byte overlong'
    }
    
    for pattern, desc in overlong_patterns.items():
        if re.search(pattern.encode('latin1'), decoded.encode('latin1')):
            print(f"Detected {desc} encoding")
```

**Unicode normalization attacks:**

```bash
# Different Unicode representations of same character
# Example: Kelvin sign (K, U+212A) vs. Latin K (U+004B)

python3 << 'EOF'
import unicodedata

# These look identical but are different
kelvin = "\u212A"  # Kelvin sign
latin_k = "\u004B"  # Latin capital K

print(f"Kelvin: {kelvin} | Latin K: {latin_k}")
print(f"Are they equal? {kelvin == latin_k}")

# After normalization
print(f"After NFKC: {unicodedata.normalize('NFKC', kelvin) == latin_k}")
EOF
```

**CTF-relevant Unicode exploits:**

**Homograph attacks (look-alike characters):**

```bash
# Cyrillic 'a' (U+0430) vs Latin 'a' (U+0061)
echo -e "Latin: example.com\nCyrillic: ex\u0430mple.com"

# Detection script
python3 -c "
domain = 'exmple.com'  # Contains Cyrillic ''
for char in domain:
    print(f'{char}: U+{ord(char):04X}')
"
```

**Best-fit mapping bypasses:**

```python
# Some systems normalize Unicode to ASCII
# Example:  (U+2133)  M

import unicodedata

def check_bypass(payload):
    normalized = unicodedata.normalize('NFKD', payload)
    ascii_folded = normalized.encode('ascii', 'ignore').decode()
    
    if payload != ascii_folded:
        print(f"Original: {payload}")
        print(f"After normalization: {ascii_folded}")
        
# Test with mathematical bold capital M
check_bypass("alicious")  # Might bypass filters looking for "Malicious"
```

**Practical log analysis workflow:**

```bash
#!/bin/bash
# analyze_encoded_logs.sh

LOG_FILE="$1"

echo "[*] Searching for Base64 patterns..."
grep -oP '[A-Za-z0-9+/]{16,}={0,2}' "$LOG_FILE" | while read encoded; do
    decoded=$(echo "$encoded" | base64 -d 2>/dev/null)
    [ $? -eq 0 ] && echo "Base64: $encoded -> $decoded"
done

echo -e "\n[*] Searching for URL encoding..."
grep -oP '%[0-9A-Fa-f]{2}' "$LOG_FILE" | head -20

echo -e "\n[*] Searching for Unicode escapes..."
grep -oP '\\u[0-9A-Fa-f]{4}' "$LOG_FILE" | head -20

echo -e "\n[*] Searching for HTML entities..."
grep -oP '&#x?[0-9A-Fa-f]+;' "$LOG_FILE" | head -20
```

**Tools for automated analysis:**

```bash
# grep with PCRE for complex patterns
grep -P '(%[0-9A-Fa-f]{2}|&#x?[0-9]+;|\\u[0-9A-Fa-f]{4})' access.log

# Using awk for field-specific analysis
awk '{print $7}' access.log | grep -P '%[0-9A-Fa-f]{2}' | \
  python3 -c "import sys, urllib.parse; [print(urllib.parse.unquote(l.strip())) for l in sys.stdin]"

# Using jq for JSON logs
cat app.log | jq -r '.request.uri' | python3 -c "
import sys, urllib.parse, base64, html
for line in sys.stdin:
    line = line.strip()
    # Try URL decode
    try: print(f'URL: {urllib.parse.unquote(line)}')
    except: pass
    # Try base64
    try: print(f'B64: {base64.b64decode(line).decode()}')
    except: pass
    # Try HTML
    try: print(f'HTML: {html.unescape(line)}')
    except: pass
"
```

**Important CTF considerations:**

1. **Encoding chains**: Attackers often combine multiple encoding methods (Base64  URL  Unicode)
2. **Case sensitivity**: Some decoders treat `%2F` and `%2f` differently
3. **Incomplete encoding**: Mixed encoded/plain text (e.g., `cat%20/etc/passwd`)
4. **Character set ambiguity**: UTF-7, UTF-16BE/LE can bypass filters expecting UTF-8
5. **Null bytes and control characters**: `%00`, `%0A`, `%0D` often indicate exploitation attempts

**[Inference]** Many WAFs and filters only check for single-encoding layers, making multi-layer encoding effective for bypass. However, specific WAF behavior depends on implementation.

---

## Hex Encoding

Hex encoding is one of the most common obfuscation techniques found in logs, particularly in web server logs, command execution traces, and network traffic captures.

### Detection and Identification

Hex-encoded data appears as strings of characters from the set `[0-9a-fA-F]`, often prefixed with identifiers:

- `0x` prefix (C-style): `0x48656c6c6f`
- `\x` prefix (Python/shell): `\x48\x65\x6c\x6c\x6f`
- `%` prefix (URL encoding): `%48%65%6c%6c%6f`
- Raw hex without prefix: `48656c6c6f`

### Decoding Tools and Commands

**xxd** - Standard hex dump and reverse tool:

```bash
# Decode hex string to ASCII
echo "48656c6c6f20576f726c64" | xxd -r -p

# Decode with \x prefix format
echo -e "\x48\x65\x6c\x6c\x6f"

# Decode from file
xxd -r -p encoded.txt > decoded.txt

# Show hex alongside ASCII (useful for mixed content)
xxd logfile.log
```

**CyberChef** - Web-based multi-stage decoder (available at `https://gchq.github.io/CyberChef/`):

- Drag "From Hex" operation into recipe
- Supports automatic delimiter detection (`0x`, `\x`, `%`, spaces)
- Chain multiple operations for nested encoding
- Can handle partial hex in mixed-format logs

**Python one-liners**:

```bash
# Decode hex string
python3 -c "import binascii; print(binascii.unhexlify('48656c6c6f').decode())"

# Decode URL-encoded hex
python3 -c "import urllib.parse; print(urllib.parse.unquote('%48%65%6c%6c%6f'))"

# Process entire log file
python3 -c "
import re, binascii
data = open('access.log').read()
for match in re.findall(r'(?:\\\\x[0-9a-fA-F]{2})+', data):
    decoded = bytes.fromhex(match.replace('\\\\x', '')).decode(errors='ignore')
    print(f'{match} -> {decoded}')
"
```

**Bash parameter expansion**:

```bash
# Decode \xHH format directly in bash
echo -e "\x48\x65\x6c\x6c\x6f\x20\x57\x6f\x72\x6c\x64"

# Extract and decode from grep output
grep -oP '\\x[0-9a-fA-F]{2}+' logfile | while read line; do echo -e "$line"; done
```

### Common CTF Scenarios

**Web server logs with hex-encoded paths**:

```
192.168.1.100 - - [29/Oct/2025:10:23:45] "GET /%2e%2e%2f%2e%2e%2f%65%74%63%2f%70%61%73%73%77%64 HTTP/1.1"
```

Decoded: `../../etc/passwd` (directory traversal)

**Command injection with hex encoding**:

```
cmd=\x63\x61\x74\x20\x2f\x65\x74\x63\x2f\x73\x68\x61\x64\x6f\x77
```

Decoded: `cat /etc/shadow`

**SQL injection payloads**:

```
' UNION SELECT 0x61646d696e, 0x70617373776f7264 --
```

The hex values decode to column names or data.

### Advanced Hex Analysis

**Mixed encoding detection**:

```bash
# Find multiple encoding schemes in single log entry
grep -P '(%[0-9a-fA-F]{2}|\\x[0-9a-fA-F]{2}|0x[0-9a-fA-F]+)' access.log
```

**Automated bulk decoding**:

```python
#!/usr/bin/env python3
import re
import sys

def decode_hex_variants(text):
    # URL encoding
    text = re.sub(r'%([0-9a-fA-F]{2})', lambda m: chr(int(m.group(1), 16)), text)
    # \x encoding
    text = re.sub(r'\\x([0-9a-fA-F]{2})', lambda m: chr(int(m.group(1), 16)), text)
    return text

for line in sys.stdin:
    print(decode_hex_variants(line.strip()))
```

Usage: `cat access.log | python3 hex_decode.py`

## Compression Artifacts

Compressed log sections often indicate data exfiltration, payload delivery, or attempts to hide large amounts of data within log fields. Compression also appears in legitimate log rotation but can conceal malicious content.

### Identifying Compressed Data

**Magic bytes (file signatures)** - First few bytes indicate compression format:

- gzip: `1f 8b`
- bzip2: `42 5a 68` (BZh)
- ZIP: `50 4b 03 04` (PK)
- zlib: `78 9c` (default compression) or `78 01`, `78 da`
- LZMA/XZ: `fd 37 7a 58 5a 00` (7zXZ)

**Detection commands**:

```bash
# Identify file types in binary log data
file suspicious.log

# Search for compression magic bytes in logs
xxd access.log | grep "1f 8b"

# Extract base64-encoded compressed data
grep -oP '[A-Za-z0-9+/]{40,}={0,2}' logfile.log | while read b64; do
    echo "$b64" | base64 -d | file -
done
```

### Decompression Tools and Techniques

**gunzip/gzip** - Handle gzip compression:

```bash
# Decompress gzip file
gunzip -c compressed.log.gz

# Decompress inline hex-encoded gzip
echo "1f8b080..." | xxd -r -p | gunzip

# Force decompression even with wrong extension
gunzip -c -S "" suspicious.dat

# Decompress and search in one command
zcat compressed.log.gz | grep "malicious"
```

**zlib decompression** - Common in network protocols and application logs:

```python
#!/usr/bin/env python3
import zlib
import sys

# Decompress zlib data from stdin
compressed = sys.stdin.buffer.read()
try:
    decompressed = zlib.decompress(compressed)
    sys.stdout.buffer.write(decompressed)
except zlib.error as e:
    print(f"Decompression failed: {e}", file=sys.stderr)
```

**Handling base64-encoded compressed data** (very common in CTFs):

```bash
# Single-stage: base64 -> gzip
echo "H4sIAAAAAAAAA..." | base64 -d | gunzip

# Multi-stage: URL decode -> base64 -> gzip
echo "SDRzSUFBQUFBQUFB..." | \
    python3 -c "import sys,urllib.parse; print(urllib.parse.unquote(sys.stdin.read()))" | \
    base64 -d | gunzip
```

**binwalk** - Automated extraction of embedded compressed data:

```bash
# Scan for embedded files and compression
binwalk logfile.bin

# Extract all detected files
binwalk -e logfile.bin

# Extract specific compression formats
binwalk --dd='gzip:gz' logfile.bin
```

**7z (p7zip)** - Universal archive extraction:

```bash
# Extract various formats (zip, rar, 7z, etc.)
7z x compressed.dat

# List contents without extraction
7z l archive.dat

# Extract to stdout for piping
7z e -so archive.dat filename.txt
```

### CTF Compression Patterns

**Nested encoding/compression** (must decode in reverse order of encoding):

```bash
# Example: data -> gzip -> base64 -> hex
# Decoding process:
echo "48347349..." | xxd -r -p | base64 -d | gunzip

# Automated nested detection with CyberChef:
# Use "Magic" operation to auto-detect encoding layers
```

**Compressed payloads in POST data**:

```
POST /upload HTTP/1.1
Content-Encoding: gzip
Content-Length: 156

[binary gzip data]
```

Extract with:

```bash
# Extract POST body from pcap
tcpflow -r capture.pcap -o output/
# Decompress extracted stream
gunzip < output/192.168.001.100.00080-192.168.001.101.54321
```

**Log rotation with compression clues**:

```bash
# Attacker may hide data in rotated logs
ls -la /var/log/
# Look for: unusual sizes, recent timestamps on old logs, non-standard compression

# Check all compressed logs for patterns
zgrep -i "password\|admin\|flag{" /var/log/*.gz
```

### Compression Ratio Analysis

[Inference] Unusually high or low compression ratios may indicate:

- High ratio: Repeated patterns (possible data exfiltration padding)
- Low ratio: Already compressed/encrypted data (pre-compressed payloads)
- No compression: Failed compression attempt (worth investigating why)

Check compression ratio:

```bash
# Compare original and compressed sizes
ls -lh original.log compressed.log.gz

# Test compression efficiency
gzip -c -9 testfile | wc -c  # Compressed size
wc -c < testfile              # Original size
```

## Encrypted Log Sections

Encrypted log sections represent the most challenging obfuscation tier. In CTF contexts, encryption typically involves symmetric algorithms with discoverable keys rather than strong cryptographic implementations.

### Identifying Encrypted Content

**Visual characteristics**:

- High entropy (appears random)
- No recognizable patterns or strings
- Often base64-encoded after encryption
- Fixed block sizes (AES: 16 bytes, DES: 8 bytes)

**Entropy analysis**:

```bash
# Calculate entropy using ent (install: apt install ent)
ent suspicious.dat
# High entropy (~7.9-8.0 bits/byte) suggests encryption or compression

# Python entropy calculation
python3 -c "
import math
from collections import Counter
data = open('suspicious.dat', 'rb').read()
entropy = -sum(count/len(data) * math.log2(count/len(data)) 
               for count in Counter(data).values())
print(f'Entropy: {entropy:.2f} bits/byte')
"
```

**OpenSSL identification**:

```bash
# Check for OpenSSL encrypted format (Salted__ header)
head -c 16 encrypted.dat | xxd
# Look for: 53 61 6c 74 65 64 5f 5f (Salted__)

# Base64-encoded OpenSSL format
echo "U2FsdGVkX1..." | base64 -d | head -c 16 | xxd
```

### Common Encryption Schemes in CTFs

**AES (Advanced Encryption Standard)** - Most common modern cipher:

```bash
# Decrypt AES-256-CBC with OpenSSL
openssl enc -d -aes-256-cbc -in encrypted.dat -out decrypted.txt -k "password"

# Specify key and IV explicitly (hex format)
openssl enc -d -aes-256-cbc -in encrypted.dat -out decrypted.txt \
    -K "0123456789abcdef..." -iv "fedcba9876543210..."

# Common AES modes in CTFs:
# -aes-128-cbc, -aes-192-cbc, -aes-256-cbc (Cipher Block Chaining)
# -aes-128-ecb, -aes-256-ecb (Electronic Codebook - weaker)
# -aes-128-ctr (Counter mode)
```

**XOR encryption** - Weak but common in CTFs:

```python
#!/usr/bin/env python3
# xor_decrypt.py
import sys

def xor_decrypt(data, key):
    key_bytes = key.encode() if isinstance(key, str) else key
    return bytes(b ^ key_bytes[i % len(key_bytes)] for i, b in enumerate(data))

# Single-byte XOR (brute force)
data = open(sys.argv[1], 'rb').read()
for key in range(256):
    result = bytes(b ^ key for b in data)
    if b'flag{' in result or b'CTF{' in result:
        print(f"Key {key:02x}: {result}")

# Multi-byte XOR with known key
# python3 xor_decrypt.py encrypted.dat "SECRET_KEY"
```

**ROT13/Caesar cipher** - Sometimes used for "encryption" in logs:

```bash
# ROT13 decode
echo "encrypted_string" | tr 'A-Za-z' 'N-ZA-Mn-za-m'

# Try all Caesar shifts
for i in {0..25}; do 
    echo "KHOOR" | tr "$(printf %${i}s | tr ' ' 'A-Z')" "A-Z"
done
```

**Base64 + XOR pattern** (very common):

```bash
# Decode base64 then XOR
echo "SGVsbG8gV29ybGQ=" | base64 -d | python3 -c "
import sys
data = sys.stdin.buffer.read()
key = b'KEY'
print(bytes(b ^ key[i % len(key)] for i, b in enumerate(data)).decode())
"
```

### Decryption Tool Arsenal

**OpenSSL** - Swiss Army knife for encryption:

```bash
# List available ciphers
openssl enc -list

# Decrypt with password-based key derivation
openssl enc -d -aes-256-cbc -pbkdf2 -in encrypted.log -out plain.log

# Decrypt without salt (some CTF implementations)
openssl enc -d -aes-256-cbc -nosalt -K KEY_HEX -iv IV_HEX -in encrypted.log
```

**CyberChef operations** for encryption:

- AES Decrypt, DES Decrypt, Triple DES Decrypt
- XOR, XOR Brute Force
- RC4, Blowfish
- Supports multiple input formats (hex, base64, UTF-8)

**hashcat** - Can brute-force weak encryption [Unverified]:

```bash
# Create wordlist or use rockyou.txt
hashcat -m <mode> -a 0 encrypted.dat wordlist.txt

# Common modes:
# -m 0: MD5 (not encryption, but used for key derivation)
# -m 10400: PDF (if logs contain encrypted PDFs)
```

**Python cryptography libraries**:

```python
#!/usr/bin/env python3
from Crypto.Cipher import AES
from Crypto.Util.Padding import unpad
import base64

# AES-CBC decryption
key = b'SIXTEEN_BYTE_KEY'  # 16, 24, or 32 bytes
iv = b'SIXTEEN_BYTE_IV!'
ciphertext = base64.b64decode("encrypted_base64_string")

cipher = AES.new(key, AES.MODE_CBC, iv)
plaintext = unpad(cipher.decrypt(ciphertext), AES.block_size)
print(plaintext.decode())
```

Install: `pip3 install pycryptodome`

### Key Recovery Strategies

**1. Hardcoded keys in logs or nearby files**:

```bash
# Search for common key patterns
grep -ri "key\|pass\|secret" /var/log/
grep -riE '[A-Fa-f0-9]{32,}' /var/log/  # Hex keys

# Check environment variables logged
grep "export.*KEY" auth.log
```

**2. Weak key derivation**:

```bash
# Try common passwords
echo -n "password123" | md5sum
echo -n "admin" | sha256sum

# Use as OpenSSL key
openssl enc -d -aes-256-cbc -K $(echo -n "password" | md5sum | cut -d' ' -f1) ...
```

**3. Known-plaintext attacks** [Inference]: If you know part of the plaintext (e.g., log format headers like `[INFO]`, timestamps), you can:

- Determine the key length (XOR)
- Verify decryption attempts
- Use differential analysis

**4. Padding oracle attacks** [Unverified - context-dependent]: If the system returns different errors for padding failures vs. other errors, this may be exploitable. Tools like `padbuster` can automate this, but this is highly CTF-scenario-specific.

### Encrypted Log Section Patterns

**Format indicators**:

```
# OpenSSL encrypted with password
Salted__[8 bytes salt][encrypted data]

# Base64 wrapper around encrypted data
-----BEGIN ENCRYPTED DATA-----
U2FsdGVkX1+...
-----END ENCRYPTED DATA-----

# Inline encrypted fields in structured logs
{"timestamp": "2025-10-29", "data": "aXYxNjpjaXBoZXJ0ZXh0"}
```

**Extraction and processing**:

```bash
# Extract encrypted section from mixed log
sed -n '/BEGIN ENCRYPTED/,/END ENCRYPTED/p' log.txt | \
    grep -v "BEGIN\|END" | \
    base64 -d > encrypted.bin

# Process JSON with encrypted fields
jq -r '.data' log.json | base64 -d | openssl enc -d -aes-256-cbc -k "password"
```

### Defense Against Analysis (What Attackers Do)

Understanding attacker techniques helps you recognize and defeat them:

**Layered encoding**: Encryption  Compression  Base64  Hex

```bash
# Reverse the entire chain
echo "3438374..." | xxd -r -p | base64 -d | gunzip | openssl enc -d -aes-256-cbc -k "key"
```

**Partial encryption**: Only sensitive parts encrypted, rest in plaintext

```bash
# Search for mixed content
grep -aoP '[\x20-\x7e]{10,}' mixed.log  # ASCII strings
xxd mixed.log | grep -v "\.\.\.\."       # Non-ASCII regions
```

**Key split across multiple log files** [Inference]:

```bash
# Concatenate parts found in different files
cat log1.key log2.key log3.key > full.key
```

## Important Related Topics

For comprehensive log analysis in CTF contexts, you should also study:

- **Log Parsing & Filtering**: Using `awk`, `sed`, `grep`, `jq` for structured and unstructured logs
- **Timestamp Analysis**: Detecting anomalies, timezone issues, and log tampering
- **Event Correlation**: Linking related events across multiple log sources
- **Steganography in Logs**: Hidden data in log formatting, whitespace, or metadata
- **Windows Event Log Analysis**: EVTX format, Event IDs, PowerShell logging
- **Network Traffic Analysis**: Combining packet captures with application logs

---

# Statistical Analysis

Statistical analysis of log data provides quantitative foundations for identifying anomalies, establishing baselines, and detecting patterns indicative of security events. In CTF contexts, statistical methods reveal hidden attack patterns, data exfiltration timing, and behavioral deviations that qualitative analysis might miss.

## Log Volume Analysis

Log volume analysis examines the quantity of log entries over time to detect unusual spikes, drops, or patterns that indicate security events.

### Fundamental Approach

**Calculating total log entries:**

```bash
wc -l /var/log/auth.log
```

**Volume analysis over time windows:**

```bash
# Hourly volume count
awk '{print $1, $2, $3}' /var/log/syslog | uniq -c

# Extract timestamps and count per hour
awk '{print substr($3,1,2)}' /var/log/apache2/access.log | sort | uniq -c

# JSON logs with jq
jq -r '.timestamp' logs.json | cut -d'T' -f1 | uniq -c
```

### Advanced Volume Analysis Techniques

**Time-series volume tracking with awk:**

```bash
# Count logs per minute
awk '{
    timestamp = $1" "$2" "$3
    gsub(/:[0-9]{2}$/, ":00", timestamp)
    count[timestamp]++
}
END {
    for (t in count) print t, count[t]
}' /var/log/syslog | sort
```

**Detecting volume anomalies:**

```bash
# Calculate mean and standard deviation
awk '{print $1}' volume_per_hour.txt | awk '{
    sum += $1
    sumsq += ($1)^2
    count++
}
END {
    mean = sum/count
    stddev = sqrt((sumsq/count) - (mean^2))
    print "Mean:", mean
    print "StdDev:", stddev
    print "Threshold (mean + 2*stddev):", mean + 2*stddev
}'
```

**Visualizing volume with gnuplot:**

```bash
# Prepare data
awk '{print NR, $1}' volume_counts.txt > plot_data.txt

# Generate plot
gnuplot -e "set terminal dumb; set title 'Log Volume Over Time'; plot 'plot_data.txt' with lines"
```

### Tool-Specific Volume Analysis

**Using grep with timestamps:**

```bash
# Apache access logs - requests per hour
grep "$(date '+%d/%b/%Y')" /var/log/apache2/access.log | cut -d'[' -f2 | cut -d':' -f2 | sort | uniq -c

# Auth logs - failed attempts per hour
grep "Failed password" /var/log/auth.log | awk '{print $1, $2, $3}' | cut -d':' -f1-2 | uniq -c
```

**Python for complex volume analysis:**

```python
#!/usr/bin/env python3
from collections import Counter
from datetime import datetime
import re

def analyze_volume(logfile):
    timestamps = []
    with open(logfile, 'r') as f:
        for line in f:
            # Extract timestamp (adjust regex for your format)
            match = re.search(r'\[(\d{2}/\w{3}/\d{4}:\d{2})', line)
            if match:
                timestamps.append(match.group(1))
    
    hourly_counts = Counter(timestamps)
    
    # Calculate statistics
    counts = list(hourly_counts.values())
    mean = sum(counts) / len(counts)
    variance = sum((x - mean) ** 2 for x in counts) / len(counts)
    stddev = variance ** 0.5
    
    print(f"Total logs: {len(timestamps)}")
    print(f"Mean per hour: {mean:.2f}")
    print(f"Std deviation: {stddev:.2f}")
    
    # Identify anomalies (> 2 standard deviations)
    threshold = mean + (2 * stddev)
    anomalies = {k: v for k, v in hourly_counts.items() if v > threshold}
    
    if anomalies:
        print("\nAnomalous hours:")
        for hour, count in sorted(anomalies.items()):
            print(f"  {hour}: {count} logs ({(count-mean)/stddev:.2f} )")

if __name__ == "__main__":
    analyze_volume('/var/log/apache2/access.log')
```

## Event Frequency Counting

Frequency counting identifies how often specific events occur, enabling detection of brute-force attempts, repeated errors, or reconnaissance activity.

### Basic Event Counting

**Counting specific events:**

```bash
# Failed SSH login attempts
grep "Failed password" /var/log/auth.log | wc -l

# HTTP 404 errors
grep " 404 " /var/log/apache2/access.log | wc -l

# Specific IP address occurrences
grep -c "192.168.1.100" /var/log/syslog
```

**Top N most frequent events:**

```bash
# Top 10 IP addresses
awk '{print $1}' /var/log/apache2/access.log | sort | uniq -c | sort -rn | head -10

# Top 10 requested URLs
awk '{print $7}' /var/log/apache2/access.log | sort | uniq -c | sort -rn | head -10

# Top failed login usernames
grep "Failed password for" /var/log/auth.log | awk '{print $11}' | sort | uniq -c | sort -rn | head -10
```

### Advanced Frequency Analysis

**Multi-field frequency analysis:**

```bash
# IP addresses with failed login attempts (IP + count)
grep "Failed password" /var/log/auth.log | awk '{print $(NF-3)}' | sort | uniq -c | sort -rn

# User-agent frequency in web logs
awk -F'"' '{print $6}' /var/log/apache2/access.log | sort | uniq -c | sort -rn | head -20

# HTTP method distribution
awk '{print $6}' /var/log/apache2/access.log | tr -d '"' | sort | uniq -c
```

**Time-based frequency analysis:**

```bash
# Failed logins per hour
grep "Failed password" /var/log/auth.log | awk '{print $1, $2, substr($3,1,2)}' | sort | uniq -c

# Requests per day
awk '{print substr($4,2,11)}' /var/log/apache2/access.log | sort | uniq -c
```

**Using datamash for statistical frequency:**

```bash
# Install if needed: apt-get install datamash

# Group and count by field
awk '{print $1}' /var/log/apache2/access.log | datamash -s groupby 1 count 1 | sort -k2 -rn | head -20

# Mean requests per IP
awk '{print $1}' /var/log/apache2/access.log | datamash -s groupby 1 count 1 | awk '{print $2}' | datamash mean 1 sstdev 1
```

### CTF-Specific Event Patterns

**Identifying scanning activity:**

```bash
# Detect port scanning (multiple port hits from single IP)
grep "Connection attempt" /var/log/syslog | awk '{print $NF, $(NF-2)}' | sort | uniq | awk '{print $1}' | uniq -c | sort -rn

# Web directory enumeration detection
awk '$9 == 404 {print $1}' /var/log/apache2/access.log | uniq -c | awk '$1 > 50 {print $2, $1}'
```

**Brute-force detection:**

```bash
# Failed SSH attempts per IP
grep "Failed password" /var/log/auth.log | awk '{print $(NF-3)}' | sort | uniq -c | awk '$1 > 5 {print "Possible brute-force from", $2, "with", $1, "attempts"}'

# Failed logins within time window (last hour)
grep "Failed password" /var/log/auth.log | awk -v cutoff="$(date -d '1 hour ago' '+%b %d %H')" '$1 " " $2 " " substr($3,1,2) >= cutoff {print $(NF-3)}' | sort | uniq -c | sort -rn
```

**Frequency-based anomaly scoring:**

```bash
# Create frequency baseline and flag outliers
awk '{print $1}' /var/log/apache2/access.log | sort | uniq -c | awk '
{
    freq[$2] = $1
    sum += $1
    sumsq += $1 * $1
    n++
}
END {
    mean = sum / n
    stddev = sqrt((sumsq / n) - (mean * mean))
    for (ip in freq) {
        z_score = (freq[ip] - mean) / stddev
        if (z_score > 3) print ip, freq[ip], "Z-score:", z_score
    }
}'
```

## Distribution Analysis

Distribution analysis examines how events spread across different dimensions (time, sources, targets, event types) to identify patterns and anomalies.

### Temporal Distribution

**Hourly distribution:**

```bash
# Extract hour from timestamp and count
awk '{print substr($3,1,2)}' /var/log/auth.log | sort | uniq -c

# Visualize hourly distribution
awk '{print substr($3,1,2)}' /var/log/auth.log | sort | uniq -c | awk '{printf "%02d:00 ", $2; for(i=0;i<$1/10;i++) printf "#"; print ""}'
```

**Day-of-week distribution:**

```bash
# Using date command to convert timestamps
awk '{print $1, $2, $3}' /var/log/syslog | while read line; do date -d "$line" "+%A" 2>/dev/null; done | sort | uniq -c

# For Apache logs
awk '{print substr($4,2,11)}' /var/log/apache2/access.log | while read d; do date -d "$d" "+%A" 2>/dev/null; done | sort | uniq -c
```

### Source/Destination Distribution

**IP address distribution analysis:**

```bash
# Source IP distribution
awk '{print $1}' /var/log/apache2/access.log | sort | uniq -c | awk '{
    if ($1 == 1) single++
    else if ($1 <= 10) low++
    else if ($1 <= 100) medium++
    else if ($1 <= 1000) high++
    else extreme++
}
END {
    print "Single access:", single
    print "Low (2-10):", low
    print "Medium (11-100):", medium
    print "High (101-1000):", high
    print "Extreme (>1000):", extreme
}'
```

**Geographic distribution (requires geoip):**

```bash
# Install: apt-get install geoip-bin geoip-database

# Count by country
awk '{print $1}' /var/log/apache2/access.log | sort -u | while read ip; do geoiplookup $ip | cut -d',' -f2; done | sort | uniq -c | sort -rn
```

### Event Type Distribution

**HTTP status code distribution:**

```bash
# Status code breakdown
awk '{print $9}' /var/log/apache2/access.log | sort | uniq -c | sort -rn

# Percentage distribution
awk '{print $9}' /var/log/apache2/access.log | sort | uniq -c | awk '{
    codes[$2] = $1
    total += $1
}
END {
    for (code in codes) {
        printf "%s: %d (%.2f%%)\n", code, codes[code], (codes[code]/total)*100
    }
}' | sort -t':' -k2 -rn
```

**Log level distribution (syslog):**

```bash
# Extract priority/severity
grep -oP '(?<=<)[0-9]+(?=>)' /var/log/syslog | awk '{
    severity = $1 % 8
    sev_names[0]="EMERG"; sev_names[1]="ALERT"; sev_names[2]="CRIT"
    sev_names[3]="ERR"; sev_names[4]="WARNING"; sev_names[5]="NOTICE"
    sev_names[6]="INFO"; sev_names[7]="DEBUG"
    print sev_names[severity]
}' | sort | uniq -c
```

### Statistical Distribution Metrics

**Calculating entropy (measure of randomness):**

```python
#!/usr/bin/env python3
import math
from collections import Counter

def calculate_entropy(data):
    """Calculate Shannon entropy of event distribution"""
    if not data:
        return 0
    
    counts = Counter(data)
    total = len(data)
    
    entropy = 0
    for count in counts.values():
        probability = count / total
        entropy -= probability * math.log2(probability)
    
    return entropy

def analyze_distribution(logfile, field_index):
    """Analyze distribution of specific field"""
    events = []
    with open(logfile, 'r') as f:
        for line in f:
            fields = line.split()
            if len(fields) > field_index:
                events.append(fields[field_index])
    
    entropy = calculate_entropy(events)
    unique_events = len(set(events))
    total_events = len(events)
    
    print(f"Total events: {total_events}")
    print(f"Unique events: {unique_events}")
    print(f"Entropy: {entropy:.4f} bits")
    print(f"Normalized entropy: {entropy/math.log2(unique_events) if unique_events > 1 else 0:.4f}")
    
    # High entropy may indicate diverse/random activity
    # Low entropy indicates repetitive patterns
    if entropy < 2:
        print(" Low entropy - highly repetitive patterns detected")
    elif entropy > math.log2(unique_events) * 0.8:
        print(" High entropy - diverse/random activity")

if __name__ == "__main__":
    # Example: analyze source IPs (field 0 in Apache logs)
    analyze_distribution('/var/log/apache2/access.log', 0)
```

**Percentile analysis:**

```bash
# Calculate percentiles for request frequencies
awk '{print $1}' /var/log/apache2/access.log | sort | uniq -c | awk '{print $1}' | sort -n | awk '
BEGIN { count = 0 }
{
    values[count++] = $1
}
END {
    # Calculate percentiles
    p50 = values[int(count * 0.50)]
    p75 = values[int(count * 0.75)]
    p90 = values[int(count * 0.90)]
    p95 = values[int(count * 0.95)]
    p99 = values[int(count * 0.99)]
    
    print "50th percentile:", p50
    print "75th percentile:", p75
    print "90th percentile:", p90
    print "95th percentile:", p95
    print "99th percentile:", p99
}'
```

### CTF Pattern Recognition with Distribution Analysis

**Detecting data exfiltration via distribution:**

```bash
# Unusual request size distribution
awk '{print $10}' /var/log/apache2/access.log | grep -v '-' | sort -n | uniq -c | awk '$2 > 1000000 {print "Large response:", $2, "bytes", "Count:", $1}'

# Timing pattern detection (requests at regular intervals)
awk '{
    gsub(/[\[\]]/, "", $4)
    split($4, dt, ":")
    timestamp = dt[1] " " dt[2] " " dt[3]
    timestamps[timestamp]++
}
END {
    for (t in timestamps) print timestamps[t], t
}' /var/log/apache2/access.log | sort -n | uniq -c | awk '$1 > 10 {print "Possible automated activity at interval with", $2, "occurrences"}'
```

**User-agent distribution anomalies:**

```bash
# Identify rare user-agents accessing many resources
awk -F'"' '{print $6}' /var/log/apache2/access.log | sort | uniq -c | sort -n | head -20 | awk '{
    if ($1 < 5) {
        ua = $0
        gsub(/^[0-9]+ /, "", ua)
        print "Rare UA with few requests:", ua, "(" $1 ")"
    }
}'
```

### Combined Statistical Analysis Script

```python
#!/usr/bin/env python3
"""
Comprehensive statistical log analysis for CTF scenarios
"""
import sys
import re
from collections import Counter, defaultdict
from datetime import datetime
import math

class LogStatAnalyzer:
    def __init__(self, logfile):
        self.logfile = logfile
        self.timestamps = []
        self.sources = []
        self.events = []
        
    def parse_apache_log(self):
        """Parse Apache access logs"""
        pattern = r'(\S+) \S+ \S+ \[([^\]]+)\] "(\S+) (\S+) \S+" (\d{3}) (\S+)'
        
        with open(self.logfile, 'r') as f:
            for line in f:
                match = re.search(pattern, line)
                if match:
                    ip, timestamp, method, url, status, size = match.groups()
                    self.sources.append(ip)
                    self.timestamps.append(timestamp)
                    self.events.append(f"{method} {status}")
    
    def volume_statistics(self):
        """Calculate volume statistics"""
        total = len(self.timestamps)
        
        # Hourly distribution
        hourly = Counter(ts.split(':')[1] for ts in self.timestamps)
        hourly_counts = list(hourly.values())
        
        mean = sum(hourly_counts) / len(hourly_counts) if hourly_counts else 0
        variance = sum((x - mean) ** 2 for x in hourly_counts) / len(hourly_counts) if hourly_counts else 0
        stddev = math.sqrt(variance)
        
        print(f"\n=== Volume Statistics ===")
        print(f"Total logs: {total}")
        print(f"Hourly mean: {mean:.2f}")
        print(f"Hourly stddev: {stddev:.2f}")
        
        # Detect anomalies
        threshold = mean + (2 * stddev)
        anomalies = {h: c for h, c in hourly.items() if c > threshold}
        if anomalies:
            print(f"\nAnomalous hours (>{threshold:.0f}):")
            for hour, count in sorted(anomalies.items()):
                print(f"  Hour {hour}: {count} logs")
    
    def frequency_analysis(self):
        """Analyze event frequencies"""
        print(f"\n=== Frequency Analysis ===")
        
        # Top sources
        source_freq = Counter(self.sources)
        print(f"\nTop 10 source IPs:")
        for ip, count in source_freq.most_common(10):
            print(f"  {ip}: {count}")
        
        # Event type distribution
        event_freq = Counter(self.events)
        print(f"\nTop 10 event types:")
        for event, count in event_freq.most_common(10):
            print(f"  {event}: {count}")
    
    def distribution_analysis(self):
        """Analyze event distributions"""
        print(f"\n=== Distribution Analysis ===")
        
        # Source entropy
        entropy = self._calculate_entropy(self.sources)
        unique_sources = len(set(self.sources))
        max_entropy = math.log2(unique_sources) if unique_sources > 1 else 0
        
        print(f"\nSource IP entropy: {entropy:.4f} bits")
        print(f"Max possible entropy: {max_entropy:.4f} bits")
        print(f"Normalized entropy: {entropy/max_entropy if max_entropy > 0 else 0:.4f}")
        
        if entropy < max_entropy * 0.5:
            print(" Low entropy: traffic dominated by few sources")
        
        # Access pattern distribution
        source_counts = Counter(self.sources)
        count_dist = Counter(source_counts.values())
        
        print(f"\nAccess distribution:")
        for freq, num_sources in sorted(count_dist.items())[:10]:
            print(f"  {num_sources} sources made {freq} requests")
    
    def _calculate_entropy(self, data):
        """Calculate Shannon entropy"""
        if not data:
            return 0
        counts = Counter(data)
        total = len(data)
        entropy = 0
        for count in counts.values():
            p = count / total
            entropy -= p * math.log2(p)
        return entropy
    
    def run_analysis(self):
        """Execute full statistical analysis"""
        print(f"Analyzing: {self.logfile}")
        self.parse_apache_log()
        self.volume_statistics()
        self.frequency_analysis()
        self.distribution_analysis()

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python3 stat_analysis.py <logfile>")
        sys.exit(1)
    
    analyzer = LogStatAnalyzer(sys.argv[1])
    analyzer.run_analysis()
```

**[Inference]**: The effectiveness of statistical analysis depends on having sufficient log volume to establish meaningful baselines. In CTF environments with limited or synthesized logs, statistical significance may be reduced.

---

## Correlation Coefficients

Correlation coefficients measure the statistical relationship between two variables in log data, helping identify connections between seemingly unrelated events.

### Pearson Correlation Coefficient

Measures linear correlation between two continuous variables, ranging from -1 (perfect negative correlation) to +1 (perfect positive correlation).

**Common applications in log analysis:**

- Correlating failed login attempts with subsequent successful logins
- Identifying relationships between network traffic volume and CPU usage
- Linking time-based patterns across multiple log sources

**Command-line calculation using awk:**

```bash
# Calculate Pearson correlation between two columns in a CSV log file
awk -F',' '{x+=$1; y+=$2; xy+=$1*$2; x2+=$1*$1; y2+=$2*$2; n++} 
END {print (n*xy - x*y) / sqrt((n*x2-x*x)*(n*y2-y*y))}' logfile.csv
```

**Using Python with pandas for log correlation:**

```bash
# Install pandas if not present
pip3 install pandas

# Python one-liner for correlation matrix
python3 -c "import pandas as pd; df=pd.read_csv('access.log', sep=' ', usecols=[0,9], names=['ip','status']); print(df.corr())"
```

**R for statistical analysis:**

```bash
# Calculate correlation from log file
Rscript -e "data <- read.csv('logfile.csv'); cor(data\$column1, data\$column2)"
```

### Spearman Rank Correlation

Non-parametric measure useful when data isn't normally distributed or contains outliers, common in real-world log data.

**Python implementation:**

```python
import pandas as pd
from scipy.stats import spearmanr

# Load log data
df = pd.read_csv('auth.log', sep='\s+', names=['timestamp', 'attempts'])
correlation, p_value = spearmanr(df['timestamp'], df['attempts'])
print(f"Spearman correlation: {correlation}, p-value: {p_value}")
```

### Practical CTF application

**Detecting coordinated attacks:**

```bash
# Extract IPs and timestamps from failed SSH attempts
grep "Failed password" /var/log/auth.log | awk '{print $1,$3,$11}' > failed_attempts.txt

# Correlate attempt frequency across different IPs
python3 << EOF
import pandas as pd
from scipy.stats import pearsonr

data = pd.read_csv('failed_attempts.txt', sep=' ', names=['date','time','ip'])
data['hour'] = pd.to_datetime(data['time']).dt.hour
pivot = data.groupby(['hour','ip']).size().unstack(fill_value=0)

# Find correlated attack patterns
for i in range(len(pivot.columns)):
    for j in range(i+1, len(pivot.columns)):
        corr, _ = pearsonr(pivot.iloc[:,i], pivot.iloc[:,j])
        if abs(corr) > 0.8:
            print(f"Correlated IPs: {pivot.columns[i]} <-> {pivot.columns[j]}: {corr:.3f}")
EOF
```

## Moving Averages

Moving averages smooth time-series log data to identify trends, detect anomalies, and establish baselines for normal system behavior.

### Simple Moving Average (SMA)

Arithmetic mean of the last N data points, useful for establishing baseline behavior.

**Calculate SMA from Apache access logs:**

```bash
# Count requests per minute and calculate 5-minute moving average
awk '{print $4}' /var/log/apache2/access.log | \
  cut -d: -f2-3 | sort | uniq -c | \
  awk '{print $1}' | \
  awk '{
    sum += $1
    arr[NR] = $1
    if (NR > 5) sum -= arr[NR-5]
    if (NR >= 5) print sum/5
  }'
```

**Using pandas for time-series moving average:**

```python
import pandas as pd

# Parse nginx access log
df = pd.read_csv('access.log', sep=r'\s+', 
                 names=['ip','timestamp','request','status','size'],
                 parse_dates=['timestamp'])
df.set_index('timestamp', inplace=True)

# Resample to 1-minute intervals and calculate 10-minute moving average
requests_per_min = df.resample('1T').size()
moving_avg = requests_per_min.rolling(window=10).mean()
print(moving_avg)
```

### Exponential Moving Average (EMA)

Gives more weight to recent observations, more responsive to recent changes in attack patterns.

**EMA calculation for real-time monitoring:**

```python
import pandas as pd

# Calculate EMA with alpha=0.3 (30% weight to new data)
df = pd.read_csv('firewall.log', parse_dates=['timestamp'])
df.set_index('timestamp', inplace=True)
blocked_per_min = df[df['action']=='BLOCK'].resample('1T').size()
ema = blocked_per_min.ewm(alpha=0.3, adjust=False).mean()
print(ema)
```

### Detecting anomalies using moving averages

**Standard deviation-based anomaly detection:**

```bash
# Detect traffic spikes exceeding 3 standard deviations from moving average
python3 << 'EOF'
import pandas as pd
import numpy as np

# Load and parse access log
df = pd.read_csv('access.log', sep=r'\s+', names=['timestamp','ip','status'], 
                 parse_dates=['timestamp'])
df.set_index('timestamp', inplace=True)

# Calculate metrics
requests = df.resample('1T').size()
moving_avg = requests.rolling(window=10).mean()
moving_std = requests.rolling(window=10).std()

# Detect anomalies
upper_bound = moving_avg + (3 * moving_std)
anomalies = requests[requests > upper_bound]

print("Detected anomalies:")
print(anomalies)
EOF
```

**Practical CTF scenario - DDoS detection:**

```bash
# Monitor real-time traffic and alert on anomalies
tail -f /var/log/nginx/access.log | \
  awk '{print strftime("%Y-%m-%d %H:%M")}' | \
  uniq -c | \
  awk '{
    sum += $1
    arr[NR] = $1
    if (NR > 5) sum -= arr[NR-5]
    avg = (NR >= 5) ? sum/5 : sum/NR
    if ($1 > avg * 3) print "ALERT: Traffic spike detected -", $1, "requests (avg:", avg, ")"
  }'
```

## Percentile Calculations

Percentiles identify threshold values in log data distributions, crucial for establishing baselines and detecting outliers.

### Basic percentile calculation

**Using awk for 95th percentile:**

```bash
# Calculate 95th percentile of response times from access log
awk '{print $NF}' access.log | sort -n | \
  awk 'BEGIN{c=0} {val[c++]=$1} END{print val[int(c*0.95)]}'
```

**Python percentile calculation:**

```python
import numpy as np
import pandas as pd

# Load log data
df = pd.read_csv('response_times.log', names=['duration'])

# Calculate percentiles
p50 = np.percentile(df['duration'], 50)  # Median
p95 = np.percentile(df['duration'], 95)
p99 = np.percentile(df['duration'], 99)

print(f"50th percentile: {p50}ms")
print(f"95th percentile: {p95}ms")
print(f"99th percentile: {p99}ms")
```

### Interquartile Range (IQR) for outlier detection

The IQR method identifies outliers as values below Q1 - 1.5IQR or above Q3 + 1.5IQR.

**IQR-based outlier detection:**

```python
import pandas as pd
import numpy as np

# Parse authentication log
df = pd.read_csv('auth_attempts.log', names=['timestamp','user','attempts'])

Q1 = df['attempts'].quantile(0.25)
Q3 = df['attempts'].quantile(0.75)
IQR = Q3 - Q1

# Define outlier boundaries
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Identify outliers
outliers = df[(df['attempts'] < lower_bound) | (df['attempts'] > upper_bound)]
print(f"Detected {len(outliers)} outlier events:")
print(outliers)
```

### Time-based percentile analysis

**Hourly percentile tracking:**

```bash
# Calculate hourly 95th percentile for request sizes
python3 << 'EOF'
import pandas as pd

df = pd.read_csv('access.log', sep=r'\s+', 
                 names=['timestamp','ip','request','status','size'],
                 parse_dates=['timestamp'])
df.set_index('timestamp', inplace=True)

# Group by hour and calculate 95th percentile
hourly_p95 = df.groupby(df.index.hour)['size'].quantile(0.95)
print("Hourly 95th percentile request sizes:")
print(hourly_p95)
EOF
```

### Practical CTF scenario - Brute force detection

**Identify accounts under attack using percentile thresholds:**

```bash
# Extract failed login attempts per user
grep "Failed password" /var/log/auth.log | \
  awk '{print $9}' | sort | uniq -c | sort -rn > failed_by_user.txt

# Calculate thresholds
python3 << 'EOF'
import pandas as pd

df = pd.read_csv('failed_by_user.txt', sep=r'\s+', names=['count','user'])

p75 = df['count'].quantile(0.75)
p90 = df['count'].quantile(0.90)
p95 = df['count'].quantile(0.95)

print(f"75th percentile: {p75} attempts")
print(f"90th percentile: {p90} attempts")
print(f"95th percentile: {p95} attempts")
print(f"\nUsers in 95th percentile (likely under attack):")
print(df[df['count'] >= p95])
EOF
```

### Combined statistical approach

**Multi-metric anomaly detection:**

```python
import pandas as pd
import numpy as np
from scipy.stats import pearsonr

# Load parsed log data
df = pd.read_csv('network.log', parse_dates=['timestamp'])
df.set_index('timestamp', inplace=True)

# Resample to 5-minute windows
resampled = df.resample('5T').agg({
    'bytes_sent': 'sum',
    'bytes_received': 'sum',
    'connections': 'count'
})

# Calculate moving averages
resampled['sent_ma'] = resampled['bytes_sent'].rolling(window=12).mean()
resampled['recv_ma'] = resampled['bytes_received'].rolling(window=12).mean()

# Calculate percentile thresholds
sent_p95 = resampled['bytes_sent'].quantile(0.95)
recv_p95 = resampled['bytes_received'].quantile(0.95)

# Detect anomalies exceeding percentile thresholds
anomalies = resampled[
    (resampled['bytes_sent'] > sent_p95) | 
    (resampled['bytes_received'] > recv_p95)
]

# Calculate correlation during anomaly periods
if len(anomalies) > 1:
    corr, p_val = pearsonr(anomalies['bytes_sent'], anomalies['bytes_received'])
    print(f"Correlation during anomalies: {corr:.3f} (p={p_val:.3f})")

print(f"\nDetected {len(anomalies)} anomalous periods:")
print(anomalies[['bytes_sent', 'bytes_received', 'connections']])
```

### Tools and utilities

**GNU datamash for quick statistical calculations:**

```bash
# Install datamash
apt-get install datamash

# Calculate multiple statistics from log column
awk '{print $10}' access.log | datamash mean 1 median 1 q1 1 q3 1 iqr 1
```

**R for advanced statistical analysis:**

```bash
# Install R statistical environment
apt-get install r-base

# Calculate comprehensive statistics
Rscript << 'EOF'
data <- read.table('response_times.txt', header=FALSE)
summary(data)
quantile(data$V1, probs=seq(0,1,0.05))
EOF
```

---

**Important subtopics to explore:**

- Time-series decomposition for seasonal pattern detection in logs
- Bayesian inference for probabilistic anomaly scoring
- Z-score normalization for cross-metric comparison
- Autoregressive models for predictive log analysis

---

# Visualization Techniques

Visualization transforms raw log data into actionable intelligence during CTF scenarios. Effective visualization reveals attack patterns, anomalies, and temporal relationships that are difficult to detect in text-based analysis.

## gnuplot usage

gnuplot is a command-line driven graphing utility ideal for quick visualizations during time-sensitive CTF challenges.

**Basic invocation:**

```bash
gnuplot -persist script.gp
```

**Common data formats for log analysis:**

For time-series log data, prepare data in space/tab-delimited format:

```
# timestamp value
1698765432 145
1698765492 203
1698765552 187
```

**Essential gnuplot commands for log visualization:**

**Plot network traffic over time:**

```gnuplot
set terminal png size 1200,800
set output 'traffic.png'
set xlabel "Time"
set ylabel "Packets/sec"
set xdata time
set timefmt "%s"
set format x "%H:%M:%S"
plot 'network.dat' using 1:2 with lines title 'Traffic Rate'
```

**Multiple data series (comparing sources):**

```gnuplot
plot 'auth.dat' using 1:2 with lines title 'Failed Logins', \
     'auth.dat' using 1:3 with lines title 'Successful Logins'
```

**Heatmap for attack distribution:**

```gnuplot
set view map
set dgrid3d 30,30
splot 'attack_coords.dat' using 1:2:3 with pm3d
```

**Common CTF scenarios:**

- Graphing authentication attempts to identify brute-force windows
- Visualizing port scan patterns by timestamp
- Plotting HTTP response codes to find application behavior changes

**Practical pipeline:**

```bash
# Extract timestamps and connection counts from Apache logs
awk '{print $4, $1}' access.log | \
  sed 's/\[//g' | \
  uniq -c | \
  awk '{print $2, $1}' > connections.dat

gnuplot -e "set term png; set output 'graph.png'; plot 'connections.dat' with lines"
```

**Performance considerations:**

- gnuplot handles datasets up to ~1M points efficiently on standard hardware
- For larger datasets, use `every` to sample: `plot 'data.dat' every 10 using 1:2`

## matplotlib for log data

matplotlib provides programmatic control for complex visualizations requiring preprocessing or correlation analysis.

**Basic setup in CTF environment:**

```python
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from datetime import datetime
```

**Parse and visualize auth logs:**

```python
import re
from collections import defaultdict

# Parse failed SSH attempts
timestamps = []
counts = defaultdict(int)

with open('/var/log/auth.log', 'r') as f:
    for line in f:
        if 'Failed password' in line:
            match = re.search(r'(\w{3}\s+\d{1,2}\s+\d{2}:\d{2}:\d{2})', line)
            if match:
                ts = datetime.strptime(match.group(1), '%b %d %H:%M:%S')
                # Add current year (auth.log doesn't include it)
                ts = ts.replace(year=datetime.now().year)
                timestamps.append(ts)

# Bin by hour
for ts in timestamps:
    hour_bin = ts.replace(minute=0, second=0)
    counts[hour_bin] += 1

# Plot
hours = sorted(counts.keys())
values = [counts[h] for h in hours]

plt.figure(figsize=(12, 6))
plt.plot(hours, values, marker='o')
plt.xlabel('Time')
plt.ylabel('Failed Auth Attempts')
plt.title('SSH Brute Force Pattern Analysis')
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('bruteforce_timeline.png', dpi=150)
```

**Multi-dimensional correlation plot:**

```python
import pandas as pd
import numpy as np

# Correlate HTTP status codes with response times
df = pd.read_csv('access_log.csv')  # Pre-parsed log data

fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))

# Response time distribution by status code
for code in [200, 404, 500]:
    subset = df[df['status'] == code]['response_time']
    ax1.hist(subset, bins=50, alpha=0.5, label=f'HTTP {code}')
ax1.set_xlabel('Response Time (ms)')
ax1.set_ylabel('Frequency')
ax1.legend()
ax1.set_title('Response Time Distribution by Status Code')

# Timeline of status codes
status_timeline = df.groupby(['timestamp', 'status']).size().unstack(fill_value=0)
status_timeline.plot(ax=ax2, kind='area', stacked=True)
ax2.set_ylabel('Request Count')
ax2.set_title('HTTP Status Code Timeline')

plt.tight_layout()
plt.savefig('http_analysis.png', dpi=150)
```

**Network traffic visualization:**

```python
import matplotlib.pyplot as plt
from scapy.all import rdpcap, TCP, IP

# Analyze pcap file for port distribution
packets = rdpcap('capture.pcap')

port_counts = defaultdict(int)
for pkt in packets:
    if TCP in pkt:
        port_counts[pkt[TCP].dport] += 1

# Plot top 20 ports
sorted_ports = sorted(port_counts.items(), key=lambda x: x[1], reverse=True)[:20]
ports, counts = zip(*sorted_ports)

plt.figure(figsize=(10, 6))
plt.bar(range(len(ports)), counts)
plt.xticks(range(len(ports)), ports, rotation=45)
plt.xlabel('Destination Port')
plt.ylabel('Packet Count')
plt.title('Port Scan Target Distribution')
plt.tight_layout()
plt.savefig('port_distribution.png')
```

**Heatmap for temporal attack patterns:**

```python
import seaborn as sns

# Create hour-of-day vs day-of-week heatmap
df['hour'] = pd.to_datetime(df['timestamp']).dt.hour
df['day'] = pd.to_datetime(df['timestamp']).dt.dayofweek

heatmap_data = df.groupby(['day', 'hour']).size().unstack(fill_value=0)

plt.figure(figsize=(14, 6))
sns.heatmap(heatmap_data, cmap='YlOrRd', annot=False, fmt='d')
plt.xlabel('Hour of Day')
plt.ylabel('Day of Week (0=Monday)')
plt.title('Attack Attempt Heatmap')
plt.savefig('attack_heatmap.png', dpi=150)
```

**Interactive exploration (useful for local CTF analysis):**

```python
import matplotlib.pyplot as plt
from matplotlib.widgets import Slider

# Interactive threshold adjustment for anomaly detection
fig, ax = plt.subplots(figsize=(12, 6))
plt.subplots_adjust(bottom=0.25)

line, = ax.plot(timestamps, values, 'b-', alpha=0.7)
threshold_line = ax.axhline(y=100, color='r', linestyle='--', label='Threshold')

ax_threshold = plt.axes([0.2, 0.1, 0.6, 0.03])
threshold_slider = Slider(ax_threshold, 'Threshold', 0, max(values), valinit=100)

def update(val):
    threshold_line.set_ydata(val)
    fig.canvas.draw_idle()

threshold_slider.on_changed(update)
plt.show()
```

**Performance tips:**

- Use `plt.ioff()` to disable interactive mode for batch processing
- For large datasets, downsample with pandas: `df.resample('1min').mean()`
- Save directly to file to avoid memory issues: `plt.savefig()` instead of `plt.show()`

## Elastic Stack visualization

The Elastic Stack (Elasticsearch, Logstash, Kibana) provides enterprise-grade visualization for large-scale log analysis. In CTF scenarios, it's particularly valuable for multi-source correlation and real-time monitoring.

**Architecture overview:**

- **Elasticsearch**: Distributed search/analytics engine (storage/query layer)
- **Logstash**: Data processing pipeline (ingestion/transformation)
- **Kibana**: Visualization interface (presentation layer)
- **Beats** (optional): Lightweight data shippers (Filebeat, Packetbeat, etc.)

**Quick deployment for CTF:**

```bash
# Docker Compose approach (recommended for CTF environments)
curl -L https://github.com/deviantony/docker-elk/archive/refs/heads/main.zip -o elk.zip
unzip elk.zip
cd docker-elk-main

# Start stack
docker-compose up -d

# Verify services
curl -X GET "localhost:9200/_cluster/health?pretty"  # Elasticsearch
curl -X GET "localhost:5601/api/status"              # Kibana
```

**Default access:**

- Kibana: http://localhost:5601
- Elasticsearch: http://localhost:9200
- Logstash: localhost:5000 (beats input), 9600 (monitoring)

**Ingesting log data with Logstash:**

**Apache access log configuration** (`/etc/logstash/conf.d/apache.conf`):

```ruby
input {
  file {
    path => "/var/log/apache2/access.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
  }
}

filter {
  grok {
    match => { "message" => "%{COMBINEDAPACHELOG}" }
  }
  date {
    match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
  }
  geoip {
    source => "clientip"
  }
  useragent {
    source => "agent"
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "apache-logs-%{+YYYY.MM.dd}"
  }
}
```

**Auth log configuration:**

```ruby
input {
  file {
    path => "/var/log/auth.log"
    start_position => "beginning"
  }
}

filter {
  grok {
    match => { 
      "message" => "%{SYSLOGTIMESTAMP:timestamp} %{SYSLOGHOST:hostname} %{DATA:program}(?:\[%{POSINT:pid}\])?: %{GREEDYDATA:log_message}" 
    }
  }
  
  if "Failed password" in [log_message] {
    mutate {
      add_field => { "event_type" => "failed_auth" }
    }
    grok {
      match => { 
        "log_message" => "Failed password for (invalid user )?%{USER:username} from %{IP:source_ip} port %{INT:source_port}" 
      }
    }
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "auth-logs-%{+YYYY.MM.dd}"
  }
}
```

**Test Logstash configuration:**

```bash
/usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/apache.conf --config.test_and_exit
```

**Ingesting packet captures with Packetbeat:**

```bash
# Install Packetbeat
curl -L -O https://artifacts.elastic.co/downloads/beats/packetbeat/packetbeat-8.11.0-amd64.deb
sudo dpkg -i packetbeat-8.11.0-amd64.deb

# Configure (edit /etc/packetbeat/packetbeat.yml)
packetbeat.interfaces.device: any
packetbeat.protocols:
  - type: http
    ports: [80, 8080, 8000]
  - type: dns
    ports: [53]
  - type: tls
    ports: [443]

output.elasticsearch:
  hosts: ["localhost:9200"]

# Start Packetbeat
sudo systemctl start packetbeat
```

**Kibana visualization workflow:**

**1. Create Index Pattern:**

- Navigate to Stack Management  Index Patterns  Create Index Pattern
- Pattern: `apache-logs-*` or `auth-logs-*`
- Time field: `@timestamp`

**2. Discover interface (initial exploration):**

- View raw log entries
- Apply filters: `clientip: 192.168.1.* AND response: 404`
- Save search for later reference

**3. Create visualizations:**

**Failed authentication timeline:**

- Visualization Type: Line Chart
- Data Source: `auth-logs-*`
- Metrics: Count
- Buckets: Date Histogram on `@timestamp` (interval: 5 minutes)
- Filters: `event_type: failed_auth`

**Geographic attack source map:**

- Visualization Type: Maps
- Layer: Choropleth (region map)
- Data Source: `apache-logs-*` or `auth-logs-*`
- Metrics: Count of documents
- Join field: `geoip.country_iso_code`

**Top attacked endpoints (data table):**

- Visualization Type: Data Table
- Metrics: Count
- Buckets:
    - Terms aggregation on `request.keyword`
    - Sub-bucket: Terms on `response`
- Filters: `response: [400 TO 599]`

**Port scan detection (metric visualization):**

- Visualization Type: Metric
- Query: `destination.port: * AND source.ip: X.X.X.X`
- Metrics: Unique count of `destination.port`
- Time range: Last 15 minutes
- Threshold: > 100 (indicates scanning behavior)

**4. Build comprehensive dashboard:**

- Combine multiple visualizations
- Add markdown widgets for context
- Set auto-refresh (e.g., 30 seconds for real-time monitoring)
- Apply global time filter and queries

**Advanced Kibana features for CTF:**

**TSVB (Time Series Visual Builder) for complex metrics:**

```
Metrics:
- Series 1: Count where response >= 500 (Server Errors)
- Series 2: Count where response >= 400 AND response < 500 (Client Errors)
- Series 3: Count where response >= 200 AND response < 300 (Success)

Panel Options:
- Show legend
- Stack series
- Time range: Last 1 hour
```

**Vega for custom visualizations** [Inference - advanced feature requiring Vega syntax knowledge]:

```json
{
  "$schema": "https://vega.github.io/schema/vega/v5.json",
  "data": {
    "url": {
      "index": "apache-logs-*",
      "body": {
        "aggs": {
          "time_buckets": {
            "date_histogram": {
              "field": "@timestamp",
              "interval": "1h"
            }
          }
        }
      }
    }
  }
}
```

**Elastic Common Schema (ECS) for standardization:** When ingesting custom logs, map fields to ECS standard:

- `source.ip` instead of `src_ip`
- `destination.port` instead of `dst_port`
- `event.action`, `event.outcome` for categorization

**Query DSL for complex searches:**

```json
GET apache-logs-*/_search
{
  "query": {
    "bool": {
      "must": [
        { "range": { "@timestamp": { "gte": "now-1h" } } },
        { "term": { "response": 404 } }
      ],
      "must_not": [
        { "match": { "request": "robots.txt" } }
      ]
    }
  },
  "aggs": {
    "top_ips": {
      "terms": { "field": "clientip", "size": 10 }
    }
  }
}
```

**Alerting (Kibana Alerting/Watcher):**

```json
PUT _watcher/watch/bruteforce_detection
{
  "trigger": {
    "schedule": { "interval": "5m" }
  },
  "input": {
    "search": {
      "request": {
        "indices": ["auth-logs-*"],
        "body": {
          "query": {
            "bool": {
              "must": [
                { "match": { "event_type": "failed_auth" } },
                { "range": { "@timestamp": { "gte": "now-5m" } } }
              ]
            }
          },
          "aggs": {
            "by_source": {
              "terms": { "field": "source_ip", "size": 10 }
            }
          }
        }
      }
    }
  },
  "condition": {
    "script": {
      "source": "ctx.payload.aggregations.by_source.buckets.stream().anyMatch(bucket -> bucket.doc_count > 20)"
    }
  },
  "actions": {
    "log_alert": {
      "logging": {
        "text": "Potential brute force from {{ctx.payload.aggregations.by_source.buckets}}"
      }
    }
  }
}
```

**Performance optimization for CTF datasets:**

- Use index lifecycle management (ILM) to delete old indices
- Adjust refresh interval: `PUT /apache-logs-*/_settings { "index.refresh_interval": "30s" }`
- Disable replicas for single-node setups: `"number_of_replicas": 0`
- Use runtime fields for ad-hoc analysis without reindexing

**Export visualizations for reporting:**

```bash
# Export dashboard as JSON
curl -X GET "localhost:5601/api/kibana/dashboards/export?dashboard=DASHBOARD_ID" > dashboard.json

# Generate PNG reports (requires X-Pack/subscription) [Unverified - may require commercial license]
# Alternative: Use browser screenshot automation with tools like Puppeteer
```

**Integration with other CTF tools:**

```bash
# Stream Elasticsearch results to jq for processing
curl -X GET "localhost:9200/apache-logs-*/_search?size=1000" | jq '.hits.hits[]._source'

# Export to CSV for external analysis
curl -X GET "localhost:9200/apache-logs-*/_search?size=10000" | \
  jq -r '.hits.hits[]._source | [.timestamp, .clientip, .request] | @csv' > export.csv
```

**Common CTF visualization scenarios:**

1. **Detecting SQL injection attempts**: Filter on request patterns containing `UNION`, `SELECT`, `' OR '1'='1`
2. **Identifying credential stuffing**: High volume of failed auths from single IP, then successful auth
3. **Command injection hunting**: Search for shell metacharacters in parameters: `;`, `|`, `&&`, backticks
4. **Exfiltration detection**: Abnormally large response sizes or long-duration connections

---

**Important subtopics for comprehensive CTF log visualization:**

- **Correlation analysis**: Cross-referencing multiple log sources (network + application + system)
- **Anomaly detection**: Statistical methods for identifying outliers in log patterns
- **Timeline analysis**: Constructing attack narratives from temporally-ordered events
- **Threat intelligence integration**: Enriching log data with known-malicious indicators (IPs, domains, file hashes)

---
## Grafana Basics

Grafana is an open-source analytics and visualization platform that queries, visualizes, and alerts on metrics and logs from multiple data sources.

### Installation and Setup

On Kali Linux, install Grafana:

```bash
sudo apt update
sudo apt install -y grafana
sudo systemctl start grafana-server
sudo systemctl enable grafana-server
```

Default access: `http://localhost:3000` (credentials: admin/admin)

### Data Source Configuration

Grafana supports multiple data sources relevant to CTF log analysis:

**Adding Elasticsearch/OpenSearch:**

```bash
# Navigate to Configuration > Data Sources > Add data source
# Select Elasticsearch
# Configure URL: http://localhost:9200
# Set Index name pattern: logstash-*
# Time field: @timestamp
```

**Adding Loki (for log aggregation):**

```bash
# Install Loki
wget https://github.com/grafana/loki/releases/download/v2.9.0/loki-linux-amd64.zip
unzip loki-linux-amd64.zip
chmod +x loki-linux-amd64

# Run Loki with config
./loki-linux-amd64 -config.file=loki-config.yaml
```

Data source URL: `http://localhost:3100`

**Adding Prometheus (for metrics):** URL: `http://localhost:9090`

### Dashboard Creation Workflow

**Creating a new dashboard:**

1. Click "+" icon  "Dashboard"  "Add new panel"
2. Select data source
3. Write query using data source query language
4. Choose visualization type
5. Configure panel options

**Query examples for log analysis:**

Elasticsearch query (Lucene syntax):

```
source:"apache" AND status:[400 TO 499]
```

LogQL (Loki) query:

```
{job="syslog"} |= "failed" | json | status >= 400
```

PromQL (Prometheus) query:

```
rate(http_requests_total{status="500"}[5m])
```

### Panel Types for CTF Analysis

**Table panel** - Raw log entries with filtering:

- Useful for examining individual events
- Configure columns to show: timestamp, source IP, destination, action, status
- Apply transformations to extract fields

**Stat panel** - Single metric display:

- Total failed login attempts
- Unique attacking IPs
- Current alert count

**Graph (Time series) panel** - Event frequency over time:

- Request rates per endpoint
- Authentication failures per minute
- Packet counts by protocol

### Variables and Templating

Variables enable dynamic dashboards that adapt to different log sources or time ranges.

**Creating a variable:**

```
Configuration > Variables > Add variable
Name: logtype
Type: Query
Data source: Elasticsearch
Query: {"find": "terms", "field": "log_type.keyword"}
```

**Using variables in queries:**

```
source:"$logtype" AND severity:error
```

**Time range variables:** Grafana provides built-in time range variables: `$__timeFrom()`, `$__timeTo()`, `$__interval`

### Annotations

Annotations mark specific events on time-based visualizations, crucial for correlating attack phases.

**Query-based annotations:**

```json
{
  "query": "event_type:exploitation AND success:true",
  "datasource": "Elasticsearch",
  "enable": true,
  "iconColor": "red"
}
```

This highlights successful exploitation attempts on all time-series panels.

## Timeline Charts

Timeline charts display events chronologically, revealing attack sequences and temporal patterns critical in CTF scenarios.

### State Timeline Visualization

The State Timeline panel shows discrete state changes over time, ideal for tracking service status, authentication states, or attack phases.

**Configuration example for authentication monitoring:**

Query (Elasticsearch):

```
auth_result:* | agg terms field:auth_result
```

Visualization settings:

- Display mode: Gradient
- Value mappings: success=1 (green), failure=0 (red), locked=-1 (yellow)
- Color scheme: By value

**Use cases:**

- Service availability tracking (up/down/degraded)
- Firewall rule state changes
- Account lock status progression
- Connection states (established/closed/filtered)

### Status History

Status History displays multiple time series as colored bars, showing state distribution across categories.

**Example: Tracking HTTP status codes across endpoints:**

Query:

```
source:"nginx" | stats count by status_code, endpoint | timechart
```

This reveals which endpoints generate specific status codes over time, identifying:

- Brute force patterns (401/403 spikes)
- Application errors (500 series)
- Not found scanning (404 floods)

### Creating Attack Phase Timelines

Effective CTF visualization requires correlating multiple log sources on a unified timeline.

**Multi-query panel configuration:**

Query A (Authentication logs):

```
log_type:"auth" | timechart count by result
```

Query B (Web access logs):

```
log_type:"apache" | timechart count by status_class
```

Query C (IDS alerts):

```
log_type:"suricata" | timechart count by severity
```

Stack these queries on a single timeline to see attack progression:

1. Reconnaissance (404 spikes)
2. Authentication attacks (auth failures)
3. Exploitation (500 errors + IDS alerts)
4. Post-exploitation (unusual access patterns)

### Interval Configuration

Timeline granularity affects pattern visibility. Configure intervals based on attack timeframe:

**High-frequency attacks (seconds to minutes):**

```
$__interval_ms: auto (typically 10s-1m)
Min interval: 10s
```

**Sustained campaigns (hours to days):**

```
$__interval: 1h
Min interval: 5m
```

**Command syntax for external timeline generation:**

Using `awk` to create timeline data from logs:

```bash
awk '{print $4, $1, $7}' /var/log/apache2/access.log | \
sed 's/\[//g' | \
sort | \
uniq -c > timeline_data.txt
```

Using `jq` for JSON logs:

```bash
jq -r '[.timestamp, .src_ip, .action] | @tsv' /var/log/app/events.json | \
sort | uniq -c
```

## Heat Maps

Heat maps visualize data density and patterns across two dimensions, typically time versus another categorical or continuous variable.

### Time-based Heat Maps

The Heatmap panel displays values as colored cells, with color intensity representing metric magnitude.

**Configuration for attack source analysis:**

Query (Elasticsearch):

```json
{
  "query": {
    "bool": {
      "must": [
        {"range": {"@timestamp": {"gte": "now-24h"}}}
      ]
    }
  },
  "aggs": {
    "time_buckets": {
      "date_histogram": {
        "field": "@timestamp",
        "interval": "1h"
      },
      "aggs": {
        "source_ips": {
          "terms": {"field": "src_ip.keyword", "size": 50}
        }
      }
    }
  }
}
```

**Data format requirements:**

- X-axis: Time buckets
- Y-axis: Categorical dimension (IPs, users, ports)
- Cell value: Metric (count, bytes, duration)

**Color scheme configuration:**

- Linear: Smooth gradient (good for continuous metrics)
- Exponential: Emphasizes outliers (good for detecting anomalies)
- Custom thresholds: Define specific ranges (e.g., <10=green, 10-100=yellow, >100=red)

### Port Scanning Detection Heat Map

Visualize scanning behavior by showing destination ports accessed per source IP.

**Query structure:**

```
stats count by src_ip, dest_port | 
where count > threshold |
eval score = count * risk_multiplier
```

**Heat map reveals:**

- Horizontal bands = Single IP scanning many ports (port scan)
- Vertical bands = Many IPs accessing single port (DDoS/focused attack)
- Scattered hot cells = Normal traffic
- Dense rectangular regions = Coordinated scanning

### Temporal Pattern Analysis

Heat maps excel at revealing periodic patterns indicating automated attacks.

**Hour-of-day vs. Day-of-week heat map:**

X-axis: Hours (0-23) Y-axis: Days (Mon-Sun) Cell color: Event count

This visualization identifies:

- Bot activity (consistent 24/7 patterns)
- Targeted attacks (specific time windows)
- Legitimate traffic variations (business hours)

**Command-line heat map generation with gnuplot:**

Prepare data:

```bash
cat /var/log/auth.log | \
grep "Failed password" | \
awk '{print $1, $2, $3}' | \
while read month day time; do
    hour=$(echo $time | cut -d: -f1)
    echo "$day $hour"
done | sort | uniq -c > heatmap_data.txt
```

Generate heat map:

```gnuplot
set terminal png size 1200,800
set output 'auth_failures_heatmap.png'
set view map
set xlabel "Hour"
set ylabel "Day"
set cblabel "Failed Attempts"
splot 'heatmap_data.txt' using 2:1:3 with image
```

### Correlation Heat Maps

Correlation heat maps show relationships between different metrics or log sources.

**Example: Service vs. Alert Type correlation**

Query configuration:

```json
{
  "x_axis": {"field": "service.keyword"},
  "y_axis": {"field": "alert_type.keyword"},
  "metric": {"sum": {"field": "severity_score"}}
}
```

**Interpretation:**

- High values indicate which services trigger which alert types
- Reveals attack paths (e.g., web service  SQLi alerts  database service  data exfil alerts)
- Identifies false positive patterns

### Geographic Heat Maps with Geomap Panel

[Inference] While Grafana supports geographic visualizations, effectiveness depends on IP geolocation data availability in your log sources.

**Prerequisites:**

- GeoIP database (MaxMind GeoLite2)
- Logstash GeoIP filter or Elasticsearch ingest pipeline
- Latitude/longitude fields in indexed logs

**Query example:**

```json
{
  "size": 0,
  "aggs": {
    "locations": {
      "geohash_grid": {
        "field": "geoip.location",
        "precision": 3
      },
      "aggs": {
        "attack_count": {"sum": {"field": "attack_score"}}
      }
    }
  }
}
```

**Geomap configuration:**

- Layer type: Heatmap
- Weight field: attack_count
- Opacity: 0.7
- Color scheme: Red-yellow gradient

### Performance Optimization

Large datasets can cause heat map rendering issues.

**Optimization strategies:**

1. **Limit Y-axis cardinality:**

```
| top limit=50 src_ip
```

2. **Pre-aggregate data:**

```bash
# Create materialized view in Elasticsearch
PUT _transform/attack_heatmap
{
  "source": {"index": "logs-*"},
  "dest": {"index": "heatmap-attacks"},
  "frequency": "1m",
  "pivot": {
    "group_by": {
      "time": {"date_histogram": {"field": "@timestamp", "fixed_interval": "5m"}},
      "source": {"terms": {"field": "src_ip.keyword"}}
    },
    "aggregations": {
      "event_count": {"value_count": {"field": "_id"}}
    }
  }
}
```

3. **Adjust time range and resolution:**

- Use appropriate `$__interval` settings
- Avoid sub-second intervals for long time ranges
- Implement dashboard time range restrictions

### Exporting Visualizations

For CTF documentation and reporting:

**PNG export:**

- Click panel title  Inspect  Panel JSON
- Or use Grafana Image Renderer plugin
- Configure via: `grafana.ini`  `[rendering]` section

**Data export:**

```bash
# Export panel data via API
curl -H "Authorization: Bearer $API_TOKEN" \
  "http://localhost:3000/api/ds/query" \
  -d @query.json > panel_data.json
```

**Automation with grafana-cli:**

```bash
# Snapshot creation
grafana-cli dashboard snapshot create --dashboard-uid XYZ123

# Export dashboard JSON
curl -H "Authorization: Bearer $API_TOKEN" \
  http://localhost:3000/api/dashboards/uid/XYZ123 > dashboard.json
```

---

**Related topics for comprehensive CTF log visualization:** Attack chain visualization using Grafana flow diagrams, alerting configuration based on visualization thresholds, integration with SIEM platforms (Splunk, ELK stack), custom plugin development for specialized CTF metrics.

---

# Scripting & Automation

## Bash Scripting for Log Analysis

### Core Concepts

Bash scripting is fundamental for rapid log analysis during CTFs. It enables real-time filtering, pattern matching, and data extraction from large log files without manual inspection.

### Essential Commands & Techniques

#### File Reading & Processing

```bash
# Read entire file
cat logfile.txt

# Read with line numbers
cat -n logfile.txt

# Read last N lines (monitoring)
tail -n 100 logfile.txt

# Follow file in real-time
tail -f /var/log/syslog

# Read first N lines
head -n 50 logfile.txt

# Process line by line
while IFS= read -r line; do
    echo "$line"
done < logfile.txt
```

#### Grep Patterns

```bash
# Basic pattern search
grep "ERROR" application.log

# Case-insensitive search
grep -i "error" application.log

# Inverted match (exclude pattern)
grep -v "DEBUG" application.log

# Count occurrences
grep -c "Failed login" auth.log

# Show line numbers
grep -n "403" access.log

# Context lines (before/after)
grep -A 3 -B 2 "Exception" error.log  # 2 before, 3 after
grep -C 5 "CRITICAL" app.log          # 5 before and after

# Multiple patterns
grep -E "error|warning|critical" logfile.txt
grep -e "pattern1" -e "pattern2" logfile.txt

# Recursive directory search
grep -r "password" /var/log/

# Only show filenames
grep -l "attack" /var/log/*.log
```

#### Awk for Field Extraction

```bash
# Print specific columns (space-delimited)
awk '{print $1, $4}' access.log

# Custom delimiter
awk -F':' '{print $1}' /etc/passwd

# Conditional processing
awk '$9 == 404 {print $0}' access.log

# Count occurrences by field
awk '{count[$1]++} END {for (ip in count) print ip, count[ip]}' access.log

# Sum numerical values
awk '{sum += $10} END {print sum}' access.log

# Filter by field value
awk '$9 >= 400 && $9 < 500 {print $7}' access.log

# Complex pattern with multiple conditions
awk '$9 == 200 && $10 > 10000 {print $1, $7, $10}' access.log
```

#### Sed for Stream Editing

```bash
# Replace pattern (first occurrence per line)
sed 's/old/new/' logfile.txt

# Replace all occurrences
sed 's/old/new/g' logfile.txt

# Delete lines matching pattern
sed '/DEBUG/d' logfile.txt

# Print only matching lines
sed -n '/ERROR/p' logfile.txt

# Extract lines between patterns
sed -n '/START/,/END/p' logfile.txt

# In-place editing (with backup)
sed -i.bak 's/sensitive/REDACTED/g' logfile.txt
```

#### Cut for Column Extraction

```bash
# Extract specific fields (delimiter: space)
cut -d' ' -f1,4 access.log

# Extract character range
cut -c1-10 logfile.txt

# Extract comma-separated values
cut -d',' -f2,5 csvfile.csv

# Tab-delimited (default)
cut -f1,3 tabfile.txt
```

#### Sort & Uniq for Analysis

```bash
# Sort alphabetically
sort logfile.txt

# Sort numerically
sort -n numbers.txt

# Reverse sort
sort -r logfile.txt

# Sort by specific field
sort -t: -k3 -n /etc/passwd  # Numeric sort on field 3, delimiter ':'

# Unique lines only
sort logfile.txt | uniq

# Count duplicates
sort logfile.txt | uniq -c

# Show only duplicates
sort logfile.txt | uniq -d

# Show only unique (no duplicates)
sort logfile.txt | uniq -u

# Case-insensitive unique
sort -f logfile.txt | uniq -i
```

### Practical CTF Scripts

#### IP Address Extraction & Analysis

```bash
#!/bin/bash
# Extract and rank IPs by frequency

logfile="$1"

echo "[*] Top 10 IP addresses:"
grep -oE '\b([0-9]{1,3}\.){3}[0-9]{1,3}\b' "$logfile" | \
    sort | uniq -c | sort -rn | head -10

echo -e "\n[*] Unique IP count:"
grep -oE '\b([0-9]{1,3}\.){3}[0-9]{1,3}\b' "$logfile" | \
    sort -u | wc -l
```

#### Failed Authentication Analysis

```bash
#!/bin/bash
# Analyze failed login attempts

logfile="/var/log/auth.log"

echo "[*] Failed SSH login attempts by user:"
grep "Failed password" "$logfile" | \
    awk '{print $(NF-5)}' | sort | uniq -c | sort -rn

echo -e "\n[*] Failed logins by IP:"
grep "Failed password" "$logfile" | \
    grep -oE '\b([0-9]{1,3}\.){3}[0-9]{1,3}\b' | \
    sort | uniq -c | sort -rn | head -20

echo -e "\n[*] Successful logins after failures (potential breach):"
grep "Accepted password" "$logfile" | \
    awk '{print $1, $2, $3, $9, $11}'
```

#### Web Server Log Analysis

```bash
#!/bin/bash
# Analyze Apache/Nginx access logs

logfile="$1"

echo "[*] HTTP Status Code Distribution:"
awk '{print $9}' "$logfile" | sort | uniq -c | sort -rn

echo -e "\n[*] Most Requested URLs:"
awk '{print $7}' "$logfile" | sort | uniq -c | sort -rn | head -20

echo -e "\n[*] Suspicious Requests (potential attacks):"
grep -E "(\.\.\/|union|select|script|exec|eval)" "$logfile" | \
    awk '{print $1, $7}' | sort -u

echo -e "\n[*] Large Response Sizes (potential data exfiltration):"
awk '$10 > 1000000 {print $1, $7, $10}' "$logfile" | sort -k3 -rn
```

#### Time-Based Analysis

```bash
#!/bin/bash
# Activity timeline analysis

logfile="$1"

echo "[*] Activity by hour:"
awk '{print $4}' "$logfile" | \
    cut -d':' -f2 | sort | uniq -c

echo -e "\n[*] Activity by date:"
awk '{print $4}' "$logfile" | \
    cut -d':' -f1 | cut -c2- | sort | uniq -c

echo -e "\n[*] Peak activity windows:"
awk '{print $4}' "$logfile" | \
    cut -d':' -f2 | sort | uniq -c | sort -rn | head -5
```

#### User Agent Analysis

```bash
#!/bin/bash
# Extract and analyze User-Agent strings

logfile="$1"

echo "[*] User-Agent distribution:"
awk -F'"' '{print $6}' "$logfile" | sort | uniq -c | sort -rn | head -10

echo -e "\n[*] Suspicious/Bot User-Agents:"
awk -F'"' '{print $6}' "$logfile" | \
    grep -iE "(bot|crawler|scanner|nikto|sqlmap|nmap)" | \
    sort | uniq -c | sort -rn

echo -e "\n[*] Empty/Missing User-Agents:"
awk -F'"' '$6 == "" || $6 == "-" {print $1}' "$logfile" | \
    sort | uniq -c
```

#### Multi-Log Correlation

```bash
#!/bin/bash
# Correlate events across multiple logs

auth_log="/var/log/auth.log"
syslog="/var/log/syslog"
apache_log="/var/log/apache2/access.log"

suspicious_ip="$1"

echo "[*] Analyzing activity for IP: $suspicious_ip"

echo -e "\n[AUTH LOG]:"
grep "$suspicious_ip" "$auth_log" | tail -20

echo -e "\n[SYSLOG]:"
grep "$suspicious_ip" "$syslog" | tail -20

echo -e "\n[WEB ACCESS]:"
grep "$suspicious_ip" "$apache_log" | tail -20

echo -e "\n[TIMELINE]:"
{
    grep "$suspicious_ip" "$auth_log" | awk '{print $1, $2, $3, "[AUTH]", $0}'
    grep "$suspicious_ip" "$syslog" | awk '{print $1, $2, $3, "[SYS]", $0}'
    grep "$suspicious_ip" "$apache_log" | awk '{print $4, "[WEB]", $0}'
} | sort
```

### Advanced Techniques

#### Process Substitution

```bash
# Compare two log files
diff <(sort log1.txt) <(sort log2.txt)

# Find IPs in file1 but not file2
comm -23 <(sort log1.txt) <(sort log2.txt)
```

#### Parallel Processing

```bash
# Process multiple files simultaneously
for logfile in /var/log/*.log; do
    (grep "ERROR" "$logfile" > "${logfile}.errors") &
done
wait
```

#### Named Pipes (FIFOs)

```bash
# Create pipeline for real-time analysis
mkfifo /tmp/logpipe
tail -f /var/log/syslog > /tmp/logpipe &
grep -i "error" /tmp/logpipe | while read line; do
    echo "[ALERT] $line"
done
```

---

## Python Log Parsing Libraries

### Core Libraries

#### Built-in Modules

**`re` - Regular Expressions**

```python
import re

# Basic pattern matching
log_line = "192.168.1.100 - - [29/Oct/2025:10:15:32] GET /admin HTTP/1.1 403"
ip_pattern = r'\b(?:\d{1,3}\.){3}\d{1,3}\b'
ip = re.search(ip_pattern, log_line)
print(ip.group())  # 192.168.1.100

# Find all occurrences
ips = re.findall(ip_pattern, log_content)

# Compile for reuse (performance optimization)
ip_regex = re.compile(ip_pattern)
matches = ip_regex.findall(log_content)

# Named groups for structured extraction
apache_pattern = r'(?P<ip>\S+) \S+ \S+ \[(?P<timestamp>[^\]]+)\] "(?P<method>\S+) (?P<url>\S+) \S+" (?P<status>\d+) (?P<size>\S+)'
match = re.search(apache_pattern, log_line)
if match:
    print(match.group('ip'))
    print(match.group('status'))
    print(match.groupdict())  # Dictionary of all named groups
```

**`datetime` - Timestamp Parsing**

```python
from datetime import datetime

# Parse common log timestamps
timestamp_str = "29/Oct/2025:10:15:32"
dt = datetime.strptime(timestamp_str, "%d/%b/%Y:%H:%M:%S")

# Syslog format
syslog_time = "Oct 29 10:15:32"
dt = datetime.strptime(syslog_time, "%b %d %H:%M:%S")

# ISO 8601
iso_time = "2025-10-29T10:15:32Z"
dt = datetime.fromisoformat(iso_time.replace('Z', '+00:00'))

# Calculate time differences
from datetime import timedelta
time_window = timedelta(hours=1)
start_time = dt - time_window
```

**`collections` - Data Structures**

```python
from collections import Counter, defaultdict

# Count occurrences
ip_list = ['192.168.1.1', '192.168.1.2', '192.168.1.1', '192.168.1.3', '192.168.1.1']
ip_counts = Counter(ip_list)
print(ip_counts.most_common(3))  # Top 3 IPs

# Group by key
ip_events = defaultdict(list)
for event in events:
    ip_events[event['ip']].append(event)
```

**`csv` - CSV Log Files**

```python
import csv

# Read CSV logs
with open('events.csv', 'r') as f:
    reader = csv.DictReader(f)
    for row in reader:
        print(row['timestamp'], row['event_type'])

# Write CSV output
with open('analysis.csv', 'w', newline='') as f:
    writer = csv.DictWriter(f, fieldnames=['ip', 'count', 'severity'])
    writer.writeheader()
    writer.writerow({'ip': '192.168.1.1', 'count': 42, 'severity': 'high'})
```

**`json` - JSON Log Processing**

```python
import json

# Parse JSON logs (common in modern applications)
with open('app.log', 'r') as f:
    for line in f:
        try:
            log_entry = json.loads(line)
            if log_entry.get('level') == 'ERROR':
                print(log_entry['message'])
        except json.JSONDecodeError:
            continue

# Pretty print JSON
print(json.dumps(log_entry, indent=2))
```

#### Specialized Log Parsing Libraries

**`python-dateutil` - Flexible Date Parsing**

```python
from dateutil import parser

# Parse various timestamp formats automatically
timestamps = [
    "2025-10-29 10:15:32",
    "Oct 29, 2025 10:15 AM",
    "29/10/2025 10:15:32",
]

for ts in timestamps:
    dt = parser.parse(ts)
    print(dt.isoformat())
```

**`pyparsing` - Complex Log Grammar**

```python
from pyparsing import Word, alphas, nums, Suppress, Regex, Group

# Define Apache log grammar
ip = Regex(r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}')
timestamp = Suppress('[') + Regex(r'[^]]+') + Suppress(']')
method = Word(alphas)
url = Regex(r'[^\s]+')
status = Word(nums)

apache_log = ip + Suppress('-') + Suppress('-') + timestamp + \
             Suppress('"') + method + url + Suppress('"') + status

# Parse line
result = apache_log.parseString(log_line)
```

**`apache-log-parser` - Apache/Nginx Logs**

```python
import apache_log_parser

# Define log format
log_format = '%h %l %u %t "%r" %>s %b "%{Referer}i" "%{User-Agent}i"'
parser = apache_log_parser.make_parser(log_format)

# Parse log line
with open('/var/log/apache2/access.log', 'r') as f:
    for line in f:
        try:
            data = parser(line)
            if data['status'] >= 400:
                print(f"Error {data['status']}: {data['request_url']}")
        except:
            continue
```

### CTF-Focused Python Scripts

#### IP Geolocation & Analysis

```python
#!/usr/bin/env python3
import re
from collections import Counter

def extract_ips(logfile):
    """Extract all IP addresses from log file"""
    ip_pattern = r'\b(?:\d{1,3}\.){3}\d{1,3}\b'
    
    with open(logfile, 'r', encoding='utf-8', errors='ignore') as f:
        content = f.read()
    
    return re.findall(ip_pattern, content)

def analyze_ips(ips):
    """Analyze IP frequency and patterns"""
    counter = Counter(ips)
    
    print("[*] Top 10 Source IPs:")
    for ip, count in counter.most_common(10):
        print(f"  {ip:<15} : {count:>6} requests")
    
    # Identify potential scanning (single IP, many requests)
    threshold = 100
    scanners = [ip for ip, count in counter.items() if count > threshold]
    
    if scanners:
        print(f"\n[!] Potential scanners (>{threshold} requests):")
        for ip in scanners:
            print(f"  {ip} : {counter[ip]} requests")
    
    return counter

if __name__ == "__main__":
    import sys
    if len(sys.argv) < 2:
        print(f"Usage: {sys.argv[0]} <logfile>")
        sys.exit(1)
    
    ips = extract_ips(sys.argv[1])
    analyze_ips(ips)
```

#### Authentication Log Parser

```python
#!/usr/bin/env python3
import re
from datetime import datetime
from collections import defaultdict

class AuthLogParser:
    def __init__(self, logfile):
        self.logfile = logfile
        self.failed_attempts = defaultdict(list)
        self.successful_logins = defaultdict(list)
    
    def parse(self):
        """Parse authentication logs"""
        failed_pattern = r'(\w+\s+\d+\s+\d+:\d+:\d+).*Failed password for (\w+) from ([\d.]+)'
        success_pattern = r'(\w+\s+\d+\s+\d+:\d+:\d+).*Accepted password for (\w+) from ([\d.]+)'
        
        with open(self.logfile, 'r') as f:
            for line in f:
                failed_match = re.search(failed_pattern, line)
                if failed_match:
                    timestamp, user, ip = failed_match.groups()
                    self.failed_attempts[ip].append({
                        'timestamp': timestamp,
                        'user': user
                    })
                
                success_match = re.search(success_pattern, line)
                if success_match:
                    timestamp, user, ip = success_match.groups()
                    self.successful_logins[ip].append({
                        'timestamp': timestamp,
                        'user': user
                    })
    
    def identify_brute_force(self, threshold=5):
        """Identify potential brute force attacks"""
        print(f"[*] IPs with >{threshold} failed attempts:")
        
        for ip, attempts in self.failed_attempts.items():
            if len(attempts) > threshold:
                users = [a['user'] for a in attempts]
                unique_users = len(set(users))
                print(f"\n  {ip}")
                print(f"    Failed attempts: {len(attempts)}")
                print(f"    Unique users tried: {unique_users}")
                print(f"    Users: {', '.join(set(users))}")
                
                # Check if any successful login followed
                if ip in self.successful_logins:
                    print(f"    [!] BREACH: Successful login after failures!")
                    for success in self.successful_logins[ip]:
                        print(f"      {success['timestamp']} - {success['user']}")
    
    def analyze_timing(self):
        """Analyze timing patterns of failed attempts"""
        print("\n[*] Failed login timing analysis:")
        
        for ip, attempts in self.failed_attempts.items():
            if len(attempts) > 10:
                timestamps = [a['timestamp'] for a in attempts]
                print(f"\n  {ip}:")
                print(f"    First attempt: {timestamps[0]}")
                print(f"    Last attempt: {timestamps[-1]}")
                print(f"    Total attempts: {len(timestamps)}")

if __name__ == "__main__":
    import sys
    if len(sys.argv) < 2:
        print(f"Usage: {sys.argv[0]} <auth.log>")
        sys.exit(1)
    
    parser = AuthLogParser(sys.argv[1])
    parser.parse()
    parser.identify_brute_force(threshold=5)
    parser.analyze_timing()
```

#### Web Log Attack Detector

```python
#!/usr/bin/env python3
import re
from collections import Counter
from urllib.parse import unquote

class WebLogAnalyzer:
    def __init__(self, logfile):
        self.logfile = logfile
        self.attack_patterns = {
            'sql_injection': r'(union.*select|insert.*into|drop.*table|exec.*xp_|;.*--|\'.*or.*\')',
            'xss': r'(<script|javascript:|onerror=|onload=|<iframe)',
            'lfi': r'(\.\./|\.\.\\|/etc/passwd|/etc/shadow|c:\\windows)',
            'rce': r'(;.*whoami|;.*cat |;.*ls |&.*cmd|`.*`|\$\(.*\))',
            'xxe': r'(<!ENTITY|<!DOCTYPE|SYSTEM|file://)',
            'traversal': r'(\.\.%2[fF]|\.\.%5[cC])',
        }
    
    def parse_apache_log(self, line):
        """Parse Apache combined log format"""
        # [Inference] This regex pattern is commonly used for Apache combined logs
        # but may not match all variations
        pattern = r'([\d.]+) \S+ \S+ \[([^\]]+)\] "(\w+) ([^"]+) [^"]+" (\d+) (\d+) "([^"]*)" "([^"]*)"'
        match = re.search(pattern, line)
        
        if match:
            return {
                'ip': match.group(1),
                'timestamp': match.group(2),
                'method': match.group(3),
                'url': unquote(match.group(4)),
                'status': int(match.group(5)),
                'size': match.group(6),
                'referer': match.group(7),
                'user_agent': match.group(8)
            }
        return None
    
    def detect_attacks(self):
        """Scan for attack patterns"""
        attacks_found = defaultdict(list)
        
        with open(self.logfile, 'r', encoding='utf-8', errors='ignore') as f:
            for line_num, line in enumerate(f, 1):
                entry = self.parse_apache_log(line)
                if not entry:
                    continue
                
                # Check URL for attack patterns
                for attack_type, pattern in self.attack_patterns.items():
                    if re.search(pattern, entry['url'], re.IGNORECASE):
                        attacks_found[attack_type].append({
                            'line': line_num,
                            'ip': entry['ip'],
                            'url': entry['url'],
                            'status': entry['status']
                        })
        
        return attacks_found
    
    def report(self):
        """Generate attack report"""
        attacks = self.detect_attacks()
        
        if not attacks:
            print("[+] No obvious attacks detected")
            return
        
        print("[!] ATTACKS DETECTED\n")
        
        for attack_type, instances in attacks.items():
            print(f"[{attack_type.upper().replace('_', ' ')}] - {len(instances)} instances")
            
            # Show unique IPs
            ips = [a['ip'] for a in instances]
            ip_counts = Counter(ips)
            print(f"  Unique IPs: {len(ip_counts)}")
            print(f"  Top attackers:")
            for ip, count in ip_counts.most_common(5):
                print(f"    {ip} : {count} attempts")
            
            # Show sample attacks
            print(f"  Sample requests:")
            for instance in instances[:3]:
                print(f"    Line {instance['line']}: {instance['url'][:100]}")
            print()

if __name__ == "__main__":
    import sys
    if len(sys.argv) < 2:
        print(f"Usage: {sys.argv[0]} <access.log>")
        sys.exit(1)
    
    analyzer = WebLogAnalyzer(sys.argv[1])
    analyzer.report()
```

#### Timeline Constructor

```python
#!/usr/bin/env python3
from datetime import datetime
from dateutil import parser as date_parser
import sys

class TimelineBuilder:
    def __init__(self):
        self.events = []
    
    def add_event(self, timestamp, source, event_type, description, severity='info'):
        """Add event to timeline"""
        try:
            dt = date_parser.parse(timestamp)
        except:
            dt = datetime.now()
        
        self.events.append({
            'timestamp': dt,
            'source': source,
            'type': event_type,
            'description': description,
            'severity': severity
        })
    
    def parse_auth_log(self, logfile):
        """Extract events from auth.log"""
        import re
        
        with open(logfile, 'r') as f:
            for line in f:
                if 'Failed password' in line:
                    match = re.search(r'(\w+ \d+ \d+:\d+:\d+).*for (\w+) from ([\d.]+)', line)
                    if match:
                        self.add_event(
                            match.group(1),
                            'auth.log',
                            'failed_login',
                            f"Failed login: {match.group(2)} from {match.group(3)}",
                            'warning'
                        )
                
                elif 'Accepted password' in line:
                    match = re.search(r'(\w+ \d+ \d+:\d+:\d+).*for (\w+) from ([\d.]+)', line)
                    if match:
                        self.add_event(
                            match.group(1),
                            'auth.log',
                            'successful_login',
                            f"Successful login: {match.group(2)} from {match.group(3)}",
                            'info'
                        )
    
    def parse_apache_log(self, logfile):
        """Extract events from Apache access log"""
        import re
        
        with open(logfile, 'r', encoding='utf-8', errors='ignore') as f:
            for line in f:
                match = re.search(r'([\d.]+) .* \[([^\]]+)\] "(\w+) ([^"]+)" (\d+)', line)
                if match:
                    ip, timestamp, method, url, status = match.groups()
                    status = int(status)
                    
                    severity = 'info'
                    if status >= 500:
                        severity = 'critical'
                    elif status >= 400:
                        severity = 'warning'
                    
                    self.add_event(
                        timestamp,
                        'access.log',
                        f'http_{status}',
                        f"{ip} {method} {url} -> {status}",
                        severity
                    )
    
    def generate_timeline(self, output_format='text'):
        """Generate sorted timeline"""
        self.events.sort(key=lambda x: x['timestamp'])
        
        if output_format == 'text':
            severity_colors = {
                'info': '',
                'warning': '[WARN]',
                'critical': '[CRIT]'
            }
            
            for event in self.events:
                severity_tag = severity_colors.get(event['severity'], '')
                print(f"{event['timestamp'].strftime('%Y-%m-%d %H:%M:%S')} "
                      f"{severity_tag:8} [{event['source']:12}] "
                      f"{event['type']:20} {event['description']}")
        
        elif output_format == 'csv':
            import csv
            writer = csv.DictWriter(sys.stdout, 
                                   fieldnames=['timestamp', 'source', 'type', 'description', 'severity'])
            writer.writeheader()
            for event in self.events:
                writer.writerow({
                    'timestamp': event['timestamp'].isoformat(),
                    'source': event['source'],
                    'type': event['type'],
                    'description': event['description'],
                    'severity': event['severity']
                })

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print(f"Usage: {sys.argv[0]} <auth.log> [access.log] [--csv]")
        sys.exit(1)
    
    timeline = TimelineBuilder()
    
    output_format = 'csv' if '--csv' in sys.argv else 'text'
    
    for logfile in sys.argv[1:]:
        if logfile.startswith('--'):
            continue
        
        if 'auth' in logfile:
            timeline.parse_auth_log(logfile)
        elif 'access' in logfile:
            timeline.parse_apache_log(logfile)
    
    timeline.generate_timeline(output_format)
```

---

## Regular Expression Building

### Fundamentals

#### Basic Metacharacters

```regex
.       # Any single character except newline
^       # Start of line
$       # End of line
*       # 0 or more repetitions
+       # 1 or more repetitions
?       # 0 or 1 repetition (optional)
{n}     # Exactly n repetitions
{n,}    # n or more repetitions
{n,m}   # Between n and m repetitions
[]      # Character class
|       # Alternation (OR)
()      # Grouping
\       # Escape special character
```

#### Character Classes

```regex
[abc]         # Match a, b, or c
[^abc]        # Match anything except a, b, or c
[a-z]         # Match any lowercase letter
[A-Z]         # Match any uppercase letter
[0-9]         # Match any digit
[a-zA-Z0-9]   # Alphanumeric
\d            # Digit [0-9]
\D            # Non-digit
\w            # Word character [a-zA-Z0-9_]
\W            # Non-word character
\s            # Whitespace [ \t\n\r\f\v]
\S            # Non-whitespace
```

### Log-Specific Patterns

#### IP Addresses

```regex
# Basic IPv4 (permissive)
\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}

# More accurate IPv4 (validates ranges)
\b(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\b

# IPv6
(?:[0-9a-fA-F]{1,4}:){7}[0-9a-fA-F]{1,4}

# IPv4 or IPv6
(?:\d{1,3}\.){3}\d{1,3}|(?:[0-9a-fA-F]{1,4}:){7}[0-9a-fA-F]{1,4}

# Private IP addresses
\b(?:10\.\d{1,3}\.\d{1,3}\.\d{1,3}|172\.(?:1[6-9]|2\d|3[01])\.\d{1,3}\.\d{1,3}|192.168.\d{1,3}.\d{1,3})\b
````

#### Timestamps

```regex
# Apache/Nginx common log format
\[(\d{2}/\w{3}/\d{4}:\d{2}:\d{2}:\d{2} [+-]\d{4})\]

# Syslog timestamp
\w{3}\s+\d{1,2}\s+\d{2}:\d{2}:\d{2}

# ISO 8601
\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(?:\.\d+)?(?:Z|[+-]\d{2}:\d{2})

# Unix timestamp (seconds since epoch)
\b\d{10}\b

# MySQL datetime
\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}

# Windows Event Log
\d{1,2}/\d{1,2}/\d{4} \d{1,2}:\d{2}:\d{2} (?:AM|PM)

# Flexible date (MM/DD/YYYY or DD/MM/YYYY)
\b\d{1,2}[-/]\d{1,2}[-/]\d{2,4}\b

# Time only (HH:MM:SS)
\b(?:[01]\d|2[0-3]):[0-5]\d:[0-5]\d\b
````

#### URLs and Paths

```regex
# Full URL
https?://[^\s]+

# URL with protocol, domain, and path
(https?://)?([\da-z\.-]+)\.([a-z\.]{2,6})([/\w \.-]*)*/?

# File path (Linux)
(?:/[\w.-]+)+

# File path (Windows)
[a-zA-Z]:\\(?:[\w.-]+\\)*[\w.-]+

# Query string parameters
[?&]([^=&]+)=([^&]*)

# Specific file extensions
\.(php|asp|aspx|jsp|cgi)\b

# Admin/sensitive paths
/(?:admin|administrator|wp-admin|phpmyadmin|manager|console)

# Directory traversal attempts
(?:\.\.[\\/])+

# URL encoding
%[0-9a-fA-F]{2}
```

#### Email Addresses

```regex
# Basic email
\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b

# More comprehensive
\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,7}\b

# Extract domain only
@([\w.-]+)
```

#### Attack Patterns

```regex
# SQL Injection
(?i)(union.*select|insert.*into|delete.*from|drop.*(?:table|database)|exec(?:ute)?.*xp_|';.*--|/\*.*\*/|@@version)

# XSS
(?i)(<script[^>]*>.*?</script>|javascript:|onerror\s*=|onload\s*=|<iframe|eval\(|alert\(|document\.cookie)

# Command Injection
[;&|`$]\s*(?:cat|ls|whoami|id|uname|wget|curl|nc|bash|sh|cmd|powershell)

# Local File Inclusion (LFI)
(?:\.\.[\\/]){2,}|/etc/passwd|/etc/shadow|c:\\windows\\system32|proc/self/environ

# Remote File Inclusion (RFI)
(?:file|php|data|ftp|http)://

# LDAP Injection
[*()!&|=]

# XML External Entity (XXE)
<!(?:DOCTYPE|ENTITY).*?SYSTEM

# Path Traversal
\.\.(?:\/|%2[fF]|%5[cC])

# Server-Side Template Injection (SSTI)
\{\{.*?\}\}|\{%.*?%\}|\${.*?}
```

#### HTTP Elements

```regex
# HTTP Method
\b(?:GET|POST|PUT|DELETE|HEAD|OPTIONS|PATCH|TRACE|CONNECT)\b

# HTTP Status Code
\b[1-5]\d{2}\b

# HTTP Status by category
\b[45]\d{2}\b  # Client/Server errors only

# User-Agent
"Mozilla/[^"]+"

# Referer header
"[^"]*" "([^"]*)"$

# Content-Length
Content-Length:\s*(\d+)
```

#### Credentials & Sensitive Data

```regex
# Password in URL/POST
(?i)(?:password|passwd|pwd)[:=]\s*[^\s&]+

# API Keys (generic patterns)
(?i)(?:api[_-]?key|apikey|access[_-]?token)[:=]\s*['"]?([a-zA-Z0-9_-]{20,})['"]?

# AWS Access Key
(?:A3T[A-Z0-9]|AKIA|AGPA|AIDA|AROA|AIPA|ANPA|ANVA|ASIA)[A-Z0-9]{16}

# Private Key headers
-----BEGIN (?:RSA |EC |OPENSSH )?PRIVATE KEY-----

# Credit Card (basic pattern - for detection only)
\b(?:\d{4}[-\s]?){3}\d{4}\b

# Social Security Number (US)
\b\d{3}-\d{2}-\d{4}\b

# JWT Token
eyJ[A-Za-z0-9_-]+\.eyJ[A-Za-z0-9_-]+\.[A-Za-z0-9_-]+
```

#### Hashes

```regex
# MD5
\b[a-fA-F0-9]{32}\b

# SHA-1
\b[a-fA-F0-9]{40}\b

# SHA-256
\b[a-fA-F0-9]{64}\b

# NTLM Hash
\b[a-fA-F0-9]{32}:[a-fA-F0-9]{32}\b

# bcrypt
\$2[aby]\$\d{2}\$[./A-Za-z0-9]{53}
```

#### Ports and Network

```regex
# Port number
:(\d{1,5})\b

# MAC Address
\b([0-9A-Fa-f]{2}[:-]){5}([0-9A-Fa-f]{2})\b

# Subnet mask
/(\d{1,2})\b

# CIDR notation
\b(?:\d{1,3}\.){3}\d{1,3}/\d{1,2}\b
```

### Advanced Regex Techniques

#### Lookahead & Lookbehind

```regex
# Positive lookahead - match "admin" only if followed by "login"
admin(?=login)

# Negative lookahead - match "admin" only if NOT followed by "logout"
admin(?!logout)

# Positive lookbehind - match IP only if preceded by "from "
(?<=from )\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}

# Negative lookbehind - match "error" not preceded by "no "
(?<!no )error

# Extract between two patterns
(?<=\[).*?(?=\])  # Extract content between [ ]
```

#### Named Groups

```regex
# Apache log with named groups (Python)
(?P<ip>\S+) \S+ \S+ \[(?P<timestamp>[^\]]+)\] "(?P<method>\w+) (?P<url>\S+) [^"]+" (?P<status>\d+) (?P<size>\S+)

# Failed login with named groups
Failed password for (?P<user>\w+) from (?P<ip>[\d.]+) port (?P<port>\d+)

# URL components
(?P<protocol>https?)://(?P<domain>[^/]+)(?P<path>/[^\s?]*)(?:\?(?P<query>[^\s]*))?
```

#### Non-Capturing Groups

```regex
# Use (?:...) when you don't need to capture
(?:https?|ftp)://[^\s]+

# Capture only what matters
(?:GET|POST) (/\S+)  # Only captures the path, not the method
```

#### Greedy vs. Non-Greedy

```regex
# Greedy (matches as much as possible)
<.*>     # Matches from first < to last >

# Non-greedy (matches as little as possible)
<.*?>    # Matches individual tags

# Example with logs
".*"     # Greedy: captures entire line if multiple quotes
".*?"    # Non-greedy: captures only first quoted string
```

### Python Regex Examples for CTF

#### Multi-Pattern Matcher

```python
#!/usr/bin/env python3
import re
from collections import defaultdict

class PatternMatcher:
    def __init__(self):
        self.patterns = {
            'ip': r'\b(?:\d{1,3}\.){3}\d{1,3}\b',
            'email': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            'url': r'https?://[^\s]+',
            'hash_md5': r'\b[a-fA-F0-9]{32}\b',
            'hash_sha1': r'\b[a-fA-F0-9]{40}\b',
            'hash_sha256': r'\b[a-fA-F0-9]{64}\b',
            'credit_card': r'\b(?:\d{4}[-\s]?){3}\d{4}\b',
            'ssn': r'\b\d{3}-\d{2}-\d{4}\b',
            'jwt': r'eyJ[A-Za-z0-9_-]+\.eyJ[A-Za-z0-9_-]+\.[A-Za-z0-9_-]+',
            'aws_key': r'(?:A3T[A-Z0-9]|AKIA|AGPA|AIDA|AROA|AIPA|ANPA|ANVA|ASIA)[A-Z0-9]{16}',
            'private_key': r'-----BEGIN (?:RSA |EC |OPENSSH )?PRIVATE KEY-----',
        }
        
        # Compile patterns for performance
        self.compiled = {name: re.compile(pattern) 
                        for name, pattern in self.patterns.items()}
    
    def scan_file(self, filename):
        """Scan file for all patterns"""
        results = defaultdict(set)
        
        with open(filename, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        
        for name, pattern in self.compiled.items():
            matches = pattern.findall(content)
            if matches:
                results[name].update(matches)
        
        return results
    
    def report(self, results):
        """Print findings"""
        if not results:
            print("[+] No sensitive patterns found")
            return
        
        print("[!] SENSITIVE DATA DETECTED\n")
        
        for pattern_name, matches in results.items():
            print(f"[{pattern_name.upper()}] - {len(matches)} unique matches")
            for match in list(matches)[:5]:  # Show first 5
                print(f"  {match}")
            if len(matches) > 5:
                print(f"  ... and {len(matches) - 5} more")
            print()

if __name__ == "__main__":
    import sys
    if len(sys.argv) < 2:
        print(f"Usage: {sys.argv[0]} <file>")
        sys.exit(1)
    
    matcher = PatternMatcher()
    results = matcher.scan_file(sys.argv[1])
    matcher.report(results)
```

#### Advanced Apache Log Parser

```python
#!/usr/bin/env python3
import re
from datetime import datetime
from collections import Counter

class ApacheLogParser:
    def __init__(self):
        # Apache Combined Log Format pattern
        self.pattern = re.compile(
            r'(?P<ip>[\d.]+) '
            r'(?P<ident>\S+) '
            r'(?P<user>\S+) '
            r'\[(?P<timestamp>[^\]]+)\] '
            r'"(?P<request>[^"]*)" '
            r'(?P<status>\d{3}) '
            r'(?P<size>\S+) '
            r'"(?P<referer>[^"]*)" '
            r'"(?P<user_agent>[^"]*)"'
        )
        
        # Request line pattern
        self.request_pattern = re.compile(
            r'(?P<method>\w+) (?P<url>\S+)(?: HTTP/(?P<version>[\d.]+))?'
        )
        
        # Attack patterns
        self.attacks = {
            'sqli': re.compile(r'(?i)(union.*select|insert.*into|delete.*from|drop.*table|exec.*xp_|\'.*or.*\'|;.*--|/\*.*\*/)'),
            'xss': re.compile(r'(?i)(<script|javascript:|onerror\s*=|onload\s*=|<iframe|eval\(|alert\()'),
            'lfi': re.compile(r'(\.\.[\\/]|/etc/passwd|/etc/shadow|c:\\windows)'),
            'rce': re.compile(r'[;&|`]\s*(?:cat|ls|whoami|wget|curl|bash|cmd)'),
        }
    
    def parse_line(self, line):
        """Parse single log line"""
        match = self.pattern.match(line)
        if not match:
            return None
        
        data = match.groupdict()
        
        # Parse request line
        request_match = self.request_pattern.match(data['request'])
        if request_match:
            data.update(request_match.groupdict())
        
        # Convert types
        data['status'] = int(data['status'])
        data['size'] = 0 if data['size'] == '-' else int(data['size'])
        
        # Parse timestamp
        try:
            data['datetime'] = datetime.strptime(
                data['timestamp'], 
                '%d/%b/%Y:%H:%M:%S %z'
            )
        except:
            pass
        
        return data
    
    def detect_attacks(self, entry):
        """Check for attack patterns"""
        detected = []
        url = entry.get('url', '')
        
        for attack_type, pattern in self.attacks.items():
            if pattern.search(url):
                detected.append(attack_type)
        
        return detected
    
    def analyze_file(self, filename):
        """Analyze entire log file"""
        stats = {
            'total': 0,
            'status_codes': Counter(),
            'methods': Counter(),
            'ips': Counter(),
            'urls': Counter(),
            'attacks': Counter(),
            'user_agents': Counter(),
            'errors': [],
        }
        
        with open(filename, 'r', encoding='utf-8', errors='ignore') as f:
            for line_num, line in enumerate(f, 1):
                entry = self.parse_line(line)
                if not entry:
                    continue
                
                stats['total'] += 1
                stats['status_codes'][entry['status']] += 1
                stats['methods'][entry.get('method', 'UNKNOWN')] += 1
                stats['ips'][entry['ip']] += 1
                stats['urls'][entry.get('url', '')] += 1
                stats['user_agents'][entry['user_agent']] += 1
                
                # Track errors
                if entry['status'] >= 400:
                    stats['errors'].append({
                        'line': line_num,
                        'ip': entry['ip'],
                        'url': entry.get('url', ''),
                        'status': entry['status']
                    })
                
                # Detect attacks
                attacks = self.detect_attacks(entry)
                for attack in attacks:
                    stats['attacks'][attack] += 1
        
        return stats
    
    def report(self, stats):
        """Generate analysis report"""
        print(f"[*] Total Requests: {stats['total']}\n")
        
        print("[*] HTTP Status Codes:")
        for status, count in stats['status_codes'].most_common():
            print(f"  {status}: {count}")
        
        print("\n[*] HTTP Methods:")
        for method, count in stats['methods'].most_common():
            print(f"  {method}: {count}")
        
        print("\n[*] Top 10 Source IPs:")
        for ip, count in stats['ips'].most_common(10):
            print(f"  {ip:<15} : {count:>6} requests")
        
        print("\n[*] Top 10 Requested URLs:")
        for url, count in stats['urls'].most_common(10):
            print(f"  {count:>4} : {url[:80]}")
        
        if stats['attacks']:
            print("\n[!] ATTACKS DETECTED:")
            for attack_type, count in stats['attacks'].most_common():
                print(f"  {attack_type.upper()}: {count} attempts")
        
        if stats['errors']:
            print(f"\n[*] Error Requests (4xx/5xx): {len(stats['errors'])}")
            print("  Sample errors:")
            for error in stats['errors'][:5]:
                print(f"    Line {error['line']}: {error['ip']} -> "
                      f"{error['url'][:50]} [{error['status']}]")
        
        print("\n[*] Top 5 User Agents:")
        for ua, count in stats['user_agents'].most_common(5):
            print(f"  {count:>4} : {ua[:80]}")

if __name__ == "__main__":
    import sys
    if len(sys.argv) < 2:
        print(f"Usage: {sys.argv[0]} <access.log>")
        sys.exit(1)
    
    parser = ApacheLogParser()
    stats = parser.analyze_file(sys.argv[1])
    parser.report(stats)
```

#### Regex-Based Log Filter

```python
#!/usr/bin/env python3
import re
import sys

class LogFilter:
    def __init__(self):
        self.filters = []
    
    def add_filter(self, pattern, filter_type='include'):
        """Add include or exclude filter"""
        self.filters.append({
            'pattern': re.compile(pattern),
            'type': filter_type
        })
    
    def should_include(self, line):
        """Determine if line passes all filters"""
        # First check exclude filters
        for f in self.filters:
            if f['type'] == 'exclude' and f['pattern'].search(line):
                return False
        
        # If no include filters, include by default
        include_filters = [f for f in self.filters if f['type'] == 'include']
        if not include_filters:
            return True
        
        # Check include filters
        for f in include_filters:
            if f['pattern'].search(line):
                return True
        
        return False
    
    def filter_file(self, input_file, output_file=None):
        """Filter log file"""
        output = open(output_file, 'w') if output_file else sys.stdout
        
        count = 0
        with open(input_file, 'r', encoding='utf-8', errors='ignore') as f:
            for line in f:
                if self.should_include(line):
                    output.write(line)
                    count += 1
        
        if output_file:
            output.close()
            print(f"[+] Filtered {count} lines to {output_file}", file=sys.stderr)
        
        return count

# Example usage
if __name__ == "__main__":
    if len(sys.argv) < 2:
        print(f"Usage: {sys.argv[0]} <logfile> [options]")
        print("\nOptions:")
        print("  -i <pattern>  : Include lines matching pattern")
        print("  -e <pattern>  : Exclude lines matching pattern")
        print("  -o <file>     : Output to file (default: stdout)")
        print("\nExamples:")
        print(f"  {sys.argv[0]} access.log -i '\\berror\\b' -e 'DEBUG'")
        print(f"  {sys.argv[0]} auth.log -i 'Failed password' -o failed.log")
        sys.exit(1)
    
    logfile = sys.argv[1]
    output_file = None
    
    filter_obj = LogFilter()
    
    i = 2
    while i < len(sys.argv):
        if sys.argv[i] == '-i' and i + 1 < len(sys.argv):
            filter_obj.add_filter(sys.argv[i + 1], 'include')
            i += 2
        elif sys.argv[i] == '-e' and i + 1 < len(sys.argv):
            filter_obj.add_filter(sys.argv[i + 1], 'exclude')
            i += 2
        elif sys.argv[i] == '-o' and i + 1 < len(sys.argv):
            output_file = sys.argv[i + 1]
            i += 2
        else:
            i += 1
    
    filter_obj.filter_file(logfile, output_file)
```

### Regex Testing & Debugging

#### Testing Patterns in Python

```python
import re

# Test pattern interactively
pattern = r'(\d{1,3}\.){3}\d{1,3}'
test_strings = [
    "192.168.1.1",
    "256.300.1.1",  # Invalid
    "10.0.0.1",
    "not an ip"
]

compiled = re.compile(pattern)
for test in test_strings:
    match = compiled.search(test)
    print(f"{test:20} : {'MATCH' if match else 'NO MATCH'}")
    if match:
        print(f"  Captured: {match.group()}")
```

#### Performance Optimization

```python
import re
import time

# Compile patterns once for reuse
ip_pattern = re.compile(r'\b(?:\d{1,3}\.){3}\d{1,3}\b')

# Bad: Recompiling every time
def slow_search(text, n=1000):
    start = time.time()
    for _ in range(n):
        re.findall(r'\b(?:\d{1,3}\.){3}\d{1,3}\b', text)
    return time.time() - start

# Good: Using compiled pattern
def fast_search(text, n=1000):
    start = time.time()
    for _ in range(n):
        ip_pattern.findall(text)
    return time.time() - start
```

### CTF-Specific Regex Patterns

#### Challenge Flag Formats

```regex
# Generic CTF flag
(?i)(?:flag|ctf)\{[a-zA-Z0-9_]+\}

# Specific formats
HTB\{[a-zA-Z0-9_]+\}              # HackTheBox
picoCTF\{[a-zA-Z0-9_]+\}          # picoCTF
flag\{[0-9a-f]{32}\}              # MD5 hash in flag
FLAG:[A-Z0-9]{32}                 # All caps with prefix
```

#### Base64 Encoded Data

```regex
# Base64 pattern
(?:[A-Za-z0-9+/]{4})*(?:[A-Za-z0-9+/]{2}==|[A-Za-z0-9+/]{3}=)?

# In Python to decode
import re
import base64

pattern = r'(?:[A-Za-z0-9+/]{4})*(?:[A-Za-z0-9+/]{2}==|[A-Za-z0-9+/]{3}=)?'
for match in re.findall(pattern, content):
    try:
        decoded = base64.b64decode(match).decode('utf-8', errors='ignore')
        if 'flag' in decoded.lower():
            print(f"Found: {decoded}")
    except:
        pass
```

#### Hex Encoded Strings

```regex
# Hex string (even length)
(?:0x)?[0-9a-fA-F]{2}(?:[0-9a-fA-F]{2})+

# Decode in Python
import re

hex_pattern = r'(?:0x)?([0-9a-fA-F]{2}(?:[0-9a-fA-F]{2})+)'
for match in re.findall(hex_pattern, content):
    try:
        decoded = bytes.fromhex(match).decode('utf-8', errors='ignore')
        print(decoded)
    except:
        pass
```

---

## Important Subtopics & Related Areas

For comprehensive CTF log analysis mastery, consider exploring:

1. **Advanced Correlation Techniques** - Multi-source log correlation, temporal analysis, behavioral anomaly detection
2. **Log Format Parsers** - Syslog-ng, Logstash, Fluentd integration and custom parsers
3. **Performance Optimization** - Stream processing, memory-efficient parsing for large datasets
4. **Automated Analysis** - Machine learning for anomaly detection, automated threat hunting
5. **Windows Event Logs** - Event ID analysis, PowerShell logging, Security Event log parsing
6. **Network Traffic Logs** - Firewall logs, IDS/IPS logs, proxy logs, NetFlow analysis

---

# Kali Linux Specific Tools

Kali Linux includes specialized log monitoring and analysis tools that are essential for real-time threat detection, log aggregation, and forensic analysis during CTF scenarios. These tools enable automated monitoring, pattern matching, and multi-source log correlation.

## logwatch

Logwatch is a customizable log analysis system that parses system logs and produces reports summarizing log entries of interest. It's particularly useful for post-exploitation analysis and identifying anomalous activities in compromised systems.

### Installation and Setup

```bash
# Install logwatch (if not present)
sudo apt update
sudo apt install logwatch

# Verify installation
logwatch --version

# Default configuration location
/etc/logwatch/conf/logwatch.conf
/usr/share/logwatch/default.conf/logwatch.conf  # Default template

# Service-specific configurations
/etc/logwatch/conf/services/
/usr/share/logwatch/scripts/services/  # Analysis scripts
```

### Basic Usage

```bash
# Generate report for today's logs
sudo logwatch

# Generate report and display to stdout
sudo logwatch --output stdout

# Analyze specific date range
sudo logwatch --range 'between -7 days and -1 days'

# Common time ranges
sudo logwatch --range yesterday
sudo logwatch --range today
sudo logwatch --range 'between 2024-10-01 and 2024-10-29'

# Specify detail level (Low = 0, Med = 5, High = 10)
sudo logwatch --detail Low
sudo logwatch --detail Med
sudo logwatch --detail High
sudo logwatch --detail 10  # Maximum detail

# Analyze specific service
sudo logwatch --service sshd
sudo logwatch --service http
sudo logwatch --service pam
sudo logwatch --service sudo

# Multiple services
sudo logwatch --service sshd --service sudo --service pam

# Output to file
sudo logwatch --output file --filename /tmp/logwatch-report.txt

# Email report (requires MTA configuration)
sudo logwatch --output mail --mailto admin@example.com
```

### Advanced Configuration

**Custom configuration file** (`/etc/logwatch/conf/logwatch.conf`):

```bash
# Create custom config
sudo nano /etc/logwatch/conf/logwatch.conf

# Example configuration
LogDir = /var/log
TmpDir = /var/cache/logwatch
Output = stdout
Format = text
Encode = none
MailTo = root
MailFrom = Logwatch
Range = yesterday
Detail = Med
Service = All
```

**Service-specific detail levels**:

```bash
# Edit service configuration
sudo nano /etc/logwatch/conf/services/sshd.conf

# Set detail level for SSH only
Detail = 10
```

**Analyzing non-standard log locations** (CTF scenarios):

```bash
# Specify custom log directory
sudo logwatch --logdir /path/to/captured/logs --output stdout

# Analyze archived logs
sudo logwatch --logdir /mnt/forensics/var/log --range all
```

### CTF-Specific Analysis Patterns

**Identifying privilege escalation attempts**:

```bash
# Focus on sudo and PAM logs
sudo logwatch --service sudo --service pam --detail High --range all

# Look for patterns in output:
# - Failed sudo attempts
# - Authentication failures followed by success
# - Unusual sudo commands
```

**SSH brute force detection**:

```bash
# Detailed SSH analysis
sudo logwatch --service sshd --detail 10 --range 'between -7 days and now'

# Key indicators in output:
# - Failed login attempts from specific IPs
# - Successful login after multiple failures
# - Login from unusual geographic locations
```

**Web server compromise indicators**:

```bash
# Apache/Nginx analysis
sudo logwatch --service http --detail High

# Look for:
# - 404 errors (reconnaissance)
# - POST requests to unusual endpoints
# - Error patterns indicating exploitation attempts
```

**System-wide anomaly detection**:

```bash
# Comprehensive report
sudo logwatch --detail High --range today --output stdout > full-report.txt

# Search for specific patterns
grep -i "failed\|error\|denied\|unauthorized" full-report.txt
grep -i "root" full-report.txt
```

### Creating Custom Service Analyzers

Logwatch uses Perl scripts to parse logs. Create custom analyzers for CTF-specific log formats:

```bash
# Custom service script location
sudo mkdir -p /etc/logwatch/scripts/services/
sudo nano /etc/logwatch/scripts/services/custom-app

# Example custom analyzer (Perl)
#!/usr/bin/perl
use strict;

my %errors;
my %warnings;

while (defined(my $line = <STDIN>)) {
    if ($line =~ /ERROR: (.+)/) {
        $errors{$1}++;
    } elsif ($line =~ /WARNING: (.+)/) {
        $warnings{$1}++;
    }
}

if (keys %errors) {
    print "\n*** Errors ***\n";
    foreach my $error (sort keys %errors) {
        print "   $error: $errors{$error} Time(s)\n";
    }
}

if (keys %warnings) {
    print "\n*** Warnings ***\n";
    foreach my $warning (sort keys %warnings) {
        print "   $warning: $warnings{$warning} Time(s)\n";
    }
}

exit(0);
```

```bash
# Make executable
sudo chmod +x /etc/logwatch/scripts/services/custom-app

# Create service configuration
sudo nano /etc/logwatch/conf/services/custom-app.conf

# Configuration content
Title = "Custom Application"
LogFile = /var/log/custom-app/*.log

# Run custom analyzer
sudo logwatch --service custom-app
```

### Logwatch Output Interpretation for CTF

**SSH section indicators**:

- **"Illegal users from"** - Brute force attempts with non-existent usernames
- **"Failed logins from"** - Failed authentication attempts
- **"Connections from"** - Successful connections (correlate with authorized access)
- **"Received disconnect"** - Abnormal disconnections

**Sudo section indicators**:

- **"Unmatched Entries"** - Commands not recognized by logwatch filters (investigate manually)
- **User X: Y Time(s)** - Sudo usage frequency (look for anomalies)
- **"session opened/closed"** - Session tracking

**PAM section indicators**:

- **"Authentication Failures"** - Failed login attempts
- **"Unknown Entries"** - Unusual authentication events
- **"Session Opened/Closed"** - User session tracking

### Limitations and Workarounds

[Inference] Logwatch limitations in CTF contexts:

1. **Delayed analysis** - Designed for periodic reports, not real-time monitoring
2. **Pre-configured services** - May not recognize custom application logs without configuration
3. **Summary-based** - Aggregates data, potentially losing granular details
4. **Perl dependency** - Custom analyzers require Perl knowledge

**Workarounds**:

```bash
# Force immediate analysis
sudo logwatch --range today --output stdout

# Preserve raw data with high detail
sudo logwatch --detail 10 --service all --save /tmp/detailed-report.txt

# Combine with grep for specific patterns
sudo logwatch --service sshd --detail 10 | grep "192.168.1.100"
```

## swatch (Simple Watcher)

Swatch (Simple Log Watcher) is a real-time log monitoring tool that watches log files and executes actions when specific patterns are detected. Critical for active monitoring during CTF attack/defense scenarios.

### Installation and Setup

```bash
# Install swatch
sudo apt update
sudo apt install swatch

# Alternative: Install from CPAN (more recent version)
sudo cpan install File::Tail
sudo cpan install Date::Calc
sudo cpan install Date::Parse

# Verify installation
swatch --version
```

### Configuration File Structure

Swatch uses configuration files to define patterns and actions. Create at `/etc/swatch/swatchrc` or `~/.swatchrc`:

```bash
# Basic configuration structure
watchfor /pattern/
    action

# Example configuration file
nano ~/.swatchrc
```

**Example swatchrc configurations**:

```bash
# Monitor SSH authentication failures
watchfor /Failed password/
    echo red
    mail addresses=admin@localhost,subject="SSH Login Failure"
    exec "/usr/local/bin/alert-script.sh"

# Detect successful root login
watchfor /Accepted password for root/
    echo bold red
    bell 3
    mail addresses=security@localhost,subject="ROOT LOGIN DETECTED"

# Monitor sudo commands
watchfor /sudo:.*COMMAND/
    echo blue
    write root

# Detect port scanning (via iptables logs)
watchfor /SRC=(\S+).*DPT=(\d+)/
    echo inverse
    pipe "/usr/local/bin/parse-portscan.sh"

# File modification detection (if logged)
watchfor /FILE MODIFIED: (.+)/
    echo yellow
    exec "echo $1 >> /tmp/modified-files.log"

# Multiple patterns for same action
watchfor /(Failed password|Invalid user|Connection closed by authenticating user)/
    echo red
    throttle 5:60  # Max 5 alerts per 60 seconds

# Case-insensitive matching
watchfor /error/i
    echo magenta

# Complex regex patterns
watchfor /(\d+\.\d+\.\d+\.\d+).*kernel:.*SYN.*DPT=(\d+)/
    echo "Port scan from $1 to port $2"
    exec "/usr/local/bin/block-ip.sh $1"
```

### Usage and Execution

**Basic execution**:

```bash
# Monitor single log file
swatch --config-file=~/.swatchrc --tail-file=/var/log/auth.log

# Monitor with explicit config
swatch -c /etc/swatch/ssh-monitor.conf -t /var/log/auth.log

# Monitor multiple files (run separate instances)
swatch -c ~/.swatchrc -t /var/log/auth.log &
swatch -c ~/.swatchrc -t /var/log/apache2/access.log &

# Examine existing log before tailing
swatch -c ~/.swatchrc --examine=/var/log/auth.log

# Tail and examine
swatch -c ~/.swatchrc --examine=/var/log/auth.log --tail-file=/var/log/auth.log

# Run as daemon
swatch -c ~/.swatchrc -t /var/log/auth.log --daemon

# Kill daemon
pkill -f "swatch.*auth.log"
```

**Configuration testing**:

```bash
# Test configuration syntax
swatch --config-file=~/.swatchrc --dump-script

# Test against sample log
echo "Failed password for invalid user test from 192.168.1.100" | swatch -c ~/.swatchrc --read-pipe=STDIN
```

### Action Types and Options

**Display actions**:

```bash
# Echo with colors
echo [normal|black|red|green|yellow|blue|magenta|cyan|white]
echo [bold|blink|inverse]

# Examples
watchfor /CRITICAL/
    echo bold red

watchfor /WARNING/
    echo yellow

watchfor /INFO/
    echo green
```

**Notification actions**:

```bash
# System bell
bell [number]  # Beep N times

watchfor /ALERT/
    bell 5

# Write to user terminal
write [user|user:tty]

watchfor /Security Alert/
    write root
    write admin:pts/0

# Send email
mail [addresses=address[,address...]][,subject=your_text]

watchfor /Attack Detected/
    mail addresses=security@localhost,subject="Security Incident"
```

**Execution actions**:

```bash
# Execute command
exec command

watchfor /Unauthorized access from (\S+)/
    exec "/usr/local/bin/firewall-block.sh $1"

# Pipe to command (sends matched line)
pipe command

watchfor /(\d+\.\d+\.\d+\.\d+)/
    pipe "/usr/local/bin/geoip-lookup.sh"
```

**Throttling** (prevent alert flooding):

```bash
# Throttle syntax: hours:minutes:seconds
throttle hours:minutes:seconds

watchfor /Failed password/
    echo red
    throttle 0:1:0  # Max once per minute

watchfor /Port scan detected/
    mail addresses=admin@localhost
    throttle 0:5:0  # Max once per 5 minutes
```

**Time-based actions**:

```bash
# When - execute only during specific times
when hours_range

watchfor /Backup started/
    when 0-6,18-23  # Only alert outside business hours
    echo yellow

# Continue - explicitly continue processing
continue

watchfor /Error/
    echo red
    continue  # Keep processing other patterns
```

### CTF-Specific Monitoring Scenarios

**Scenario 1: Real-time intrusion detection**:

```bash
# Create monitoring config
cat > /tmp/intrusion-monitor.conf << 'EOF'
# SSH brute force
watchfor /Failed password for .* from (\S+)/
    echo red
    exec "echo $(date) - Brute force from $1 >> /tmp/attacks.log"
    throttle 0:0:30

# Successful login after failures (potential compromise)
watchfor /Accepted password for (\S+) from (\S+)/
    echo bold red
    bell 3
    exec "echo $(date) - SUCCESSFUL LOGIN: User $1 from $2 >> /tmp/successful-logins.log"

# Privilege escalation
watchfor /sudo:.*COMMAND=\/(bin|sbin)\/(su|bash|sh)/
    echo inverse
    exec "echo $(date) - PRIVILEGE ESCALATION ATTEMPT >> /tmp/privesc.log"

# New user creation
watchfor /useradd|adduser/
    echo bold yellow
    exec "echo $(date) - NEW USER CREATED >> /tmp/user-changes.log"
EOF

# Start monitoring
sudo swatch -c /tmp/intrusion-monitor.conf -t /var/log/auth.log --daemon
```

**Scenario 2: Web application attack monitoring**:

```bash
# Apache/Nginx attack detection
cat > /tmp/web-monitor.conf << 'EOF'
# SQL injection attempts
watchfor /(union.*select|exec\(|script>|\.\.\/)/i
    echo red
    pipe "awk '{print $1}' >> /tmp/attacker-ips.log"

# Directory traversal
watchfor /\.\.\/|\.\.%2F/i
    echo yellow
    exec "echo $(date) - Directory traversal attempt >> /tmp/web-attacks.log"

# Shellshock attempts
watchfor /\(\) \{/
    echo bold red
    bell 5

# Command injection patterns
watchfor /;.*\||&&|\$\(.*\)|`.*`/
    echo inverse
    exec "echo $(date) - Command injection attempt >> /tmp/web-attacks.log"
EOF

# Monitor Apache access log
sudo swatch -c /tmp/web-monitor.conf -t /var/log/apache2/access.log --daemon
```

**Scenario 3: File integrity monitoring**:

```bash
# Monitor audit logs for file changes (requires auditd)
cat > /tmp/file-monitor.conf << 'EOF'
# Sensitive file modifications
watchfor /type=PATH.*name="\/etc\/(passwd|shadow|sudoers|ssh\/)/
    echo bold red
    mail addresses=admin@localhost,subject="Critical File Modified"
    exec "echo $(date) - Critical file modification detected >> /tmp/file-changes.log"

# Binary modifications
watchfor /type=PATH.*name="\/bin\/|\/sbin\/|\/usr\/bin\//
    echo red
    exec "echo $(date) - Binary modification detected >> /tmp/binary-changes.log"
EOF

sudo swatch -c /tmp/file-monitor.conf -t /var/log/audit/audit.log --daemon
```

**Scenario 4: Network activity monitoring**:

```bash
# Monitor iptables/netfilter logs
cat > /tmp/network-monitor.conf << 'EOF'
# Port scan detection (SYN packets to multiple ports)
watchfor /SRC=(\S+).*DPT=(\d+).*SYN/
    echo "Port scan from $1 to port $2"
    exec "echo $1 >> /tmp/scanners.log"
    throttle 0:0:10

# Blocked connections
watchfor /\[UFW BLOCK\].*SRC=(\S+)/
    pipe "sort | uniq -c | sort -rn >> /tmp/blocked-ips.log"
EOF

sudo swatch -c /tmp/network-monitor.conf -t /var/log/syslog --daemon
```

### Automated Response Actions

**IP blocking script** (`/usr/local/bin/block-ip.sh`):

```bash
#!/bin/bash
IP=$1
if [ -z "$IP" ]; then
    exit 1
fi

# Add to iptables
iptables -A INPUT -s $IP -j DROP

# Log action
echo "$(date) - Blocked IP: $IP" >> /var/log/auto-blocked.log

# Optional: Add to persistent blocklist
echo $IP >> /etc/blocked-ips.txt
```

**Alert script** (`/usr/local/bin/alert-script.sh`):

```bash
#!/bin/bash
# Send alert to monitoring system
MESSAGE="Security alert: $(date)"
curl -X POST http://monitoring-server/api/alert \
    -H "Content-Type: application/json" \
    -d "{\"message\": \"$MESSAGE\"}"
```

### Managing Swatch Processes

```bash
# List running swatch processes
ps aux | grep swatch

# Start multiple watchers
for logfile in /var/log/auth.log /var/log/apache2/access.log /var/log/syslog; do
    swatch -c ~/.swatchrc -t "$logfile" --daemon --pid-file="/tmp/swatch-$(basename $logfile).pid"
done

# Stop specific watcher
kill $(cat /tmp/swatch-auth.log.pid)

# Stop all swatch processes
pkill -f swatch

# Restart watcher (e.g., after config change)
kill $(cat /tmp/swatch-auth.log.pid)
swatch -c ~/.swatchrc -t /var/log/auth.log --daemon --pid-file=/tmp/swatch-auth.log.pid
```

### Limitations and Considerations

[Inference] Swatch limitations in CTF environments:

1. **Performance impact** - Complex regex on high-volume logs can cause lag
2. **No log rotation handling** - May lose events during rotation (use with logrotate's `copytruncate`)
3. **Single-line matching only** - Cannot correlate multi-line events natively
4. **No built-in persistence** - Doesn't track state across restarts

**Workarounds**:

```bash
# Handle log rotation
watchfor /./  # Matches any line
    exec "test -f /var/log/auth.log || kill -HUP $$"  # Restart on rotation

# Simple correlation (track patterns in temp file)
watchfor /Failed password for root from (\S+)/
    exec "echo $1 >> /tmp/failed-root-ips.log; [ $(grep -c $1 /tmp/failed-root-ips.log) -gt 5 ] && /usr/local/bin/block-ip.sh $1"
```

## MultiTail

MultiTail is an advanced log monitoring tool that displays multiple log files simultaneously in split-screen mode with color highlighting, filtering, and merging capabilities. Essential for correlating events across multiple sources in real-time.

### Installation and Setup

```bash
# Install multitail
sudo apt update
sudo apt install multitail

# Verify installation
multitail -V

# Configuration file location
/etc/multitail.conf  # System-wide configuration
~/.multitailrc       # User-specific configuration
```

### Basic Usage

```bash
# Monitor single file
multitail /var/log/auth.log

# Monitor multiple files in split view
multitail /var/log/auth.log /var/log/syslog

# Monitor with labels
multitail -l "SSH Logs" /var/log/auth.log -l "System Logs" /var/log/syslog

# Horizontal split (default is vertical for 2 files)
multitail -s 2 /var/log/auth.log /var/log/apache2/access.log

# Vertical split explicitly
multitail -sV 2 /var/log/auth.log /var/log/syslog

# Monitor command output
multitail -l "Network Connections" -c "netstat -tuln" /var/log/syslog

# Monitor with auto-refresh of command
multitail -l "Active SSH" -c "ss -tnp | grep :22" -R 2 /var/log/auth.log
# -R 2 = refresh every 2 seconds

# Follow file by name (handles log rotation)
multitail -F /var/log/auth.log

# Monitor growing file with specific encoding
multitail -e utf-8 /var/log/auth.log
```

### Advanced Monitoring Configurations

**Multi-source correlation**:

```bash
# Monitor SSH, web, and system logs simultaneously
multitail -s 3 \
    -l "SSH Authentication" /var/log/auth.log \
    -l "Apache Access" /var/log/apache2/access.log \
    -l "System Messages" /var/log/syslog

# Four-way split
multitail -s 4 \
    /var/log/auth.log \
    /var/log/apache2/access.log \
    /var/log/apache2/error.log \
    /var/log/syslog

# Custom layout (2 columns, 2 rows)
multitail -s 2 -sn 2 \
    /var/log/auth.log /var/log/syslog \
    /var/log/apache2/access.log /var/log/apache2/error.log
```

**Merging logs with timestamps**:

```bash
# Merge multiple logs into single view (chronological)
multitail -M 0 /var/log/auth.log /var/log/apache2/access.log

# Merge with source identification
multitail -M 0 --mark-interval 2 /var/log/auth.log /var/log/syslog

# Merge and add timestamp prefix
multitail -M 0 -tf /var/log/auth.log /var/log/syslog
```

**Color schemes and filtering**:

```bash
# Use specific color scheme
multitail -cS apache /var/log/apache2/access.log

# Available schemes (check with -l)
multitail -cS syslog /var/log/syslog
multitail -cS squid /var/log/squid/access.log
multitail -cS postfix /var/log/mail.log

# Filter lines containing pattern (case-insensitive)
multitail -e "failed" /var/log/auth.log

# Inverse filter (exclude lines)
multitail -ev "systemd" /var/log/syslog

# Multiple filters
multitail -e "failed" -e "error" /var/log/auth.log

# Regular expression filter
multitail -ex "192\.168\.1\.\d+" /var/log/auth.log
```

**Buffering and history**:

```bash
# Set buffer size (lines)
multitail -m 10000 /var/log/auth.log
# Default is 1000 lines

# Load entire file into buffer first
multitail -i /var/log/auth.log

# Start from end of file (ignore existing content)
multitail -f /var/log/auth.log
```

### Interactive Commands

While MultiTail is running, use these keyboard shortcuts:

```
Basic Navigation:
  q           - Quit
  h           - Help screen
  b           - Select window to view (when multiple windows)
  SPACE       - Pause/resume scrolling
  
Filtering:
  /           - Search forward
  ?           - Search backward
  n           - Next search result
  N           - Previous search result
  f           - Add filter
  F           - Remove filter
  
Display Control:
  0-9         - Switch to window N
  t           - Add timestamp to lines
  T           - Toggle timestamp format
  w           - Wrap/unwrap long lines
  z           - Clear window
  
Advanced:
  m           - Show menu
  a           - Add window (follow new file)
  d           - Delete current window
  s           - Show statistics
  S           - Subwindow configuration
  r           - Restart window (reload file)
  R           - Restart all windows
  
Marking:
  +           - Mark current line
  -           - Show marked lines
  *           - Clear marks
```

### CTF-Specific Monitoring Scenarios

**Scenario 1: Attack surface monitoring**:

```bash
# Monitor all critical service logs simultaneously
multitail -s 4 \
    -cS syslog -l "AUTH" /var/log/auth.log \
    -cS apache -l "WEB" /var/log/apache2/access.log \
    -cS apache -l "WEB ERRORS" /var/log/apache2/error.log \
    -l "FIREWALL" /var/log/ufw.log
```

**Scenario 2: Incident timeline reconstruction**:

```bash
# Merge all relevant logs chronologically
multitail -M 0 --mergeall \
    --mark-interval 10 \
    /var/log/auth.log \
    /var/log/apache2/access.log \
    /var/log/syslog \
    /var/log/kern.log

# Save merged output to file for analysis
multitail -M 0 /var/log/auth.log /var/log/syslog | tee /tmp/merged-timeline.log
```

**Scenario 3: Real-time attack detection dashboard**:

```bash
# Create live monitoring dashboard
multitail -s 3 \
    -l "Failed SSH Logins" -e "Failed password" /var/log/auth.log \
    -l "Web Attacks" -ex "(union|select|script|\.\.\/)" /var/log/apache2/access.log \
    -l "Sudo Commands" -e "sudo:.*COMMAND" /var/log/auth.log
```

**Scenario 4: Process and log correlation**:

```bash
# Monitor logs alongside process list
multitail -s 2 \
    -l "Auth Logs" /var/log/auth.log \
    -l "Active Processes" -c "ps auxf" -R 5

# Network connections with logs
multitail -s 2 \
    -l "Network Connections" -c "netstat -tnp" -R 2 \
    -l "System Logs" /var/log/syslog
```

**Scenario 5: Multi-host monitoring** (via SSH):

```bash
# Monitor logs from multiple remote hosts
multitail -s 2 \
    -l "Local SSH" /var/log/auth.log \
    -l "Remote Server 1" -c "ssh user@server1 'tail -f /var/log/auth.log'" \
    -l "Remote Server 2" -c "ssh user@server2 'tail -f /var/log/auth.log'"

# Merged view of multiple remote sources
multitail -M 0 \
    /var/log/auth.log \
    -c "ssh user@server1 'tail -f /var/log/auth.log'" \
    -c "ssh user@server2 'tail -f /var/log/auth.log'"
```

### Custom Color Schemes

Create custom color schemes for specific log formats in `/etc/multitail.conf` or `~/.multitailrc`:

```bash
# Example: Custom CTF attack detection scheme
nano ~/.multitailrc

# Add custom color scheme
colorscheme:ctf-attacks
cs_re:red:failed|error|denied|unauthorized
cs_re:yellow:warning|suspicious
cs_re:green:success|accepted|authenticated
cs_re:magenta:\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b
cs_re:cyan:root|admin|sudo
cs_re_s:blue,black,bold:ATTACK|INTRUSION|COMPROMISE

# Apply custom scheme
multitail -cS ctf-attacks /var/log/auth.log
```

**Pre-configured scheme examples**:

```bash
# SSH monitoring with colors
colorscheme:ssh-monitor
cs_re:red:^.*Failed password.*$
cs_re:green:^.*Accepted password.*$
cs_re:yellow:^.*Invalid user.*$
cs_re:cyan:^.*session opened.*$
cs_re:magenta:^.*session closed.*$

# Web server attack detection
colorscheme:web-attacks
cs_re:red,black,bold:union.*select|script>|\.\.\/|;.*\||exec\(
cs_re:yellow:' OR '|' AND '|--|\#|\/\*
cs_re:green:^[0-9\.]+ - - \[.*\] "GET
cs_re:magenta:^[0-9\.]+ - - \[.*\] "POST
```

### Filtering Strategies for CTF

**Whitelist approach** (show only relevant lines):

```bash
# Show only authentication-related lines
multitail -e "authentication|password|sudo|su|login" /var/log/auth.log

# Show only HTTP errors
multitail -ex '"[45]\d{2} ' /var/log/apache2/access.log
```

**Blacklist approach** (hide noise):

```bash
# Hide routine systemd messages
multitail -ev "systemd|Starting|Started|Stopped" /var/log/syslog

# Hide successful web requests (focus on errors)
multitail -ev '" 200 ' /var/log/apache2/access.log
```

**IP-based filtering**:

```bash
# Monitor specific IP address activity
multitail -e "192\.168\.1\.100" -M 0 /var/log/auth.log /var/log/apache2/access.log

# Exclude internal network (show only external)
multitail -ev "192\.168\.|10\.|172\.16\." /var/log/apache2/access.log
```

### Performance Optimization

```bash
# Limit buffer size for performance
multitail -m 500 /var/log/auth.log

# Update interval (reduce CPU usage)
multitail -u 1000 /var/log/auth.log
# Updates every 1000ms (1 second)

# Disable specific features for speed
multitail -D /var/log/auth.log  # Disable diff mode
multitail -n /var/log/auth.log  # Disable line numbers
```

### Saving and Scripting

**Save filtered output**:

```bash
# Save to file while viewing
multitail /var/log/auth.log | tee /tmp/auth-filtered.log

# Save specific timeframe
multitail -i /var/log/auth.log -e "2024-10-29" | tee /tmp/today-events.log

# Automated filtering and saving
multitail -e "Failed password" /var/log/auth.log | \
    awk '{print $1, $2, $3, $(NF-3), $(NF-2), $NF}' > /tmp/failed-logins.txt
```

**Scripted monitoring**:

```bash
#!/bin/bash
# CTF monitoring script

LOG_DIR="/var/log"
SESSION_NAME="ctf-monitor"

# Start multitail with predefined configuration
multitail -s 4 \
    -cS syslog -l "AUTH" "$LOG_DIR/auth.log" \
    -cS apache -l "WEB" "$LOG_DIR/apache2/access.log" \
    -e "Failed\|Error" -l "ERRORS" "$LOG_DIR/syslog" \
    -c "ss -tnp | grep ESTABLISHED" -R 5 -l "CONNECTIONS"
```

### Integration with Other Tools

**Combine with grep for pre-filtering**:

```bash
# Monitor pre-filtered log
tail -f /var/log/auth.log | grep -E "Failed|Accepted" | multitail -

# Multiple pre-filtered sources
multitail -s 2 \
    -c "tail -f /var/log/auth.log | grep Failed" \
    -c "tail -f /var/log/apache2/access.log | grep -E '404|500'"
````

**Pipe to analysis tools**:
```bash
# Extract IPs from multitail output
multitail /var/log/auth.log | grep "Failed password" | awk '{print $(NF-3)}' | sort | uniq -c

# Real-time counting of events
multitail -M 0 /var/log/auth.log /var/log/syslog | \
    grep -o "Failed password" | \
    uniq -c
````

**Integration with journalctl**:

```bash
# Monitor systemd journal alongside traditional logs
multitail -s 2 \
    -l "Traditional Logs" /var/log/auth.log \
    -l "Systemd Journal" -c "journalctl -f -u ssh.service"

# Filter journal output
multitail -s 2 \
    -l "SSH Service" -c "journalctl -f -u ssh --priority=warning" \
    -l "Auth Logs" /var/log/auth.log
```

### Configuration File Options

**System-wide configuration** (`/etc/multitail.conf`):

```bash
# Edit system config
sudo nano /etc/multitail.conf

# Common configuration options:

# Default number of lines to buffer
maxlines:5000

# Default scrollback buffer
scrollback:10000

# Abbreviate filenames in status line
abbreviate_fields:yes

# Hide heartbeat messages
hide_heartbeat:yes

# Conversion filters for binary logs
conversion-filter:auditd:/usr/bin/ausearch -i --input-logs
conversion-filter:syslog:/usr/bin/journalctl -o short --file

# Default color scheme
defaultcscheme:colorscheme:syslog
```

**User-specific configuration** (`~/.multitailrc`):

```bash
# Personal preferences
nano ~/.multitailrc

# Window-specific settings
check_mail:0
#Disable mail checking

# Set default buffer
maxlines:10000

# Color schemes (as shown earlier)
colorscheme:myscheme
cs_re:red:CRITICAL|ALERT
cs_re:yellow:WARNING
cs_re:green:SUCCESS|OK
cs_re:blue:INFO|DEBUG

# Command aliases
# Cannot create direct command aliases, but can document common commands in comments
# Frequently used: multitail -cS myscheme -s 2 /var/log/auth.log /var/log/syslog
```

### Troubleshooting and Common Issues

**Issue 1: Terminal size problems**

```bash
# Force terminal dimensions
multitail -tn 50 -tw 200 /var/log/auth.log
# -tn = terminal height (lines)
# -tw = terminal width (columns)

# Better: resize terminal before starting
resize
multitail /var/log/auth.log
```

**Issue 2: Performance with large files**

```bash
# Skip existing content
multitail -f /var/log/huge-file.log

# Reduce buffer size
multitail -m 100 /var/log/huge-file.log

# Sample every Nth line
multitail -n 10 /var/log/huge-file.log
# Shows every 10th line
```

**Issue 3: UTF-8/encoding issues**

```bash
# Specify encoding explicitly
multitail -e utf-8 /var/log/auth.log

# Convert encoding on the fly
multitail -c "iconv -f ISO-8859-1 -t UTF-8 /var/log/legacy.log | tail -f"
```

**Issue 4: Handling binary logs**

```bash
# Monitor binary log with conversion
multitail -c "strings /var/log/binary.log | tail -f"

# Use conversion filter
multitail --conversion-filter=binary:strings /var/log/binary.log
```

**Issue 5: Log rotation handling**

```bash
# Follow by name (automatically handles rotation)
multitail -F /var/log/auth.log

# Monitor both rotated and current
multitail -s 2 /var/log/auth.log /var/log/auth.log.1
```

### Advanced CTF Scenarios

**Scenario 1: Competition environment - Full visibility dashboard**

```bash
#!/bin/bash
# comprehensive-monitor.sh - Full CTF defense monitoring

multitail \
    -s 6 \
    -cS syslog -l "AUTHENTICATION" -e "authentication|password|login|ssh" /var/log/auth.log \
    -cS apache -l "WEB ACCESS" -ex "200 OK" /var/log/apache2/access.log \
    -cS apache -l "WEB ERRORS" /var/log/apache2/error.log \
    -l "FIREWALL BLOCKS" -e "UFW BLOCK|DROP" /var/log/kern.log \
    -l "CRITICAL ALERTS" -c "tail -f /var/log/syslog | grep -i 'critical\|alert\|emergency'" \
    -l "ACTIVE CONNECTIONS" -c "watch -n 2 'ss -tnp | grep ESTABLISHED'" -R 2
```

**Scenario 2: Forensic analysis - Post-incident timeline**

```bash
# Reconstruct attack timeline from archived logs
multitail -M 0 --mergeall \
    -i /forensics/auth.log \
    -i /forensics/apache2/access.log \
    -i /forensics/syslog \
    -i /forensics/kern.log \
    -tf | tee /tmp/incident-timeline.log

# Filter for specific attacker IP
multitail -M 0 \
    -i /forensics/auth.log \
    -i /forensics/apache2/access.log \
    -e "192.168.100.50" | tee /tmp/attacker-activity.log
```

**Scenario 3: Distributed system monitoring**

```bash
# Monitor multiple team services
multitail -s 4 \
    -l "DB Server" -c "ssh db-server 'tail -f /var/log/mysql/error.log'" \
    -l "Web Server 1" -c "ssh web1 'tail -f /var/log/apache2/access.log'" \
    -l "Web Server 2" -c "ssh web2 'tail -f /var/log/apache2/access.log'" \
    -l "App Server" -c "ssh app 'tail -f /var/log/application.log'"
```

**Scenario 4: Attack pattern detection**

```bash
# Create focused attack detection view
multitail -s 3 \
    -l "SQL Injection" -ex "union.*select|'.*or.*'='" /var/log/apache2/access.log \
    -l "XSS Attempts" -ex "script>|javascript:|onerror=" /var/log/apache2/access.log \
    -l "Path Traversal" -ex "\.\./|%2e%2e" /var/log/apache2/access.log

# Simultaneously monitor defensive actions
multitail -s 4 \
    -l "Attack Attempts" -ex "union|script|\.\./" /var/log/apache2/access.log \
    -l "Failed Auth" -e "Failed password" /var/log/auth.log \
    -l "Blocked IPs" -e "UFW BLOCK" /var/log/kern.log \
    -l "Firewall Rules" -c "iptables -L -n -v | head -20" -R 5
```

**Scenario 5: Custom application monitoring**

```bash
# Monitor custom CTF application with multiple components
multitail -s 4 \
    -l "App Stdout" -c "docker logs -f ctf-app 2>&1 | grep -v DEBUG" \
    -l "App Errors" -c "docker logs -f ctf-app 2>&1 | grep ERROR" \
    -l "Database" -c "docker logs -f ctf-db" \
    -l "System" /var/log/syslog
```

### Comparison and Combined Usage Strategies

**When to use each tool**:

|Tool|Best For|Real-time|Complexity|Resource Usage|
|---|---|---|---|---|
|logwatch|Scheduled reports, summaries|No|Low|Low|
|swatch|Pattern-based alerting, automation|Yes|Medium|Medium|
|multitail|Visual correlation, live monitoring|Yes|High|Medium-High|

**Combined usage workflow**:

```bash
# 1. Use MultiTail for initial triage and monitoring
multitail -s 2 /var/log/auth.log /var/log/apache2/access.log

# 2. Deploy swatch for automated responses to patterns found
cat > /tmp/incident-response.conf << 'EOF'
watchfor /Failed password.*from (\S+)/
    exec "/usr/local/bin/block-ip.sh $1"
    
watchfor /Accepted password for root/
    mail addresses=admin@localhost,subject="ROOT LOGIN"
EOF

swatch -c /tmp/incident-response.conf -t /var/log/auth.log --daemon

# 3. Use logwatch for comprehensive post-incident analysis
sudo logwatch --range 'between 2024-10-29 and today' --detail High --output file --filename /tmp/incident-report.txt
```

**Integrated monitoring script**:

```bash
#!/bin/bash
# integrated-monitor.sh - Combined approach for CTF scenarios

# Start background alerting with swatch
swatch -c /etc/swatch/critical-alerts.conf -t /var/log/auth.log --daemon --pid-file=/tmp/swatch.pid

# Launch visual monitoring with multitail
multitail -s 3 \
    -l "Authentication" /var/log/auth.log \
    -l "Web Activity" /var/log/apache2/access.log \
    -l "Alerts" -c "tail -f /tmp/swatch-alerts.log" -R 1

# On exit, stop swatch
trap "kill $(cat /tmp/swatch.pid 2>/dev/null) 2>/dev/null" EXIT

# Generate summary report periodically (background)
(
    while true; do
        sleep 3600  # Every hour
        logwatch --range today --output file --filename "/tmp/hourly-report-$(date +%H).txt"
    done
) &
```

### Security Considerations

**Permissions and access**:

```bash
# Check log file permissions
ls -la /var/log/auth.log
# Should be readable only by root or syslog group

# Run with appropriate privileges
sudo multitail /var/log/auth.log

# For CTF defense, create dedicated monitoring user
sudo useradd -r -s /bin/false logmonitor
sudo usermod -aG adm logmonitor  # Grant log read access
sudo -u logmonitor multitail /var/log/auth.log
```

**Preventing log analysis detection** [Inference - attacker's perspective]:

```bash
# Attackers may detect monitoring processes
ps aux | grep -E 'swatch|multitail|logwatch'

# Stealth monitoring (though ethically questionable in CTF defense)
# Rename processes or use screen/tmux detached sessions
screen -dmS hidden-monitor multitail /var/log/auth.log
```

**Log tampering detection**:

```bash
# Monitor logs for signs of tampering
multitail -s 2 \
    -l "Log File" /var/log/auth.log \
    -l "File Integrity" -c "sha256sum /var/log/auth.log" -R 30

# Using swatch to alert on unexpected log modifications
watchfor /LOG CLEARED|logger/i
    echo bold red
    mail addresses=admin@localhost,subject="Possible Log Tampering"
```

### Performance Benchmarks and Optimization

[Inference - based on typical tool behavior]:

**Resource usage comparison** (approximate):

- **logwatch**: Minimal during execution (Perl process), CPU spike during report generation
- **swatch**: ~5-10MB RAM per instance, low CPU (event-driven)
- **multitail**: 20-50MB RAM per file monitored, moderate CPU for screen updates

**Optimization strategies**:

```bash
# For high-volume logs, use sampling
multitail -n 5 /var/log/high-volume.log  # Show every 5th line

# Reduce screen update frequency
multitail -u 2000 /var/log/auth.log  # Update every 2 seconds

# Use filters to reduce processing
swatch -c ~/.swatchrc -t /var/log/auth.log | grep "Failed"  # Pre-filter with grep

# Limit buffer sizes
multitail -m 500 /var/log/auth.log  # 500 lines buffer instead of default 1000
```

**Memory management**:

```bash
# Monitor tool resource usage
watch -n 5 'ps aux | grep -E "swatch|multitail|logwatch" | grep -v grep'

# Set process limits for swatch
ulimit -v 100000  # Limit virtual memory to ~100MB
swatch -c ~/.swatchrc -t /var/log/auth.log --daemon
```

### Essential Subtopics for Further Study

- **Log rotation handling with logrotate** - Ensuring continuous monitoring across log rotations
- **Centralized logging with rsyslog/syslog-ng** - Aggregating logs from multiple sources
- **SIEM integration** - Feeding multitail/swatch alerts to enterprise security systems
- **Custom parser development** - Creating logwatch service parsers for proprietary applications
- **Regular expression optimization** - Improving swatch pattern matching performance
- **Terminal multiplexing with tmux/screen** - Persistent monitoring sessions
- **Systemd journal integration** - Monitoring journalctl output with these tools
- **Automated incident response scripting** - Building comprehensive response chains with swatch

---

## Practical CTF Log Monitoring Setup

### Complete defense monitoring setup:

```bash
#!/bin/bash
# ctf-defense-setup.sh - Complete monitoring infrastructure

# 1. Configure logwatch for periodic reports
sudo tee /etc/cron.hourly/ctf-logwatch << 'EOF'
#!/bin/bash
/usr/sbin/logwatch --range today --detail High --service all --output file --filename /var/log/reports/logwatch-$(date +%Y%m%d-%H%M).txt
EOF
sudo chmod +x /etc/cron.hourly/ctf-logwatch

# 2. Deploy swatch for critical alerts
sudo tee /etc/swatch/critical.conf << 'EOF'
watchfor /Failed password for root/
    echo bold red
    exec "echo $(date) - ROOT LOGIN ATTEMPT from $(echo $0 | awk '{print $(NF-3)}') >> /var/log/critical-events.log"
    mail addresses=admin@localhost,subject="CRITICAL: Root Login Attempt"

watchfor /(union.*select|script>|\.\.\/)/i
    echo red
    exec "/usr/local/bin/alert-web-attack.sh"
    throttle 0:1:0
EOF

sudo tee /etc/systemd/system/swatch-critical.service << 'EOF'
[Unit]
Description=Swatch Critical Alert Monitor
After=network.target

[Service]
Type=simple
ExecStart=/usr/bin/swatch -c /etc/swatch/critical.conf -t /var/log/auth.log
Restart=always

[Install]
WantedBy=multi-user.target
EOF

sudo systemctl enable swatch-critical
sudo systemctl start swatch-critical

# 3. Create multitail monitoring script
sudo tee /usr/local/bin/ctf-monitor << 'EOF'
#!/bin/bash
multitail -s 4 \
    -cS syslog -l "AUTH" /var/log/auth.log \
    -cS apache -l "WEB" /var/log/apache2/access.log \
    -l "ERRORS" -e "error|failed|denied" /var/log/syslog \
    -l "CRITICAL" -c "tail -f /var/log/critical-events.log"
EOF

sudo chmod +x /usr/local/bin/ctf-monitor

echo "CTF monitoring setup complete!"
echo "Run 'ctf-monitor' to start visual monitoring"
echo "Check /var/log/reports/ for logwatch reports"
echo "Check systemctl status swatch-critical for alert service"
```

This comprehensive guide covers the essential Kali Linux log monitoring tools for CTF scenarios, providing practical commands, configurations, and real-world application strategies for effective log analysis and threat detection.

---

## ccze (Colorized Log Viewer)

`ccze` is a robust log colorizer that makes pattern recognition significantly faster by applying color-coding to different log components based on log format detection.

### Installation and Verification

```bash
# Verify installation (pre-installed on most Kali versions)
ccze --version

# If not installed
sudo apt update && sudo apt install ccze -y
```

### Basic Usage

**Standard log viewing:**

```bash
# Pipe any log through ccze
tail -f /var/log/apache2/access.log | ccze

# View static log file
ccze < /var/log/syslog

# Colorize command output
journalctl -xe | ccze -A
```

**Supported log formats (auto-detected):**

- Apache access/error logs
- Syslog
- Postfix mail logs
- SSH logs (auth.log)
- Squid proxy logs
- Generic system logs

### Key Options

```bash
# Force ANSI color mode (for piping/redirecting)
tail -f access.log | ccze -A

# Specify log format explicitly
ccze -m apache < access.log
ccze -m syslog < messages.log

# Available format modules: apache, squid, syslog, postfix, procmail, php, etc.
ccze -l  # List all available modules

# Disable automatic format detection
ccze -p syslog < mixed.log

# Raw ANSI output (preserves colors in redirects)
tail -f app.log | ccze -A > colored-output.log
```

### CTF Application Scenarios

**Analyzing web server logs:**

```bash
# Colorize Apache/nginx access logs for quick visual parsing
cat access.log | ccze -A | less -R

# Monitor in real-time with color
tail -f /var/log/apache2/access.log | ccze -A

# Combined with grep (note: grep after ccze removes colors)
cat access.log | ccze -A | grep -E "404|500"

# Better approach: grep first, then colorize
grep -E "404|500" access.log | ccze -A
```

**SSH authentication analysis:**

```bash
# Colorize auth logs for failed login detection
cat /var/log/auth.log | ccze -A | less -R

# Highlight authentication patterns
grep "Failed password\|Accepted password" /var/log/auth.log | ccze -A

# Monitor live authentication attempts
tail -f /var/log/auth.log | ccze -A
```

**Custom log parsing:**

```bash
# Even works with application logs that match known formats
cat custom-app.log | ccze -A

# For completely custom formats, ccze will still colorize IPs, dates, numbers
python3 exploit.py 2>&1 | ccze -A
```

### Performance Considerations

[Inference] `ccze` adds minimal overhead for small to medium log files but may slow down processing of very large files (>1GB). For bulk analysis, use standard tools first, then colorize filtered results.

**Efficient workflow:**

```bash
# Filter THEN colorize (faster)
grep "pattern" huge.log | ccze -A

# Rather than
cat huge.log | ccze -A | grep "pattern"  # Slower
```

## lnav (Log Navigator)

`lnav` (Log File Navigator) is an advanced log viewer with automatic format detection, SQL querying capabilities, syntax highlighting, and built-in log analysis features.

### Installation

```bash
# Check if installed
lnav --version

# Install if needed
sudo apt update && sudo apt install lnav -y

# Alternative: Install latest from GitHub
wget https://github.com/tstack/lnav/releases/download/v0.12.2/lnav-0.12.2-x86_64-linux-musl.zip
unzip lnav-*.zip
sudo mv lnav-*/lnav /usr/local/bin/
```

### Basic Navigation

**Opening logs:**

```bash
# Single file
lnav /var/log/apache2/access.log

# Multiple files (merged by timestamp)
lnav /var/log/apache2/*.log

# All logs in directory
lnav /var/log/

# Recursive directory watching
lnav -r /var/log/

# Remote logs via SSH
ssh user@target "tail -f /var/log/app.log" | lnav

# Follow mode (like tail -f)
lnav -t /var/log/syslog
```

### Keyboard Navigation

```
Movement:
  j/k           - Up/down one line
  Space/b       - Page down/up
  g/G           - Jump to top/bottom
  /pattern      - Search forward
  ?pattern      - Search backward
  n/N           - Next/previous match
  
Filtering:
  i             - Toggle filter input
  o             - Toggle OUT filter (hide matches)
  CTRL-R        - Clear filters
  TAB           - Toggle filters on/off
  
Time Navigation:
  t             - Jump to specific time
  T             - Jump to relative time
  1-6           - Jump to log percentile marks
  
Views:
  q             - Quit
  CTRL-L        - Redraw screen
  :             - Enter command mode
  ;             - Execute SQL query
  
Bookmarks:
  m             - Set bookmark
  M             - Set named bookmark
  u/U           - Next/previous bookmark
  c             - Clear bookmarks
```

### Automatic Format Detection

[Unverified] `lnav` claims to automatically detect and parse over 50 common log formats including:

- Syslog (RFC3164, RFC5424)
- Apache/nginx access logs
- Common Log Format (CLF)
- Combined Log Format
- JSON logs
- Kubernetes logs
- Docker logs
- AWS CloudWatch/CloudTrail logs
- Strace output
- Custom regex-based formats

**Viewing parsed fields:**

```bash
# Inside lnav, press 'p' to toggle pretty-print mode
# Shows structured view of log fields
```

### SQL Querying (Powerful Feature)

`lnav` allows SQL queries against log data, treating each log format as a virtual table.

**Entering SQL mode:**

```
Press ; (semicolon) inside lnav
```

**Basic SQL queries:**

```sql
-- Count requests by status code
;SELECT sc_status, COUNT(*) as count FROM access_log GROUP BY sc_status ORDER BY count DESC

-- Find top IP addresses
;SELECT c_ip, COUNT(*) as requests FROM access_log GROUP BY c_ip ORDER BY requests DESC LIMIT 10

-- Filter by time range
;SELECT * FROM access_log WHERE log_time > datetime('2024-01-01 00:00:00')

-- Find 404 errors with specific paths
;SELECT log_time, c_ip, cs_uri_stem FROM access_log WHERE sc_status = 404 AND cs_uri_stem LIKE '%admin%'

-- Calculate average response time
;SELECT AVG(time_taken) as avg_ms FROM access_log WHERE sc_status = 200

-- Requests by hour
;SELECT strftime('%Y-%m-%d %H:00', log_time) as hour, COUNT(*) FROM access_log GROUP BY hour

-- Join multiple log types (if viewing multiple formats)
;SELECT a.log_time, a.c_ip, s.log_body FROM access_log a JOIN syslog_log s ON a.log_time = s.log_time
```

**Available table names (depends on detected format):**

- `access_log` - Web server access logs
- `error_log` - Error logs
- `syslog_log` - Syslog entries
- `generic_log` - Unknown formats
- Custom names based on format files

**View schema:**

```sql
-- List available columns
;SELECT * FROM access_log LIMIT 1

-- Or use .schema command
:.schema access_log
```

### Advanced Filtering

**Text filtering:**

```
# Inside lnav:
i                           # Enter filter mode
regex: .*flag.*            # Show only lines matching regex
<ENTER>

o                           # Enter OUT filter mode  
regex: .*noise.*           # Hide lines matching regex
<ENTER>

TAB                         # Toggle all filters on/off
CTRL-R                      # Clear all filters
```

**Time-based filtering:**

```bash
# Jump to specific time
t                           # Press 't' in lnav
2024-01-15 14:30:00        # Enter timestamp

# Relative time jump
T                           # Press 'T'
-1h                        # Go back 1 hour
+30m                       # Forward 30 minutes
```

### CTF-Specific Usage Patterns

**Analyzing web server exploitation attempts:**

```bash
lnav /var/log/apache2/access.log

# Inside lnav SQL mode:
;SELECT c_ip, cs_uri_stem, COUNT(*) as attempts FROM access_log 
 WHERE sc_status >= 400 OR cs_uri_stem LIKE '%..%' 
 GROUP BY c_ip, cs_uri_stem ORDER BY attempts DESC

# Filter for common attack patterns
i
regex: .*(union|select|script|\.\.\/|%00).*
```

**Correlating multi-service logs:**

```bash
# Open all relevant logs at once (auto-merged by timestamp)
lnav /var/log/apache2/access.log /var/log/auth.log /var/log/syslog

# Use SQL to correlate
;SELECT a.log_time, a.c_ip, s.log_body 
 FROM access_log a 
 JOIN syslog_log s ON abs(julianday(a.log_time) - julianday(s.log_time)) < 0.0001
 WHERE a.c_ip = '192.168.1.100'
```

**Extracting specific data patterns:**

```bash
# Search for flags in logs
/                           # Enter search mode
flag\{[^}]+\}              # Regex for flag format

# Use SQL to extract
;SELECT log_body FROM syslog_log WHERE log_body LIKE '%flag{%'
```

**Performance monitoring:**

```bash
# Analyze response time patterns
;SELECT 
  CASE 
    WHEN time_taken < 100 THEN 'fast'
    WHEN time_taken < 1000 THEN 'medium'
    ELSE 'slow'
  END as speed,
  COUNT(*) as count
FROM access_log 
GROUP BY speed
```

### Custom Format Definition

[Inference] For logs that `lnav` doesn't auto-detect, you can define custom formats using JSON configuration files.

**Location:** `~/.lnav/formats/installed/`

**Example custom format (custom-app.json):**

```json
{
  "custom_app_log": {
    "title": "Custom Application Log",
    "description": "Custom CTF app logs",
    "regex": {
      "basic": {
        "pattern": "^(?<timestamp>\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}) \\[(?<level>\\w+)\\] (?<message>.*)$"
      }
    },
    "level": {
      "error": "ERROR",
      "warning": "WARN"
    },
    "timestamp-format": [
      "%Y-%m-%d %H:%M:%S"
    ],
    "sample": [
      {
        "line": "2024-01-15 14:30:22 [INFO] User logged in successfully"
      }
    ]
  }
}
```

**Test custom format:**

```bash
# Verify format loads
lnav -i /path/to/custom-app.json

# Use with logs
lnav custom-app.log
```

### Scripting and Automation

**Command-line SQL execution:**

```bash
# Execute SQL query from command line
lnav -n -c ";SELECT * FROM access_log WHERE sc_status = 500" access.log

# Export results to file
lnav -n -c ";SELECT * FROM access_log" access.log > output.txt

# Combine with other tools
lnav -n -c ";SELECT c_ip FROM access_log WHERE sc_status = 404" access.log | sort | uniq -c
```

**Batch commands:**

```bash
# Create command file
cat > analyze.lnav << 'EOF'
:filter-in .*error.*
:write-to filtered-errors.txt
EOF

# Execute commands
lnav -f analyze.lnav /var/log/app.log
```

### Session Management

```bash
# Save current session (filters, bookmarks, positions)
# Inside lnav:
:save-session session-name

# Load saved session
lnav -S session-name /var/log/*.log

# List sessions
:session list
```

## angle-grinder (agrind)

`angle-grinder` (command: `agrind`) is a modern log analysis tool designed for fast filtering, aggregating, and transforming log data using a query language similar to SQL but optimized for log processing.

### Installation

```bash
# Not typically pre-installed on Kali, needs manual installation

# Method 1: Download binary from GitHub
wget https://github.com/rcoh/angle-grinder/releases/download/v0.19.2/agrind-v0.19.2-x86_64-unknown-linux-musl.tar.gz
tar -xzf agrind-*.tar.gz
sudo mv agrind /usr/local/bin/
agrind --version

# Method 2: Install via cargo (Rust)
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
cargo install ag

# Method 3: Build from source
git clone https://github.com/rcoh/angle-grinder
cd angle-grinder
cargo build --release
sudo cp target/release/agrind /usr/local/bin/
```

### Basic Syntax

```bash
agrind 'QUERY' < logfile
cat logfile | agrind 'QUERY'
tail -f logfile | agrind 'QUERY'
```

**Query structure:**

```
agrind '[json | logfmt | parse "pattern"] | [filters] | [aggregations] | [transformations]'
```

### Parsing Input Formats

**JSON logs:**

```bash
# Automatically parse JSON
cat app.json | agrind 'json | fields status, ip'

# CloudWatch logs example
cat cloudwatch.json | agrind 'json | fields .message, .timestamp'

# Nested JSON access
cat nested.json | agrind 'json | fields user.id, request.method'
```

**Logfmt format (key=value pairs):**

```bash
# Parse logfmt
echo 'level=info msg="user login" user=admin ip=10.0.0.1' | agrind 'logfmt | fields msg, user, ip'

# Real example
cat heroku.log | agrind 'logfmt | where level == "error"'
```

**Custom regex parsing:**

```bash
# Parse Apache Common Log Format
cat access.log | agrind 'parse "* * * [*] \"* * *\" * *" as ip, ident, user, date, method, path, proto, status, bytes'

# Parse custom format
cat app.log | agrind 'parse "[*] *: *" as timestamp, level, message'

# Multiple patterns (try each until match)
cat mixed.log | agrind 'parse "* *" as ts, msg | parse "ERROR: *" as error_msg'
```

### Filtering

**Where clauses:**

```bash
# Exact match
agrind 'json | where status == 200'

# Numeric comparison
agrind 'json | where response_time > 1000'

# String contains
agrind 'json | where path contains "admin"'

# Multiple conditions
agrind 'json | where status >= 400 and method == "POST"'

# Regex matching
agrind 'json | where message ~= "error|fail"'

# Negation
agrind 'json | where status != 404'
```

**Common CTF filters:**

```bash
# Find authentication failures
agrind 'json | where message contains "failed" and message contains "auth"'

# Filter SQL injection attempts
agrind 'parse "* \"* * *\" *" as ip, method, path, proto, status | where path ~= "(union|select|drop)"'

# Find privilege escalation attempts
agrind 'json | where user == "www-data" and command contains "sudo"'
```

### Aggregations

**Count operations:**

```bash
# Count all lines
agrind 'count'

# Count by field
agrind 'json | count by status'

# Count distinct values
agrind 'json | count_distinct by ip'

# Multiple grouping
agrind 'json | count by method, status'
```

**Statistical aggregations:**

```bash
# Sum
agrind 'json | sum(bytes) by host'

# Average
agrind 'json | avg(response_time) by endpoint'

# Min/Max
agrind 'json | min(timestamp), max(timestamp)'

# Percentiles
agrind 'json | p50(response_time), p95(response_time), p99(response_time) by service'
```

**Multiple aggregations:**

```bash
agrind 'json | count, avg(response_time), p95(response_time) by endpoint'
```

### Transformations

**Field operations:**

```bash
# Select specific fields
agrind 'json | fields ip, method, path, status'

# Field aliasing
agrind 'json | fields ip as client_ip, path as uri'

# Computed fields
agrind 'json | fields *, (bytes / 1024) as kb'
```

**Sorting:**

```bash
# Sort by field
agrind 'json | count by ip | sort by count'

# Descending sort (default)
agrind 'json | count by ip | sort by count desc'

# Ascending sort
agrind 'json | count by status | sort by status asc'

# Limit results
agrind 'json | count by ip | sort by count | limit 10'
```

**Time operations:**

```bash
# Parse timestamps
agrind 'json | parse timestamp as datetime'

# Time-based grouping
agrind 'json | count by timeslice(timestamp, 5m)'  # 5-minute buckets

# Available time units: s, m, h, d
agrind 'json | avg(response_time) by timeslice(timestamp, 1h)'
```

### CTF-Specific Examples

**Web access log analysis:**

```bash
# Find most accessed endpoints
cat access.log | agrind 'parse "* * * [*] \"* * *\" * *" as ip, ident, user, date, method, path, proto, status, bytes | count by path | sort by count desc | limit 20'

# Find attack patterns
cat access.log | agrind 'parse "* * * [*] \"* * *\" * *" as ip, ident, user, date, method, path, proto, status, bytes | where path ~= "\.\.|%00|union|script|alert" | fields ip, path, status'

# Response code distribution by IP
cat access.log | agrind 'parse "* * * [*] \"* * *\" * *" as ip, ident, user, date, method, path, proto, status, bytes | count by ip, status'

# Suspicious 4xx followed by 2xx from same IP
cat access.log | agrind 'parse "* * * [*] \"* * *\" * *" as ip, ident, user, date, method, path, proto, status, bytes | where status >= 400 or status < 300 | fields date, ip, path, status'
```

**Application log forensics:**

```bash
# Extract flags from JSON logs
cat app.json | agrind 'json | where message ~= "flag\{.*\}" | fields timestamp, user, message'

# Find privilege changes
cat audit.json | agrind 'json | where event_type == "permission_change" | fields timestamp, user, target_user, old_role, new_role'

# Detect brute force attempts
cat auth.json | agrind 'json | where event == "login_failed" | count by source_ip, user | where count > 10'
```

**AWS CloudWatch log parsing:**

```bash
# Parse Lambda logs
cat lambda-logs.json | agrind 'json | where message contains "ERROR" | fields timestamp, requestId, message'

# API Gateway analysis
cat apigateway.json | agrind 'json | fields ip, path, status, responseLatency | where status >= 400 or responseLatency > 1000'

# Count by HTTP method and status
cat apigateway.json | agrind 'json | count by httpMethod, status | sort by count desc'
```

**Real-time monitoring:**

```bash
# Live tail with filtering
tail -f /var/log/app.log | agrind 'json | where level == "ERROR" | fields timestamp, message'

# Live aggregation (updates every 5 seconds)
tail -f access.log | agrind 'parse "* * * [*] \"* * *\" * *" as ip, ident, user, date, method, path, proto, status, bytes | count by status'

# Alert on threshold
tail -f app.log | agrind 'json | where response_time > 5000 | fields timestamp, endpoint, response_time'
```

### Performance Optimization

**Efficient query patterns:**

```bash
# Filter early in pipeline (faster)
agrind 'json | where status == 404 | count by ip'  # Good

# Avoid late filtering
agrind 'json | count by ip, status | where status == 404'  # Slower

# Parse only needed fields
agrind 'parse "* * * [*] \"* * *\" * *" as ip, _, _, _, method, path, _, status, _ | fields ip, method, path, status'
```

**Large file handling:**

```bash
# Use standard Unix tools for initial filtering
grep "ERROR" huge.log | agrind 'json | fields message'

# Stream processing instead of loading entire file
cat huge.log | agrind 'json | where level == "error"' > errors.txt
```

### Comparison with Other Tools

**agrind vs awk:**

- agrind: Better for structured logs (JSON, logfmt), built-in aggregations
- awk: Better for simple pattern matching, more universally available

**agrind vs jq:**

- agrind: Optimized for log aggregation, time-series operations
- jq: Better for complex JSON transformations, more mature

**agrind vs lnav:**

- agrind: Command-line streaming, scriptable, fast one-off queries
- lnav: Interactive exploration, multiple format support, SQL queries

### Combined Tool Workflows

**Multi-stage analysis:**

```bash
# Stage 1: ccze for quick visual scan
cat access.log | ccze -A | less -R

# Stage 2: agrind for specific analysis
cat access.log | agrind 'parse "* * * [*] \"* * *\" * *" as ip, ident, user, date, method, path, proto, status, bytes | where status >= 400 | count by ip, status'

# Stage 3: lnav for interactive drill-down
lnav access.log
# Then use SQL queries inside lnav
```

**Chaining tools:**

```bash
# Extract, filter, colorize
cat app.json | agrind 'json | where level == "ERROR"' | ccze -A

# Parse, aggregate, then load into lnav
cat access.log | agrind 'parse "* * * [*] \"* * *\" * *" as ip, ident, user, date, method, path, proto, status, bytes | where status == 404' > 404s.log
lnav 404s.log

# Real-time monitoring with visual feedback
tail -f app.log | agrind 'json | where level in ["ERROR", "WARN"]' | ccze -A
```

---

**Important related topics:**

- jq (JSON processing - essential for cloud logs)
- goaccess (real-time web log analyzer with dashboard)
- Elasticsearch/Kibana log aggregation (for large-scale CTF infrastructure)
- Custom log parsing with Python/Perl for proprietary formats

---

# Forensic Log Analysis

Forensic log analysis in CTF scenarios requires maintaining evidence integrity while extracting actionable intelligence. Proper preservation, documentation, and verification ensure findings are defensible and analysis is reproducible.

## Log Artifact Preservation

Log preservation prevents tampering, data loss, and ensures forensic soundness. In CTF contexts, this includes capturing volatile data, creating forensic copies, and maintaining read-only evidence.

**Immediate acquisition priorities:**

```bash
# Capture system state before log collection
date -u > acquisition_timestamp.txt
uname -a >> acquisition_timestamp.txt
uptime >> acquisition_timestamp.txt

# Check if logs are being actively written
lsof | grep -E '\.log$|/var/log/' > open_log_files.txt

# Identify log rotation status (may lose data if rotation occurs)
ls -lah /var/log/*.log > log_file_status.txt
```

**Creating forensic copies:**

```bash
# Using dd for bit-by-bit copy (preserves slack space, deleted data)
dd if=/var/log/apache2/access.log of=access.log.forensic bs=4K conv=noerror,sync status=progress

# Calculate hash before and after to verify integrity
sha256sum /var/log/apache2/access.log > original.sha256
sha256sum access.log.forensic > copy.sha256

# Create compressed archive with metadata preservation
tar -czf logs_$(date +%Y%m%d_%H%M%S).tar.gz \
    --preserve-permissions \
    --preserve-order \
    --numeric-owner \
    --absolute-names \
    /var/log/

# For entire partition (advanced scenarios)
dd if=/dev/sda1 of=partition.dd bs=512 conv=noerror,sync
```

**Write-blocking and read-only mounting:**

```bash
# Mount filesystem as read-only to prevent modifications
mount -o ro,noload /dev/sdb1 /mnt/evidence

# Verify read-only status
mount | grep /mnt/evidence
# Should show: (ro,noload)

# For analysis without mounting (safer)
losetup -r /dev/loop0 evidence.dd
mount -o ro /dev/loop0 /mnt/evidence

# Set immutable flag on copied logs (prevents accidental modification)
chattr +i access.log.forensic
# Remove with: chattr -i access.log.forensic
```

**Memory-resident log capture (volatile data):**

```bash
# Capture logs from memory before they're written to disk
strings /proc/$(pgrep syslog)/mem | grep -a "pattern" > memory_logs.txt

# Dump system journal from memory
journalctl --since "1 hour ago" --no-pager > journal_snapshot.txt

# Capture kernel ring buffer (dmesg)
dmesg -T > dmesg_snapshot.txt

# Network connection logs (ephemeral)
ss -tunap > network_connections.txt
netstat -anp > netstat_snapshot.txt
```

**Live log collection script:**

```bash
#!/bin/bash
# preserve_logs.sh - Forensically sound log collection

EVIDENCE_DIR="evidence_$(date +%Y%m%d_%H%M%S)"
mkdir -p "$EVIDENCE_DIR"

# Document collection process
exec > >(tee "$EVIDENCE_DIR/collection.log")
exec 2>&1

echo "[*] Starting log preservation at $(date -u)"
echo "[*] Operator: $(whoami)@$(hostname)"
echo "[*] Evidence directory: $EVIDENCE_DIR"

# Function to collect and hash
collect_artifact() {
    local source="$1"
    local dest="$EVIDENCE_DIR/$(basename $source)"
    
    if [ -f "$source" ]; then
        echo "[+] Collecting: $source"
        cp -a "$source" "$dest"
        
        # Calculate hashes
        md5sum "$source" >> "$EVIDENCE_DIR/hashes.md5"
        sha256sum "$source" >> "$EVIDENCE_DIR/hashes.sha256"
        
        # Document metadata
        stat "$source" >> "$EVIDENCE_DIR/metadata.txt"
        ls -lah "$source" >> "$EVIDENCE_DIR/metadata.txt"
    else
        echo "[-] Not found: $source"
    fi
}

# Collect common log locations
for log in /var/log/auth.log /var/log/syslog /var/log/apache2/*.log /var/log/nginx/*.log; do
    collect_artifact "$log"
done

# Collect running process logs
lsof -p $(pgrep apache2 | head -1) 2>/dev/null > "$EVIDENCE_DIR/apache_open_files.txt"

# Create timeline
echo "[*] Creating timeline..."
find /var/log -type f -printf "%T+ %p\n" | sort > "$EVIDENCE_DIR/timeline.txt"

# Final hash of entire evidence collection
echo "[*] Calculating evidence package hash..."
tar -czf "${EVIDENCE_DIR}.tar.gz" "$EVIDENCE_DIR"
sha256sum "${EVIDENCE_DIR}.tar.gz" > "${EVIDENCE_DIR}.tar.gz.sha256"

echo "[*] Collection complete at $(date -u)"
```

**Preserving Windows logs (when applicable):**

```bash
# On Windows systems (via WSL or remote access)
# Export Event Logs
wevtutil epl Security Security.evtx /ow:true
wevtutil epl System System.evtx /ow:true
wevtutil epl Application Application.evtx /ow:true

# Convert to readable format
wevtutil qe Security /f:text > Security.txt

# Using PowerShell for forensic export
# Get-WinEvent -LogName Security | Export-Csv security_log.csv
```

**Docker/Container log preservation:**

```bash
# Export container logs before container stops
docker logs container_name > container_logs.txt 2>&1

# Export all container logs
for container in $(docker ps -a -q); do
    docker logs $container > "logs_${container}.txt" 2>&1
done

# Copy logs from container filesystem
docker cp container_name:/var/log/app.log ./app.log.forensic

# Export container metadata
docker inspect container_name > container_metadata.json
```

## Chain of Custody

Chain of custody documentation tracks who accessed evidence, when, where, and why. Essential for proving evidence integrity in formal investigations and CTF scoring disputes.

**Chain of custody document template:**

```bash
# Create custody log
cat > chain_of_custody.txt << 'EOF'
CHAIN OF CUSTODY LOG
====================

Case/CTF ID: 
Evidence Item: 
Description: 

ACQUISITION
-----------
Date/Time (UTC): 
Collected By: 
Source Location: 
Collection Method: 
Original Hash (SHA-256): 

CUSTODY TRANSFERS
-----------------
[Date/Time] | [From] | [To] | [Purpose] | [Hash Verified: Y/N]

ANALYSIS SESSIONS
-----------------
[Date/Time] | [Analyst] | [Actions Taken] | [Tools Used] | [Hash After: ]

STORAGE LOCATION
----------------
Current Location: 
Access Controls: 
EOF

# Automated custody logging
log_custody_event() {
    local event_type="$1"
    local description="$2"
    
    echo "[$(date -u '+%Y-%m-%d %H:%M:%S UTC')] $event_type: $description (User: $(whoami))" \
        >> chain_of_custody.txt
}

# Usage examples
log_custody_event "ACCESS" "Opened evidence file for analysis"
log_custody_event "COPY" "Created working copy for string extraction"
log_custody_event "HASH_VERIFY" "Verified integrity - SHA256 matches"
```

**Automated custody tracking script:**

```bash
#!/bin/bash
# custody_wrapper.sh - Tracks all access to evidence files

EVIDENCE_FILE="$1"
CUSTODY_LOG="custody_$(basename $EVIDENCE_FILE).log"
HASH_LOG="hashes_$(basename $EVIDENCE_FILE).log"

# Initial hash
INITIAL_HASH=$(sha256sum "$EVIDENCE_FILE" | awk '{print $1}')
echo "$(date -u '+%Y-%m-%d %H:%M:%S') | INITIAL | $INITIAL_HASH | $(whoami)" >> "$HASH_LOG"

# Log access
echo "$(date -u '+%Y-%m-%d %H:%M:%S') | ACCESS | $(whoami) | $EVIDENCE_FILE" >> "$CUSTODY_LOG"

# Execute intended command on evidence
"${@:2}"  # Run remaining arguments as command

# Post-operation hash verification
POST_HASH=$(sha256sum "$EVIDENCE_FILE" | awk '{print $1}')
echo "$(date -u '+%Y-%m-%d %H:%M:%S') | POST-OP | $POST_HASH | $(whoami)" >> "$HASH_LOG"

# Alert if hash changed
if [ "$INITIAL_HASH" != "$POST_HASH" ]; then
    echo "[!] WARNING: Evidence hash changed during operation!" | tee -a "$CUSTODY_LOG"
    echo "    Before: $INITIAL_HASH"
    echo "    After:  $POST_HASH"
fi

# Usage: ./custody_wrapper.sh evidence.log grep "malicious"
```

**Working copy vs. original separation:**

```bash
# Create write-protected original and working copy
ORIGINAL="access.log"
WORKING="${ORIGINAL}.working"

# Preserve original with immutable flag
cp -a "$ORIGINAL" "${ORIGINAL}.original"
chattr +i "${ORIGINAL}.original"
sha256sum "${ORIGINAL}.original" > "${ORIGINAL}.original.sha256"

# Create working copy for analysis
cp -a "$ORIGINAL" "$WORKING"

# All analysis on working copy only
grep "suspicious" "$WORKING" > findings.txt

# Verify original unchanged
sha256sum -c "${ORIGINAL}.original.sha256"
```

**Metadata preservation:**

```bash
# Capture comprehensive file metadata
capture_metadata() {
    local file="$1"
    local output="${file}.metadata.txt"
    
    {
        echo "=== File Metadata ==="
        echo "File: $file"
        echo "Collection Time: $(date -u)"
        echo ""
        
        echo "=== File Statistics ==="
        stat "$file"
        echo ""
        
        echo "=== Extended Attributes ==="
        getfattr -d "$file" 2>/dev/null || echo "None"
        echo ""
        
        echo "=== File Type ==="
        file "$file"
        echo ""
        
        echo "=== Timestamps (Detailed) ==="
        stat -c "Access: %x%nModify: %y%nChange: %z%nBirth: %w" "$file"
        echo ""
        
        echo "=== Inode Information ==="
        stat -c "Inode: %i%nLinks: %h%nBlocks: %b" "$file"
        echo ""
        
        echo "=== Ownership & Permissions ==="
        ls -lahn "$file"
        
    } > "$output"
    
    echo "[+] Metadata saved to: $output"
}

# Usage
capture_metadata "/var/log/auth.log"
```

**Digital signature for evidence integrity:**

```bash
# Sign evidence with GPG for tamper-evidence
gpg --output evidence.log.sig --detach-sign evidence.log

# Verify signature later
gpg --verify evidence.log.sig evidence.log

# Create signed manifest of all evidence
find evidence_dir/ -type f -exec sha256sum {} \; > manifest.txt
gpg --clearsign manifest.txt
# Produces manifest.txt.asc with embedded signature
```

## Hash Verification

Cryptographic hashing proves evidence hasn't been altered. Essential for establishing integrity throughout analysis lifecycle.

**Hash algorithm selection:**

```bash
# MD5 (128-bit) - Fast but collision vulnerabilities exist
md5sum file.log

# SHA-1 (160-bit) - Deprecated, known collision attacks
sha1sum file.log

# SHA-256 (256-bit) - Current standard, widely accepted
sha256sum file.log

# SHA-512 (512-bit) - Higher security margin
sha512sum file.log

# BLAKE2 (faster than SHA-256, secure) - Less universal tool support
b2sum file.log

# Generate multiple hashes for redundancy
{
    md5sum file.log
    sha1sum file.log
    sha256sum file.log
    sha512sum file.log
} > file.log.hashes
```

**Batch hash generation:**

```bash
# Hash all files in evidence directory
find evidence/ -type f -exec sha256sum {} \; > evidence_hashes.sha256

# Recursive hashing with metadata
find /var/log -type f -printf "%p\t" -exec sha256sum {} \; | \
    awk -F'\t' '{print $1 "\t" $2}' > complete_hash_manifest.txt

# Hash with file size (detect truncation)
find evidence/ -type f -exec sh -c 'echo "$(sha256sum "$1" | cut -d" " -f1) $(stat -c%s "$1") $1"' _ {} \; \
    > hashes_with_size.txt
```

**Continuous hash verification:**

```bash
#!/bin/bash
# verify_integrity.sh - Periodic integrity checking

HASH_FILE="evidence_hashes.sha256"
ALERT_LOG="integrity_alerts.log"

# Initial hash creation
if [ ! -f "$HASH_FILE" ]; then
    echo "[*] Creating initial hash baseline..."
    find evidence/ -type f -exec sha256sum {} \; > "$HASH_FILE"
    echo "[+] Baseline created: $HASH_FILE"
fi

# Verification function
verify_integrity() {
    echo "[*] Verifying integrity at $(date -u)"
    
    if sha256sum -c "$HASH_FILE" --quiet 2>&1 | grep -v "OK"; then
        echo "[!] ALERT: Hash mismatch detected at $(date -u)" | tee -a "$ALERT_LOG"
        sha256sum -c "$HASH_FILE" 2>&1 | grep FAILED | tee -a "$ALERT_LOG"
        return 1
    else
        echo "[+] All hashes verified successfully"
        return 0
    fi
}

# Continuous monitoring mode
if [ "$1" == "--monitor" ]; then
    while true; do
        verify_integrity
        sleep 300  # Check every 5 minutes
    done
else
    verify_integrity
fi
```

**Hash verification strategies:**

```bash
# Verify single file
sha256sum -c file.sha256

# Verify only specific files (ignore missing)
sha256sum -c --ignore-missing evidence_hashes.sha256

# Verify with verbose output
sha256sum -c -w evidence_hashes.sha256
# Shows warnings for improperly formatted lines

# Parallel verification (faster for large datasets)
cat evidence_hashes.sha256 | parallel --colsep '\s+' 'echo {2} {1} | sha256sum -c -'

# Verify and log results
sha256sum -c evidence_hashes.sha256 2>&1 | tee verification_$(date +%Y%m%d_%H%M%S).log
```

**Timeline-based hash verification:**

```bash
# Create hash timeline showing when files were last modified
create_hash_timeline() {
    local dir="$1"
    
    find "$dir" -type f -printf "%T+ %p\n" | sort | while read timestamp filepath; do
        hash=$(sha256sum "$filepath" | awk '{print $1}')
        echo "$timestamp | $hash | $filepath"
    done > hash_timeline.txt
}

# Usage
create_hash_timeline "/var/log"

# Compare timelines to detect backdating or time manipulation
diff hash_timeline_day1.txt hash_timeline_day2.txt
```

**Streaming hash calculation (large files):**

```bash
# For files too large to load into memory
# Calculate hash while copying
pv large_log.log | tee >(sha256sum > large_log.sha256) | \
    cat > large_log.forensic

# Verify during decompression
gzip -dc compressed.log.gz | tee >(sha256sum > decompressed.sha256) | \
    cat > decompressed.log

# Hash multiple streams simultaneously
tee >(md5sum > file.md5) >(sha256sum > file.sha256) < original_file > /dev/null
```

**Merkle tree verification (advanced):**

```bash
# Hierarchical hashing for large log sets
# Useful when verifying subset of files without recalculating all hashes

# Simple 2-level Merkle tree
calculate_merkle_root() {
    # Level 1: Hash individual files
    find evidence/ -type f -exec sha256sum {} \; | sort > level1_hashes.txt
    
    # Level 2: Hash the hash list
    sha256sum level1_hashes.txt | awk '{print $1}' > merkle_root.txt
    
    echo "[+] Merkle root: $(cat merkle_root.txt)"
}

# Verify subset without full recalculation
verify_merkle_subset() {
    local file="$1"
    
    # Check if file hash exists in level 1
    sha256sum "$file" | grep -Ff - level1_hashes.txt
    
    if [ $? -eq 0 ]; then
        echo "[+] File verified in merkle tree"
    else
        echo "[!] File not found or modified"
    fi
}
```

**Hash-based duplicate detection:**

```bash
# Find duplicate logs (saves storage, identifies log replication)
find /var/log -type f -exec sha256sum {} \; | \
    sort | \
    awk '{if (seen[$1]++) print $2 " (duplicate of " prev[$1] ")"; prev[$1]=$2}'

# Group files by hash
find evidence/ -type f -exec sha256sum {} \; | \
    awk '{print $1}' | sort | uniq -c | sort -rn
# Output shows: <count> <hash>
```

**Integrity verification in analysis pipelines:**

```bash
# Wrapper to verify hash before and after every operation
safe_analyze() {
    local evidence="$1"
    local operation="$2"
    
    # Pre-operation hash
    local pre_hash=$(sha256sum "$evidence" | awk '{print $1}')
    echo "[*] Pre-operation hash: $pre_hash"
    
    # Perform analysis on working copy
    local working="${evidence}.tmp"
    cp -a "$evidence" "$working"
    
    eval "$operation" "$working"
    
    # Verify original unchanged
    local post_hash=$(sha256sum "$evidence" | awk '{print $1}')
    
    if [ "$pre_hash" != "$post_hash" ]; then
        echo "[!] ERROR: Evidence modified during analysis!"
        return 1
    else
        echo "[+] Original evidence integrity verified"
    fi
    
    # Clean up
    rm "$working"
}

# Usage
safe_analyze "evidence.log" "grep 'attack' > findings.txt <"
```

**Cross-platform hash verification:**

```python
#!/usr/bin/env python3
# verify_hashes.py - Platform-independent verification

import hashlib
import sys

def calculate_hash(filepath, algorithm='sha256'):
    """Calculate hash of file using specified algorithm"""
    hash_obj = hashlib.new(algorithm)
    
    try:
        with open(filepath, 'rb') as f:
            # Read in chunks to handle large files
            for chunk in iter(lambda: f.read(4096), b''):
                hash_obj.update(chunk)
        return hash_obj.hexdigest()
    except IOError as e:
        print(f"[!] Error reading {filepath}: {e}", file=sys.stderr)
        return None

def verify_hash_file(hash_file):
    """Verify hashes from hash file (format: hash filepath)"""
    failed = []
    passed = 0
    
    with open(hash_file, 'r') as f:
        for line in f:
            parts = line.strip().split(None, 1)
            if len(parts) != 2:
                continue
                
            expected_hash, filepath = parts
            calculated_hash = calculate_hash(filepath)
            
            if calculated_hash == expected_hash:
                passed += 1
                print(f"[+] {filepath}: OK")
            else:
                failed.append(filepath)
                print(f"[!] {filepath}: FAILED")
    
    print(f"\n[*] Verification complete: {passed} passed, {len(failed)} failed")
    return len(failed) == 0

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: verify_hashes.py <hash_file>")
        sys.exit(1)
    
    success = verify_hash_file(sys.argv[1])
    sys.exit(0 if success else 1)
```

**Distributed verification (multiple analysts):**

```bash
# Create signed hash manifest for distribution
sha256sum evidence/*.log > manifest.txt
gpg --clearsign manifest.txt

# Each analyst verifies independently
gpg --verify manifest.txt.asc
sha256sum -c manifest.txt

# Consensus verification (multiple analysts must agree)
# Each analyst generates their own hashes
sha256sum evidence/*.log > analyst1_hashes.txt
sha256sum evidence/*.log > analyst2_hashes.txt

# Compare results
diff analyst1_hashes.txt analyst2_hashes.txt
# No output = consensus achieved
```

**Important forensic considerations:**

1. **Hash before any operation**: Calculate baseline hashes immediately upon evidence acquisition
2. **Multiple algorithms**: Use at least two different hash algorithms (e.g., SHA-256 + SHA-512) for redundancy
3. **Document everything**: Every hash calculation, verification, and access should be logged
4. **Time synchronization**: Ensure system clock accuracy (use NTP) for accurate timestamps
5. **Secure storage**: Store hash files separately from evidence (prevents simultaneous tampering)
6. **Automate verification**: Manual verification is error-prone; use scripts for consistency

**[Inference]** Hash mismatches typically indicate file corruption, tampering, or accidental modification. In CTF scenarios, they may also indicate anti-forensics techniques or environmental issues with the challenge infrastructure.

**Critical tool availability:**

- `sha256sum`, `md5sum`: Included in GNU coreutils (standard on Kali Linux)
- `b2sum`: Available in coreutils 8.26
- `chattr`: Part of e2fsprogs package
- `gpg`: GnuPG, typically pre-installed

---

## Write-Blocker Usage

Write-blockers prevent any modification to evidence media during analysis, ensuring forensic integrity. In CTF scenarios, this principle extends to preventing accidental alterations of log files, disk images, and system artifacts.

### Hardware vs Software Write-Blockers

**Hardware write-blockers** - Physical devices that intercept write commands:

- Not typically available in CTF environments
- Mentioned for completeness in forensic methodology
- Examples include Tableau, CRU WiebeTech devices

**Software write-blockers** - Implemented through OS-level controls:

**Linux loop device with read-only flag**:

```bash
# Mount disk image read-only
losetup -r -f disk.img
losetup -a  # List loop devices to find assignment (e.g., /dev/loop0)

# Mount the loop device
mkdir /mnt/evidence
mount -o ro,noexec,noload /dev/loop0 /mnt/evidence

# Verify read-only status
grep /mnt/evidence /proc/mounts | grep -o "ro"

# Cleanup after analysis
umount /mnt/evidence
losetup -d /dev/loop0
```

**Block device read-only mode**:

```bash
# Set device to read-only (requires root)
blockdev --setro /dev/sdb

# Verify read-only status
blockdev --getro /dev/sdb  # Returns 1 if read-only

# Mount with multiple read-only safeguards
mount -o ro,norecovery,noload /dev/sdb1 /mnt/evidence
```

**LVM volume read-only activation**:

```bash
# Activate logical volume read-only
vgchange -a y --readonly volume_group

# Mount LVM volume read-only
mount -o ro /dev/volume_group/logical_volume /mnt/evidence
```

### Forensic Disk Image Creation

Always work on copies, never original evidence:

**dd - Traditional disk imaging**:

```bash
# Create bit-for-bit copy with progress indicator
dd if=/dev/sdb of=evidence.img bs=4M status=progress conv=noerror,sync

# Create compressed image
dd if=/dev/sdb bs=4M conv=noerror,sync | gzip -c > evidence.img.gz

# Hash original and copy for verification
md5sum /dev/sdb > original.md5
md5sum evidence.img > copy.md5
```

**dcfldd - Enhanced forensic dd**:

```bash
# Install: apt install dcfldd

# Create image with built-in hashing
dcfldd if=/dev/sdb of=evidence.img hash=md5,sha256 hashlog=hashes.txt bs=4M

# Create split images (useful for large disks)
dcfldd if=/dev/sdb of=evidence.img bs=4M split=2G hashwindow=1G

# Wipe free space (not for evidence preservation, but for preparing test media)
dcfldd of=/dev/sdb pattern=00
```

**ewfacquire - Expert Witness Format (E01)**:

```bash
# Install: apt install ewf-tools

# Create forensic image in E01 format (includes metadata and compression)
ewfacquire -t evidence_disk -C "CTF Case 2025" -D "Suspicious server logs" \
    -e "investigator@ctf.local" -m removable -M logical -c best \
    -f encase6 /dev/sdb

# Verify E01 image integrity
ewfverify evidence_disk.E01

# Mount E01 image read-only
mkdir /mnt/ewf
ewfmount evidence_disk.E01 /mnt/ewf
mount -o ro /mnt/ewf/ewf1 /mnt/evidence
```

**Creating images from files or partitions**:

```bash
# Image entire directory structure
tar -czf logs_backup.tar.gz --preserve-permissions /var/log/

# Create forensic timeline with MAC times preserved
rsync -a --archive /var/log/ /evidence/logs/

# Calculate hash tree for verification
find /var/log -type f -exec sha256sum {} \; > log_hashes.txt
```

### File-Level Write Protection

**Immutable flag (Linux ext2/3/4)**:

```bash
# Make log file immutable (cannot be modified or deleted, even by root)
chattr +i /var/log/important.log

# Verify attributes
lsattr /var/log/important.log
# Output: ----i--------e----- /var/log/important.log

# Remove immutable flag (for cleanup after CTF)
chattr -i /var/log/important.log

# Append-only mode (can add, cannot modify existing content)
chattr +a /var/log/append-only.log
```

**Copy-on-write snapshots (btrfs/ZFS)**:

```bash
# Create btrfs snapshot before analysis
btrfs subvolume snapshot -r /mnt/logs /mnt/logs_snapshot

# Create ZFS snapshot
zfs snapshot tank/logs@forensic_capture

# Clone snapshot for analysis
zfs clone tank/logs@forensic_capture tank/logs_analysis
```

### Verification and Chain of Custody

**Hash verification workflow**:

```bash
# Generate cryptographic hashes before analysis
sha256sum evidence.img > evidence.sha256
md5sum evidence.img > evidence.md5

# Create detailed hash manifest
cat > manifest.txt <<EOF
Filename: evidence.img
Size: $(stat -c%s evidence.img)
MD5: $(md5sum evidence.img | cut -d' ' -f1)
SHA256: $(sha256sum evidence.img | cut -d' ' -f1)
Date: $(date -u +"%Y-%m-%d %H:%M:%S UTC")
Analyst: investigator_id
EOF

# Verify integrity after analysis
sha256sum -c evidence.sha256
```

**Timestamping evidence**:

```bash
# Record file metadata before analysis
stat evidence.img > evidence_metadata.txt

# Create forensic timeline
find /mnt/evidence -type f -printf "%T+ %p\n" > timeline.txt
```

### Common CTF Write-Blocker Scenarios

**Analyzing suspicious disk images**:

```bash
# Safe analysis workflow
1. Verify image hash matches challenge description
2. Mount read-only: mount -o ro,loop,noexec disk.img /mnt/analysis
3. Create working copy if modifications needed: cp disk.img working_copy.img
4. Never analyze directly on original
```

**Protecting live system logs during capture**:

```bash
# Capture system state without modification
# Use external media for output, never write to suspect system

# Capture running processes and network connections
ps aux > /external_media/process_list.txt
netstat -tulpn > /external_media/network_state.txt

# Copy logs to external media
rsync -av --no-modify-window /var/log/ /external_media/logs/
```

## Evidence Extraction

Evidence extraction involves retrieving log data from various storage media, file systems, and formats while maintaining forensic integrity. CTF challenges often hide logs in non-standard locations or formats.

### File System Analysis Tools

**The Sleuth Kit (TSK)** - Comprehensive file system analysis suite:

```bash
# Install: apt install sleuthkit

# List partition layout
mmls disk.img

# Examine file system details
fsstat -o 2048 disk.img  # -o specifies partition offset in sectors

# List all files and directories (including deleted)
fls -r -o 2048 disk.img

# Extract specific inode/file
icat -o 2048 disk.img 15234 > extracted_log.txt

# Search for specific filename
fls -r -o 2048 disk.img | grep -i "auth.log"

# Timeline generation (MAC times)
fls -r -m / -o 2048 disk.img > timeline.body
mactime -b timeline.body -d > timeline.csv
```

**Autopsy** - Graphical interface for TSK:

```bash
# Install: apt install autopsy
autopsy

# Access via browser: http://localhost:9999/autopsy
# Create case -> Add data source -> Analyze file system
# Useful for: visualizing file systems, keyword searches, timeline analysis
```

**Mounting various file systems**:

**ext2/ext3/ext4**:

```bash
# Mount with read-only, no journal replay
mount -o ro,noload,norecovery /dev/loop0 /mnt/evidence

# View superblock information
dumpe2fs /dev/loop0 | less

# Check for deleted inodes
debugfs /dev/loop0
debugfs: lsdel  # List deleted inodes
debugfs: quit
```

**NTFS (Windows)**:

```bash
# Install NTFS-3G: apt install ntfs-3g

# Mount NTFS read-only
mount -t ntfs-3g -o ro /dev/loop0 /mnt/evidence

# Examine NTFS metadata
ntfsinfo -m /dev/loop0

# List files including ADS (Alternate Data Streams)
ntfsls -a -l /dev/loop0
```

**FAT32/exFAT**:

```bash
# Mount FAT32 read-only
mount -t vfat -o ro /dev/loop0 /mnt/evidence

# FAT forensics with fls
fls -f fat32 -r disk.img
```

**APFS (Apple)**:

```bash
# APFS support limited on Linux
# Install apfs-fuse (third-party): apt install apfs-fuse

# Mount read-only
apfs-fuse -o ro disk.img /mnt/evidence
```

### Log File Location Discovery

**Common log locations (Linux)**:

```bash
# Standard log directories
/var/log/              # System and application logs
/var/log/journal/      # systemd journal
~/.bash_history        # User command history
~/.local/share/        # User application data
/tmp/                  # Temporary files (may contain logs)
/var/cache/            # Cached logs

# Search for log files by extension
find /mnt/evidence -type f \( -name "*.log" -o -name "*.log.*" \)

# Search for log files by content signature
find /mnt/evidence -type f -exec file {} \; | grep -i "text\|ASCII"

# Find files modified in specific timeframe
find /mnt/evidence -type f -newermt "2025-10-01" ! -newermt "2025-10-29"
```

**Windows log locations**:

```bash
# Windows Event Logs
C:\Windows\System32\winevt\Logs\*.evtx

# IIS web server logs
C:\inetpub\logs\LogFiles\

# Application-specific logs
C:\ProgramData\<application>\logs\

# Extract from mounted NTFS
find /mnt/evidence/Windows/System32/winevt/Logs -name "*.evtx"
```

### Structured Log Parsing

**systemd journal extraction**:

```bash
# View journal from disk image
journalctl --directory=/mnt/evidence/var/log/journal

# Export to JSON for analysis
journalctl --directory=/mnt/evidence/var/log/journal -o json > journal.json

# Filter by service
journalctl --directory=/mnt/evidence/var/log/journal -u ssh.service

# Filter by time range
journalctl --directory=/mnt/evidence/var/log/journal --since "2025-10-01" --until "2025-10-29"

# Extract specific fields
journalctl --directory=/mnt/evidence/var/log/journal -o json | \
    jq -r '[._SOURCE_REALTIME_TIMESTAMP, .MESSAGE] | @tsv'
```

**Windows Event Log parsing (EVTX)**:

```bash
# Install python-evtx: pip3 install python-evtx

# Convert EVTX to XML
evtx_dump.py Security.evtx > security.xml

# Parse with xmllint
xmllint --format security.xml | grep -A 10 "EventID.*4624"

# Use evtx_dump with filtering
evtx_dump.py --json Security.evtx | jq '.[] | select(.event_id == 4624)'
```

**Alternative: evtxexport (libevtx)**:

```bash
# Install: apt install libevtx-utils

# Export to human-readable format
evtxexport -f text Security.evtx

# Export to XML
evtxexport -f xml Security.evtx

# Export specific records
evtxexport -r 100-200 Security.evtx
```

### Binary Log Extraction

**syslog binary format parsing**:

```bash
# Extract structured data from binary syslog
strings /mnt/evidence/var/log/messages | less

# Better: use syslog parser
# Most syslog is text-based, but proprietary formats exist
```

**utmp/wtmp/btmp logs (user login records)**:

```bash
# Parse wtmp (login history)
last -f /mnt/evidence/var/log/wtmp

# Parse btmp (failed logins)
lastb -f /mnt/evidence/var/log/btmp

# Parse utmp (current users)
who -H /mnt/evidence/var/run/utmp

# Dump raw binary content
utmpdump /mnt/evidence/var/log/wtmp
```

**lastlog (last login times)**:

```bash
# Display last login information
lastlog -u username -f /mnt/evidence/var/log/lastlog
```

### Database Log Extraction

**SQLite databases** (common in application logs):

```bash
# Identify SQLite databases
file /mnt/evidence/app/data/*.db
strings database.db | grep -i "create table"

# Extract tables
sqlite3 database.db ".tables"
sqlite3 database.db "SELECT * FROM logs;" > extracted_logs.txt

# Export entire database
sqlite3 database.db .dump > database_dump.sql

# Recover deleted SQLite entries (see Deleted Log Recovery section)
```

**MySQL/MariaDB logs**:

```bash
# Binary logs location
/var/lib/mysql/mysql-bin.*

# Parse binary logs
mysqlbinlog /mnt/evidence/var/lib/mysql/mysql-bin.000001 > binlog.sql

# Error logs
cat /mnt/evidence/var/log/mysql/error.log

# Query logs (if enabled)
cat /mnt/evidence/var/lib/mysql/hostname.log
```

**PostgreSQL logs**:

```bash
# Log locations
/var/lib/postgresql/*/main/log/
/var/log/postgresql/

# Parse PostgreSQL CSV logs
cat /mnt/evidence/var/log/postgresql/postgresql-*.csv | csvtool readable -
```

### Memory-Based Log Extraction

**Analyzing memory dumps for logs** [Inference]:

```bash
# Install Volatility 3: pip3 install volatility3

# List processes in memory dump
vol -f memory.dmp windows.pslist

# Extract command line arguments (may contain logging commands)
vol -f memory.dmp windows.cmdline

# Search for log strings in memory
strings -e l memory.dmp | grep -E "(ERROR|WARN|INFO|DEBUG)" > memory_logs.txt

# Targeted string extraction
strings memory.dmp | grep -i "password\|token\|flag{"
```

### Network Packet Log Extraction

**PCAP/PCAPNG analysis**:

```bash
# Extract HTTP objects (may include logs transmitted via HTTP)
tcpflow -r capture.pcap -o output/

# Extract specific protocol data
tshark -r capture.pcap -Y "http" -T fields -e http.request.uri

# Search for log patterns in packet payloads
strings capture.pcap | grep -E "^\[.*\].*ERROR"

# Export specific TCP streams
tshark -r capture.pcap -z follow,tcp,ascii,0 > stream0.txt
```

**Extracting logs from network traffic**:

```bash
# Syslog over network (UDP/514)
tshark -r capture.pcap -Y "syslog" -T fields -e syslog.msg

# HTTP access logs transmitted to SIEM
tshark -r capture.pcap -Y "http.request.method == POST && http.request.uri contains '/log'" \
    -T fields -e http.file_data
```

### Container and Virtualization Logs

**Docker container logs**:

```bash
# Extract from disk image
cat /mnt/evidence/var/lib/docker/containers/*/.*-json.log

# Parse Docker JSON logs
jq -r '[.time, .log] | @csv' container-json.log
```

**VMware virtual machine logs**:

```bash
# VMware logs location
/var/log/vmware/*.log
*.vmx.log (in VM directory)

# Extract from VMDK
vmware-mount /mnt/evidence/disk.vmdk /mnt/vmware
# Then access mounted file system
```

### Artifact Extraction Tools

**bulk_extractor** - High-speed artifact extraction:

```bash
# Install: apt install bulk-extractor

# Extract artifacts including logs, emails, URLs
bulk_extractor -o output_dir disk.img

# View extracted artifacts
cat output_dir/email.txt
cat output_dir/url.txt
cat output_dir/telephone.txt

# Search for specific patterns with custom regex
bulk_extractor -o output_dir -e wordlist -F wordlist.txt disk.img
```

**foremost** - File carving based on headers/footers:

```bash
# Install: apt install foremost

# Carve files from disk image
foremost -t all -i disk.img -o carved_files/

# Carve specific types (e.g., archives that might contain logs)
foremost -t zip,tar,gz -i disk.img -o archives/

# Check carved files
ls -lh carved_files/
```

**scalpel** - Advanced file carving:

```bash
# Install: apt install scalpel

# Configure file types in /etc/scalpel/scalpel.conf
# Uncomment desired file types

# Run scalpel
scalpel disk.img -o carved_output/

# More precise than foremost for fragmented files
```

## Deleted Log Recovery

Deleted log recovery is crucial in CTF scenarios where attackers attempt to cover their tracks. Multiple techniques exist depending on file system, deletion method, and time elapsed since deletion.

### Understanding Deletion Mechanisms

**File system deletion behavior**:

- Linux (ext4): Inode marked as free, data blocks marked as available but not immediately overwritten
- NTFS: MFT entry marked as deleted, clusters marked as available
- FAT32: First byte of filename changed to 0xE5
- SSD TRIM: May immediately destroy deleted data [Unverified - depends on TRIM implementation]

**Immediate vs delayed recovery**:

```bash
# Stop all disk activity immediately after accidental deletion
# Every write operation may overwrite deleted data

# Remount read-only to prevent overwrites
mount -o remount,ro /dev/sda1
```

### File System-Level Recovery

**extundelete** - ext3/ext4 recovery tool:

```bash
# Install: apt install extundelete

# List deleted files
extundelete /dev/sdb1 --ls --inode 2

# Recover all deleted files
extundelete /dev/sdb1 --restore-all

# Recover files from specific directory
extundelete /dev/sdb1 --restore-directory /var/log

# Recover files deleted after specific time
extundelete /dev/sdb1 --after $(date -d "2025-10-01" +%s) --restore-all

# Recovered files appear in ./RECOVERED_FILES/
```

**TestDisk** - Multi-file system recovery:

```bash
# Install: apt install testdisk

# Interactive mode
testdisk disk.img

# Menu navigation:
# 1. Select disk/image
# 2. Choose partition type (Intel for most Linux/Windows)
# 3. Select partition -> Advanced -> Undelete
# 4. Navigate and select files to recover
# 5. Press 'C' to copy to safe location
```

**PhotoRec** - File carving (despite name, recovers all file types):

```bash
# Install: apt install testdisk (includes photorec)

# Interactive mode
photorec disk.img

# Carves files based on signatures
# Useful when file system metadata is damaged
# Recovers to recup_dir.1/, recup_dir.2/, etc.

# Command-line mode for automation
photorec /d recovery_output/ /cmd disk.img search
```

**debugfs** - Low-level ext2/ext3/ext4 debugging:

```bash
# Open file system in debugfs
debugfs -R "lsdel" /dev/sdb1

# List deleted inodes with details
debugfs -R "lsdel" /dev/sdb1 | awk '$1 ~ /[0-9]+/ {print}'

# Dump specific deleted inode
debugfs -R "dump <15234> /tmp/recovered_file" /dev/sdb1

# Example workflow:
debugfs /dev/sdb1
debugfs: lsdel
debugfs: ls -d /var/log  # Find inode numbers
debugfs: logdump -i <inode>  # View inode changes
debugfs: dump <inode> /tmp/recovered.log
debugfs: quit
```

### NTFS Deleted File Recovery

**ntfsundelete** - NTFS recovery tool:

```bash
# Install: apt install ntfs-3g

# Scan for deleted files
ntfsundelete /dev/sdb1

# Recover all files
ntfsundelete -u -m "*" /dev/sdb1

# Recover specific file by name
ntfsundelete -u -m "access.log" /dev/sdb1

# Recover files by percentage intact
ntfsundelete -u -p 90 /dev/sdb1  # Recover files 90%+ intact

# Output to directory
ntfsundelete -u -d /recovery_output/ /dev/sdb1
```

**NTFS MFT parsing**:

```bash
# Install analyzeMFT: pip3 install analyzeMFT

# Parse MFT (includes deleted entries)
analyzeMFT.py -f /mnt/evidence/\$MFT -o mft_analysis.csv

# Filter deleted files
grep -i "deleted" mft_analysis.csv | grep -i ".log"
```

### SQLite Database Recovery

**SQLite deleted record recovery**:

```bash
# Install sqlite3: apt install sqlite3

# Dump entire database including free pages
sqlite3 database.db ".dump" > full_dump.sql

# Strings-based recovery (crude but effective)
strings database.db | grep -E "^INSERT INTO|^CREATE TABLE"

# Identify table structure
sqlite3 database.db ".schema logs"

# Check for rollback journal or WAL files
ls -la database.db-journal database.db-wal
```

**sqlparse** - Advanced SQLite forensics:

```bash
# Install: pip3 install sqlparse

# Parse deleted records from freelist
python3 << 'EOF'
import sqlite3
conn = sqlite3.connect('database.db')
cursor = conn.cursor()

# Access freelist pages (contains deleted data)
cursor.execute("PRAGMA freelist_count;")
print(f"Free pages: {cursor.fetchone()[0]}")

# Dump page content
cursor.execute("PRAGMA page_count;")
total_pages = cursor.fetchone()[0]

with open('deleted_data.txt', 'wb') as f:
    for page in range(1, total_pages + 1):
        try:
            cursor.execute(f"PRAGMA page_size;")
            page_size = cursor.fetchone()[0]
            # Read raw page (requires low-level access)
        except:
            pass
conn.close()
EOF
```

**undark** - SQLite deleted record recovery tool:

```bash
# Install: git clone https://github.com/inflex/undark && cd undark && make

# Recover deleted entries
./undark -i database.db --freespace

# Verbose output with offset information
./undark -i database.db --freespace -v

# Output to file
./undark -i database.db --freespace > recovered_records.txt
```

### Journal and Temporary File Recovery

**systemd journal recovery**:

```bash
# Journal files location
/var/log/journal/*/

# Verify journal integrity
journalctl --verify

# Recover from corrupted journal
journalctl --file=/mnt/evidence/var/log/journal/*/system.journal

# Extract even with corruption (may produce partial results)
journalctl --file=corrupted.journal -o json | jq -r '.MESSAGE' 2>/dev/null
```

**Recovering from swap space**:

```bash
# Swap may contain log data paged out from memory
# Identify swap partition
swapon -s

# Dump swap space (after unmounting)
swapoff /dev/sda2
dd if=/dev/sda2 of=swap.dump bs=4M

# Search for log patterns
strings swap.dump | grep -E "^\[.*\].*" > recovered_logs.txt

# Extract specific strings
strings swap.dump | grep -i "flag{\|password\|token"
```

**Temporary file recovery**:

```bash
# /tmp directory often contains rotated or temporary logs
# May persist even after deletion if not cleared on boot

# Search for log-like files in temp
find /tmp -name "*.log" -o -name "*.tmp" -o -name ".*log"

# vim swap files (may contain log edits)
find ~ -name ".*.swp" -o -name ".*.swx"

# Recover from vim swap
vim -r /tmp/.logfile.swp
```

### Memory-Based Log Recovery

**Analyzing process memory for logs**:

```bash
# Dump process memory (requires gcore from gdb)
gcore -o process_dump <pid>

# Search memory dump for log patterns
strings process_dump.* | grep -E "\[(ERROR|INFO|WARN)\]"

# Extract specific log formats
strings process_dump.* | grep -oP '\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}.*'
```

**Using /proc for live recovery**:

```bash
# Read deleted file if process still has handle open
lsof | grep deleted | grep ".log"
# Note the PID and FD (file descriptor)

# Recover via /proc
cat /proc/<PID>/fd/<FD> > recovered.log

# Example: If syslog process (PID 1234) has deleted auth.log open at FD 5
cat /proc/1234/fd/5 > recovered_auth.log
```

### Advanced Recovery Techniques

**File carving with custom signatures**:

```bash
# Create custom scalpel configuration for log files
cat >> /etc/scalpel/scalpel.conf << 'EOF'
# Common log header patterns
txt	y	10000000	\[20	\n	# Syslog format
txt	y	10000000	{"timestamp"	}	# JSON logs
txt	y	10000000	<Event	</Event>	# XML logs
EOF

# Run with custom config
scalpel -c /etc/scalpel/scalpel.conf -o carved/ disk.img
```

**Binary log reconstruction**:

```bash
# When log files are partially overwritten, reconstruct readable portions
dd if=/dev/sdb1 bs=512 skip=START_SECTOR count=NUM_SECTORS | \
    strings | \
    grep -E "^\[.*\]" > partial_recovery.txt
```

**Analyzing file slack space** [Inference]:

```bash
# Slack space between end of file and end of allocated block
# May contain remnants of deleted logs

# View slack space (requires bmap and dd)
bmap --mode slack /dev/sdb1 > slack_sectors.txt

# Extract slack space sectors
while read sector; do
    dd if=/dev/sdb1 bs=512 skip=$sector count=1 2>/dev/null | strings
done < slack_sectors.txt
```

### Recovery Validation and Analysis

**Verify recovered log integrity**:

```bash
# Check for valid log format
file recovered.log

# Validate timestamps are sequential
grep -oP '\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}' recovered.log | sort -c

# Check for corruption patterns
grep -E "[\x00-\x08\x0B-\x0C\x0E-\x1F]" recovered.log  # Non-printable chars
```

**Timeline reconstruction from fragments**:

```bash
# Combine multiple recovered fragments
cat recovered_*.log | sort -t' ' -k1,1 > timeline.log

# Remove duplicates (may occur with overlapping recovery)
sort timeline.log | uniq > cleaned_timeline.log

# Fill gaps by correlating with other sources
comm -23 <(sort known_events.log) <(sort recovered.log)
```

### CTF-Specific Recovery Scenarios

**Scenario: Logs deleted with shred/srm**:

```bash
# shred overwrites data multiple times - recovery is difficult or impossible
# Check for:
# 1. Backup copies
find /var/backups -name "*log*"

# 2. Remote syslog servers (logs may exist remotely)
grep "@@" /etc/rsyslog.conf

# 3. Process memory if deletion just occurred
ps aux | grep log
cat /proc/<PID>/fd/* 2>/dev/null
```

**Scenario: Timestamp manipulation detection**:

```bash
# Attacker modified log timestamps to hide activity
# Compare different timestamp sources:

# File system MAC times
stat suspicious.log

# Embedded log timestamps
head -n1 suspicious.log | grep -oP '\d{4}-\d{2}-\d{2}'

# Journal metadata
journalctl --file=system.journal | head -n1

# Discrepancies suggest tampering
```

**Scenario: Partial log rotation interference**:

```bash
# Attacker interrupted log rotation, causing fragmented logs

# Find all log rotation artifacts
ls -la /var/log/*.log* /var/log/*.gz

# Reconstruct timeline
for f in /var/log/syslog*; do
    echo "=== $f ==="
    zcat -f $f | head -n1
    zcat -f $f | tail -n1
done

# Merge chronologically
for f in $(ls -t /var/log/syslog*); do zcat -f $f; done | sort
```

## Important Related Topics

For comprehensive forensic log analysis, consider studying:

- **Timeline Analysis**: Creating super-timelines with log2timeline/plaso across multiple artifact sources
- **Log Integrity Verification**: Detecting tampered logs using hash chains, signatures, and anomaly detection
- **Steganography Detection**: Finding logs hidden in image files, whitespace, or other non-obvious locations
- **Anti-Forensics Techniques**: Understanding common log destruction and obfuscation methods used by adversaries
- **Chain of Custody Documentation**: Proper evidence handling, documentation, and reporting for CTF write-ups

---

# CTF-Specific Techniques

CTF challenges often embed flags, clues, and hidden data within logs using creative encoding, steganography, and obfuscation techniques. Recognizing these patterns requires systematic analysis beyond standard log review.

## Flag Format Recognition

Flag format recognition involves identifying competition-specific flag patterns within log data through regex matching, encoding detection, and context analysis.

### Common Flag Formats

**Standard CTF flag patterns:**

```bash
# Generic flag{...} format
grep -oE 'flag\{[^}]+\}' logfile.txt

# CTF-specific formats
grep -oE 'CTF\{[^}]+\}' logfile.txt
grep -oE 'HTB\{[^}]+\}' logfile.txt  # HackTheBox
grep -oE 'picoCTF\{[^}]+\}' logfile.txt

# Case-insensitive search
grep -ioE 'flag\{[^}]+\}' logfile.txt

# With surrounding context
grep -ioE '.{0,20}flag\{[^}]+\}.{0,20}' logfile.txt
```

**Custom format detection:**

```bash
# Alphanumeric flags with specific lengths
grep -oE '\b[A-Za-z0-9]{32}\b' logfile.txt  # 32-char flags (common MD5 length)
grep -oE '\b[A-F0-9]{40}\b' logfile.txt     # 40-char hex (SHA1 length)

# UUID-style flags
grep -oE '[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}' logfile.txt

# Base64-encoded flags (typical padding patterns)
grep -oE '\b[A-Za-z0-9+/]{20,}={0,2}\b' logfile.txt
```

### Multi-Encoding Flag Detection

**Hexadecimal encoded flags:**

```bash
# Extract hex strings and decode
grep -oE '([0-9a-fA-F]{2}[[:space:]]?){10,}' logfile.txt | tr -d ' ' | xxd -r -p

# Alternative decoding
echo "666c61677b746573747d" | xxd -r -p

# Python for batch hex decoding
python3 -c "
import re
import binascii
with open('logfile.txt', 'r') as f:
    content = f.read()
    hex_patterns = re.findall(r'([0-9a-fA-F]{2}[\\s]?){10,}', content)
    for match in hex_patterns:
        try:
            decoded = binascii.unhexlify(match.replace(' ', ''))
            if b'flag' in decoded.lower():
                print(decoded.decode('utf-8', errors='ignore'))
        except:
            pass
"
```

**Base64 encoded flags:**

```bash
# Find and decode Base64 strings
grep -oE '\b[A-Za-z0-9+/]{20,}={0,2}\b' logfile.txt | while read line; do
    decoded=$(echo "$line" | base64 -d 2>/dev/null)
    if echo "$decoded" | grep -qi "flag"; then
        echo "Found: $decoded (from $line)"
    fi
done

# URL-safe Base64 (base64url)
grep -oE '\b[A-Za-z0-9_-]{20,}\b' logfile.txt | while read line; do
    # Convert base64url to standard base64
    echo "$line" | tr '_-' '/+' | base64 -d 2>/dev/null | grep -i "flag"
done
```

**ROT13/Caesar cipher:**

```bash
# ROT13 decoding
grep -oE '\b[A-Za-z]{20,}\b' logfile.txt | tr 'A-Za-z' 'N-ZA-Mn-za-m'

# All Caesar shifts (brute force)
echo "synt{grfg}" | for i in {1..25}; do
    echo "Shift $i: $(echo 'synt{grfg}' | tr "$(echo {a..z} {A..Z} | tr -d ' ')" "$(echo {a..z} {A..Z} | tr -d ' ' | sed "s/^.\{$i\}\(.*\)/\1/")$(echo {a..z} {A..Z} | tr -d ' ' | sed "s/\(^.\{$i\}\).*/\1/")")"
done
```

**URL encoding:**

```bash
# Decode URL-encoded strings
grep -oE '%[0-9A-Fa-f]{2}' logfile.txt | while read line; do
    echo -e "$(echo "$line" | sed 's/%/\\x/g')"
done | tr -d '\n'

# Python URL decode
python3 -c "
import urllib.parse
import re
with open('logfile.txt', 'r') as f:
    content = f.read()
    # Find URL-encoded patterns
    encoded = re.findall(r'(?:%[0-9A-Fa-f]{2}){4,}', content)
    for match in encoded:
        decoded = urllib.parse.unquote(match)
        if 'flag' in decoded.lower():
            print(decoded)
"
```

### Advanced Format Recognition

**Multi-layer encoding detection:**

```python
#!/usr/bin/env python3
"""
Multi-layer encoding detector for CTF flags
"""
import re
import base64
import binascii
import urllib.parse
import codecs

class FlagDecoder:
    def __init__(self, data):
        self.data = data
        self.findings = []
    
    def try_base64(self, text):
        """Attempt Base64 decoding"""
        try:
            # Standard Base64
            decoded = base64.b64decode(text)
            if self._is_printable(decoded):
                return decoded.decode('utf-8', errors='ignore')
        except:
            pass
        
        try:
            # Base64 URL-safe
            text_fixed = text.replace('-', '+').replace('_', '/')
            # Add padding if needed
            padding = 4 - (len(text_fixed) % 4)
            if padding != 4:
                text_fixed += '=' * padding
            decoded = base64.b64decode(text_fixed)
            if self._is_printable(decoded):
                return decoded.decode('utf-8', errors='ignore')
        except:
            pass
        return None
    
    def try_hex(self, text):
        """Attempt hex decoding"""
        # Remove spaces and common delimiters
        text_clean = text.replace(' ', '').replace(':', '').replace('-', '')
        if len(text_clean) % 2 != 0:
            return None
        try:
            decoded = binascii.unhexlify(text_clean)
            if self._is_printable(decoded):
                return decoded.decode('utf-8', errors='ignore')
        except:
            pass
        return None
    
    def try_url_decode(self, text):
        """Attempt URL decoding"""
        try:
            decoded = urllib.parse.unquote(text)
            if decoded != text:  # Something was decoded
                return decoded
        except:
            pass
        return None
    
    def try_rot13(self, text):
        """Apply ROT13"""
        try:
            return codecs.decode(text, 'rot_13')
        except:
            return None
    
    def _is_printable(self, data):
        """Check if data is mostly printable ASCII"""
        if isinstance(data, bytes):
            printable_count = sum(32 <= b < 127 for b in data)
            return printable_count / len(data) > 0.8 if data else False
        return True
    
    def detect_flag_format(self, text):
        """Detect common flag formats"""
        patterns = [
            (r'flag\{[^}]+\}', 'Generic flag{}'),
            (r'CTF\{[^}]+\}', 'CTF{}'),
            (r'HTB\{[^}]+\}', 'HackTheBox{}'),
            (r'\b[A-F0-9]{32}\b', 'MD5 hash format'),
            (r'\b[A-F0-9]{40}\b', 'SHA1 hash format'),
        ]
        
        for pattern, description in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                return matches, description
        return None, None
    
    def recursive_decode(self, text, depth=0, max_depth=5):
        """Recursively try decoding with multiple encodings"""
        if depth > max_depth:
            return
        
        # Check for flag format at current level
        matches, desc = self.detect_flag_format(text)
        if matches:
            self.findings.append({
                'depth': depth,
                'matches': matches,
                'description': desc,
                'decoded_text': text
            })
        
        # Try each decoding method
        decoders = [
            ('Base64', self.try_base64),
            ('Hex', self.try_hex),
            ('URL', self.try_url_decode),
            ('ROT13', self.try_rot13),
        ]
        
        for method, decoder in decoders:
            result = decoder(text)
            if result and result != text:
                # Found a valid decoding, recurse
                self.recursive_decode(result, depth + 1, max_depth)
    
    def scan_file(self, filepath):
        """Scan file for encoded flags"""
        with open(filepath, 'r', errors='ignore') as f:
            content = f.read()
        
        # Look for suspicious patterns
        suspicious_patterns = [
            r'\b[A-Za-z0-9+/]{20,}={0,2}\b',  # Base64-like
            r'\b(?:[0-9a-fA-F]{2}[:\s-]?){8,}\b',  # Hex-like
            r'(?:%[0-9A-Fa-f]{2}){4,}',  # URL-encoded
        ]
        
        for pattern in suspicious_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                self.recursive_decode(match)
        
        return self.findings

if __name__ == "__main__":
    import sys
    if len(sys.argv) < 2:
        print("Usage: python3 flag_decoder.py <logfile>")
        sys.exit(1)
    
    decoder = FlagDecoder(None)
    findings = decoder.scan_file(sys.argv[1])
    
    if findings:
        print(f"\n=== Found {len(findings)} potential flags ===\n")
        for i, finding in enumerate(findings, 1):
            print(f"[{i}] Depth: {finding['depth']} | Type: {finding['description']}")
            print(f"    Matches: {finding['matches']}")
            if finding['depth'] > 0:
                print(f"    Decoded text: {finding['decoded_text'][:100]}...")
            print()
    else:
        print("No flags found.")
```

**Context-based flag extraction:**

```bash
# Extract flags from HTTP request parameters
grep -oE 'flag=[^&\s]+' logfile.txt | cut -d'=' -f2 | while read encoded; do
    echo "Original: $encoded"
    echo "URL decoded: $(echo "$encoded" | python3 -c "import sys, urllib.parse; print(urllib.parse.unquote(sys.stdin.read()))")"
    echo "---"
done

# Extract from User-Agent headers
awk -F'"' '/User-Agent/ {print $6}' logfile.txt | grep -i flag

# Extract from custom headers
grep -E "X-Flag|X-CTF|X-Secret" logfile.txt
```

## Hidden Data in Logs

Hidden data in logs can be embedded through field manipulation, whitespace encoding, timing channels, or log injection techniques.

### Whitespace and Invisible Character Analysis

**Detecting hidden whitespace patterns:**

```bash
# Show all whitespace characters
cat -A logfile.txt | grep -E '\^I|\$'

# Count different whitespace types
sed -n l logfile.txt | grep -oE '\t| ' | sort | uniq -c

# Extract binary from spaces/tabs (width-based encoding)
# Assuming tab=1, space=0
cat logfile.txt | sed 's/\t/1/g; s/ /0/g' | tr -d '\n' | fold -w8 | while read byte; do
    echo "obase=16; ibase=2; $byte" | bc
done | xxd -r -p
```

**Unicode zero-width character detection:**

```python
#!/usr/bin/env python3
"""
Detect zero-width Unicode characters used for steganography
"""
import sys

ZERO_WIDTH_CHARS = {
    '\u200B': 'ZERO WIDTH SPACE',
    '\u200C': 'ZERO WIDTH NON-JOINER',
    '\u200D': 'ZERO WIDTH JOINER',
    '\uFEFF': 'ZERO WIDTH NO-BREAK SPACE',
    '\u180E': 'MONGOLIAN VOWEL SEPARATOR',
}

def detect_zero_width(filepath):
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()
    
    findings = []
    for line_num, line in enumerate(content.split('\n'), 1):
        for char in ZERO_WIDTH_CHARS:
            if char in line:
                positions = [i for i, c in enumerate(line) if c == char]
                findings.append({
                    'line': line_num,
                    'char': ZERO_WIDTH_CHARS[char],
                    'count': len(positions),
                    'positions': positions
                })
    
    return findings

def extract_binary_from_zwc(filepath):
    """Extract binary data from zero-width characters"""
    # Common encoding: \u200B = 0, \u200C = 1
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()
    
    binary = content.replace('\u200B', '0').replace('\u200C', '1')
    # Remove non-binary characters
    binary = ''.join(c for c in binary if c in '01')
    
    if len(binary) % 8 == 0 and binary:
        # Convert binary to text
        text = ''.join(chr(int(binary[i:i+8], 2)) for i in range(0, len(binary), 8))
        return text
    return None

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python3 zwc_detector.py <file>")
        sys.exit(1)
    
    findings = detect_zero_width(sys.argv[1])
    if findings:
        print(f"\n=== Zero-Width Characters Detected ===\n")
        for f in findings:
            print(f"Line {f['line']}: {f['char']} ({f['count']} occurrences)")
    
    extracted = extract_binary_from_zwc(sys.argv[1])
    if extracted:
        print(f"\n=== Extracted Data ===\n{extracted}")
```

### Field-Based Data Hiding

**Least significant bit (LSB) in numeric fields:**

```bash
# Extract LSBs from status codes
awk '{print $9}' /var/log/apache2/access.log | while read code; do
    echo -n $((code & 1))
done | fold -w8 | while read byte; do
    printf "\\x$(printf '%x' $((2#$byte)))"
done

# Extract LSBs from timestamps
awk '{print $4}' /var/log/apache2/access.log | grep -oE '[0-9]{2}:[0-9]{2}:[0-9]{2}' | \
    tr -d ':' | while read time; do
        echo -n $((10#${time: -1} & 1))
    done | fold -w8
```

**ASCII value manipulation:**

```python
#!/usr/bin/env python3
"""
Extract hidden data from ASCII value differences
"""
import re

def extract_from_ascii_delta(logfile, field_pattern):
    """
    Extract data from differences between expected and actual ASCII values
    Example: 'A' (65) instead of 'a' (97) = difference of 32
    """
    with open(logfile, 'r') as f:
        content = f.read()
    
    matches = re.findall(field_pattern, content)
    
    # Analyze character frequency
    deltas = []
    for match in matches:
        for char in match:
            if char.isalpha():
                # Compare with lowercase version
                expected = ord(char.lower())
                actual = ord(char)
                delta = actual - expected
                if delta != 0:
                    deltas.append(delta)
    
    return deltas

def extract_from_field_length(logfile):
    """Extract data based on field length variations"""
    with open(logfile, 'r') as f:
        lines = f.readlines()
    
    lengths = []
    for line in lines:
        fields = line.split()
        if len(fields) > 6:  # Example: URL field
            url = fields[6]
            lengths.append(len(url))
    
    # Convert lengths to potential encoding
    # E.g., length mod 256 = ASCII character
    message = ''.join(chr(l % 256) for l in lengths if 32 <= l % 256 < 127)
    return message

if __name__ == "__main__":
    import sys
    if len(sys.argv) < 2:
        print("Usage: python3 field_extractor.py <logfile>")
        sys.exit(1)
    
    result = extract_from_field_length(sys.argv[1])
    if result:
        print(f"Extracted from field lengths: {result}")
```

### Timing Channel Analysis

**Inter-arrival time analysis:**

```python
#!/usr/bin/env python3
"""
Extract data from timing channels in logs
"""
from datetime import datetime
import re

def extract_from_timing(logfile, timestamp_pattern):
    """
    Extract data from time differences between events
    """
    timestamps = []
    
    with open(logfile, 'r') as f:
        for line in f:
            match = re.search(timestamp_pattern, line)
            if match:
                # Parse timestamp (adjust format as needed)
                try:
                    ts_str = match.group(0)
                    # Example Apache format: [01/Jan/2025:12:34:56 +0000]
                    ts = datetime.strptime(ts_str.strip('[]').split()[0], 
                                          '%d/%b/%Y:%H:%M:%S')
                    timestamps.append(ts)
                except:
                    pass
    
    # Calculate inter-arrival times in milliseconds
    if len(timestamps) < 2:
        return None
    
    deltas = []
    for i in range(1, len(timestamps)):
        delta = (timestamps[i] - timestamps[i-1]).total_seconds() * 1000
        deltas.append(int(delta))
    
    # Method 1: Delta mod 256 = ASCII
    message1 = ''.join(chr(d % 256) for d in deltas if 32 <= d % 256 < 127)
    
    # Method 2: Delta > threshold = 1, else = 0
    threshold = sum(deltas) / len(deltas)
    binary = ''.join('1' if d > threshold else '0' for d in deltas)
    message2 = ''.join(chr(int(binary[i:i+8], 2)) 
                      for i in range(0, len(binary) - 7, 8)
                      if 32 <= int(binary[i:i+8], 2) < 127)
    
    return {
        'method1_ascii_mod': message1,
        'method2_binary': message2,
        'delta_stats': {
            'count': len(deltas),
            'mean': sum(deltas) / len(deltas),
            'min': min(deltas),
            'max': max(deltas)
        }
    }

if __name__ == "__main__":
    import sys
    if len(sys.argv) < 2:
        print("Usage: python3 timing_extractor.py <logfile>")
        sys.exit(1)
    
    # Apache log timestamp pattern
    pattern = r'\[\d{2}/\w{3}/\d{4}:\d{2}:\d{2}:\d{2} [+-]\d{4}\]'
    
    result = extract_from_timing(sys.argv[1], pattern)
    if result:
        print(f"\n=== Timing Channel Analysis ===")
        print(f"Delta stats: {result['delta_stats']}")
        print(f"\nMethod 1 (ASCII mod): {result['method1_ascii_mod'][:100]}")
        print(f"Method 2 (Binary threshold): {result['method2_binary'][:100]}")
```

### Log Injection Pattern Detection

**Detecting injected data:**

```bash
# Unusual field counts (extra fields may contain hidden data)
awk '{print NF}' logfile.txt | sort | uniq -c

# Extract abnormal fields
awk 'NF > 10' logfile.txt  # Assuming normal logs have <10 fields

# Look for comment-style injections
grep -E '#|//|<!--' logfile.txt

# Detect null-byte separated data
grep -a $'\x00' logfile.txt | od -A x -t x1z -v
```

**Multi-line log analysis (data split across entries):**

```python
#!/usr/bin/env python3
"""
Reconstruct data split across multiple log entries
"""
import re

def reconstruct_split_data(logfile, marker_pattern):
    """
    Reconstruct data split with sequence markers
    Example: part1_of_3, part2_of_3, part3_of_3
    """
    parts = {}
    
    with open(logfile, 'r') as f:
        for line in f:
            # Look for sequence markers
            match = re.search(r'part(\d+)_of_(\d+)', line, re.IGNORECASE)
            if match:
                part_num = int(match.group(1))
                total_parts = int(match.group(2))
                
                # Extract data after marker
                data_match = re.search(r'part\d+_of_\d+[:\s]+([^\s]+)', line, re.IGNORECASE)
                if data_match:
                    parts[part_num] = data_match.group(1)
    
    # Reconstruct in order
    if parts:
        max_part = max(parts.keys())
        reconstructed = ''.join(parts.get(i, '') for i in range(1, max_part + 1))
        return reconstructed
    
    return None

def extract_sequential_fields(logfile, field_index):
    """Extract specific field from sequential log entries"""
    data = []
    
    with open(logfile, 'r') as f:
        for line in f:
            fields = line.split()
            if len(fields) > field_index:
                data.append(fields[field_index])
    
    # Concatenate and try to decode
    concatenated = ''.join(data)
    
    # Try different decodings
    results = {}
    
    # As hex
    try:
        results['hex'] = bytes.fromhex(concatenated).decode('utf-8', errors='ignore')
    except:
        pass
    
    # As base64
    try:
        import base64
        results['base64'] = base64.b64decode(concatenated).decode('utf-8', errors='ignore')
    except:
        pass
    
    return results

if __name__ == "__main__":
    import sys
    if len(sys.argv) < 2:
        print("Usage: python3 reconstruct_data.py <logfile>")
        sys.exit(1)
    
    reconstructed = reconstruct_split_data(sys.argv[1], r'part\d+')
    if reconstructed:
        print(f"Reconstructed data: {reconstructed}")
    
    sequential = extract_sequential_fields(sys.argv[1], 0)
    if sequential:
        print(f"\nSequential field extraction:")
        for method, data in sequential.items():
            if data and 'flag' in data.lower():
                print(f"  {method}: {data}")
```

## Steganography in Logs

Steganography in logs involves hiding data within legitimate-looking log content using advanced encoding, format manipulation, or cryptographic techniques.

### Image-Based Steganography in Logs

**Extracting embedded images from logs:**

```bash
# Find base64-encoded images
grep -oE 'data:image/[a-z]+;base64,[A-Za-z0-9+/]+=*' logfile.txt | \
    cut -d',' -f2 | base64 -d > extracted_image.png

# Extract images from hex dumps in logs
grep -oE '([0-9a-fA-F]{2}[[:space:]]?){100,}' logfile.txt | \
    tr -d ' ' | xxd -r -p > extracted_data.bin

# Check file type
file extracted_data.bin

# Use steghide if PNG/JPG found
steghide extract -sf extracted_image.png
```

**Analyzing extracted images:**

```bash
# Install steganalysis tools
apt-get install steghide stegdetect zsteg outguess

# PNG analysis with zsteg
zsteg extracted_image.png --all

# JPEG analysis
steghide info extracted_image.jpg
outguess -r extracted_image.jpg output.txt

# Check LSB planes
zsteg extracted_image.png -a -E b1,rgb,lsb

# Binwalk for embedded files
binwalk -e extracted_image.png
```

### Text-Based Steganography Techniques

**Acrostic analysis (first letters of lines):**

```bash
# Extract first character of each line
awk '{print substr($0,1,1)}' logfile.txt | tr -d '\n'

# First character of specific fields
awk '{print substr($7,1,1)}' logfile.txt | tr -d '\n'

# Last character of each line
awk '{print substr($0,length($0),1)}' logfile.txt | tr -d '\n'
```

**Word spacing analysis:**

```python
#!/usr/bin/env python3
"""
Extract data from word spacing patterns
"""
import re

def analyze_word_spacing(logfile):
    """
    Analyze spacing between words for hidden data
    Single space = 0, Double space = 1
    """
    with open(logfile, 'r') as f:
        content = f.read()
    
    # Find patterns of spaces
    binary = ''
    lines = content.split('\n')
    
    for line in lines:
        # Count consecutive spaces
        spaces = re.findall(r' +', line)
        for space in spaces:
            if len(space) == 1:
                binary += '0'
            elif len(space) == 2:
                binary += '1'
            else:
                # Multiple spaces might encode more data
                binary += bin(len(space))[2:].zfill(3)
    
    # Convert binary to ASCII
    if len(binary) >= 8:
        message = ''
        for i in range(0, len(binary) - 7, 8):
            byte = binary[i:i+8]
            char_code = int(byte, 2)
            if 32 <= char_code < 127:
                message += chr(char_code)
        
        return message
    
    return None

def analyze_line_length_encoding(logfile):
    """Extract data from line length variations"""
    with open(logfile, 'r') as f:
        lines = f.readlines()
    
    # Remove empty lines
    lines = [l.rstrip() for l in lines if l.strip()]
    
    lengths = [len(l) for l in lines]
    
    # Method 1: Even length = 0, Odd length = 1
    binary = ''.join('0' if l % 2 == 0 else '1' for l in lengths)
    message1 = ''
    for i in range(0, len(binary) - 7, 8):
        byte = binary[i:i+8]
        char_code = int(byte, 2)
        if 32 <= char_code < 127:
            message1 += chr(char_code)
    
    # Method 2: Length mod 256 = ASCII
    message2 = ''.join(chr(l % 256) for l in lengths if 32 <= l % 256 < 127)
    
    return {
        'binary_parity': message1,
        'length_mod': message2
    }

if __name__ == "__main__":
    import sys
    if len(sys.argv) < 2:
        print("Usage: python3 text_stego.py <logfile>")
        sys.exit(1)
    
    spacing_result = analyze_word_spacing(sys.argv[1])
    if spacing_result:
        print(f"Spacing analysis: {spacing_result}")
    
    length_result = analyze_line_length_encoding(sys.argv[1])
    print(f"\nLine length analysis:")
    print(f"  Binary parity: {length_result['binary_parity'][:100]}")
    print(f"  Length mod: {length_result['length_mod'][:100]}")
```

### Format-Specific Steganography

**JSON log steganography:**

```python
#!/usr/bin/env python3
"""
Extract hidden data from JSON logs
"""
import json
import re
import sys
import base64
import binascii

def extract_from_json_keys(logfile):
    """Extract data from JSON key ordering or naming"""
    with open(logfile, 'r') as f:
        lines = f.readlines()
    
    # Collect all keys in order
    all_keys = []
    for line in lines:
        try:
            obj = json.loads(line)
            all_keys.extend(obj.keys())
        except:
            pass
    
    # Method 1: First letter of each key
    message = ''.join(k[0] for k in all_keys if k)
    
    # Method 2: Unusual key names (look for encoded data)
    suspicious = []
    for key in all_keys:
        # Check if key looks like encoded data
        if re.match(r'^[A-Za-z0-9+/]+=*$', key) and len(key) > 10:
            try:
                decoded = base64.b64decode(key).decode('utf-8', errors='ignore')
                if 'flag' in decoded.lower():
                    suspicious.append(decoded)
            except:
                pass
    
    return {
        'key_acrostic': message,
        'suspicious_keys': suspicious
    }


def extract_from_json_values(logfile):
    """Extract hidden data from JSON value patterns"""
    with open(logfile, 'r') as f:
        lines = f.readlines()
    
    findings = []
    
    for line_num, line in enumerate(lines, 1):
        try:
            obj = json.loads(line)
            
            # Look for unusual value patterns
            for key, value in obj.items():
                if isinstance(value, str):
                    # Check for base64-encoded data
                    if re.match(r'^[A-Za-z0-9+/]{20,}={0,2}$', value):
                        try:
                            decoded = base64.b64decode(value).decode('utf-8', errors='ignore')
                            if 'flag' in decoded.lower() or len(decoded) > 10:
                                findings.append({
                                    'line': line_num,
                                    'key': key,
                                    'encoded': value[:50],
                                    'decoded': decoded
                                })
                        except:
                            pass

                    # Check for hex-encoded values
                    if re.match(r'^[0-9a-fA-F]{16,}$', value):
                        try:
                            decoded = binascii.unhexlify(value).decode('utf-8', errors='ignore')
                            if decoded and len(decoded) > 5:
                                findings.append({
                                    'line': line_num,
                                    'key': key,
                                    'encoded': value[:50],
                                    'decoded': decoded
                                })
                        except:
                            pass
        except:
            pass

    return findings


if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python3 json_stego.py <logfile>")
        sys.exit(1)

    key_results = extract_from_json_keys(sys.argv[1])
    print(f"\n=== JSON Key Analysis ===")
    print(f"Key acrostic: {key_results['key_acrostic'][:100]}")
    if key_results['suspicious_keys']:
        print(f"Suspicious keys found: {key_results['suspicious_keys']}")

    value_results = extract_from_json_values(sys.argv[1])
    if value_results:
        print(f"\n=== JSON Value Analysis ===")
        for finding in value_results[:10]:  # Show first 10
            print(f"Line {finding['line']}, Key '{finding['key']}':")
            print(f"  Encoded: {finding['encoded']}...")
            print(f"  Decoded: {finding['decoded'][:100]}")

````

**XML/HTML log steganography:**
```bash
# Extract data from HTML comments
grep -oE '<!--.*?-->' logfile.html | sed 's/<!--//g; s/-->//g'

# Extract from attributes
grep -oE 'data-[a-z]+="[^"]*"' logfile.html | cut -d'"' -f2

# Extract from element IDs (potential encoding)
grep -oE 'id="[^"]*"' logfile.html | cut -d'"' -f2 | tr -d '\n'
````

```python
#!/usr/bin/env python3
"""
Extract hidden data from XML/HTML logs
"""
import re
from html.parser import HTMLParser

class HiddenDataExtractor(HTMLParser):
    def __init__(self):
        super().__init__()
        self.hidden_data = []
        self.comment_data = []
        
    def handle_comment(self, data):
        """Extract data from HTML comments"""
        self.comment_data.append(data.strip())
        
        # Check for encoded data in comments
        if re.match(r'^[A-Za-z0-9+/]+=*$', data.strip()):
            try:
                import base64
                decoded = base64.b64decode(data.strip())
                if b'flag' in decoded.lower():
                    self.hidden_data.append(decoded.decode('utf-8', errors='ignore'))
            except:
                pass
    
    def handle_starttag(self, tag, attrs):
        """Extract data from unusual attributes"""
        attr_dict = dict(attrs)
        
        # Check custom data attributes
        for key, value in attr_dict.items():
            if key.startswith('data-') or key.startswith('x-'):
                if len(value) > 20:
                    self.hidden_data.append(f"{tag}.{key}: {value}")
        
        # Check for unusual style/class values
        if 'style' in attr_dict:
            # Extract hex colors that might encode data
            colors = re.findall(r'#([0-9a-fA-F]{6})', attr_dict['style'])
            if colors:
                # Try to decode hex values
                hex_string = ''.join(colors)
                try:
                    import binascii
                    decoded = binascii.unhexlify(hex_string)
                    if len(decoded) > 5:
                        self.hidden_data.append(f"Style colors: {decoded.decode('utf-8', errors='ignore')}")
                except:
                    pass

def extract_from_xml_attributes(logfile):
    """Extract data from XML attribute patterns"""
    with open(logfile, 'r') as f:
        content = f.read()
    
    # Extract all attribute values
    attributes = re.findall(r'\w+="([^"]*)"', content)
    
    # Concatenate all attribute values
    concatenated = ''.join(attributes)
    
    results = {}
    
    # Try hex decode
    if re.match(r'^[0-9a-fA-F]+$', concatenated):
        try:
            import binascii
            results['hex'] = binascii.unhexlify(concatenated).decode('utf-8', errors='ignore')
        except:
            pass
    
    # Try base64 decode
    try:
        import base64
        results['base64'] = base64.b64decode(concatenated).decode('utf-8', errors='ignore')
    except:
        pass
    
    return results

if __name__ == "__main__":
    import sys
    if len(sys.argv) < 2:
        print("Usage: python3 xml_stego.py <logfile>")
        sys.exit(1)
    
    parser = HiddenDataExtractor()
    with open(sys.argv[1], 'r') as f:
        parser.feed(f.read())
    
    print(f"\n=== HTML/XML Hidden Data ===")
    print(f"Comments found: {len(parser.comment_data)}")
    for comment in parser.comment_data[:5]:
        print(f"  {comment[:100]}")
    
    print(f"\nHidden data extracted: {len(parser.hidden_data)}")
    for data in parser.hidden_data:
        print(f"  {data[:100]}")
    
    attr_results = extract_from_xml_attributes(sys.argv[1])
    if attr_results:
        print(f"\n=== Attribute Concatenation ===")
        for method, data in attr_results.items():
            if data and len(data) > 10:
                print(f"{method}: {data[:100]}")
```

### Cryptographic Steganography

**XOR-based data hiding:**

```python
#!/usr/bin/env python3
"""
Extract XOR-encoded data from logs
"""

def xor_decode(data, key):
    """XOR decode with repeating key"""
    if isinstance(data, str):
        data = data.encode()
    if isinstance(key, str):
        key = key.encode()
    
    result = bytearray()
    for i, byte in enumerate(data):
        result.append(byte ^ key[i % len(key)])
    
    return result

def brute_force_xor_single_byte(data):
    """Try all single-byte XOR keys"""
    if isinstance(data, str):
        data = data.encode()
    
    results = []
    for key in range(256):
        decoded = bytes(b ^ key for b in data)
        
        # Check if result is mostly printable ASCII
        printable_count = sum(32 <= b < 127 for b in decoded)
        if printable_count / len(decoded) > 0.8:
            decoded_str = decoded.decode('utf-8', errors='ignore')
            if 'flag' in decoded_str.lower():
                results.append({
                    'key': key,
                    'key_char': chr(key) if 32 <= key < 127 else f'\\x{key:02x}',
                    'decoded': decoded_str
                })
    
    return results

def extract_xor_from_logs(logfile):
    """Extract potential XOR-encoded data from logs"""
    with open(logfile, 'r') as f:
        content = f.read()
    
    # Look for hex-encoded data
    hex_patterns = re.findall(r'\b([0-9a-fA-F]{20,})\b', content)
    
    all_results = []
    for pattern in hex_patterns:
        try:
            import binascii
            data = binascii.unhexlify(pattern)
            
            # Try single-byte XOR
            results = brute_force_xor_single_byte(data)
            if results:
                all_results.extend(results)
        except:
            pass
    
    return all_results

if __name__ == "__main__":
    import sys
    import re
    
    if len(sys.argv) < 2:
        print("Usage: python3 xor_stego.py <logfile>")
        sys.exit(1)
    
    results = extract_xor_from_logs(sys.argv[1])
    
    if results:
        print(f"\n=== XOR Decoded Results ===")
        for r in results[:10]:  # Show first 10
            print(f"Key: {r['key']} ({r['key_char']})")
            print(f"Decoded: {r['decoded'][:100]}")
            print()
```

**Frequency analysis for substitution ciphers:**

```python
#!/usr/bin/env python3
"""
Frequency analysis for detecting substitution ciphers in logs
"""
from collections import Counter
import string

def frequency_analysis(text):
    """Perform frequency analysis on text"""
    # Remove non-alphabetic characters
    letters = [c.lower() for c in text if c.isalpha()]
    
    if not letters:
        return None
    
    freq = Counter(letters)
    total = len(letters)
    
    # Calculate frequencies
    freq_percent = {char: (count / total) * 100 for char, count in freq.items()}
    
    # Expected English letter frequencies (approximate)
    english_freq = {
        'e': 12.70, 't': 9.06, 'a': 8.17, 'o': 7.51, 'i': 6.97,
        'n': 6.75, 's': 6.33, 'h': 6.09, 'r': 5.99, 'd': 4.25,
        'l': 4.03, 'c': 2.78, 'u': 2.76, 'm': 2.41, 'w': 2.36
    }
    
    # Calculate chi-squared statistic
    chi_squared = 0
    for char in string.ascii_lowercase:
        observed = freq_percent.get(char, 0)
        expected = english_freq.get(char, 1.0)
        chi_squared += ((observed - expected) ** 2) / expected
    
    return {
        'frequencies': freq_percent,
        'chi_squared': chi_squared,
        'top_chars': freq.most_common(10),
        'is_likely_english': chi_squared < 100  # [Inference] Threshold is heuristic
    }

def detect_substitution_cipher(logfile):
    """Detect potential substitution ciphers in log fields"""
    suspicious_fields = []
    
    with open(logfile, 'r') as f:
        for line_num, line in enumerate(f, 1):
            fields = line.split()
            
            for field_idx, field in enumerate(fields):
                # Only analyze fields with mostly letters
                if len(field) > 20 and sum(c.isalpha() for c in field) / len(field) > 0.8:
                    analysis = frequency_analysis(field)
                    
                    if analysis and analysis['chi_squared'] > 50:  # Not English-like
                        suspicious_fields.append({
                            'line': line_num,
                            'field': field_idx,
                            'text': field[:50],
                            'chi_squared': analysis['chi_squared'],
                            'top_chars': analysis['top_chars']
                        })
    
    return suspicious_fields

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python3 freq_analysis.py <logfile>")
        sys.exit(1)
    
    results = detect_substitution_cipher(sys.argv[1])
    
    if results:
        print(f"\n=== Potential Substitution Ciphers ===")
        for r in results[:5]:
            print(f"Line {r['line']}, Field {r['field']}")
            print(f"Text: {r['text']}...")
            print(f"Chi-squared: {r['chi_squared']:.2f}")
            print(f"Top characters: {r['top_chars'][:5]}")
            print()
```

### Comprehensive CTF Log Steganography Scanner

```python
#!/usr/bin/env python3
"""
Comprehensive CTF log steganography scanner
Combines multiple techniques for maximum coverage
"""
import re
import base64
import binascii
from collections import Counter

class CTFLogScanner:
    def __init__(self, logfile):
        self.logfile = logfile
        self.findings = []
        
    def scan_all(self):
        """Run all scanning techniques"""
        print(f"[*] Scanning {self.logfile}...")
        
        self.scan_flag_formats()
        self.scan_encodings()
        self.scan_whitespace()
        self.scan_field_patterns()
        self.scan_timing()
        
        return self.findings
    
    def scan_flag_formats(self):
        """Scan for common flag formats"""
        patterns = [
            (r'flag\{[^}]+\}', 'Generic flag{}'),
            (r'CTF\{[^}]+\}', 'CTF{}'),
            (r'HTB\{[^}]+\}', 'HackTheBox{}'),
            (r'\b[A-F0-9]{32}\b', 'MD5-like'),
            (r'\b[A-F0-9]{40}\b', 'SHA1-like'),
        ]
        
        with open(self.logfile, 'r', errors='ignore') as f:
            content = f.read()
        
        for pattern, desc in patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            if matches:
                self.findings.append({
                    'type': 'Flag Format',
                    'description': desc,
                    'matches': matches
                })
    
    def scan_encodings(self):
        """Scan for encoded data"""
        with open(self.logfile, 'r', errors='ignore') as f:
            content = f.read()
        
        # Base64
        b64_patterns = re.findall(r'\b[A-Za-z0-9+/]{20,}={0,2}\b', content)
        for pattern in b64_patterns[:10]:  # Limit to first 10
            try:
                decoded = base64.b64decode(pattern).decode('utf-8', errors='ignore')
                if 'flag' in decoded.lower() or len(decoded) > 10:
                    self.findings.append({
                        'type': 'Base64 Encoding',
                        'encoded': pattern[:50],
                        'decoded': decoded[:100]
                    })
            except:
                pass
        
        # Hex
        hex_patterns = re.findall(r'\b(?:[0-9a-fA-F]{2}[:\s-]?){8,}\b', content)
        for pattern in hex_patterns[:10]:
            try:
                clean = pattern.replace(':', '').replace(' ', '').replace('-', '')
                decoded = binascii.unhexlify(clean).decode('utf-8', errors='ignore')
                if decoded and len(decoded) > 5:
                    self.findings.append({
                        'type': 'Hex Encoding',
                        'encoded': pattern[:50],
                        'decoded': decoded[:100]
                    })
            except:
                pass
    
    def scan_whitespace(self):
        """Scan for whitespace-based encoding"""
        with open(self.logfile, 'r', errors='ignore') as f:
            lines = f.readlines()
        
        # Check for unusual whitespace patterns
        unusual_lines = []
        for i, line in enumerate(lines, 1):
            # Multiple consecutive spaces
            if '  ' in line or '\t' in line:
                unusual_lines.append(i)
        
        if unusual_lines:
            self.findings.append({
                'type': 'Whitespace Anomaly',
                'description': f'Unusual whitespace on {len(unusual_lines)} lines',
                'lines': unusual_lines[:10]
            })
    
    def scan_field_patterns(self):
        """Scan for patterns in log fields"""
        with open(self.logfile, 'r', errors='ignore') as f:
            lines = f.readlines()
        
        # Check for acrostics (first char of each line)
        acrostic = ''.join(line[0] if line else '' for line in lines)
        if 'flag' in acrostic.lower():
            self.findings.append({
                'type': 'Acrostic Pattern',
                'data': acrostic[:100]
            })
        
        # Check line lengths
        lengths = [len(line.rstrip()) for line in lines if line.strip()]
        if lengths:
            # Even/odd binary encoding
            binary = ''.join('0' if l % 2 == 0 else '1' for l in lengths)
            if len(binary) >= 8:
                message = ''
                for i in range(0, len(binary) - 7, 8):
                    byte = binary[i:i+8]
                    char_code = int(byte, 2)
                    if 32 <= char_code < 127:
                        message += chr(char_code)
                
                if 'flag' in message.lower():
                    self.findings.append({
                        'type': 'Line Length Encoding',
                        'data': message[:100]
                    })
    
    def scan_timing(self):
        """Scan for timing-based patterns"""
        # Look for timestamp patterns
        timestamp_pattern = r'\b\d{2}:\d{2}:\d{2}\b'
        
        with open(self.logfile, 'r', errors='ignore') as f:
            content = f.read()
        
        timestamps = re.findall(timestamp_pattern, content)
        
        if len(timestamps) > 5:
            # Extract seconds
            seconds = [int(ts.split(':')[2]) for ts in timestamps]
            
            # Check if seconds encode ASCII
            message = ''.join(chr(s) for s in seconds if 32 <= s < 127)
            
            if 'flag' in message.lower():
                self.findings.append({
                    'type': 'Timing Channel (Seconds)',
                    'data': message[:100]
                })
    
    def print_findings(self):
        """Print all findings"""
        if not self.findings:
            print("\n[!] No suspicious patterns found")
            return
        
        print(f"\n[+] Found {len(self.findings)} suspicious patterns:\n")
        
        for i, finding in enumerate(self.findings, 1):
            print(f"[{i}] {finding['type']}")
            for key, value in finding.items():
                if key != 'type':
                    print(f"    {key}: {value}")
            print()

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python3 ctf_log_scanner.py <logfile>")
        sys.exit(1)
    
    scanner = CTFLogScanner(sys.argv[1])
    scanner.scan_all()
    scanner.print_findings()
```

### Quick Reference Commands

**One-liner flag hunters:**

```bash
# Multi-encoding flag search
cat logfile.txt | grep -oE '\b[A-Za-z0-9+/]{20,}={0,2}\b' | while read enc; do echo "$enc" | base64 -d 2>/dev/null | grep -i "flag" && echo "Found in: $enc"; done

# Hex pattern search and decode
grep -oE '([0-9a-fA-F]{2}[[:space:]]?){10,}' logfile.txt | tr -d ' ' | xxd -r -p | strings | grep -i flag

# ROT13 all text
cat logfile.txt | tr 'A-Za-z' 'N-ZA-Mn-za-m' | grep -i "flag"

# Extract all unique "suspicious" strings (alphanumeric, 20+ chars)
grep -oE '\b[A-Za-z0-9]{20,}\b' logfile.txt | sort -u > suspicious.txt
```

**Automated multi-decode pipeline:**

```bash
#!/bin/bash
# multi_decode.sh - Try multiple decoding methods on input

INPUT="$1"

echo "[*] Testing Base64..."
echo "$INPUT" | base64 -d 2>/dev/null && echo

echo "[*] Testing Hex..."
echo "$INPUT" | xxd -r -p 2>/dev/null && echo

echo "[*] Testing URL decode..."
python3 -c "import urllib.parse; print(urllib.parse.unquote('$INPUT'))" 2>/dev/null && echo

echo "[*] Testing ROT13..."
echo "$INPUT" | tr 'A-Za-z' 'N-ZA-Mn-za-m' && echo

echo "[*] Testing Base64 -> Hex..."
echo "$INPUT" | base64 -d 2>/dev/null | xxd -r -p 2>/dev/null && echo

echo "[*] Testing Hex -> Base64..."
echo "$INPUT" | xxd -r -p 2>/dev/null | base64 -d 2>/dev/null && echo
```

**[Inference]**: The effectiveness of steganography detection depends on understanding the CTF creator's encoding methods. Multiple passes with different techniques increase success probability.

---

**Important subtopics for further study:**

- **Polyglot file analysis** - Files that are valid in multiple formats simultaneously
- **Null-byte injection techniques** - Using null bytes to hide data or bypass filters
- **DNS tunneling in logs** - Extracting data from DNS query patterns
- **Covert channels via HTTP headers** - Custom headers as data exfiltration channels

---
## Code Obfuscation

Code obfuscation in CTF logs involves identifying and decoding disguised commands, payloads, or flags hidden within log entries.

### Base64 encoding detection and decoding

Base64 is the most common encoding found in CTF logs, often used to hide commands in web server logs or authentication attempts.

**Automated Base64 detection:**

```bash
# Extract and decode all Base64 strings from logs
grep -oE '[A-Za-z0-9+/]{20,}={0,2}' access.log | while read encoded; do
    decoded=$(echo "$encoded" | base64 -d 2>/dev/null)
    if [ $? -eq 0 ]; then
        echo "Encoded: $encoded"
        echo "Decoded: $decoded"
        echo "---"
    fi
done
```

**Python script for comprehensive Base64 hunting:**

```python
import re
import base64
import sys

def find_base64(log_file):
    # Pattern for potential Base64 (minimum 16 chars)
    pattern = r'[A-Za-z0-9+/]{16,}={0,2}'
    
    with open(log_file, 'r', errors='ignore') as f:
        for line_num, line in enumerate(f, 1):
            matches = re.findall(pattern, line)
            for match in matches:
                try:
                    decoded = base64.b64decode(match).decode('utf-8', errors='ignore')
                    # Check if decoded content is readable
                    if decoded.isprintable():
                        print(f"Line {line_num}: {match}")
                        print(f"Decoded: {decoded}\n")
                except:
                    continue

find_base64('access.log')
```

**URL-safe Base64 variant:**

```bash
# Some CTFs use URL-safe Base64 (- and _ instead of + and /)
echo "SGVsbG8gV29ybGQh" | base64 -d
echo "SGVsbG8_V29ybGQh" | tr '_-' '/+' | base64 -d
```

### Hexadecimal encoding

Hex-encoded payloads appear frequently in SQL injection attempts and command injection logs.

**Extract and decode hex strings:**

```bash
# Find hex patterns and decode
grep -oE '(0x)?[0-9a-fA-F]{10,}' suspicious.log | while read hex; do
    hex_clean=$(echo "$hex" | sed 's/^0x//')
    echo "$hex_clean" | xxd -r -p
    echo ""
done
```

**Python hex decoder with pattern matching:**

```python
import re
import binascii

def decode_hex_in_logs(log_file):
    # Match hex strings with or without 0x prefix
    patterns = [
        r'0x([0-9a-fA-F]+)',
        r'\\x([0-9a-fA-F]{2})+',
        r'%([0-9a-fA-F]{2})+'
    ]
    
    with open(log_file, 'r', errors='ignore') as f:
        for line in f:
            for pattern in patterns:
                matches = re.findall(pattern, line)
                if matches:
                    print(f"Original: {line.strip()}")
                    for match in matches:
                        try:
                            decoded = bytes.fromhex(match).decode('utf-8', errors='ignore')
                            if decoded.isprintable():
                                print(f"Decoded hex: {decoded}")
                        except:
                            pass
                    print("---")

decode_hex_in_logs('access.log')
```

### URL encoding (Percent encoding)

Web application logs often contain URL-encoded payloads used to bypass filters.

**Multi-level URL decoding:**

```bash
# URL decode with multiple iterations (common in WAF bypass)
python3 << 'EOF'
import urllib.parse
import sys

def recursive_url_decode(encoded, max_depth=5):
    decoded = encoded
    for i in range(max_depth):
        new_decode = urllib.parse.unquote(decoded)
        if new_decode == decoded:
            break
        decoded = new_decode
        print(f"Level {i+1}: {decoded}")
    return decoded

# Read from stdin or file
for line in sys.stdin:
    if '%' in line:
        print(f"Original: {line.strip()}")
        recursive_url_decode(line.strip())
        print("---")
EOF
```

**Extract URL-encoded payloads from access logs:**

```bash
# Find and decode URL-encoded patterns
grep -oE '%[0-9a-fA-F]{2}' access.log | \
    tr -d '\n' | \
    python3 -c "import sys, urllib.parse; print(urllib.parse.unquote(sys.stdin.read()))"
```

### Unicode and UTF-8 encoding tricks

**Unicode escape sequence detection:**

```python
import re
import codecs

def decode_unicode_escapes(log_file):
    patterns = [
        (r'\\u([0-9a-fA-F]{4})', 'unicode_escape'),
        (r'\\x([0-9a-fA-F]{2})', 'unicode_escape'),
        (r'&#(\d+);', 'html'),
        (r'&#x([0-9a-fA-F]+);', 'html_hex')
    ]
    
    with open(log_file, 'r', errors='ignore') as f:
        content = f.read()
        
        # Unicode escapes
        if '\\u' in content or '\\x' in content:
            try:
                decoded = codecs.decode(content, 'unicode_escape')
                print("Unicode decoded content:")
                print(decoded[:500])
            except:
                pass
        
        # HTML entities
        if '&#' in content:
            # Decimal entities
            for match in re.finditer(r'&#(\d+);', content):
                char = chr(int(match.group(1)))
                print(f"HTML entity &#{{match.group(1)}}; = {char}")
            
            # Hex entities
            for match in re.finditer(r'&#x([0-9a-fA-F]+);', content):
                char = chr(int(match.group(1), 16))
                print(f"HTML entity &#x{match.group(1)}; = {char}")

decode_unicode_escapes('access.log')
```

### ROT13 and Caesar cipher

**Quick ROT13 detection and decoding:**

```bash
# Check for ROT13 encoded flags
grep -i "flag\|ctf\|key" *.log | tr 'A-Za-z' 'N-ZA-Mn-za-m'

# Try all Caesar cipher shifts
for shift in {1..25}; do
    echo "Shift $shift:"
    cat suspicious.txt | tr "$(echo {A..Z} {a..z} | tr -d ' ')" \
        "$(echo {A..Z} {a..z} | tr -d ' ' | sed "s/^.\{$shift\}\(.*\)\(.\{$shift\}\)$/\2\1/")"
done
```

### XOR obfuscation

XOR encoding is common in malware logs and command payloads.

**Single-byte XOR bruteforce:**

```python
def xor_bruteforce(hex_string):
    data = bytes.fromhex(hex_string)
    
    for key in range(256):
        result = bytes([b ^ key for b in data])
        try:
            decoded = result.decode('utf-8', errors='strict')
            if decoded.isprintable() and 'flag' in decoded.lower():
                print(f"XOR key: {key} (0x{key:02x})")
                print(f"Decoded: {decoded}\n")
        except:
            pass

# Example usage
hex_data = "2c3f3f3e686f3f3e6b"  # Example from CTF log
xor_bruteforce(hex_data)
```

**Multi-byte XOR key detection:**

```python
def detect_xor_key(data, max_keylen=16):
    from collections import Counter
    
    # Try different key lengths
    for keylen in range(1, max_keylen + 1):
        blocks = [data[i::keylen] for i in range(keylen)]
        key = []
        
        for block in blocks:
            # Assume most common byte XORs to space (0x20)
            if len(block) > 0:
                most_common = Counter(block).most_common(1)[0][0]
                key.append(most_common ^ 0x20)
        
        # Try to decode with this key
        decoded = bytes([data[i] ^ key[i % len(key)] for i in range(len(data))])
        try:
            text = decoded.decode('utf-8', errors='strict')
            if text.isprintable():
                print(f"Possible key length: {keylen}")
                print(f"Key: {bytes(key).hex()}")
                print(f"Decoded: {text[:100]}...\n")
        except:
            pass

# Load binary log data
with open('encoded.log', 'rb') as f:
    detect_xor_key(f.read())
```

### Combined obfuscation layers

**Multi-layer decoder for nested encoding:**

```python
import base64
import urllib.parse
import codecs
import binascii

def progressive_decode(encoded_string):
    """
    Attempts multiple decoding strategies in sequence
    """
    current = encoded_string
    steps = []
    
    while True:
        changed = False
        
        # Try URL decode
        url_decoded = urllib.parse.unquote(current)
        if url_decoded != current:
            current = url_decoded
            steps.append("URL decode")
            changed = True
            continue
        
        # Try Base64
        try:
            b64_decoded = base64.b64decode(current).decode('utf-8', errors='strict')
            if b64_decoded.isprintable() and b64_decoded != current:
                current = b64_decoded
                steps.append("Base64 decode")
                changed = True
                continue
        except:
            pass
        
        # Try hex decode
        try:
            hex_decoded = bytes.fromhex(current.replace('0x', '').replace('\\x', '')).decode('utf-8')
            if hex_decoded != current:
                current = hex_decoded
                steps.append("Hex decode")
                changed = True
                continue
        except:
            pass
        
        # Try unicode escape
        try:
            unicode_decoded = codecs.decode(current, 'unicode_escape')
            if unicode_decoded != current:
                current = unicode_decoded
                steps.append("Unicode escape")
                changed = True
                continue
        except:
            pass
        
        # No more decoding possible
        break
    
    print(f"Decoding steps: {' -> '.join(steps)}")
    print(f"Final result: {current}")
    return current

# Example: nested encoding from CTF log entry
progressive_decode("JTI1MzJaJTI1MzJYJTI1MzJa")
```

## Puzzle Solving from Logs

CTF logs often contain flags or clues distributed across multiple entries, requiring pattern recognition and data correlation.

### Flag format recognition

**Automated flag extraction:**

```bash
# Common flag formats: flag{...}, CTF{...}, FLAG{...}
grep -oE '(flag|ctf|FLAG|CTF)\{[^\}]+\}' *.log

# Case-insensitive with extended patterns
grep -iE '(flag|ctf|key|password)\{[a-zA-Z0-9_]+\}' *.log

# Hex-encoded flags
grep -oE '[0-9a-fA-F]{32,}' *.log | while read hash; do
    echo "$hash" | xxd -r -p 2>/dev/null | grep -i "flag\|ctf"
done
```

**Python-based intelligent flag finder:**

```python
import re
import os

def find_flags(log_dir):
    flag_patterns = [
        r'flag\{[^}]+\}',
        r'CTF\{[^}]+\}',
        r'FLAG\{[^}]+\}',
        r'[A-Fa-f0-9]{32}',  # MD5-like
        r'[A-Fa-f0-9]{40}',  # SHA1-like
        r'[A-Za-z0-9+/]{32,}={0,2}'  # Base64-like
    ]
    
    findings = {}
    
    for root, dirs, files in os.walk(log_dir):
        for file in files:
            if file.endswith('.log'):
                filepath = os.path.join(root, file)
                with open(filepath, 'r', errors='ignore') as f:
                    content = f.read()
                    
                    for pattern in flag_patterns:
                        matches = re.findall(pattern, content, re.IGNORECASE)
                        if matches:
                            findings[file] = findings.get(file, []) + matches
    
    for file, flags in findings.items():
        print(f"\n{file}:")
        for flag in set(flags):
            print(f"  {flag}")

find_flags('/var/log')
```

### Timestamp-based puzzles

Flags may be split across log entries ordered by timestamp, IP address, or other fields.

**Extract entries in chronological order:**

```bash
# Sort web logs by timestamp and concatenate specific field
awk '{print $4, $7}' access.log | \
    sort -t'[' -k2 | \
    awk '{print $2}' | \
    tr -d '\n'
```

**Time-based correlation script:**

```python
import pandas as pd
from datetime import datetime

def solve_time_puzzle(log_file):
    # Parse log with timestamps
    data = []
    with open(log_file, 'r') as f:
        for line in f:
            # Extract timestamp and relevant field
            parts = line.split()
            if len(parts) >= 4:
                timestamp = parts[3].strip('[]')
                payload = parts[-1] if len(parts) > 4 else ""
                data.append({'time': timestamp, 'payload': payload})
    
    df = pd.DataFrame(data)
    df['time'] = pd.to_datetime(df['time'], format='%d/%b/%Y:%H:%M:%S', errors='coerce')
    df = df.dropna().sort_values('time')
    
    # Concatenate payloads in chronological order
    result = ''.join(df['payload'].values)
    print(f"Chronological reconstruction: {result}")
    
    # Check for patterns at specific intervals
    df['minute'] = df['time'].dt.minute
    by_minute = df.groupby('minute')['payload'].apply(''.join)
    print("\nBy minute grouping:")
    print(by_minute)

solve_time_puzzle('access.log')
```

### IP address puzzles

**IP-to-ASCII conversion:**

```python
def ip_to_ascii(ip_list):
    """
    Convert IP octets to ASCII characters
    Common CTF technique
    """
    result = ""
    for ip in ip_list:
        octets = ip.split('.')
        for octet in octets:
            char_code = int(octet)
            if 32 <= char_code <= 126:  # Printable ASCII
                result += chr(char_code)
    return result

# Extract IPs from log and convert
ips = ['102.108.97.103', '123.116.101.115', '116.95.115.101']
print(ip_to_ascii(ips))  # Output: flag{tes...}
```

**Sort by IP and extract pattern:**

```bash
# Extract and sort IPs, then concatenate specific fields
awk '{print $1, $7}' access.log | \
    sort -t. -k1,1n -k2,2n -k3,3n -k4,4n | \
    awk '{print $2}' | \
    grep -o . | \
    tr -d '\n'
```

### User-Agent string analysis

User-Agent fields frequently contain hidden messages in CTF challenges.

**Extract and analyze User-Agents:**

```bash
# Extract unique User-Agent strings
grep "User-Agent" access.log | \
    cut -d'"' -f6 | \
    sort -u > user_agents.txt

# Look for suspicious or custom User-Agents
grep -v "Mozilla\|Chrome\|Safari\|Firefox" user_agents.txt

# Decode if Base64 encoded
cat user_agents.txt | while read ua; do
    echo "$ua" | base64 -d 2>/dev/null
done
```

**Python User-Agent puzzle solver:**

```python
import re

def analyze_user_agents(log_file):
    user_agents = []
    
    with open(log_file, 'r') as f:
        for line in f:
            ua_match = re.search(r'"([^"]*)" "[^"]*"$', line)
            if ua_match:
                user_agents.append(ua_match.group(1))
    
    # Check for patterns
    print(f"Total User-Agents: {len(user_agents)}")
    print(f"Unique: {len(set(user_agents))}")
    
    # Extract first character of each unique UA
    first_chars = ''.join([ua[0] for ua in sorted(set(user_agents))])
    print(f"First characters: {first_chars}")
    
    # Look for custom/non-standard UAs
    suspicious = [ua for ua in user_agents if not any(
        browser in ua for browser in ['Mozilla', 'Chrome', 'Safari', 'Firefox', 'Edge']
    )]
    
    if suspicious:
        print("\nSuspicious User-Agents:")
        for ua in set(suspicious):
            print(f"  {ua}")

analyze_user_agents('access.log')
```

### HTTP status code patterns

**Extract information from status code sequences:**

```bash
# Extract status codes in order
awk '{print $9}' access.log | grep -E '^[0-9]{3}$' > status_codes.txt

# Convert to ASCII
while read code; do
    # Some CTFs use status codes as ASCII values
    printf "\\x$(printf '%x' $code)"
done < status_codes.txt
```

### Log file steganography

**Check for hidden data in log file metadata:**

```bash
# Check file for appended data after legitimate log entries
tail -c 1000 access.log | xxd | grep -v "0a00 0000"

# Look for binary data
strings -n 10 access.log | grep -i "flag\|ctf\|key"

# Check for alternate data streams (Windows logs)
find . -name "*.log" -exec powershell Get-Item {} -Stream * \;
```

**Extract data from whitespace patterns:**

```python
def extract_whitespace_data(log_file):
    """
    Some CTFs hide data in trailing whitespace patterns
    """
    with open(log_file, 'r') as f:
        binary_string = ""
        for line in f:
            # Count trailing spaces/tabs
            trailing = len(line) - len(line.rstrip())
            if trailing > 0:
                # Odd = 1, Even = 0 (example encoding)
                binary_string += '1' if trailing % 2 else '0'
        
        # Convert binary to ASCII
        if binary_string:
            n = int(binary_string, 2)
            result = n.to_bytes((n.bit_length() + 7) // 8, 'big').decode('utf-8', errors='ignore')
            print(f"Whitespace decoded: {result}")

extract_whitespace_data('access.log')
```

## Multi-stage Challenges

Multi-stage CTF log challenges require solving sequential puzzles where each stage reveals information needed for the next.

### Stage identification

**Detecting stage markers:**

```bash
# Look for stage indicators in logs
grep -iE "(stage|phase|level|step).*[0-9]" *.log

# Find numbered patterns
grep -oE "(1st|2nd|3rd|[0-9]+th)" *.log | sort | uniq -c
```

**Automated stage extractor:**

```python
import re
import os

def identify_stages(log_dir):
    stage_data = {}
    
    for file in os.listdir(log_dir):
        if file.endswith('.log'):
            with open(os.path.join(log_dir, file), 'r', errors='ignore') as f:
                content = f.read()
                
                # Look for stage markers
                markers = re.findall(r'(stage|level|step)[\s_-]*(\d+)', content, re.IGNORECASE)
                
                for marker_type, stage_num in markers:
                    key = f"{marker_type}_{stage_num}"
                    if key not in stage_data:
                        stage_data[key] = []
                    stage_data[key].append(file)
    
    print("Identified stages:")
    for stage in sorted(stage_data.keys()):
        print(f"\n{stage}:")
        for file in stage_data[stage]:
            print(f"  {file}")

identify_stages('/var/log/ctf')
```

### Cross-log correlation

**Correlate entries across multiple log files:**

```python
import pandas as pd
from datetime import datetime

def correlate_logs(log_files):
    """
    Merge multiple logs by timestamp to find related events
    """
    all_data = []
    
    for log_file in log_files:
        with open(log_file, 'r', errors='ignore') as f:
            for line in f:
                # Basic log parsing (adjust for your format)
                parts = line.split()
                if len(parts) >= 4:
                    try:
                        timestamp = ' '.join(parts[3:5]).strip('[]')
                        all_data.append({
                            'time': pd.to_datetime(timestamp, format='%d/%b/%Y:%H:%M:%S'),
                            'source': log_file,
                            'content': line.strip()
                        })
                    except:
                        pass
    
    df = pd.DataFrame(all_data)
    df = df.sort_values('time')
    
    # Find events within 1 second of each other across different sources
    df['time_diff'] = df['time'].diff().dt.total_seconds()
    correlated = df[df['time_diff'] < 1]
    
    print("Correlated events across logs:")
    print(correlated[['time', 'source', 'content']])

correlate_logs(['access.log', 'auth.log', 'syslog'])
```

### Session tracking across stages

**Track user sessions through multiple stages:**

```bash
# Extract session IDs and track progression
grep -oE 'session=[a-zA-Z0-9]+' access.log | \
    cut -d= -f2 | \
    sort | uniq -c | \
    sort -rn

# Follow specific session through logs
SESSION="abc123def456"
grep "$SESSION" *.log | sort
```

**Python session analyzer:**

```python
import re
from collections import defaultdict

def track_sessions(log_file):
    sessions = defaultdict(list)
    
    with open(log_file, 'r') as f:
        for line_num, line in enumerate(f, 1):
            # Extract session ID (adjust pattern for your logs)
            session_match = re.search(r'session[=:]([a-zA-Z0-9]+)', line)
            if session_match:
                session_id = session_match.group(1)
                sessions[session_id].append({
                    'line': line_num,
                    'content': line.strip()
                })
    
    # Analyze session progression
    print(f"Found {len(sessions)} unique sessions\n")
    
    for session_id, events in sorted(sessions.items(), key=lambda x: len(x[1]), reverse=True):
        print(f"Session {session_id}: {len(events)} events")
        if len(events) > 5:  # Focus on active sessions
            print("  Event sequence:")
            for event in events[:10]:  # First 10 events
                print(f"    Line {event['line']}: {event['content'][:80]}")
            print()

track_sessions('access.log')
```

### Chained decoding challenges

**Progressive challenge solver:**

```python
import base64
import hashlib
import re

def solve_chained_challenge(log_file):
    """
    Solves multi-stage challenges where each answer leads to next clue
    """
    with open(log_file, 'r') as f:
        content = f.read()
    
    print("Stage 1: Find initial clue...")
    # Look for starting marker
    stage1_match = re.search(r'START:([A-Za-z0-9+/=]+)', content)
    if not stage1_match:
        print("No starting point found")
        return
    
    current = stage1_match.group(1)
    print(f"Found: {current}")
    
    print("\nStage 2: Decode Base64...")
    stage2 = base64.b64decode(current).decode('utf-8')
    print(f"Decoded: {stage2}")
    
    print("\nStage 3: Extract next clue from decoded content...")
    # Look for hash or encoded data in decoded content
    stage3_match = re.search(r'([0-9a-fA-F]{32,})', stage2)
    if stage3_match:
        hash_value = stage3_match.group(1)
        print(f"Found hash: {hash_value}")
        
        print("\nStage 4: Search for hash in logs...")
        hash_matches = re.findall(f'{hash_value}.*', content)
        if hash_matches:
            print(f"Hash context: {hash_matches[0]}")
            
            # Extract next encoded data from context
            final_match = re.search(r'FINAL:([A-Za-z0-9+/=]+)', hash_matches[0])
            if final_match:
                flag = base64.b64decode(final_match.group(1)).decode('utf-8')
                print(f"\n=== FLAG FOUND: {flag} ===")

solve_chained_challenge('challenge.log')
```

### Dependency resolution

**Build dependency graph from logs:**

```python
import re
from collections import defaultdict

def build_dependency_graph(log_file):
    """
    Parse logs to identify which stages depend on previous stages
    """
    dependencies = defaultdict(set)
    stage_data = {}
    
    with open(log_file, 'r') as f:
        for line in f:
            # Look for stage declarations
            stage_match = re.search(r'Stage(\d+):', line)
            if stage_match:
                stage_num = int(stage_match.group(1))
                stage_data[stage_num] = line.strip()
                
                # Look for dependencies
                dep_match = re.search(r'requires Stage(\d+)', line)
                if dep_match:
                    dep_num = int(dep_match.group(1))
                    dependencies[stage_num].add(dep_num)
    
    # Topological sort for solving order
    def topo_sort(stages, deps):
        visited = set()
        stack = []
        
        def visit(stage):
            if stage in visited:
                return
            visited.add(stage)
            for dep in deps.get(stage, []):
                visit(dep)
            stack.append(stage)
        
        for stage in stages:
            visit(stage)
        
        return stack
    
    order = topo_sort(stage_data.keys(), dependencies)
    
    print("Stage solving order:")
    for idx, stage in enumerate(order, 1):
        print(f"{idx}. Stage {stage}")
        if dependencies[stage]:
            print(f"   Depends on: {', '.join(map(str, dependencies[stage]))}")

build_dependency_graph('multistage.log')
```

### Time-delayed challenges

**Extract events that occur at specific intervals:**

```python
import pandas as pd
from datetime import timedelta

def find_timed_sequence(log_file, interval_seconds=60):
    """
    Find log entries that occur at regular intervals
    """
    data = []
    
    with open(log_file, 'r') as f:
        for line in f:
            # Parse timestamp (adjust format)
            match = re.search(r'\[(\d{2}/\w{3}/\d{4}:\d{2}:\d{2}:\d{2})', line)
            if match:
                timestamp = pd.to_datetime(match.group(1), format='%d/%b/%Y:%H:%M:%S')
                data.append({'time': timestamp, 'line': line.strip()})
    
    df = pd.DataFrame(data).sort_values('time')
    
    # Find sequences with consistent intervals
    df['time_diff'] = df['time'].diff().dt.total_seconds()
    
    # Filter for target interval (5 seconds tolerance)
    sequences = df[
        (df['time_diff'] >= interval_seconds - 5) & 
        (df['time_diff'] <= interval_seconds + 5)
    ]
    
    print(f"Found {len(sequences)} entries at ~{interval_seconds}s intervals:")
    for _, row in sequences.iterrows():
        print(f"{row['time']}: {row['line'][:100]}")

find_timed_sequence('access.log', interval_seconds=60)
```

### Parallel challenge branches

**Detect and solve parallel challenge paths:**

```bash
# Identify different challenge tracks by unique identifiers
grep -oE 'track[A-Z]' challenge.log | sort | uniq

# Extract each track separately
for track in A B C; do
    echo "=== Track $track ==="
    grep "track$track" challenge.log | \
        grep -oE 'data:[A-Za-z0-9+/=]+' | \
        cut -d: -f2 | \
        while read data; do
            echo "$data" | base64 -d
        done
    echo ""
done
```

---

**Important related topics:**

- Steganography in log file formats (PCAP, binary logs)
- Frequency analysis for substitution ciphers in logs
- Network protocol analysis for application-layer puzzles
- SQL injection payload reconstruction from WAF logs

---

# Log File Formats

Understanding log file formats is critical for effective parsing, analysis, and tool selection during CTF challenges. Different formats offer varying levels of structure, readability, and parsing complexity.

## Syslog format (RFC 3164, RFC 5424)

Syslog is the de facto standard for system logging on Unix-like systems. Two primary RFCs define syslog formats with significant structural differences.

### RFC 3164 (BSD Syslog - Legacy Format)

**Structure:**

```
<Priority>Timestamp Hostname Process[PID]: Message
```

**Priority calculation:**

```
Priority = Facility  8 + Severity
```

**Facilities (0-23):**

```
0  = kern     (kernel messages)
1  = user     (user-level messages)
2  = mail     (mail system)
3  = daemon   (system daemons)
4  = auth     (security/authorization)
5  = syslog   (syslogd internal)
6  = lpr      (line printer)
7  = news     (network news)
8  = uucp     (UUCP subsystem)
9  = cron     (clock daemon)
10 = authpriv (security/authorization - private)
16 = local0   (local use 0-7)
...
23 = local7
```

**Severities (0-7):**

```
0 = Emergency (emerg)   - System unusable
1 = Alert     (alert)   - Action must be taken immediately
2 = Critical  (crit)    - Critical conditions
3 = Error     (err)     - Error conditions
4 = Warning   (warning) - Warning conditions
5 = Notice    (notice)  - Normal but significant
6 = Info      (info)    - Informational
7 = Debug     (debug)   - Debug-level messages
```

**Example RFC 3164 log entries:**

```
<34>Oct 11 22:14:15 mymachine su: 'su root' failed for lonvick on /dev/pts/8
<13>Feb  5 17:32:18 10.0.0.99 Use the BFG!
<165>Aug 24 05:34:00 CST 1987 mymachine myproc[10]: %% It's time to make the do-nuts.
```

**Breaking down priority <34>:**

```
Priority = 34
Facility = 34 / 8 = 4 (auth)
Severity = 34 % 8 = 2 (critical)
```

**Timestamp format (RFC 3164):**

```
Mmm dd hh:mm:ss
```

**Critical limitation**: No year, no timezone, no subsecond precision.

**Common RFC 3164 patterns:**

**Standard auth log:**

```
Oct 29 14:23:45 webserver sshd[12345]: Failed password for invalid user admin from 192.168.1.100 port 54321 ssh2
```

**Apache error log (syslog format):**

```
<27>Oct 29 14:30:12 webserver apache2: [error] [client 10.0.0.5] File does not exist: /var/www/html/admin.php
```

**Parsing RFC 3164 with regex:**

```bash
# Basic pattern
^<(\d+)>(\w{3}\s+\d{1,2}\s+\d{2}:\d{2}:\d{2})\s+(\S+)\s+([^:\[\]]+)(?:\[(\d+)\])?:\s+(.*)$

# Groups:
# 1: Priority
# 2: Timestamp
# 3: Hostname
# 4: Process name
# 5: PID (optional)
# 6: Message
```

**Python parsing example:**

```python
import re
from datetime import datetime

RFC3164_PATTERN = re.compile(
    r'^<(?P<priority>\d+)>'
    r'(?P<timestamp>\w{3}\s+\d{1,2}\s+\d{2}:\d{2}:\d{2})\s+'
    r'(?P<hostname>\S+)\s+'
    r'(?P<process>[^:\[\]]+)'
    r'(?:\[(?P<pid>\d+)\])?:\s+'
    r'(?P<message>.*)$'
)

def parse_rfc3164(line):
    match = RFC3164_PATTERN.match(line)
    if not match:
        return None
    
    data = match.groupdict()
    priority = int(data['priority'])
    
    return {
        'facility': priority // 8,
        'severity': priority % 8,
        'timestamp': data['timestamp'],
        'hostname': data['hostname'],
        'process': data['process'],
        'pid': data['pid'],
        'message': data['message']
    }

# Example usage
log_line = "<34>Oct 29 14:23:45 webserver sshd[12345]: Failed password for admin from 192.168.1.100"
parsed = parse_rfc3164(log_line)
print(f"Facility: {parsed['facility']} (auth), Severity: {parsed['severity']} (critical)")
```

**Using logger command to generate RFC 3164 logs:**

```bash
# Basic usage
logger "Test message"

# Specify facility and severity
logger -p auth.warning "Failed login attempt"
logger -p local0.info "Custom application message"

# Include process name and PID
logger -t myapp[$$] "Application started"

# Remote syslog server
logger -n 192.168.1.10 -P 514 "Remote log message"
```

### RFC 5424 (Modern Syslog)

**Structure:**

```
<Priority>Version Timestamp Hostname Application PID MessageID [StructuredData] Message
```

**Example RFC 5424 log entry:**

```
<165>1 2003-10-11T22:14:15.003Z mymachine.example.com evntslog - ID47 [exampleSDID@32473 iut="3" eventSource="Application" eventID="1011"] An application event log entry...
```

**Key improvements over RFC 3164:**

- Version number (always `1`)
- ISO 8601 timestamp with timezone and microsecond precision
- Message ID for categorization
- Structured Data (SD-PARAMS in key-value format)
- Explicit NIL values (`-`) for missing fields

**Field breakdown:**

```
<Priority>    : <165> (Facility 20/local4, Severity 5/notice)
Version       : 1
Timestamp     : 2003-10-11T22:14:15.003Z (ISO 8601 with milliseconds)
Hostname      : mymachine.example.com
Application   : evntslog
PID           : - (NIL, not provided)
MessageID     : ID47
Structured Data: [exampleSDID@32473 iut="3" eventSource="Application" eventID="1011"]
Message       : An application event log entry...
```

**Structured Data format:**

```
[SD-ID@EnterpriseNumber param1="value1" param2="value2"][SD-ID2 param3="value3"]
```

**Common structured data examples:**

**timeQuality (clock information):**

```
[timeQuality tzKnown="1" isSynced="1" syncAccuracy="60000"]
```

**origin (detailed source info):**

```
[origin ip="192.168.1.100" enterpriseId="32473" software="sshd" swVersion="8.2p1"]
```

**meta (additional context):**

```
[meta sequenceId="1234" sysUpTime="3600000" language="en"]
```

**Parsing RFC 5424 with Python:**

```python
import re
from datetime import datetime

RFC5424_PATTERN = re.compile(
    r'^<(?P<priority>\d+)>'
    r'(?P<version>\d+)\s+'
    r'(?P<timestamp>\S+)\s+'
    r'(?P<hostname>\S+)\s+'
    r'(?P<app_name>\S+)\s+'
    r'(?P<procid>\S+)\s+'
    r'(?P<msgid>\S+)\s+'
    r'(?P<structured_data>(?:\[.*?\])+|-)\s*'
    r'(?P<message>.*)$',
    re.DOTALL
)

def parse_structured_data(sd_string):
    """Parse structured data elements into dictionary."""
    if sd_string == '-':
        return {}
    
    sd_dict = {}
    # Pattern for [SD-ID param="value" param2="value2"]
    sd_pattern = re.compile(r'\[([^\s\]]+)\s+([^\]]+)\]')
    
    for match in sd_pattern.finditer(sd_string):
        sd_id = match.group(1)
        params_str = match.group(2)
        
        # Parse key="value" pairs
        params = {}
        param_pattern = re.compile(r'(\w+)="([^"]*)"')
        for param_match in param_pattern.finditer(params_str):
            params[param_match.group(1)] = param_match.group(2)
        
        sd_dict[sd_id] = params
    
    return sd_dict

def parse_rfc5424(line):
    match = RFC5424_PATTERN.match(line)
    if not match:
        return None
    
    data = match.groupdict()
    priority = int(data['priority'])
    
    return {
        'facility': priority // 8,
        'severity': priority % 8,
        'version': int(data['version']),
        'timestamp': data['timestamp'],
        'hostname': data['hostname'],
        'app_name': data['app_name'],
        'procid': data['procid'],
        'msgid': data['msgid'],
        'structured_data': parse_structured_data(data['structured_data']),
        'message': data['message']
    }

# Example
log_line = '<165>1 2025-10-29T14:23:45.123456Z webserver sshd 12345 ID47 [auth@32473 result="fail" user="admin"] Failed password'
parsed = parse_rfc5424(log_line)
print(parsed['structured_data'])  # {'auth@32473': {'result': 'fail', 'user': 'admin'}}
```

**Using rsyslog to generate RFC 5424:**

```bash
# /etc/rsyslog.conf or /etc/rsyslog.d/50-default.conf
# Use RSYSLOG_SyslogProtocol23Format template
*.* @@(o)192.168.1.10:514;RSYSLOG_SyslogProtocol23Format

# Or define custom template
template(name="RFC5424Format" type="string"
  string="<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% %msgid% %structured-data% %msg%\n"
)

# Apply template
*.* action(type="omfwd" target="192.168.1.10" port="514" protocol="tcp" template="RFC5424Format")
```

**Practical CTF considerations:**

**Extracting auth failures from mixed syslog formats:**

```bash
# RFC 3164
grep "Failed password" /var/log/auth.log | \
  awk '{print $1, $2, $3, $9, $11, $13}' | \
  sort | uniq -c | sort -rn

# RFC 5424 with structured data
grep "Failed password" /var/log/syslog | \
  grep -oP '\[auth@\d+ result="fail" user="\K[^"]+' | \
  sort | uniq -c | sort -rn
```

**Converting between formats:**

```python
def rfc3164_to_rfc5424(rfc3164_line):
    """Convert RFC 3164 to RFC 5424 format."""
    parsed = parse_rfc3164(rfc3164_line)
    if not parsed:
        return None
    
    # Construct RFC 5424 timestamp (add year and timezone)
    timestamp = datetime.now().strftime('%Y') + ' ' + parsed['timestamp']
    dt = datetime.strptime(timestamp, '%Y %b %d %H:%M:%S')
    iso_timestamp = dt.isoformat() + 'Z'
    
    priority = parsed['facility'] * 8 + parsed['severity']
    procid = parsed['pid'] if parsed['pid'] else '-'
    
    return (f"<{priority}>1 {iso_timestamp} {parsed['hostname']} "
            f"{parsed['process']} {procid} - - {parsed['message']}")
```

**Common syslog parsing tools:**

**Using `rsyslog` imfile module:**

```bash
# /etc/rsyslog.d/custom.conf
module(load="imfile")

input(type="imfile"
      File="/var/log/app/custom.log"
      Tag="customapp"
      Severity="info"
      Facility="local0")
```

**Using `syslog-ng` for advanced parsing:**

```bash
# /etc/syslog-ng/syslog-ng.conf
source s_custom {
    file("/var/log/app/custom.log" 
         flags(no-parse) 
         program_override("customapp"));
};

parser p_custom {
    csv-parser(
        columns("timestamp", "level", "user", "action")
        delimiters(",")
    );
};

destination d_parsed {
    file("/var/log/parsed/${level}_${action}.log" 
         template("${timestamp} ${user} ${action}\n"));
};

log {
    source(s_custom);
    parser(p_custom);
    destination(d_parsed);
};
```

## JSON logs

JSON (JavaScript Object Notation) logs provide structured, machine-readable format ideal for complex data and automated parsing. Increasingly common in modern applications and containerized environments.

**Advantages:**

- Native hierarchical structure (nested objects/arrays)
- Self-describing schema
- No ambiguous delimiters
- Direct parsing in most languages
- Easy integration with NoSQL databases

**Disadvantages:**

- Higher storage overhead (verbose field names)
- Less human-readable than plain text
- Potential performance impact for high-volume logging

### Common JSON log formats

**Application log (structured):**

```json
{
  "timestamp": "2025-10-29T14:23:45.123456Z",
  "level": "ERROR",
  "logger": "com.example.app.AuthService",
  "message": "Authentication failed",
  "context": {
    "username": "admin",
    "source_ip": "192.168.1.100",
    "attempt_number": 5,
    "reason": "invalid_credentials"
  },
  "thread": "http-nio-8080-exec-3",
  "trace_id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890"
}
```

**Web server log (JSON format):**

```json
{
  "timestamp": "2025-10-29T14:25:10.456Z",
  "remote_addr": "10.0.0.5",
  "request_method": "POST",
  "request_uri": "/api/login",
  "status": 401,
  "body_bytes_sent": 145,
  "http_referer": "https://example.com/login",
  "http_user_agent": "Mozilla/5.0 (X11; Linux x86_64) Chrome/118.0.0.0",
  "request_time": 0.234,
  "upstream_response_time": 0.198,
  "ssl_protocol": "TLSv1.3",
  "ssl_cipher": "TLS_AES_256_GCM_SHA384"
}
```

**Docker container logs (automatically JSON):**

```json
{
  "log": "192.168.1.100 - - [29/Oct/2025:14:30:00 +0000] \"GET /admin HTTP/1.1\" 404 162\n",
  "stream": "stdout",
  "time": "2025-10-29T14:30:00.123456789Z"
}
```

**AWS CloudWatch logs:**

```json
{
  "timestamp": 1698765432000,
  "message": "User login attempt",
  "logStream": "i-0123456789abcdef/var/log/auth.log",
  "logGroup": "/aws/ec2/production",
  "awsRequestId": "a1b2c3d4-e5f6-7890-abcd-ef1234567890"
}
```

### Parsing JSON logs

**Command-line with jq:**

```bash
# Extract specific field
cat app.log | jq '.context.username'

# Filter by log level
cat app.log | jq 'select(.level == "ERROR")'

# Filter by timestamp range
cat app.log | jq 'select(.timestamp >= "2025-10-29T14:00:00Z" and .timestamp <= "2025-10-29T15:00:00Z")'

# Extract multiple fields
cat app.log | jq '{time: .timestamp, user: .context.username, ip: .context.source_ip}'

# Count occurrences by field
cat app.log | jq -r '.context.username' | sort | uniq -c | sort -rn

# Complex filter: failed logins from specific IP
cat app.log | jq 'select(.level == "ERROR" and .context.source_ip == "192.168.1.100" and (.message | contains("failed")))'

# Nested field extraction
cat app.log | jq '.context.metadata.geo_location.country'

# Array handling
echo '{"users": ["alice", "bob", "charlie"]}' | jq '.users[]'
echo '{"users": ["alice", "bob", "charlie"]}' | jq '.users | length'

# Create histogram of status codes
cat web.log | jq -r '.status' | sort | uniq -c | sort -rn

# Calculate average response time
cat web.log | jq -s 'map(.request_time) | add / length'

# Top 10 user agents
cat web.log | jq -r '.http_user_agent' | sort | uniq -c | sort -rn | head -10

# Pretty print minified JSON
cat minified.log | jq '.'

# Compact output (remove whitespace)
cat app.log | jq -c '.'

# Convert to CSV
cat app.log | jq -r '[.timestamp, .level, .message] | @csv'

# Filter and transform
cat app.log | jq 'select(.level == "ERROR") | {time: .timestamp, error: .message, user: .context.username}'
```

**Python parsing:**

```python
import json
from datetime import datetime
from collections import defaultdict

# Basic parsing
with open('app.log', 'r') as f:
    for line in f:
        try:
            log_entry = json.loads(line)
            if log_entry.get('level') == 'ERROR':
                print(f"{log_entry['timestamp']}: {log_entry['message']}")
        except json.JSONDecodeError:
            print(f"Invalid JSON: {line}")
            continue

# Aggregation example: count errors by username
error_counts = defaultdict(int)

with open('app.log', 'r') as f:
    for line in f:
        try:
            log_entry = json.loads(line)
            if log_entry.get('level') == 'ERROR':
                username = log_entry.get('context', {}).get('username', 'unknown')
                error_counts[username] += 1
        except json.JSONDecodeError:
            continue

# Sort by count descending
for user, count in sorted(error_counts.items(), key=lambda x: x[1], reverse=True):
    print(f"{user}: {count} errors")

# Time-series analysis
from datetime import datetime, timedelta

def parse_timestamp(ts_str):
    """Parse ISO 8601 timestamp."""
    return datetime.fromisoformat(ts_str.replace('Z', '+00:00'))

# Count events per hour
hourly_counts = defaultdict(int)

with open('app.log', 'r') as f:
    for line in f:
        try:
            log_entry = json.loads(line)
            timestamp = parse_timestamp(log_entry['timestamp'])
            hour_key = timestamp.replace(minute=0, second=0, microsecond=0)
            hourly_counts[hour_key] += 1
        except (json.JSONDecodeError, KeyError, ValueError):
            continue

for hour in sorted(hourly_counts.keys()):
    print(f"{hour}: {hourly_counts[hour]} events")

# Nested field extraction with safe navigation
def safe_get(dct, *keys):
    """Safely extract nested dictionary values."""
    for key in keys:
        try:
            dct = dct[key]
        except (KeyError, TypeError):
            return None
    return dct

with open('app.log', 'r') as f:
    for line in f:
        log_entry = json.loads(line)
        geo_country = safe_get(log_entry, 'context', 'metadata', 'geo_location', 'country')
        if geo_country:
            print(f"Country: {geo_country}")
```

**Streaming JSON logs with Python:**

```python
import json
import sys

# Process large JSON log files line-by-line (JSONL format)
def process_json_stream(filename, filter_func=None):
    """
    Process JSON logs without loading entire file into memory.
    
    Args:
        filename: Path to JSONL file
        filter_func: Optional function to filter log entries (returns bool)
    """
    with open(filename, 'r') as f:
        for line_num, line in enumerate(f, 1):
            try:
                log_entry = json.loads(line)
                
                if filter_func is None or filter_func(log_entry):
                    yield log_entry
                    
            except json.JSONDecodeError as e:
                print(f"Line {line_num}: Invalid JSON - {e}", file=sys.stderr)
                continue

# Example usage
def is_auth_failure(log_entry):
    return (log_entry.get('level') == 'ERROR' and 
            'auth' in log_entry.get('message', '').lower())

for entry in process_json_stream('app.log', filter_func=is_auth_failure):
    print(f"{entry['timestamp']}: {entry['context']['username']}")
```

### Generating JSON logs

**Python logging to JSON:**

```python
import logging
import json
from datetime import datetime

class JSONFormatter(logging.Formatter):
    def format(self, record):
        log_entry = {
            'timestamp': datetime.utcnow().isoformat() + 'Z',
            'level': record.levelname,
            'logger': record.name,
            'message': record.getMessage(),
            'module': record.module,
            'function': record.funcName,
            'line': record.lineno
        }
        
        # Add extra fields if present
        if hasattr(record, 'context'):
            log_entry['context'] = record.context
        
        # Add exception info if present
        if record.exc_info:
            log_entry['exception'] = self.formatException(record.exc_info)
        
        return json.dumps(log_entry)

# Setup logger
logger = logging.getLogger('app')
handler = logging.FileHandler('app.log')
handler.setFormatter(JSONFormatter())
logger.addHandler(handler)
logger.setLevel(logging.INFO)

# Log with context
logger.error('Authentication failed', extra={
    'context': {
        'username': 'admin',
        'source_ip': '192.168.1.100',
        'attempt_number': 5
    }
})
```

**Nginx JSON log format:**

```nginx
log_format json_combined escape=json
'{'
  '"timestamp":"$time_iso8601",'
  '"remote_addr":"$remote_addr",'
  '"request_method":"$request_method",'
  '"request_uri":"$request_uri",'
  '"status":$status,'
  '"body_bytes_sent":$body_bytes_sent,'
  '"http_referer":"$http_referer",'
  '"http_user_agent":"$http_user_agent",'
  '"request_time":$request_time,'
  '"upstream_response_time":"$upstream_response_time"'
'}';

access_log /var/log/nginx/access.log json_combined;
```

**Apache JSON log format (requires mod_log_config):**

```apache
LogFormat "{\"timestamp\":\"%{%Y-%m-%dT%H:%M:%S}t.%{usec_frac}tZ\", \"remote_addr\":\"%a\", \"request_method\":\"%m\", \"request_uri\":\"%U\", \"query_string\":\"%q\", \"status\":%>s, \"bytes_sent\":%B, \"referer\":\"%{Referer}i\", \"user_agent\":\"%{User-agent}i\", \"response_time\":%D}" json_log

CustomLog /var/log/apache2/access.log json_log
```

### CTF-specific JSON log analysis

**Detecting SQL injection attempts:**

```bash
# Look for SQL keywords in request URIs
cat web.log | jq 'select(.request_uri | test("(union|select|insert|update|delete|drop|exec|script)"; "i"))'

# Extract suspicious patterns
cat web.log | jq -r 'select(.request_uri | test("'\'|--|;|union|0x[0-9a-f]+"; "i")) | "\(.timestamp) \(.remote_addr) \(.request_uri)"'
```

**Identifying command injection:**

```bash
# Shell metacharacters in parameters
cat app.log | jq 'select(.context.parameters // "" | test("[;&|`$(){}]"))'
```

**Brute force detection:**

```bash
# Count failed auth attempts by IP
cat app.log | jq -r 'select(.level == "ERROR" and (.message | contains("auth"))) | .context.source_ip' | sort | uniq -c | sort -rn

# Timeline of failures for specific IP
cat app.log | jq 'select(.context.source_ip == "192.168.1.100" and .level == "ERROR") | {time: .timestamp, user: .context.username}'
```

**Session hijacking indicators:**

```bash
# Multiple IPs using same session ID
cat web.log | jq -r '[.session_id, .remote_addr] | @tsv' | sort | uniq | awk '{sess[$1]++; if(sess[$1] > 1) print $1}'
```

**Data exfiltration patterns:**

```bash
# Large response sizes
cat web.log | jq 'select(.body_bytes_sent > 1048576) | {time: .timestamp, ip: .remote_addr, bytes: .body_bytes_sent, uri: .request_uri}'

# Unusual download patterns
cat web.log | jq 'select(.request_uri | endswith(".zip", ".tar", ".gz", ".7z")) | {time: .timestamp, ip: .remote_addr, file: .request_uri}'
```

## CSV logs

CSV (Comma-Separated Values) provides simple, flat structure for tabular log data. Common in legacy systems, exports, and simplified logging scenarios.

**Advantages:**

- Simple, widely supported format
- Easy to import into spreadsheets/databases
- Compact representation for flat data
- Human-readable

**Disadvantages:**

- No native support for hierarchical data
- Delimiter conflicts (commas in data)
- Inconsistent quoting/escaping standards
- No schema enforcement
- Difficult to handle multiline entries

### CSV log structure

**Basic format:**

```csv
timestamp,level,source_ip,username,action,result
2025-10-29T14:23:45Z,ERROR,192.168.1.100,admin,login,failed
2025-10-29T14:24:01Z,ERROR,192.168.1.100,admin,login,failed
2025-10-29T14:24:15Z,INFO,192.168.1.100,admin,login,success
```

**With header row:**

```csv
timestamp,remote_addr,method,uri,status,bytes,user_agent
2025-10-29T14:25:10Z,10.0.0.5,POST,/api/login,401,145,"Mozilla/5.0 (X11; Linux x86_64)"
2025-10-29T14:25:30Z,10.0.0.5,GET,/dashboard,200,8192,"Mozilla/5.0 (X11; Linux x86_64)"
```

**Handling commas and quotes:**

```csv
timestamp,username,message
2025-10-29T14:30:00Z,user1,"Login failed, invalid password"
2025-10-29T14:31:00Z,user2,"User said: ""I forgot my password"""
```

**Common escaping rules (RFC 4180):**

- Fields containing commas, quotes, or newlines must be quoted
- Quotes inside quoted fields are escaped by doubling: `""`
- Each record on separate line (CRLF or LF)

### Parsing CSV logs

**Command-line with awk:**

```bash
# Extract specific column (2nd column)
awk -F',' '{print $2}' access.csv

# Filter by condition (status code 404)
awk -F',' '$5 == 404 {print $0}' access.csv

# Sum values (total bytes sent)
awk -F',' 'NR > 1 {sum += $6} END {print sum}' access.csv

# Count occurrences by field
awk -F',' 'NR > 1 {count[$2]++} END {for (ip in count) print ip, count[ip]}' access.csv | sort -k2 -rn

# Filter by timestamp range
awk -F',' '$1 >= "2025-10-29T14:00:00Z" && $1 <= "2025-10-29T15:00:00Z"' access.csv
```

**Using csvkit tools:**

```bash
# Install csvkit
pip install csvkit

# View CSV in formatted table
csvlook access.csv | less -S

# Get column statistics
csvstat access.csv

# Extract specific columns
csvcut -c timestamp,status,bytes access.csv

# Filter rows
csvgrep -c status -m 404 access.csv

# Sort by column
csvsort -c bytes -r access.csv | head -10

# Convert to JSON
csvjson access.csv > access.json

# SQL queries on CSV
csvsql --query "SELECT status, COUNT(*) as count FROM access GROUP BY status" access.csv

# Join two CSV files
csvjoin -c username users.csv access.csv
```

**Python with csv module:**

```python
import csv
from collections import defaultdict

# Basic reading
with open('access.csv', 'r') as f:
    reader = csv.DictReader(f)
    for row in reader:
        print(f"{row['timestamp']}: {row['status']}")

# Manual column specification (no header)
with open('access.csv', 'r') as f:
    reader = csv.reader(f)
    fieldnames = next(reader)  # Skip header
    for row in reader:
        timestamp, ip, method, uri, status, bytes_sent = row
        if status == '404':
            print(f"{timestamp}: 404 from {ip} for {uri}")

# Count by column value
status_counts = defaultdict(int)

with open('access.csv', 'r') as f:
    reader = csv.DictReader(f)
    for row in reader:
        status_counts[row['status']] += 1

for status, count in sorted(status_counts.items()):
    print(f"{status}: {count}")

# Filtering and writing
with open('access.csv', 'r') as infile, open('errors.csv', 'w', newline='') as outfile:
    reader = csv.DictReader(infile)
    writer = csv.DictWriter(outfile, fieldnames=reader.fieldnames)
    writer.writeheader()
    
    for row in reader:
        if int(row['status']) >= 400:
            writer.writerow(row)

# Aggregation by time window
from datetime import datetime, timedelta
from collections import defaultdict

hourly_counts = defaultdict(int)

with open('access.csv', 'r') as f:
    reader = csv.DictReader(f)
    for row in reader:
        timestamp = datetime.fromisoformat(row['timestamp'].replace('Z', '+00:00'))
        hour_key = timestamp.replace(minute=0, second=0, microsecond=0)
        hourly_counts[hour_key] += 1

for hour in sorted(hourly_counts.keys()):
    print(f"{hour}: {hourly_counts[hour]} requests")

# Handling quoted fields with embedded commas/newlines
with open('complex.csv', 'r') as f:
    reader = csv.reader(f, quoting=csv.QUOTE_ALL)
    for row in reader:
        # Properly handles: "value with, comma","value with ""quotes"""
        print(row)
```

**Python with pandas (recommended for complex analysis):**

```python
import pandas as pd

# Read CSV with automatic type inference
df = pd.read_csv('access.csv', parse_dates=['timestamp'])

# Basic info
print(df.info())
print(df.describe())

# Filter by condition
errors = df[df['status'] >= 400]
print(errors.head())

# Count by column
print(df['status'].value_counts())

# Group by and aggregate
ip_summary = df.groupby('remote_addr').agg({
    'status': 'count',
    'bytes': 'sum'
}).rename(columns={'status': 'request_count', 'bytes': 'total_bytes'})
print(ip_summary.sort_values('request_count', ascending=False).head(10))

# Time-based analysis
df['hour'] = df['timestamp'].dt.hour
hourly = df.groupby('hour').size()
print(hourly)

# Multiple conditions
suspicious = df[
    (df['status'] == 404) & 
    (df['uri'].str.contains('admin|config|.env', case=False, na=False))
]
print(suspicious[['timestamp', 'remote_addr', 'uri']])

# Calculate percentiles
print(df['bytes'].quantile([0.25, 0.5, 0.75, 0.95, 0.99]))

# Detect anomalies (requests > 3 standard deviations)
mean_bytes = df['bytes'].mean()
std_bytes = df['bytes'].std()
anomalies = df[df['bytes'] > mean_bytes + 3 * std_bytes]
print(f"Found {len(anomalies)} anomalous requests")

# Pivot table
pivot = df.pivot_table(
    values='bytes',
    index='remote_addr',
    columns='status',
    aggfunc='sum',
    fill_value=0
)
print(pivot)

# Time window analysis (resample)
df.set_index('timestamp', inplace=True)
requests_per_5min = df.resample('5min').size()
print(requests_per_5min)

# Rolling window (moving average)
df['rolling_avg_bytes'] = df['bytes'].rolling(window=100).mean()

# Export filtered results
errors.to_csv('errors_only.csv', index=False)
```

**Handling malformed CSV files:**

```python
import csv

# Skip bad lines
with open('malformed.csv', 'r') as f:
    reader = csv.reader(f)
    for i, row in enumerate(reader):
        try:
            # Expected number of columns
            if len(row) != 7:
                print(f"Line {i+1}: Invalid column count: {len(row)}")
                continue
            # Process valid row
            timestamp, ip, method, uri, status, bytes_sent, user_agent = row
            print(f"{timestamp}: {status}")
        except Exception as e:
            print(f"Line {i+1}: Error - {e}")
            continue

# Force quoting for problematic data
with open('output.csv', 'w', newline='') as f:
    writer = csv.writer(f, quoting=csv.QUOTE_NONNUMERIC)
    writer.writerow(['timestamp', 'message'])
    writer.writerow(['2025-10-29T14:30:00Z', 'Message with, comma'])

# Custom delimiter (tab-separated)
with open('access.tsv', 'r') as f:
    reader = csv.DictReader(f, delimiter='\t')
    for row in reader:
        print(row)

# Handle different line terminators
with open('windows.csv', 'r', newline='') as f:
    reader = csv.reader(f)
    # Automatically handles CRLF, LF, or CR
```

### Converting between log formats

**Syslog to CSV:**

```python
import csv
import re
from datetime import datetime

RFC3164_PATTERN = re.compile(
    r'^<(\d+)>(\w{3}\s+\d{1,2}\s+\d{2}:\d{2}:\d{2})\s+(\S+)\s+([^:\[\]]+)(?:\[(\d+)\])?:\s+(.*)$'
)

with open('syslog.log', 'r') as infile, open('syslog.csv', 'w', newline='') as outfile:
    writer = csv.writer(outfile)
    writer.writerow(['timestamp', 'hostname', 'process', 'pid', 'facility', 'severity', 'message'])
    
    for line in infile:
        match = RFC3164_PATTERN.match(line)
        if match:
            priority = int(match.group(1))
            facility = priority // 8
            severity = priority % 8
            
            writer.writerow([
                match.group(2),  # timestamp
                match.group(3),  # hostname
                match.group(4),  # process
                match.group(5) or '',  # pid
                facility,
                severity,
                match.group(6)   # message
            ])
```

**JSON to CSV:**

```python
import json
import csv

# Flatten JSON structure
def flatten_json(data, parent_key='', sep='_'):
    """Flatten nested JSON into single-level dictionary."""
    items = []
    for k, v in data.items():
        new_key = f"{parent_key}{sep}{k}" if parent_key else k
        if isinstance(v, dict):
            items.extend(flatten_json(v, new_key, sep=sep).items())
        elif isinstance(v, list):
            items.append((new_key, json.dumps(v)))
        else:
            items.append((new_key, v))
    return dict(items)

# Convert JSONL to CSV
with open('app.log', 'r') as infile:
    # Read first line to determine fields
    first_line = json.loads(infile.readline())
    flat_first = flatten_json(first_line)
    fieldnames = list(flat_first.keys())
    
    # Reset to beginning
    infile.seek(0)
    
    with open('app.csv', 'w', newline='') as outfile:
        writer = csv.DictWriter(outfile, fieldnames=fieldnames)
        writer.writeheader()
        
        for line in infile:
            try:
                log_entry = json.loads(line)
                flat_entry = flatten_json(log_entry)
                writer.writerow(flat_entry)
            except json.JSONDecodeError:
                continue
```

**CSV to JSON:**

```python
import csv
import json

with open('access.csv', 'r') as infile, open('access.json', 'w') as outfile:
    reader = csv.DictReader(infile)
    
    for row in reader:
        # Convert to JSON object per line (JSONL format)
        json.dump(row, outfile)
        outfile.write('\n')

# Or create JSON array
with open('access.csv', 'r') as infile, open('access_array.json', 'w') as outfile:
    reader = csv.DictReader(infile)
    data = list(reader)
    json.dump(data, outfile, indent=2)
```

### CTF-specific CSV analysis

**Detecting brute force attacks:**

```bash
# Count failed login attempts by IP
csvgrep -c result -m failed auth.csv | \
  csvcut -c source_ip | \
  tail -n +2 | \
  sort | uniq -c | sort -rn

# Using awk
awk -F',' '$6 == "failed" {ips[$3]++} END {for (ip in ips) if (ips[ip] > 10) print ip, ips[ip]}' auth.csv | sort -k2 -rn
```

**Identifying SQL injection attempts:**

```python
import csv
import re

sql_patterns = [
    r"union\s+select",
    r"'\s*or\s*'1'\s*=\s*'1",
    r";\s*drop\s+table",
    r"exec\s*\(",
    r"0x[0-9a-f]+",
    r"--\s*$"
]

with open('access.csv', 'r') as f:
    reader = csv.DictReader(f)
    for row in reader:
        uri = row['uri'].lower()
        for pattern in sql_patterns:
            if re.search(pattern, uri, re.IGNORECASE):
                print(f"{row['timestamp']} - {row['remote_addr']} - {row['uri']}")
                break
```

**Time-based correlation:**

```python
import csv
from datetime import datetime, timedelta

# Find IPs that had failed auth followed by success within 5 minutes
events = []

with open('auth.csv', 'r') as f:
    reader = csv.DictReader(f)
    for row in reader:
        events.append({
            'timestamp': datetime.fromisoformat(row['timestamp'].replace('Z', '+00:00')),
            'ip': row['source_ip'],
            'username': row['username'],
            'result': row['result']
        })

# Sort by timestamp
events.sort(key=lambda x: x['timestamp'])

# Find pattern: failed attempts followed by success
successful_bruteforce = []

for i, event in enumerate(events):
    if event['result'] == 'failed':
        # Look ahead for success from same IP within 5 minutes
        for j in range(i+1, len(events)):
            next_event = events[j]
            time_diff = next_event['timestamp'] - event['timestamp']
            
            if time_diff > timedelta(minutes=5):
                break
            
            if (next_event['ip'] == event['ip'] and 
                next_event['result'] == 'success'):
                successful_bruteforce.append({
                    'ip': event['ip'],
                    'failed_time': event['timestamp'],
                    'success_time': next_event['timestamp'],
                    'username': next_event['username']
                })
                break

for entry in successful_bruteforce:
    print(f"IP {entry['ip']} - Failed at {entry['failed_time']}, Success at {entry['success_time']} as {entry['username']}")
```

**Anomaly detection with statistics:**

```python
import csv
import pandas as pd
import numpy as np

# Load data
df = pd.read_csv('access.csv')
df['timestamp'] = pd.to_datetime(df['timestamp'])

# Detect anomalous request sizes
Q1 = df['bytes'].quantile(0.25)
Q3 = df['bytes'].quantile(0.75)
IQR = Q3 - Q1

# Outliers: values below Q1 - 1.5*IQR or above Q3 + 1.5*IQR
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = df[(df['bytes'] < lower_bound) | (df['bytes'] > upper_bound)]
print(f"Found {len(outliers)} anomalous requests by size")
print(outliers[['timestamp', 'remote_addr', 'uri', 'bytes']].head(10))

# Detect unusual request frequency
df.set_index('timestamp', inplace=True)
requests_per_minute = df.resample('1min')['remote_addr'].count()

mean_rate = requests_per_minute.mean()
std_rate = requests_per_minute.std()

# Periods with abnormally high traffic
high_traffic = requests_per_minute[requests_per_minute > mean_rate + 2 * std_rate]
print(f"\nHigh traffic periods:")
print(high_traffic)
```

**User agent analysis:**

```bash
# Extract unique user agents
csvcut -c user_agent access.csv | tail -n +2 | sort -u

# Count by user agent
csvcut -c user_agent access.csv | tail -n +2 | sort | uniq -c | sort -rn

# Find suspicious/unusual user agents
csvcut -c user_agent access.csv | grep -iE "(bot|crawler|scan|nmap|sqlmap|nikto|metasploit)"

# Empty or very short user agents (often bots/scripts)
awk -F',' 'length($7) < 10 && NR > 1 {print $0}' access.csv
```

**Geographic analysis (if IP geolocation data available):**

```python
import csv
from collections import Counter

# Assuming CSV has country column from GeoIP enrichment
country_counts = Counter()

with open('access_geoip.csv', 'r') as f:
    reader = csv.DictReader(f)
    for row in reader:
        country_counts[row['country']] += 1

print("Top 10 countries:")
for country, count in country_counts.most_common(10):
    print(f"{country}: {count}")

# Find IPs from unexpected countries for your service
expected_countries = ['US', 'CA', 'GB', 'DE']

with open('access_geoip.csv', 'r') as f:
    reader = csv.DictReader(f)
    for row in reader:
        if row['country'] not in expected_countries:
            print(f"Unexpected: {row['remote_addr']} from {row['country']} accessing {row['uri']}")
```

### Performance considerations

**Large CSV file handling:**

```python
import csv

# Process in chunks (memory-efficient)
chunk_size = 10000
chunk = []

with open('huge.csv', 'r') as f:
    reader = csv.DictReader(f)
    
    for i, row in enumerate(reader):
        chunk.append(row)
        
        if len(chunk) >= chunk_size:
            # Process chunk
            process_chunk(chunk)
            chunk = []
    
    # Process remaining rows
    if chunk:
        process_chunk(chunk)

# Using pandas with chunking
for chunk in pd.read_csv('huge.csv', chunksize=10000):
    # Process each chunk
    result = chunk[chunk['status'] == 404]
    result.to_csv('filtered.csv', mode='a', header=False, index=False)
```

**Optimizing CSV parsing:**

```bash
# Use binary search for sorted CSV (by timestamp)
# Requires CSV to be sorted first
sort -t',' -k1 access.csv > access_sorted.csv

# Quick filtering with grep before parsing
grep "404" access.csv > errors_404.csv

# Parallel processing with GNU parallel
cat access.csv | parallel --pipe --block 10M 'csvgrep -c status -m 404'

# Using awk for speed (faster than Python for simple operations)
time awk -F',' '$5 == 404' access.csv > errors.csv
```

### Best practices for CSV logs in CTF

**When CSV is appropriate:**

- Flat, tabular data without nested structures
- Need for quick import into spreadsheet tools
- Simple log exports from databases
- Known, fixed schema

**When to avoid CSV:**

- Complex nested data (use JSON)
- Multiline log entries
- Data containing many commas or quotes
- Need for schema validation
- Real-time streaming (syslog or JSON better)

**CSV generation best practices:**

```python
import csv

# Always include header row
with open('output.csv', 'w', newline='') as f:
    writer = csv.writer(f)
    writer.writerow(['timestamp', 'level', 'message'])
    writer.writerow(['2025-10-29T14:30:00Z', 'ERROR', 'Auth failed'])

# Use appropriate quoting
with open('output.csv', 'w', newline='') as f:
    writer = csv.writer(f, quoting=csv.QUOTE_MINIMAL)  # Only quote when necessary
    # or csv.QUOTE_ALL, csv.QUOTE_NONNUMERIC, csv.QUOTE_NONE

# Handle None/NULL values consistently
with open('output.csv', 'w', newline='') as f:
    writer = csv.writer(f)
    writer.writerow(['timestamp', 'optional_field'])
    writer.writerow(['2025-10-29T14:30:00Z', None])  # Empty field
    # Consider using explicit NULL string or empty string

# Include ISO 8601 timestamps (sortable, unambiguous)
timestamp = datetime.utcnow().isoformat() + 'Z'

# Escape properly for Excel compatibility (prevents formula injection)
def excel_safe(value):
    """Prevent CSV injection attacks."""
    if isinstance(value, str) and value.startswith(('=', '+', '-', '@')):
        return "'" + value  # Prefix with single quote
    return value
```

---

**Important related topics:**

- **Log parsing libraries**: Understanding purpose-built tools like Logstash's grok patterns, Python's `parse` library, and regex best practices for complex log formats
- **Log normalization**: Converting diverse log formats into unified schema for correlation analysis
- **Custom log formats**: Parsing proprietary or application-specific log formats commonly encountered in CTF challenges
- **Binary log formats**: Handling Windows Event Logs (EVTX), database logs, and other non-text formats

---

## Binary Logs

Binary logs store data in non-human-readable formats, offering performance advantages and space efficiency but requiring specialized tools for analysis.

### Understanding Binary Log Structure

Binary logs typically contain:

- Fixed-size headers with metadata (version, timestamp, record count)
- Record entries with type identifiers and length fields
- Structured data fields (integers, timestamps, flags)
- Optional compression or encoding

**Common binary log types in CTF scenarios:**

- Database transaction logs (MySQL binlog, PostgreSQL WAL)
- System logs (utmp/wtmp/btmp on Linux)
- Application-specific logs (Packet captures, custom formats)
- Kernel logs (kmsg)

### Linux utmp/wtmp/btmp Analysis

These binary files track user login sessions and authentication attempts.

**File locations:**

```bash
/var/run/utmp    # Currently logged-in users
/var/log/wtmp    # Historical login records
/var/log/btmp    # Failed login attempts
```

**Reading with standard utilities:**

```bash
# View current users
who
w
users

# View login history
last
last -f /var/log/wtmp

# View failed login attempts (requires root)
sudo lastb
sudo lastb -f /var/log/btmp

# Detailed user session info
lastlog
```

**Manual binary parsing with utmpdump:**

```bash
# Convert binary to text format
utmpdump /var/log/wtmp > wtmp.txt

# Convert text back to binary
utmpdump -r < wtmp.txt > wtmp.binary

# Extract specific timeframe
utmpdump /var/log/wtmp | grep "2025-10-29"
```

**Structure examination with hexdump:**

```bash
# View raw structure
hexdump -C /var/run/utmp | head -n 50

# Extract specific fields (example: PID field at offset 8, 4 bytes)
hexdump -s 384 -n 4 -e '1/4 "%d\n"' /var/run/utmp
```

**Python script for custom utmp parsing:**

```python
import struct
import time

# utmp structure (simplified, architecture-dependent)
UTMP_STRUCT = "hi32s4s32s256shhiii4i20s"
UTMP_SIZE = struct.calcsize(UTMP_STRUCT)

def parse_utmp(filename):
    with open(filename, 'rb') as f:
        while True:
            data = f.read(UTMP_SIZE)
            if not data:
                break
            
            record = struct.unpack(UTMP_STRUCT, data)
            ut_type = record[0]
            ut_pid = record[1]
            ut_line = record[2].decode('utf-8', errors='ignore').strip('\x00')
            ut_id = record[3].decode('utf-8', errors='ignore').strip('\x00')
            ut_user = record[4].decode('utf-8', errors='ignore').strip('\x00')
            ut_host = record[5].decode('utf-8', errors='ignore').strip('\x00')
            ut_tv_sec = record[8]
            
            if ut_type == 7:  # USER_PROCESS
                timestamp = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(ut_tv_sec))
                print(f"{timestamp} | User: {ut_user} | Host: {ut_host} | TTY: {ut_line}")

parse_utmp('/var/log/wtmp')
```

**[Inference] The exact structure may vary across different Linux distributions and architectures (32-bit vs 64-bit).**

### MySQL Binary Logs

MySQL binary logs record database modifications for replication and point-in-time recovery.

**Enabling binary logging:**

```sql
-- Check if enabled
SHOW VARIABLES LIKE 'log_bin';

-- View binary log files
SHOW BINARY LOGS;

-- View current binary log position
SHOW MASTER STATUS;
```

Configuration in `/etc/mysql/my.cnf`:

```ini
[mysqld]
log-bin=/var/log/mysql/mysql-bin.log
server-id=1
binlog_format=ROW
```

**Analyzing with mysqlbinlog:**

```bash
# View entire binary log
mysqlbinlog /var/log/mysql/mysql-bin.000001

# Filter by database
mysqlbinlog --database=webapp /var/log/mysql/mysql-bin.000001

# Time-based extraction
mysqlbinlog --start-datetime="2025-10-29 10:00:00" \
            --stop-datetime="2025-10-29 11:00:00" \
            /var/log/mysql/mysql-bin.000001

# Output to SQL file for replay
mysqlbinlog mysql-bin.000001 > replay.sql

# Filter specific operations
mysqlbinlog mysql-bin.000001 | grep -i "DELETE FROM users"

# Verbose output with row data
mysqlbinlog -vv mysql-bin.000001
```

**Extracting specific event types:**

```bash
# Find all INSERT statements
mysqlbinlog mysql-bin.000001 | awk '/### INSERT/,/COMMIT/'

# Track admin user modifications
mysqlbinlog mysql-bin.000001 | grep -A 20 "UPDATE.*admin"

# Count operations per table
mysqlbinlog mysql-bin.000001 | grep "### INSERT INTO" | cut -d'`' -f2 | sort | uniq -c
```

**CTF use cases:**

- Recovering deleted data
- Identifying SQL injection artifacts
- Tracking unauthorized data modifications
- Reconstructing attack timelines

### systemd Journal Binary Logs

systemd stores logs in binary format at `/var/log/journal/` or `/run/log/journal/`.

**Accessing with journalctl:**

```bash
# View all logs
journalctl

# Follow live logs
journalctl -f

# View logs since boot
journalctl -b

# Specific time range
journalctl --since "2025-10-29 08:00" --until "2025-10-29 09:00"

# Filter by service
journalctl -u ssh.service
journalctl -u apache2.service -u mysql.service

# Filter by priority (0=emerg, 3=err, 6=info)
journalctl -p err

# View kernel messages only
journalctl -k

# Output in JSON format
journalctl -o json > journal.json
journalctl -o json-pretty

# Specific process ID
journalctl _PID=1234

# Export binary journal
journalctl -o export > journal.export
```

**Advanced filtering:**

```bash
# Multiple field filters
journalctl _SYSTEMD_UNIT=sshd.service _UID=0

# Failed service starts
journalctl -u '*' --no-pager | grep -i failed

# Authentication events
journalctl _COMM=sshd | grep -i "failed\|accepted"

# Disk usage by journals
journalctl --disk-usage

# Verify journal integrity
journalctl --verify
```

**Extracting journal files for offline analysis:**

```bash
# Copy journal directory
sudo cp -r /var/log/journal /path/to/investigation/

# Analyze copied journals
journalctl --directory=/path/to/investigation/journal

# Merge multiple journal files
journalctl --file=/path/to/system.journal --file=/path/to/user.journal
```

**Binary format inspection:**

```bash
# Journal file format identification
file /var/log/journal/*/system.journal

# Low-level structure examination
sudo hexdump -C /var/log/journal/*/system.journal | head -n 50
```

### Packet Capture Files (PCAP/PCAPNG)

While technically not logs, packet captures are binary files frequently analyzed in CTF challenges.

**Basic analysis with tcpdump:**

```bash
# Read pcap file
tcpdump -r capture.pcap

# Filter specific protocol
tcpdump -r capture.pcap tcp
tcpdump -r capture.pcap 'port 80 or port 443'

# Extract specific host traffic
tcpdump -r capture.pcap 'host 192.168.1.100'

# ASCII output for payloads
tcpdump -A -r capture.pcap

# Hex + ASCII output
tcpdump -X -r capture.pcap

# Save filtered results
tcpdump -r capture.pcap 'tcp port 80' -w filtered.pcap
```

**Advanced analysis with tshark:**

```bash
# Display all fields
tshark -r capture.pcap -V

# Extract HTTP objects
tshark -r capture.pcap --export-objects http,output_dir/

# Statistics summary
tshark -r capture.pcap -q -z io,stat,1

# Protocol hierarchy
tshark -r capture.pcap -q -z io,phs

# Conversations
tshark -r capture.pcap -q -z conv,tcp

# Extract specific fields
tshark -r capture.pcap -T fields -e ip.src -e ip.dst -e tcp.port

# Follow TCP stream
tshark -r capture.pcap -q -z follow,tcp,ascii,0
```

**Converting between formats:**

```bash
# PCAP to PCAPNG
editcap capture.pcap capture.pcapng

# Merge multiple captures
mergecap -w merged.pcap capture1.pcap capture2.pcap capture3.pcap

# Split by size
editcap -c 10000 large.pcap split.pcap

# Split by time
editcap -i 3600 capture.pcap hourly.pcap
```

## Windows EVT/EVTX

Windows Event Logs store system, security, and application events in proprietary binary formats.

### EVT Format (Windows XP/2003 and earlier)

Legacy format with limited structure and search capabilities.

**File locations:**

- `C:\Windows\System32\config\` (System, Security, Application)
- `C:\Windows\System32\winevt\Logs\` (may contain legacy exports)

**Analysis on Linux with python-evtx:**

```bash
# Install dependencies
pip3 install python-evtx

# Convert to XML
evtx_dump.py System.evt > System.xml

# Convert to JSON
evtx_dump_json.py Security.evt > Security.json
```

**Using libesedb for older evt parsing:**

```bash
# Install
sudo apt install libesedb-utils

# Export evt structure
esedbexport -t logs Security.evt
```

### EVTX Format (Windows Vista and later)

Modern XML-based binary format with improved querying and filtering.

**File locations:**

```
C:\Windows\System32\winevt\Logs\System.evtx
C:\Windows\System32\winevt\Logs\Security.evtx
C:\Windows\System32\winevt\Logs\Application.evtx
C:\Windows\System32\winevt\Logs\Microsoft-Windows-*/
```

**Analysis on Windows with PowerShell:**

```powershell
# Read EVTX file
Get-WinEvent -Path C:\Path\To\Security.evtx

# Filter by Event ID
Get-WinEvent -Path Security.evtx -FilterHashtable @{ID=4624}

# Multiple Event IDs (successful and failed logons)
Get-WinEvent -Path Security.evtx -FilterHashtable @{ID=4624,4625}

# Time-based filtering
$StartTime = Get-Date "2025-10-29 08:00:00"
$EndTime = Get-Date "2025-10-29 10:00:00"
Get-WinEvent -Path Security.evtx -FilterHashtable @{StartTime=$StartTime; EndTime=$EndTime}

# Export to CSV
Get-WinEvent -Path Security.evtx | 
  Select-Object TimeCreated, ID, Message | 
  Export-Csv -Path events.csv -NoTypeInformation

# Export to JSON
Get-WinEvent -Path Security.evtx | 
  ConvertTo-Json | 
  Out-File events.json

# Complex XPath query
Get-WinEvent -Path Security.evtx -FilterXPath "*[System[(EventID=4624)] and EventData[Data[@Name='TargetUserName']='admin']]"
```

**Analysis on Linux with python-evtx:**

```bash
# Install
pip3 install python-evtx

# Convert to XML
evtx_dump.py Security.evtx > Security.xml

# Convert to JSON with timeline
evtx_dump_json.py --all Security.evtx > Security.json

# Extract specific Event IDs
evtx_dump.py Security.evtx | grep -A 20 "EventID>4625"
```

**Using evtxexport (libevtx):**

```bash
# Install
sudo apt install libevtx-utils

# Export to multiple formats
evtxexport -f text Security.evtx
evtxexport -f xml Security.evtx
evtxexport -f json Security.evtx

# Export to specific directory
evtxexport -t /output/directory Security.evtx
```

**Chainsaw - Fast EVTX searching:**

```bash
# Install
wget https://github.com/WithSecureLabs/chainsaw/releases/latest/download/chainsaw_x86_64-unknown-linux-gnu.tar.gz
tar -xzf chainsaw_x86_64-unknown-linux-gnu.tar.gz

# Hunt for suspicious activity
./chainsaw hunt Security.evtx -s sigma/ --mapping mappings/sigma-mapping.yml

# Search for specific patterns
./chainsaw search Security.evtx -t "mimikatz"

# Dump timeline
./chainsaw dump Security.evtx -o timeline.json
```

### Critical Windows Event IDs

**Security log events:**

```bash
# Authentication
4624  # Successful logon
4625  # Failed logon
4634  # Logoff
4648  # Logon with explicit credentials
4672  # Special privileges assigned
4720  # User account created
4726  # User account deleted
4732  # Member added to security-enabled local group
4738  # User account changed
4740  # User account locked

# Object access
4663  # Attempt to access object
4656  # Handle to object requested
4658  # Handle to object closed

# Process tracking
4688  # Process created
4689  # Process exited

# Policy changes
4719  # System audit policy changed
4906  # CrashOnAuditFail value changed
```

**System log events:**

```bash
7034  # Service crashed
7035  # Service start/stop
7036  # Service state change
7040  # Service start type changed
1102  # Audit log cleared
```

**PowerShell logging:**

```bash
4103  # Module logging
4104  # Script block logging
4105  # Command start
4106  # Command complete
```

**Extracting specific attack indicators:**

```bash
# Find RDP connections (Event ID 4624, Logon Type 10)
evtx_dump.py Security.evtx | grep -B5 -A15 "EventID>4624" | grep -A10 "LogonType>10"

# Lateral movement detection (Event ID 4648)
evtx_dump.py Security.evtx | grep -B5 -A20 "EventID>4648"

# Account lockouts
evtx_dump.py Security.evtx | grep -B5 -A15 "EventID>4740"

# Scheduled task creation (Event ID 4698)
evtx_dump.py Security.evtx | grep -B5 -A25 "EventID>4698"
```

### Parsing EVTX Structure

EVTX files contain multiple chunks with headers and records.

**Structure overview:**

- File header (128 bytes): Magic number "ElfFile\0", version, chunk count
- Chunks (65536 bytes each): Contains multiple event records
- Event records: Variable length, contains XML data

**Manual structure examination:**

```bash
# View file header
xxd -l 128 Security.evtx

# Check magic number
head -c 8 Security.evtx | xxd

# Extract chunk headers
dd if=Security.evtx bs=65536 count=1 | xxd | head -n 20
```

**Python script for basic EVTX parsing:**

```python
import struct
from xml.etree import ElementTree as ET

def parse_evtx_header(filename):
    with open(filename, 'rb') as f:
        magic = f.read(8)
        if magic != b'ElfFile\x00':
            print("Not a valid EVTX file")
            return
        
        # Read header fields
        first_chunk_num = struct.unpack('<Q', f.read(8))[0]
        last_chunk_num = struct.unpack('<Q', f.read(8))[0]
        next_record_id = struct.unpack('<Q', f.read(8))[0]
        
        print(f"First chunk: {first_chunk_num}")
        print(f"Last chunk: {last_chunk_num}")
        print(f"Next record ID: {next_record_id}")
        print(f"Total chunks: {last_chunk_num - first_chunk_num + 1}")

parse_evtx_header('Security.evtx')
```

**Using Plaso/log2timeline for timeline creation:**

```bash
# Install
sudo apt install plaso-tools

# Create timeline
log2timeline.py timeline.plaso Security.evtx

# Export to CSV
psort.py -o l2tcsv -w timeline.csv timeline.plaso

# Filter during export
psort.py -o l2tcsv -w filtered.csv timeline.plaso "event_identifier:4624"
```

## Proprietary Formats

Many applications use custom binary or structured formats requiring vendor-specific tools or reverse engineering.

### Identifying Unknown Formats

**Initial reconnaissance:**

```bash
# File type identification
file unknown.log

# Check for magic numbers
xxd unknown.log | head -n 10

# Strings extraction
strings unknown.log | head -n 50
strings -e l unknown.log  # Little-endian 16-bit (UTF-16LE)
strings -e b unknown.log  # Big-endian 16-bit (UTF-16BE)

# Entropy analysis (high entropy suggests compression/encryption)
ent unknown.log

# Look for embedded file signatures
binwalk unknown.log
foremost unknown.log
```

**Identifying structure patterns:**

```bash
# Find repeating byte sequences (potential record delimiters)
xxd unknown.log | cut -c 11-58 | sort | uniq -c | sort -rn | head -n 20

# Frequency analysis
xxd -p unknown.log | fold -w2 | sort | uniq -c | sort -rn | head -n 20

# Look for ASCII markers
hexdump -C unknown.log | grep -i "log\|event\|record\|entry"
```

### Syslog Variants

While syslog is text-based, some implementations use binary framing or structured formats.

**Syslog-ng binary format:**

```bash
# Reading binary syslog-ng persist file
syslog-ng-ctl stats
syslog-ng-ctl query get "*"

# Convert to text
strings /var/lib/syslog-ng/syslog-ng.persist
```

**Rsyslog binary queue files (*.qi):**

Located in `/var/spool/rsyslog/` or `/var/lib/rsyslog/`

```bash
# View queue statistics
rsyslogd -N1 -f /etc/rsyslog.conf

# Recover messages (requires rsyslog source code knowledge)
# [Unverified] No standard tool exists for direct queue file parsing
```

### Cisco IOS Binary Logs

Cisco devices can export logs in binary format via NetFlow or proprietary protocols.

**NetFlow v5 parsing:**

```bash
# Using nfdump
nfdump -r netflow.file

# Filter by source IP
nfdump -r netflow.file 'src ip 192.168.1.100'

# Statistics
nfdump -r netflow.file -s srcip -n 20

# Output to CSV
nfdump -r netflow.file -o csv > netflow.csv
```

**SFlow parsing:**

```bash
# Using sflowtool
sflowtool -l < sflow.dat

# JSON output
sflowtool -j < sflow.dat > sflow.json
```

### Microsoft Exchange Logs

Exchange uses multiple proprietary formats including ESE database format.

**Analyzing Exchange transaction logs (.log files):**

```bash
# Using esedbexport (libesedb)
esedbexport Exchange.edb

# Extract specific tables
esedbexport -t messages Exchange.edb

# View database info
esedbinfo Exchange.edb
```

**Interpreting message tracking logs:**

Located in `C:\Program Files\Microsoft\Exchange Server\V15\TransportRoles\Logs\MessageTracking\`

```powershell
# On Windows
Get-MessageTrackingLog -Start "10/29/2025 08:00:00" -End "10/29/2025 10:00:00"

# Filter by sender
Get-MessageTrackingLog -Sender "attacker@example.com"

# Export to CSV
Get-MessageTrackingLog | Export-Csv tracking.csv
```

On Linux (CSV format):

```bash
# Parse CSV logs
csvcut -c Timestamp,Sender,Recipients,MessageSubject MSGTRK*.log | csvlook

# Filter specific events
grep "DELIVER" MSGTRK*.log | cut -d',' -f1,4,5,6
```

### SQLite-based Logs

Many modern applications store logs in SQLite databases.

**Identifying SQLite files:**

```bash
# Check file signature
file app.db
xxd app.db | head -n 1  # Should show "SQLite format 3"
```

**Analyzing with sqlite3:**

```bash
# Open database
sqlite3 app.db

# List tables
.tables

# Describe table structure
.schema logs

# Query logs
SELECT timestamp, level, message FROM logs WHERE level='ERROR';

# Export to CSV
.mode csv
.output logs.csv
SELECT * FROM logs;
.quit

# Export to JSON (requires json1 extension)
sqlite3 app.db "SELECT json_group_array(json_object('timestamp', timestamp, 'message', message)) FROM logs;" > logs.json
```

**Recovering deleted records:**

```bash
# Using sqlite-analyzer
sqlite-analyzer app.db

# Carving with strings
strings app.db | grep -E "[0-9]{4}-[0-9]{2}-[0-9]{2}"

# Using undark for forensic recovery
git clone https://github.com/inflex/undark
cd undark
gcc undark.c -o undark
./undark -i app.db --freelist --csv > recovered.csv
```

### Protocol Buffers (Protobuf) Logs

Google Protocol Buffers provide efficient binary serialization.

**Identifying protobuf data:**

```bash
# Look for characteristic byte patterns
hexdump -C data.pb | grep -E "^[0-9a-f]+ +(0[8-9a-f]|1[0-9a-f])"

# Check for varint encoding patterns
```

**Decoding without schema (blackbox approach):**

```bash
# Using protoc --decode_raw
protoc --decode_raw < data.pb

# Using protobuf_inspector
pip3 install protobuf-inspector
protobuf_inspector < data.pb
```

**Decoding with known .proto schema:**

```bash
# Compile proto file
protoc --decode=LogEntry log.proto < data.pb

# Using Python
python3 << EOF
import log_pb2
with open('data.pb', 'rb') as f:
    entry = log_pb2.LogEntry()
    entry.ParseFromString(f.read())
    print(entry)
EOF
```

### Apache Avro Binary Logs

Avro stores schema with data, enabling self-describing binary formats.

**Reading Avro files:**

```bash
# Install avro-tools
sudo apt install avro-tools

# View schema
avro-tools getschema logs.avro

# Convert to JSON
avro-tools tojson logs.avro > logs.json

# View metadata
avro-tools getmeta logs.avro

# Count records
avro-tools count logs.avro
```

**Python parsing:**

```python
from avro.datafile import DataFileReader
from avro.io import DatumReader

with open('logs.avro', 'rb') as f:
    reader = DataFileReader(f, DatumReader())
    for record in reader:
        print(record)
    reader.close()
```

### MessagePack Logs

MessagePack provides efficient binary JSON-like serialization.

**Unpacking MessagePack data:**

```bash
# Install msgpack-tools
pip3 install msgpack-python

# Python one-liner
python3 -c "import msgpack, sys; print(msgpack.unpackb(sys.stdin.buffer.read()))" < data.msgpack
```

**Streaming MessagePack logs:**

```python
import msgpack

with open('stream.msgpack', 'rb') as f:
    unpacker = msgpack.Unpacker(f, raw=False)
    for obj in unpacker:
        print(obj)
```

### Custom Format Reverse Engineering Strategy

When encountering completely unknown binary formats:

**Step 1: Static analysis**

```bash
# Gather all available intelligence
file binary.log
xxd binary.log | head -n 100
strings binary.log
binwalk binary.log
```

**Step 2: Pattern identification**

```bash
# Find fixed-position headers
xxd binary.log | awk '{print $2$3$4$5}' | sort | uniq -c | sort -rn

# Identify record boundaries
xxd binary.log | grep -E "(00 00 00 00|FF FF FF FF|0A 0D)" --color=always
```

**Step 3: Hypothesis and testing**

Common patterns to look for:

- Length fields (2 or 4 bytes before variable data)
- Type identifiers (1 byte indicating record type)
- Timestamps (Unix epoch: 4 bytes, epoch milliseconds: 8 bytes)
- Null terminators for strings
- Checksum/CRC fields at record end

**Step 4: Automated structure extraction**

```bash
# Using Kaitai Struct for format description
# Create .ksy file describing structure
kaitai-struct-compiler -t python format.ksy
python3 parse_logs.py binary.log
```

**Example Kaitai Struct definition:**

```yaml
meta:
  id: custom_log
  file-extension: log
  endian: le
seq:
  - id: magic
    contents: [0x4C, 0x4F, 0x47, 0x00]  # "LOG\0"
  - id: version
    type: u2
  - id: records
    type: record
    repeat: eos
types:
  record:
    seq:
      - id: timestamp
        type: u4
      - id: level
        type: u1
      - id: message_len
        type: u2
      - id: message
        type: str
        size: message_len
        encoding: UTF-8
```

**Step 5: Scripted extraction**

```python
import struct

def parse_custom_log(filename):
    with open(filename, 'rb') as f:
        # Verify magic
        magic = f.read(4)
        if magic != b'LOG\x00':
            raise ValueError("Invalid magic number")
        
        version = struct.unpack('<H', f.read(2))[0]
        print(f"Version: {version}")
        
        while True:
            # Read record
            data = f.read(7)  # timestamp(4) + level(1) + len(2)
            if not data or len(data) < 7:
                break
            
            timestamp, level, msg_len = struct.unpack('<IbH', data)
            message = f.read(msg_len).decode('utf-8')
            
            print(f"{timestamp} [{level}] {message}")

parse_custom_log('binary.log')
```

---

**Related topics:** Log file compression formats (gzip, bzip2, xz), encrypted log containers, log integrity verification techniques, automated format fingerprinting tools.

---

# Data Extraction

Data extraction from logs is a critical skill in CTF challenges and forensic analysis. Efficiently identifying and extracting IPs, emails, and URLs reveals attack vectors, data exfiltration attempts, communication patterns, and often flags hidden in log data.

## IP Address Extraction

IP addresses in logs indicate attack sources, compromised hosts, command-and-control servers, and lateral movement patterns.

### Basic Extraction with grep

**IPv4 extraction:**

```bash
# Basic IPv4 pattern (matches 0.0.0.0 to 999.999.999.999 - overly permissive)
grep -oE '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}' logfile.txt

# More accurate IPv4 (still allows invalid ranges like 999.x.x.x)
grep -oE '([0-9]{1,3}\.){3}[0-9]{1,3}' logfile.txt

# Strict IPv4 validation regex (only valid 0-255 ranges)
grep -oP '(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)' logfile.txt

# Simple practical pattern (good balance)
grep -oE '\b([0-9]{1,3}\.){3}[0-9]{1,3}\b' logfile.txt
```

**IPv6 extraction:**

```bash
# Full IPv6 format (8 groups of 4 hex digits)
grep -oE '([0-9a-fA-F]{1,4}:){7}[0-9a-fA-F]{1,4}' logfile.txt

# IPv6 with :: compression
grep -oE '(([0-9a-fA-F]{1,4}:){1,7}:|:((:[0-9a-fA-F]{1,4}){1,7}|:))' logfile.txt

# Comprehensive IPv6 pattern
grep -oP '(([0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|([0-9a-fA-F]{1,4}:){1,7}:|([0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|([0-9a-fA-F]{1,4}:){1,5}(:[0-9a-fA-F]{1,4}){1,2}|([0-9a-fA-F]{1,4}:){1,4}(:[0-9a-fA-F]{1,4}){1,3}|([0-9a-fA-F]{1,4}:){1,3}(:[0-9a-fA-F]{1,4}){1,4}|([0-9a-fA-F]{1,4}:){1,2}(:[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:((:[0-9a-fA-F]{1,4}){1,6})|:((:[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(:[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(ffff(:0{1,4}){0,1}:){0,1}((25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])\.){3,3}(25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])|([0-9a-fA-F]{1,4}:){1,4}:((25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])\.){3,3}(25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9]))' logfile.txt
```

### Extraction with Analysis

**Extract and count unique IPs:**

```bash
# Extract, sort, count
grep -oE '\b([0-9]{1,3}\.){3}[0-9]{1,3}\b' access.log | sort | uniq -c | sort -rn

# Top 10 most frequent IPs
grep -oE '\b([0-9]{1,3}\.){3}[0-9]{1,3}\b' access.log | sort | uniq -c | sort -rn | head -10

# Count total unique IPs
grep -oE '\b([0-9]{1,3}\.){3}[0-9]{1,3}\b' access.log | sort -u | wc -l
```

**Extract IPs with context:**

```bash
# Show 2 lines before and after each IP match
grep -B2 -A2 -E '\b192\.168\.1\.100\b' logfile.txt

# Extract IP with timestamp from Apache logs
grep -oP '\d+\.\d+\.\d+\.\d+ - - \[\K[^\]]+' access.log

# Extract IP and HTTP status code
awk '{print $1, $9}' access.log | sort | uniq -c

# IPs with failed login attempts
grep "Failed password" /var/log/auth.log | grep -oE '\b([0-9]{1,3}\.){3}[0-9]{1,3}\b' | sort | uniq -c | sort -rn
```

### Advanced IP Extraction

**Using awk for specific fields:**

```bash
# Extract first field (IP) from Apache/nginx logs
awk '{print $1}' access.log | sort -u

# Extract IPs only from POST requests
awk '$6 == "\"POST" {print $1}' access.log | sort | uniq -c

# Extract source and destination IPs from firewall logs
awk '{print $src_ip, $dst_ip}' firewall.log

# Multiple conditions
awk '$9 >= 400 {print $1, $7, $9}' access.log  # IP, URL, status for errors
```

**Using sed for extraction and transformation:**

```bash
# Extract IPs and format as CSV
grep -oE '\b([0-9]{1,3}\.){3}[0-9]{1,3}\b' logfile.txt | sed 's/^/"/;s/$/"/' | paste -sd,

# Replace IPs with [REDACTED] for sanitized output
sed -E 's/\b([0-9]{1,3}\.){3}[0-9]{1,3}\b/[REDACTED]/g' logfile.txt

# Extract IPs and create hosts file format
grep -oE '\b([0-9]{1,3}\.){3}[0-9]{1,3}\b' targets.log | awk '{print $1 " target-" NR ".local"}'
```

**Using Python for complex extraction:**

```python
#!/usr/bin/env python3
import re
import sys
from collections import Counter

# Strict IPv4 regex
ipv4_pattern = re.compile(r'\b(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\b')

def extract_ips(logfile):
    ips = []
    with open(logfile, 'r', errors='ignore') as f:
        for line in f:
            ips.extend(ipv4_pattern.findall(line))
    return ips

def filter_private_ips(ips):
    """Remove RFC1918 private IPs"""
    public_ips = []
    for ip in ips:
        octets = [int(x) for x in ip.split('.')]
        # Skip 10.x.x.x, 172.16-31.x.x, 192.168.x.x, 127.x.x.x
        if octets[0] == 10:
            continue
        if octets[0] == 172 and 16 <= octets[1] <= 31:
            continue
        if octets[0] == 192 and octets[1] == 168:
            continue
        if octets[0] == 127:
            continue
        public_ips.append(ip)
    return public_ips

if __name__ == '__main__':
    if len(sys.argv) < 2:
        print(f"Usage: {sys.argv[0]} <logfile>")
        sys.exit(1)
    
    all_ips = extract_ips(sys.argv[1])
    public_ips = filter_private_ips(all_ips)
    
    print(f"Total IPs found: {len(all_ips)}")
    print(f"Unique IPs: {len(set(all_ips))}")
    print(f"Public IPs: {len(public_ips)}")
    print(f"\nTop 10 IPs:")
    for ip, count in Counter(all_ips).most_common(10):
        print(f"{ip}: {count}")
```

**Usage:**

```bash
chmod +x extract_ips.py
./extract_ips.py access.log
```

### IP Geolocation and Intelligence

**Using geoiplookup (MaxMind database):**

```bash
# Install GeoIP tools
sudo apt install geoip-bin geoip-database -y

# Lookup single IP
geoiplookup 8.8.8.8

# Batch lookup from extracted IPs
grep -oE '\b([0-9]{1,3}\.){3}[0-9]{1,3}\b' access.log | sort -u | while read ip; do
    echo "$ip: $(geoiplookup $ip)"
done

# Extract only IPs from specific country
grep -oE '\b([0-9]{1,3}\.){3}[0-9]{1,3}\b' access.log | sort -u | while read ip; do
    result=$(geoiplookup $ip)
    if echo "$result" | grep -q "CN"; then
        echo "$ip"
    fi
done
```

**Using whois for IP ownership:**

```bash
# Lookup IP owner
whois 192.0.2.1

# Extract organization from whois
whois 8.8.8.8 | grep -i "orgname\|netname"

# Batch whois lookup
cat ips.txt | while read ip; do
    echo "=== $ip ==="
    whois $ip | grep -E "OrgName|NetRange|Country"
done
```

**Using curl with IP intelligence APIs:**

```bash
# ip-api.com (free, no key required, rate limited)
curl -s "http://ip-api.com/json/8.8.8.8" | jq '.'

# Batch lookup
cat suspicious_ips.txt | while read ip; do
    curl -s "http://ip-api.com/json/$ip" | jq -r '[.query, .country, .isp] | @csv'
    sleep 1  # Rate limiting
done

# AbuseIPDB (requires API key)
API_KEY="your_key_here"
curl -s -G https://api.abuseipdb.com/api/v2/check \
    --data-urlencode "ipAddress=8.8.8.8" \
    -H "Key: $API_KEY" \
    -H "Accept: application/json" | jq '.'
```

### CTF-Specific IP Extraction Scenarios

**Finding C2 server IPs:**

```bash
# Extract IPs from outbound connections in firewall logs
grep "ACCEPT.*OUT" firewall.log | grep -oE 'DST=([0-9]{1,3}\.){3}[0-9]{1,3}' | cut -d= -f2 | sort | uniq -c | sort -rn

# Find IPs contacting unusual ports
awk '$6 !~ /80|443|22|53/ {print $1, $6}' connection.log | sort | uniq -c
```

**Identifying attack sources:**

```bash
# IPs with high 4xx/5xx error rates
awk '$9 >= 400 {print $1}' access.log | sort | uniq -c | sort -rn | head -20

# IPs attempting path traversal
grep -E '\.\./|\.\.\%2[fF]' access.log | grep -oE '\b([0-9]{1,3}\.){3}[0-9]{1,3}\b' | sort -u

# IPs with SQL injection attempts
grep -iE "union.*select|' or |1=1" access.log | awk '{print $1}' | sort | uniq -c | sort -rn
```

**Correlation across multiple log types:**

```bash
# Find IPs present in both web logs and auth logs (potential pivot)
comm -12 <(grep -oE '\b([0-9]{1,3}\.){3}[0-9]{1,3}\b' access.log | sort -u) \
         <(grep -oE '\b([0-9]{1,3}\.){3}[0-9]{1,3}\b' auth.log | sort -u)
```

## Email Extraction

Email addresses in logs reveal user accounts, phishing attempts, data exfiltration targets, and sometimes encode flags in CTF challenges.

### Basic Email Extraction

**Using grep with regex:**

```bash
# Basic email pattern
grep -oE '[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}' logfile.txt

# More permissive pattern (includes more special chars)
grep -oE '[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,6}' logfile.txt

# Stricter pattern with word boundaries
grep -oE '\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b' logfile.txt

# Extract with Perl-compatible regex (PCRE)
grep -oP '[\w\.-]+@[\w\.-]+\.\w+' logfile.txt
```

**Extract and deduplicate:**

```bash
# Unique emails
grep -oE '\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b' maillog.txt | sort -u

# Count occurrences
grep -oE '\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b' maillog.txt | sort | uniq -c | sort -rn

# Extract only the domain part
grep -oE '\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b' maillog.txt | cut -d@ -f2 | sort -u
```

### Advanced Email Extraction

**Extract with context:**

```bash
# Show line with email and surrounding context
grep -B2 -A2 -E '\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b' logfile.txt

# Extract emails from specific log field
awk '{for(i=1;i<=NF;i++) if($i ~ /@/) print $i}' logfile.txt

# Extract from JSON logs
jq -r '.email' user-logs.json | grep -v null

# From mail server logs (Postfix)
grep "from=<" /var/log/mail.log | grep -oP 'from=<\K[^>]+' | sort -u
grep "to=<" /var/log/mail.log | grep -oP 'to=<\K[^>]+' | sort -u
```

**Using sed for extraction:**

```bash
# Extract and clean
sed -n 's/.*\([A-Za-z0-9._%+-]\+@[A-Za-z0-9.-]\+\.[A-Za-z]\{2,\}\).*/\1/p' logfile.txt

# Extract from quoted strings
sed -n 's/.*"\([^@]\+@[^"]\+\)".*/\1/p' logfile.txt
```

**Python script for advanced extraction:**

```python
#!/usr/bin/env python3
import re
import sys
from collections import Counter

# RFC 5322 compliant email regex (simplified)
email_pattern = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')

def extract_emails(logfile):
    emails = []
    with open(logfile, 'r', errors='ignore') as f:
        for line in f:
            emails.extend(email_pattern.findall(line))
    return emails

def categorize_by_domain(emails):
    """Group emails by domain"""
    domain_map = {}
    for email in emails:
        domain = email.split('@')[1].lower()
        if domain not in domain_map:
            domain_map[domain] = []
        domain_map[domain].append(email)
    return domain_map

def find_suspicious_patterns(emails):
    """Identify potentially suspicious emails"""
    suspicious = []
    for email in emails:
        # Check for common suspicious patterns
        if re.search(r'\d{5,}', email):  # Many digits
            suspicious.append((email, "many_digits"))
        if email.count('.') > 3:  # Too many dots
            suspicious.append((email, "excessive_dots"))
        if re.search(r'(admin|root|test|temp|noreply)@', email, re.I):  # Common targets
            suspicious.append((email, "privileged_account"))
    return suspicious

if __name__ == '__main__':
    if len(sys.argv) < 2:
        print(f"Usage: {sys.argv[0]} <logfile>")
        sys.exit(1)
    
    emails = extract_emails(sys.argv[1])
    
    print(f"Total emails found: {len(emails)}")
    print(f"Unique emails: {len(set(emails))}")
    
    print("\n=== Top 10 Most Common ===")
    for email, count in Counter(emails).most_common(10):
        print(f"{email}: {count}")
    
    print("\n=== Emails by Domain ===")
    domains = categorize_by_domain(emails)
    for domain, domain_emails in sorted(domains.items(), key=lambda x: len(x[1]), reverse=True)[:5]:
        print(f"{domain}: {len(domain_emails)} emails")
    
    print("\n=== Suspicious Patterns ===")
    suspicious = find_suspicious_patterns(set(emails))
    for email, reason in suspicious[:10]:
        print(f"{email} - {reason}")
```

### Email Validation and Filtering

**Filter by domain:**

```bash
# Extract only specific domain
grep -oE '\b[A-Za-z0-9._%+-]+@example\.com\b' logfile.txt | sort -u

# Exclude specific domains
grep -oE '\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b' logfile.txt | grep -v '@gmail.com' | sort -u

# Extract only corporate emails (exclude free providers)
grep -oE '\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b' logfile.txt | grep -vE '@(gmail|yahoo|hotmail|outlook)\.com' | sort -u
```

**Validate email format:**

```bash
# Basic validation: check for @ and domain
awk -F@ 'NF==2 && $2 ~ /\./ {print}' extracted_emails.txt

# More strict validation
grep -E '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}$' extracted_emails.txt
```

### CTF-Specific Email Extraction

**Finding encoded flags in emails:**

```bash
# Extract emails, check for flag patterns
grep -oE '\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b' logs.txt | grep -E 'flag|ctf|[A-Z0-9]{32}'

# Base64 decode email usernames (sometimes flags are encoded)
grep -oE '\b[A-Za-z0-9._%+-]+@' logs.txt | cut -d@ -f1 | base64 -d 2>/dev/null

# ROT13 on email addresses
grep -oE '\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b' logs.txt | tr 'A-Za-z' 'N-ZA-Mn-za-m'
```

**Phishing campaign analysis:**

```bash
# Find emails with suspicious sender-recipient patterns
awk '{print $1, $2}' mail.log | sort | uniq -c | sort -rn  # Bulk sending

# Extract subject lines with emails
grep -B5 "To:" mail.log | grep -E "Subject:|To:" | paste - -

# Find emails from external domains to internal recipients
grep "to=" mail.log | grep -oP 'from=<\K[^>]+' | grep -vE '@internal\.com' | sort | uniq -c | sort -rn
```

**Data exfiltration detection:**

```bash
# Large attachments to external emails
awk '/to=.*@external/ && /size=[0-9]{7,}/ {print $0}' mail.log

# Unusual recipient patterns (many external domains)
grep -oP 'to=<\K[^>]+' mail.log | grep -vE '@(company|internal)\.com' | cut -d@ -f2 | sort | uniq -c | sort -rn
```

## URL Extraction

URL extraction reveals accessed resources, attack vectors, data exfiltration endpoints, and C2 infrastructure.

### Basic URL Extraction

**HTTP/HTTPS URLs:**

```bash
# Basic URL pattern
grep -oE 'https?://[^[:space:]"]+' logfile.txt

# More comprehensive (includes query strings)
grep -oE 'https?://[A-Za-z0-9./?=_%:-]+' logfile.txt

# With Perl regex (handles more edge cases)
grep -oP 'https?://[^\s<>"{}|\\^`\[\]]+' logfile.txt

# Extract only domains from URLs
grep -oE 'https?://[^/]+' logfile.txt | sed 's/https\?:\/\///' | sort -u
```

**FTP and other protocols:**

```bash
# Multiple protocol support
grep -oE '(https?|ftp|file)://[^[:space:]"]+' logfile.txt

# All common protocols
grep -oE '(https?|ftp|ftps|ssh|telnet|file|smb)://[A-Za-z0-9./?=_%:-]+' logfile.txt
```

### Extraction from Specific Log Types

**Apache/nginx access logs:**

```bash
# Extract requested URLs (path only)
awk '{print $7}' access.log | sort -u

# Full URL (requires parsing Host header or using combined format)
awk '{print "http://" $11 $7}' access.log | tr -d '"' | sort -u

# Extract URLs with query strings
awk '{print $7}' access.log | grep '?' | sort -u

# URLs returning specific status
awk '$9 == 404 {print $7}' access.log | sort | uniq -c | sort -rn
```

**From JSON logs:**

```bash
# Extract URL field from JSON
jq -r '.url' app-logs.json | grep -v null

# Multiple URL fields
jq -r '.request.url, .referrer' app-logs.json | grep http

# CloudWatch logs
jq -r '.message' cloudwatch.json | grep -oE 'https?://[^[:space:]"]+' | sort -u
```

**From browser history/SQL databases:**

```bash
# Chrome history (SQLite)
sqlite3 ~/.config/google-chrome/Default/History "SELECT url, title, visit_count FROM urls ORDER BY visit_count DESC LIMIT 100"

# Firefox history
sqlite3 ~/.mozilla/firefox/*.default/places.sqlite "SELECT url, title, visit_count FROM moz_places ORDER BY visit_count DESC LIMIT 100"

# Extract URLs from SQLite database
sqlite3 database.db "SELECT url FROM urls" | grep -oE 'https?://[^[:space:]]+'
```

### Advanced URL Analysis

**Extract and parse URL components:**

```bash
# Using Python urlparse
python3 << 'EOF'
from urllib.parse import urlparse
import sys

with open('urls.txt', 'r') as f:
    for line in f:
        url = line.strip()
        parsed = urlparse(url)
        print(f"{parsed.scheme}://{parsed.netloc}{parsed.path}")
EOF

# Extract domains only
grep -oE 'https?://[^/]+' urls.txt | sed 's|https\?://||' | sort -u

# Extract paths only
grep -oE 'https?://[^/]+(.*)' urls.txt | sed 's|https\?://[^/]*||' | sort -u

# Extract query parameters
grep -oP 'https?://[^?]+\?\K.*' urls.txt
```

**Using awk for detailed parsing:**

```bash
# Extract domain and path separately
awk -F'/' '{print $3, "/" $4 "/" $5}' < <(grep -oE 'https?://[^[:space:]]+' logfile.txt)

# Count URLs by domain
grep -oE 'https?://[^/]+' logfile.txt | sed 's|https\?://||' | sort | uniq -c | sort -rn

# Extract and count unique paths per domain
awk '{match($0, /https?:\/\/[^\/]+/, domain); match($0, /https?:\/\/[^\/]+(.*)/, path); print domain[0], path[1]}' logfile.txt | sort | uniq -c
```

**Python script for comprehensive URL extraction:**

```python
#!/usr/bin/env python3
import re
import sys
from urllib.parse import urlparse, parse_qs
from collections import Counter

# Comprehensive URL regex
url_pattern = re.compile(
    r'https?://'  # Protocol
    r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+[A-Z]{2,6}\.?|'  # Domain
    r'localhost|'  # Localhost
    r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'  # IP
    r'(?::\d+)?'  # Optional port
    r'(?:/?|[/?]\S+)', 
    re.IGNORECASE
)

def extract_urls(logfile):
    urls = []
    with open(logfile, 'r', errors='ignore') as f:
        for line in f:
            urls.extend(url_pattern.findall(line))
    return urls

def analyze_urls(urls):
    """Analyze URL patterns"""
    results = {
        'total': len(urls),
        'unique': len(set(urls)),
        'domains': Counter(),
        'protocols': Counter(),
        'paths': Counter(),
        'suspicious': []
    }
    
    for url in urls:
        parsed = urlparse(url)
        results['domains'][parsed.netloc] += 1
        results['protocols'][parsed.scheme] += 1
        results['paths'][parsed.path] += 1
        
        # Check for suspicious patterns
        if any(pattern in url.lower() for pattern in ['cmd=', 'exec=', '../', 'union', 'select']):
            results['suspicious'].append(url)
        
        # Check for base64 in parameters
        if parsed.query:
            params = parse_qs(parsed.query)
            for key, values in params.items():
                for value in values:
                    if len(value) > 20 and re.match(r'^[A-Za-z0-9+/=]+$', value):
                        results['suspicious'].append(f"{url} - Possible base64: {key}={value[:50]}...")
    
    return results

if __name__ == '__main__':
    if len(sys.argv) < 2:
        print(f"Usage: {sys.argv[0]} <logfile>")
        sys.exit(1)
    
    urls = extract_urls(sys.argv[1])
    results = analyze_urls(urls)
    
    print(f"Total URLs: {results['total']}")
    print(f"Unique URLs: {results['unique']}")
    
    print("\n=== Top 10 Domains ===")
    for domain, count in results['domains'].most_common(10):
        print(f"{domain}: {count}")
    
    print("\n=== Protocols ===")
    for protocol, count in results['protocols'].items():
        print(f"{protocol}: {count}")
    
    print("\n=== Top 10 Paths ===")
    for path, count in results['paths'].most_common(10):
        print(f"{path}: {count}")
    
    if results['suspicious']:
        print("\n=== Suspicious URLs (first 20) ===")
        for url in results['suspicious'][:20]:
            print(url)
```

### URL Decoding and Normalization

**URL decoding:**

```bash
# Python URL decode
python3 -c "import sys; from urllib.parse import unquote; print(unquote(sys.argv[1]))" "http%3A%2F%2Fexample.com%2Fpath"

# Batch decode URLs from file
while read url; do
    python3 -c "from urllib.parse import unquote; print(unquote('$url'))"
done < encoded_urls.txt

# Decode using Perl
echo "http%3A%2F%2Fexample.com" | perl -pe 's/%([0-9a-f]{2})/chr(hex($1))/eig'
```

**Double URL encoding detection:**

```bash
# Find double-encoded URLs (often used in bypasses)
grep -E '%25[0-9A-F]{2}' access.log

# Decode twice
python3 << 'EOF'
from urllib.parse import unquote
url = "http://example.com/%252e%252e%252f"
print("Once:", unquote(url))
print("Twice:", unquote(unquote(url)))
EOF
```

### CTF-Specific URL Extraction

**Finding attack patterns:**

```bash
# Path traversal attempts
grep -oE 'https?://[^[:space:]"]+' access.log | grep -E '\.\./|\.\.%2[fF]|%2e%2e'

# SQL injection attempts
grep -oE 'https?://[^[:space:]"]+' access.log | grep -iE "union.*select|'.*or.*1=1|sleep\(|benchmark\("

# XSS attempts
grep -oE 'https?://[^[:space:]"]+' access.log | grep -iE '<script|javascript:|onerror=|onload='

# Command injection
grep -oE 'https?://[^[:space:]"]+' access.log | grep -E ';.*||&&|`|$(|%0a'

# Local File Inclusion (LFI)
grep -oE 'https?://[^[:space:]"]+' access.log | grep -E '(file://|/etc/passwd|/proc/|..)'
````

**Extracting URLs with sensitive parameters:**
```bash
# Find URLs with potential credentials
grep -oE 'https?://[^[:space:]"]+' access.log | grep -iE '(password|token|api_key|secret|auth)='

# Extract API endpoints
grep -oE 'https?://[^[:space:]"]+/api/[^[:space:]"]+' access.log | sort -u

# Find admin panels
grep -oE 'https?://[^[:space:]"]+' access.log | grep -iE '/(admin|dashboard|panel|manage)' | sort -u

# Extract file downloads
grep -oE 'https?://[^[:space:]"]+\.(zip|tar|gz|rar|pdf|doc|xls|csv|sql|bak)' access.log | sort -u
````

**Analyzing query parameters:**

```bash
# Extract all unique parameter names
grep -oP '\?[^[:space:]"]+' access.log | grep -oP '[?&]\K[^=]+(?==)' | sort -u

# Find parameters with suspicious values
grep -oE '\?[^[:space:]"]+' access.log | grep -E '(id|file|page|url|redirect)=[^&[:space:]"]+' | sort -u

# Parameters with long values (potential payload)
grep -oP '\?[^[:space:]"]+' access.log | awk -F'[=&]' '{for(i=1;i<=NF;i+=2) if(length($i) > 50) print}'
```

**Extracting external/redirect URLs:**

```bash
# Find redirect parameters
grep -oE 'https?://[^[:space:]"]+' access.log | grep -E '(redirect|url|next|return|goto)=' | grep -oP '=\Khttps?://[^&[:space:]"]+' | sort -u

# Extract referrer URLs from logs
awk -F'"' '{print $4}' access.log | grep http | sort -u

# Find URLs pointing to external domains
grep -oE 'https?://[^/]+' access.log | sed 's|https\?://||' | grep -vE '^(localhost|127\.0\.0\.1|internal\.domain\.com)' | sort -u
```

**C2 and exfiltration detection:**

```bash
# Unusual domains with high request frequency
grep -oE 'https?://[^/]+' access.log | sed 's|https\?://||' | sort | uniq -c | sort -rn | head -20

# Beaconing detection (regular intervals)
awk '{print $4, $7}' access.log | grep -E 'unusual-domain' | cut -d: -f1 | uniq -c

# Data exfiltration via URL parameters (long query strings)
awk '{print $7}' access.log | awk 'length($0) > 500' | head -20

# Base64 encoded data in URLs
grep -oE 'https?://[^[:space:]"]+' access.log | grep -oP '=[A-Za-z0-9+/]{30,}={0,2}' | while read encoded; do
    echo "Encoded: $encoded"
    echo "Decoded: $(echo $encoded | base64 -d 2>/dev/null)"
    echo "---"
done
```

**URL fuzzing pattern detection:**

```bash
# Sequential ID enumeration
awk '{print $7}' access.log | grep -oP 'id=\K[0-9]+' | sort -n | uniq | head -20

# Wordlist-based fuzzing detection
awk '{print $7}' access.log | cut -d'?' -f1 | sort | uniq -c | awk '$1 == 1' | wc -l  # Unique paths (possible fuzzing)

# Find endpoints accessed only once (possible discovery phase)
awk '{print $7}' access.log | sort | uniq -c | awk '$1 == 1 {print $2}' | head -50
```

### Defanging and Safe Display

**Defang URLs for safe sharing:**

```bash
# Replace http:// with hxxp://
sed 's/http/hxxp/g' urls.txt

# Replace dots in domain
sed 's/\([a-z0-9]\)\.\([a-z0-9]\)/\1[.]\2/g' urls.txt

# Combined defanging
sed -e 's/http/hxxp/g' -e 's/\./[.]/g' urls.txt

# Python defanging script
python3 << 'EOF'
import sys
for line in sys.stdin:
    defanged = line.strip().replace('http', 'hxxp').replace('.', '[.]')
    print(defanged)
EOF
```

### Cross-Pattern Extraction (IPs, Emails, URLs Combined)

**Extract all IOCs (Indicators of Compromise):**

```bash
# Combined extraction script
cat << 'EOF' > extract_iocs.sh
#!/bin/bash

LOGFILE="$1"

echo "=== IP Addresses ==="
grep -oE '\b([0-9]{1,3}\.){3}[0-9]{1,3}\b' "$LOGFILE" | sort -u | head -20

echo -e "\n=== Email Addresses ==="
grep -oE '\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b' "$LOGFILE" | sort -u | head -20

echo -e "\n=== URLs ==="
grep -oE 'https?://[^[:space:]"]+' "$LOGFILE" | sort -u | head -20

echo -e "\n=== Domains (from URLs) ==="
grep -oE 'https?://[^/]+' "$LOGFILE" | sed 's|https\?://||' | sort -u | head -20

echo -e "\n=== Suspicious Patterns ==="
grep -iE '(flag|ctf|password|secret|token|admin|root)' "$LOGFILE" | head -10
EOF

chmod +x extract_iocs.sh
./extract_iocs.sh access.log
```

**Python comprehensive extraction:**

```python
#!/usr/bin/env python3
import re
import sys
from collections import defaultdict

class IOCExtractor:
    def __init__(self):
        self.ipv4_pattern = re.compile(r'\b(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\b')
        self.email_pattern = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b')
        self.url_pattern = re.compile(r'https?://[^\s<>"{}|\\^`\[\]]+', re.IGNORECASE)
        self.domain_pattern = re.compile(r'\b(?:[a-z0-9](?:[a-z0-9-]{0,61}[a-z0-9])?\.)+[a-z]{2,}\b', re.IGNORECASE)
        
    def extract_all(self, logfile):
        iocs = {
            'ips': set(),
            'emails': set(),
            'urls': set(),
            'domains': set()
        }
        
        with open(logfile, 'r', errors='ignore') as f:
            for line in f:
                iocs['ips'].update(self.ipv4_pattern.findall(line))
                iocs['emails'].update(self.email_pattern.findall(line))
                iocs['urls'].update(self.url_pattern.findall(line))
                
                # Extract domains from URLs
                for url in self.url_pattern.findall(line):
                    domain_match = re.search(r'https?://([^/:]+)', url)
                    if domain_match:
                        iocs['domains'].add(domain_match.group(1))
        
        return iocs
    
    def filter_private_ips(self, ips):
        """Remove private IP ranges"""
        public_ips = set()
        for ip in ips:
            octets = [int(x) for x in ip.split('.')]
            if not (octets[0] == 10 or
                    (octets[0] == 172 and 16 <= octets[1] <= 31) or
                    (octets[0] == 192 and octets[1] == 168) or
                    octets[0] == 127):
                public_ips.add(ip)
        return public_ips
    
    def generate_report(self, iocs):
        print("=" * 60)
        print("IOC EXTRACTION REPORT")
        print("=" * 60)
        
        print(f"\n[+] IPv4 Addresses: {len(iocs['ips'])} unique")
        public_ips = self.filter_private_ips(iocs['ips'])
        print(f"    Public IPs: {len(public_ips)}")
        for ip in sorted(public_ips)[:20]:
            print(f"    - {ip}")
        
        print(f"\n[+] Email Addresses: {len(iocs['emails'])} unique")
        for email in sorted(iocs['emails'])[:20]:
            print(f"    - {email}")
        
        print(f"\n[+] URLs: {len(iocs['urls'])} unique")
        for url in sorted(iocs['urls'])[:20]:
            print(f"    - {url}")
        
        print(f"\n[+] Domains: {len(iocs['domains'])} unique")
        for domain in sorted(iocs['domains'])[:20]:
            print(f"    - {domain}")

if __name__ == '__main__':
    if len(sys.argv) < 2:
        print(f"Usage: {sys.argv[0]} <logfile>")
        sys.exit(1)
    
    extractor = IOCExtractor()
    iocs = extractor.extract_all(sys.argv[1])
    extractor.generate_report(iocs)
```

**Usage:**

```bash
chmod +x ioc_extractor.py
./ioc_extractor.py /var/log/apache2/access.log
```

### STIX/IOC Format Export

**Convert extracted data to STIX format (for threat intelligence platforms):**

```python
#!/usr/bin/env python3
import json
import sys
import re
from datetime import datetime

def extract_and_convert_to_stix(logfile):
    """Extract IOCs and convert to STIX 2.1 format"""
    
    # Extract patterns (simplified)
    ip_pattern = re.compile(r'\b(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\b')
    url_pattern = re.compile(r'https?://[^\s<>"{}|\\^`\[\]]+', re.IGNORECASE)
    
    ips = set()
    urls = set()
    
    with open(logfile, 'r', errors='ignore') as f:
        for line in f:
            ips.update(ip_pattern.findall(line))
            urls.update(url_pattern.findall(line))
    
    # Create STIX bundle
    stix_bundle = {
        "type": "bundle",
        "id": f"bundle--{datetime.now().strftime('%Y%m%d-%H%M%S')}",
        "objects": []
    }
    
    # Add IP indicators
    for ip in ips:
        indicator = {
            "type": "indicator",
            "spec_version": "2.1",
            "id": f"indicator--{ip.replace('.', '-')}",
            "created": datetime.utcnow().isoformat() + "Z",
            "modified": datetime.utcnow().isoformat() + "Z",
            "pattern": f"[ipv4-addr:value = '{ip}']",
            "pattern_type": "stix",
            "valid_from": datetime.utcnow().isoformat() + "Z"
        }
        stix_bundle["objects"].append(indicator)
    
    # Add URL indicators
    for url in list(urls)[:100]:  # Limit for practicality
        indicator = {
            "type": "indicator",
            "spec_version": "2.1",
            "id": f"indicator--url-{hash(url)}",
            "created": datetime.utcnow().isoformat() + "Z",
            "modified": datetime.utcnow().isoformat() + "Z",
            "pattern": f"[url:value = '{url}']",
            "pattern_type": "stix",
            "valid_from": datetime.utcnow().isoformat() + "Z"
        }
        stix_bundle["objects"].append(indicator)
    
    return json.dumps(stix_bundle, indent=2)

if __name__ == '__main__':
    if len(sys.argv) < 2:
        print(f"Usage: {sys.argv[0]} <logfile> > output.stix")
        sys.exit(1)
    
    print(extract_and_convert_to_stix(sys.argv[1]))
```

**Usage:**

```bash
chmod +x stix_converter.py
./stix_converter.py access.log > indicators.stix
```

### Integration with Threat Intelligence

**Check extracted IPs against AbuseIPDB:**

```bash
#!/bin/bash
# Requires API key from abuseipdb.com

API_KEY="your_api_key_here"
LOGFILE="$1"

# Extract unique IPs
IPS=$(grep -oE '\b([0-9]{1,3}\.){3}[0-9]{1,3}\b' "$LOGFILE" | sort -u)

echo "Checking IPs against AbuseIPDB..."

for ip in $IPS; do
    response=$(curl -s -G https://api.abuseipdb.com/api/v2/check \
        --data-urlencode "ipAddress=$ip" \
        -H "Key: $API_KEY" \
        -H "Accept: application/json")
    
    abuse_score=$(echo "$response" | jq -r '.data.abuseConfidenceScore')
    
    if [ "$abuse_score" -gt 50 ]; then
        echo "[!] High abuse score: $ip (Score: $abuse_score)"
        echo "$response" | jq '.data'
    fi
    
    sleep 1  # Rate limiting
done
```

**Check URLs against VirusTotal:**

```bash
#!/bin/bash
# Requires VirusTotal API key

VT_API_KEY="your_virustotal_api_key"
LOGFILE="$1"

# Extract unique URLs
URLS=$(grep -oE 'https?://[^[:space:]"]+' "$LOGFILE" | sort -u | head -50)

echo "Checking URLs against VirusTotal..."

for url in $URLS; do
    # URL encode
    encoded_url=$(python3 -c "import sys; from urllib.parse import quote; print(quote(sys.argv[1], safe=''))" "$url")
    
    # Check URL
    response=$(curl -s -X GET \
        "https://www.virustotal.com/api/v3/urls/$encoded_url" \
        -H "x-apikey: $VT_API_KEY")
    
    malicious=$(echo "$response" | jq -r '.data.attributes.last_analysis_stats.malicious')
    
    if [ "$malicious" != "null" ] && [ "$malicious" -gt 0 ]; then
        echo "[!] Malicious URL detected: $url (Detections: $malicious)"
    fi
    
    sleep 15  # VT rate limiting
done
```

### Performance Optimization for Large Logs

**Parallel processing with GNU Parallel:**

```bash
# Install GNU Parallel
sudo apt install parallel -y

# Extract IPs in parallel
cat huge.log | parallel --pipe --block 10M 'grep -oE "\b([0-9]{1,3}\.){3}[0-9]{1,3}\b"' | sort -u > ips.txt

# Process multiple log files in parallel
find /var/log -name "*.log" | parallel 'echo "Processing {}"; grep -oE "https?://[^[:space:]]+" {} >> all_urls.txt'

# Parallel URL analysis
cat urls.txt | parallel -j4 'curl -s -I {} | head -1'
```

**Using ripgrep for faster extraction:**

```bash
# Install ripgrep (faster than grep for large files)
sudo apt install ripgrep -y

# Extract IPs with ripgrep
rg -oIN '\b([0-9]{1,3}\.){3}[0-9]{1,3}\b' logfile.txt | sort -u

# Extract URLs
rg -oIN 'https?://[^\s<>"{}|\\^`\[\]]+' logfile.txt | sort -u

# Recursive search in directory
rg -oIN 'flag\{[^}]+\}' /var/log/
```

**Memory-efficient processing:**

```bash
# Stream processing for huge files (doesn't load entire file)
while IFS= read -r line; do
    echo "$line" | grep -oE '\b([0-9]{1,3}\.){3}[0-9]{1,3}\b'
done < huge.log | sort -u > ips.txt

# Process in chunks
split -l 100000 huge.log chunk_
for chunk in chunk_*; do
    grep -oE 'https?://[^[:space:]"]+' "$chunk" >> urls.txt
    rm "$chunk"
done
sort -u urls.txt > unique_urls.txt
```

---

**Important related topics:**

- Regular expression optimization and benchmarking
- Hash extraction (MD5, SHA-1, SHA-256) from logs
- File path extraction and analysis
- MAC address extraction from network logs
- Credit card/PII detection and masking (for compliance)
- CVE identifier extraction from vulnerability scanner logs

---

## Username Enumeration

Username enumeration extracts valid user accounts from authentication logs, access patterns, and application behavior. Attackers use this for targeted attacks; defenders use it to identify compromised accounts.

**Linux authentication log analysis:**

```bash
# Extract successful logins from auth.log
grep "Accepted" /var/log/auth.log | awk '{print $9}' | sort -u

# Extract failed login attempts
grep "Failed password" /var/log/auth.log | awk '{print $9}' | sort | uniq -c | sort -rn

# Extract both successful and failed authentication users
grep -E "(Accepted|Failed)" /var/log/auth.log | \
    awk '{print $9}' | sort -u > all_users.txt

# SSH authentication patterns
grep "sshd" /var/log/auth.log | \
    grep -oP "user \K\w+" | sort | uniq -c | sort -rn

# sudo usage (privilege escalation tracking)
grep "sudo:" /var/log/auth.log | \
    grep -oP "USER=\K\w+" | sort | uniq -c | sort -rn
```

**Detailed SSH session analysis:**

```bash
# Extract connection patterns: user, source IP, timestamp
grep "Accepted" /var/log/auth.log | \
    awk '{print $1, $2, $3, $9, $11}' | \
    column -t > ssh_successful_logins.txt

# Format: Month Day Time Username SourceIP

# Failed login attempts with source IPs
grep "Failed password" /var/log/auth.log | \
    awk '{print $1, $2, $3, "User:", $9, "From:", $11}' | \
    column -t > ssh_failed_attempts.txt

# Identify brute force attempts (multiple failures from same IP)
grep "Failed password" /var/log/auth.log | \
    awk '{print $11}' | sort | uniq -c | sort -rn | head -20

# Extract unique username/IP combinations
grep "Failed password" /var/log/auth.log | \
    awk '{print $9 "@" $11}' | sort -u
```

**Web application username enumeration:**

```bash
# Apache/Nginx access logs - extract usernames from login attempts
# POST requests to login endpoints
grep "POST /login" /var/log/apache2/access.log | \
    grep -oP 'username=\K[^&\s]+' | sort -u

# URL-encoded usernames
grep "POST /login" /var/log/apache2/access.log | \
    python3 -c "
import sys, re, urllib.parse
for line in sys.stdin:
    matches = re.findall(r'username=([^&\s]+)', line)
    for match in matches:
        print(urllib.parse.unquote(match))
" | sort -u

# JSON POST body username extraction
grep "POST /api/login" /var/log/nginx/access.log | \
    grep -oP '"username":"?\K[^",:}]+' | sort -u

# HTTP Basic Auth username extraction
grep "Authorization: Basic" /var/log/apache2/access.log | \
    awk '{print $NF}' | base64 -d | cut -d: -f1 | sort -u
```

**Timing-based username enumeration detection:**

```bash
# Identify username enumeration via response time differences
# Valid users may have different response times than invalid users

awk '/POST \/login/ {
    user = $0; 
    getline; 
    if ($10 ~ /^[0-9]+$/) {
        match(user, /username=([^&\s]+)/, arr);
        printf "%s %s\n", arr[1], $10
    }
}' /var/log/apache2/access.log | \
    awk '{sum[$1]+=$2; count[$1]++} 
         END {for (user in sum) print user, sum[user]/count[user]}' | \
    sort -k2 -n
```

**Database query log username extraction:**

```bash
# MySQL general query log
grep "SELECT.*FROM users WHERE" /var/log/mysql/general.log | \
    grep -oP "username\s*=\s*'\K[^']+" | sort -u

# PostgreSQL log
grep "SELECT.*FROM users WHERE" /var/log/postgresql/postgresql.log | \
    grep -oP "username\s*=\s*'\K[^']+" | sort -u

# Extract usernames from failed authentication queries
grep -i "authentication failed\|access denied" /var/log/mysql/error.log | \
    grep -oP "user\s*'\K[^']+" | sort -u
```

**Windows Event Log username enumeration:**

```bash
# On Windows systems or when analyzing exported EVTX files
# Using evtx_dump (install: pip install python-evtx)

evtx_dump.py Security.evtx | grep "EventID 4624" | \
    grep -oP "TargetUserName\">\K[^<]+" | sort -u

# Successful logons (EventID 4624)
evtx_dump.py Security.evtx | grep "EventID 4624" -A 20 | \
    grep "TargetUserName" | cut -d'>' -f2 | cut -d'<' -f1 | sort -u

# Failed logons (EventID 4625)
evtx_dump.py Security.evtx | grep "EventID 4625" -A 20 | \
    grep "TargetUserName" | cut -d'>' -f2 | cut -d'<' -f1 | \
    sort | uniq -c | sort -rn

# Using Windows PowerShell (when available)
# Get-WinEvent -FilterHashtable @{LogName='Security';ID=4624} | 
#   Select-Object -ExpandProperty Properties | 
#   Where-Object {$_.Value -match '^[a-zA-Z]'} | 
#   Select-Object -Unique Value
```

**Application-specific username extraction:**

```bash
# Email server logs (Postfix example)
grep "sasl_username" /var/log/mail.log | \
    grep -oP "sasl_username=\K[^,\s]+" | sort -u

# FTP server logs
grep "USER" /var/log/vsftpd.log | \
    awk '{print $NF}' | sort -u

# Docker container logs
docker logs container_name 2>&1 | \
    grep -i "user\|username\|login" | \
    grep -oP "user[name]*[=:]\s*\K\w+" | sort -u
```

**Comprehensive username extraction script:**

```bash
#!/bin/bash
# extract_usernames.sh - Multi-source username enumeration

OUTPUT_DIR="username_extraction_$(date +%Y%m%d_%H%M%S)"
mkdir -p "$OUTPUT_DIR"

echo "[*] Extracting usernames from multiple sources..."

# System logs
echo "[+] Analyzing system authentication logs..."
{
    grep -h "Accepted\|Failed" /var/log/auth.log* 2>/dev/null | \
        grep -oP "(user )?\K\w+(?= from)" || true
} | sort -u > "$OUTPUT_DIR/system_users.txt"

# Web logs
echo "[+] Analyzing web server logs..."
{
    grep -h "username=" /var/log/apache2/access.log* /var/log/nginx/access.log* 2>/dev/null | \
        grep -oP "username=\K[^&\s]+" || true
} | python3 -c "
import sys, urllib.parse
for line in sys.stdin:
    try: print(urllib.parse.unquote(line.strip()))
    except: pass
" | sort -u > "$OUTPUT_DIR/web_users.txt"

# Mail logs
echo "[+] Analyzing mail server logs..."
{
    grep -h "sasl_username" /var/log/mail.log* 2>/dev/null | \
        grep -oP "sasl_username=\K[^,\s]+" || true
} | sort -u > "$OUTPUT_DIR/email_users.txt"

# System users (baseline comparison)
echo "[+] Extracting system account list..."
cut -d: -f1 /etc/passwd | sort > "$OUTPUT_DIR/system_accounts.txt"

# Combine and deduplicate
echo "[+] Creating combined user list..."
cat "$OUTPUT_DIR"/*.txt | sort -u > "$OUTPUT_DIR/all_users.txt"

# Statistics
echo ""
echo "=== Extraction Summary ==="
wc -l "$OUTPUT_DIR"/*.txt

# Identify users not in /etc/passwd (potential external accounts)
comm -23 "$OUTPUT_DIR/all_users.txt" "$OUTPUT_DIR/system_accounts.txt" \
    > "$OUTPUT_DIR/external_users.txt"

echo ""
echo "[+] External/application users found: $(wc -l < "$OUTPUT_DIR/external_users.txt")"
echo "[+] Results saved to: $OUTPUT_DIR"
```

**Pattern-based username validation:**

```bash
# Filter valid username formats (reduce false positives)
filter_valid_usernames() {
    grep -E "^[a-zA-Z0-9._-]{3,32}$" | \
    grep -v -E "^(null|none|admin123|test|root123)$"
}

# Apply filter
cat extracted_users.txt | filter_valid_usernames > valid_users.txt

# Identify suspicious patterns
cat extracted_users.txt | grep -E "(admin|root|system|test)" > suspicious_users.txt
```

## Credential Harvesting

Credential harvesting extracts passwords, tokens, API keys, and authentication material from logs. Often found in cleartext due to misconfigurations, debug logging, or application errors.

**Cleartext password detection:**

```bash
# Generic password patterns in logs
grep -iE "(password|passwd|pwd)[=:\s]+" /var/log/*.log | \
    grep -v "hashed\|encrypted\|\*\*\*"

# Common parameter names
grep -E "(password|passwd|pwd|pass|secret|token|api_key)=" /var/log/apache2/access.log | \
    grep -oP "(password|passwd|pwd)=\K[^&\s]+" | head -20

# URL-encoded passwords
grep "password=" /var/log/nginx/access.log | \
    python3 -c "
import sys, re, urllib.parse
for line in sys.stdin:
    match = re.search(r'password=([^&\s]+)', line)
    if match:
        try:
            decoded = urllib.parse.unquote(match.group(1))
            if not any(x in decoded for x in ['***', 'hidden', 'redacted']):
                print(decoded)
        except: pass
" | sort -u
```

**HTTP Basic Authentication extraction:**

```bash
# Extract and decode Basic Auth credentials
grep "Authorization: Basic" /var/log/apache2/access.log | \
    grep -oP "Basic \K[A-Za-z0-9+/=]+" | \
    while read encoded; do
        decoded=$(echo "$encoded" | base64 -d 2>/dev/null)
        if [ $? -eq 0 ]; then
            echo "$decoded"
        fi
    done | sort -u

# Format: username:password

# Extract with context (timestamp, IP, endpoint)
grep "Authorization: Basic" /var/log/apache2/access.log | \
    awk '{
        for(i=1; i<=NF; i++) {
            if($i=="Basic") {
                cmd="echo " $(i+1) " | base64 -d 2>/dev/null"
                cmd | getline creds
                close(cmd)
                print $1, $4, $7, creds
            }
        }
    }'
```

**POST data credential extraction:**

```bash
# Apache/Nginx POST body logging (if enabled)
# Extract credentials from POST requests

# JSON POST bodies
grep "POST" /var/log/apache2/access.log | \
    grep -oP '\{"[^}]*password[^}]*\}' | \
    python3 -c "
import sys, json
for line in sys.stdin:
    try:
        data = json.loads(line.strip())
        if 'password' in data:
            print(f\"User: {data.get('username', 'N/A')} | Pass: {data['password']}\")
    except: pass
"

# Form-encoded POST data
grep "POST" /var/log/apache2/other_vhosts_access.log | \
    grep -E "username=.*password=" | \
    python3 -c "
import sys, re, urllib.parse
for line in sys.stdin:
    user_match = re.search(r'username=([^&\s]+)', line)
    pass_match = re.search(r'password=([^&\s]+)', line)
    if user_match and pass_match:
        user = urllib.parse.unquote(user_match.group(1))
        pwd = urllib.parse.unquote(pass_match.group(1))
        print(f'{user}:{pwd}')
" | sort -u
```

**API key and token extraction:**

```bash
# Common API key patterns
grep -oP "api[_-]?key[=:\s]+\K[A-Za-z0-9_-]{20,}" /var/log/*.log

# Bearer tokens
grep -oP "Authorization:\s*Bearer\s+\K[A-Za-z0-9._-]+" /var/log/nginx/access.log

# JWT tokens (JSON Web Tokens)
grep -oP "eyJ[A-Za-z0-9_-]+\.eyJ[A-Za-z0-9_-]+\.[A-Za-z0-9_-]+" /var/log/*.log | \
    sort -u > jwt_tokens.txt

# AWS keys (access key ID pattern)
grep -oP "AKIA[0-9A-Z]{16}" /var/log/*.log

# Generic secret patterns
grep -iE "(secret|token|api[_-]?key)[=:\s]+[A-Za-z0-9+/=_-]{20,}" /var/log/*.log | \
    grep -v "example\|sample\|test123"
```

**JWT token decoding:**

```bash
# Decode JWT tokens from logs
decode_jwt() {
    local token="$1"
    
    # Split token into parts
    IFS='.' read -r header payload signature <<< "$token"
    
    # Decode header and payload (Base64url)
    echo "=== Header ==="
    echo "$header" | base64 -d 2>/dev/null | jq . 2>/dev/null || echo "$header" | base64 -d 2>/dev/null
    
    echo -e "\n=== Payload ==="
    echo "$payload" | base64 -d 2>/dev/null | jq . 2>/dev/null || echo "$payload" | base64 -d 2>/dev/null
    
    echo -e "\n=== Signature ==="
    echo "$signature"
}

# Extract and decode all JWTs
grep -oP "eyJ[A-Za-z0-9_-]+\.eyJ[A-Za-z0-9_-]+\.[A-Za-z0-9_-]+" /var/log/app.log | \
    while read jwt; do
        echo "=== Token: ${jwt:0:30}... ==="
        decode_jwt "$jwt"
        echo ""
    done
```

**Session cookie extraction:**

```bash
# Extract session cookies from web logs
grep -oP "Cookie: [^\"]*" /var/log/apache2/access.log | \
    grep -oP "(PHPSESSID|JSESSIONID|session_id|auth_token)=\K[^;\s]+" | \
    sort -u

# Extract Set-Cookie headers (server responses, requires full logs)
grep "Set-Cookie:" /var/log/apache2/access.log | \
    grep -oP "Set-Cookie: \K[^;]+"

# ASP.NET session IDs
grep -oP "ASP\.NET_SessionId=\K[a-z0-9]+" /var/log/nginx/access.log
```

**Database connection strings:**

```bash
# MySQL/MariaDB connection strings
grep -iE "mysql://|jdbc:mysql:" /var/log/*.log | \
    grep -oP "(mysql://|jdbc:mysql:)[^\"'\s]+" | \
    while read conn; do
        echo "$conn" | python3 -c "
import sys
from urllib.parse import urlparse
url = sys.stdin.read().strip()
parsed = urlparse(url)
if parsed.username and parsed.password:
    print(f'Host: {parsed.hostname}')
    print(f'User: {parsed.username}')
    print(f'Pass: {parsed.password}')
    print(f'DB: {parsed.path.lstrip(\"/\")}')
    print('---')
"
    done

# PostgreSQL connection strings
grep -iP "postgresql://|postgres://" /var/log/*.log | \
    grep -oP "(postgresql|postgres)://[^\"'\s]+"

# MongoDB connection strings
grep -iP "mongodb://" /var/log/*.log | \
    grep -oP "mongodb://[^\"'\s]+"
```

**Email credentials (SMTP/IMAP):**

```bash
# SMTP authentication
grep -E "AUTH LOGIN|AUTH PLAIN" /var/log/mail.log | \
    grep -oP "AUTH (LOGIN|PLAIN) \K[A-Za-z0-9+/=]+" | \
    while read encoded; do
        echo "$encoded" | base64 -d 2>/dev/null
        echo ""
    done

# IMAP login attempts
grep "LOGIN" /var/log/dovecot.log | \
    grep -oP "user=<\K[^>]+" | sort -u
```

**Comprehensive credential harvesting script:**

```bash
#!/bin/bash
# harvest_credentials.sh - Extract authentication material

OUTPUT_DIR="credentials_$(date +%Y%m%d_%H%M%S)"
mkdir -p "$OUTPUT_DIR"

echo "[*] Harvesting credentials from logs..."

# HTTP Basic Auth
echo "[+] Extracting HTTP Basic Auth credentials..."
grep -rh "Authorization: Basic" /var/log/ 2>/dev/null | \
    grep -oP "Basic \K[A-Za-z0-9+/=]+" | \
    while read encoded; do
        decoded=$(echo "$encoded" | base64 -d 2>/dev/null)
        [ $? -eq 0 ] && echo "$decoded"
    done | sort -u > "$OUTPUT_DIR/http_basic_auth.txt"

# POST form credentials
echo "[+] Extracting POST form credentials..."
grep -rh "POST" /var/log/ 2>/dev/null | \
    grep -E "username=.*password=" | \
    python3 -c "
import sys, re, urllib.parse
for line in sys.stdin:
    user = re.search(r'username=([^&\s]+)', line)
    pwd = re.search(r'password=([^&\s]+)', line)
    if user and pwd:
        print(f'{urllib.parse.unquote(user.group(1))}:{urllib.parse.unquote(pwd.group(1))}')
" | sort -u > "$OUTPUT_DIR/post_credentials.txt"

# API keys and tokens
echo "[+] Extracting API keys..."
{
    grep -roP "api[_-]?key[=:\s]+\K[A-Za-z0-9_-]{20,}" /var/log/ 2>/dev/null
    grep -roP "Authorization:\s*Bearer\s+\K[A-Za-z0-9._-]+" /var/log/ 2>/dev/null
} | sort -u > "$OUTPUT_DIR/api_keys.txt"

# JWT tokens
echo "[+] Extracting JWT tokens..."
grep -roP "eyJ[A-Za-z0-9_-]+\.eyJ[A-Za-z0-9_-]+\.[A-Za-z0-9_-]+" /var/log/ 2>/dev/null | \
    sort -u > "$OUTPUT_DIR/jwt_tokens.txt"

# Database connection strings
echo "[+] Extracting database credentials..."
{
    grep -roP "(mysql|postgresql|mongodb)://[^\"'\s]+" /var/log/ 2>/dev/null
} | sort -u > "$OUTPUT_DIR/db_connections.txt"

# AWS keys
echo "[+] Extracting AWS keys..."
grep -roP "AKIA[0-9A-Z]{16}" /var/log/ 2>/dev/null | \
    sort -u > "$OUTPUT_DIR/aws_keys.txt"

# Summary
echo ""
echo "=== Harvest Summary ==="
for file in "$OUTPUT_DIR"/*.txt; do
    count=$(wc -l < "$file")
    [ $count -gt 0 ] && echo "$(basename $file): $count items"
done

echo ""
echo "[+] Results saved to: $OUTPUT_DIR"
```

**Credential validation patterns:**

```bash
# Validate extracted credentials format
validate_credentials() {
    local cred_file="$1"
    
    # Check for username:password format
    grep ":" "$cred_file" | while IFS=: read user pass; do
        # Basic validation
        if [ ${#user} -ge 3 ] && [ ${#pass} -ge 6 ]; then
            # Exclude common false positives
            if ! echo "$user" | grep -qE "(http|https|ftp|example)"; then
                echo "$user:$pass"
            fi
        fi
    done
}

# Usage
validate_credentials post_credentials.txt > validated_creds.txt
```

## Hash Extraction

Hash extraction identifies password hashes, checksums, and cryptographic material for offline cracking or integrity verification. Common in authentication logs, database dumps, and configuration files.

**Linux password hash extraction:**

```bash
# /etc/shadow hash extraction (requires root)
sudo grep -v "^#\|^$\|:\*:\|:!:" /etc/shadow | \
    awk -F: '{print $1":"$2}' > shadow_hashes.txt

# Format: username:$algorithm$salt$hash

# Extract only specific hash types
# MD5 ($1$)
sudo grep '^\w*:\$1\$' /etc/shadow | awk -F: '{print $1":"$2}'

# SHA-256 ($5$)
sudo grep '^\w*:\$5\$' /etc/shadow | awk -F: '{print $1":"$2}'

# SHA-512 ($6$) - modern Linux default
sudo grep '^\w*:\$6\$' /etc/shadow | awk -F: '{print $1":"$2}'

# yescrypt ($y$) - newer systems
sudo grep '^\w*:\$y\$' /etc/shadow | awk -F: '{print $1":"$2}'
```

**Hash type identification:**

```bash
# Identify hash algorithm by prefix
identify_hash_type() {
    local hash="$1"
    
    case "$hash" in
        \$1\$*)  echo "MD5 crypt" ;;
        \$2a\$*|\$2b\$*|\$2y\$*) echo "bcrypt" ;;
        \$5\$*)  echo "SHA-256 crypt" ;;
        \$6\$*)  echo "SHA-512 crypt" ;;
        \$y\$*)  echo "yescrypt" ;;
        \$apr1\$*) echo "Apache MD5" ;;
        {SHA}*)  echo "SHA-1 (Base64)" ;;
        {SSHA}*) echo "Salted SHA-1 (Base64)" ;;
        [a-f0-9]{32}) echo "MD5 (raw)" ;;
        [a-f0-9]{40}) echo "SHA-1 (raw)" ;;
        [a-f0-9]{64}) echo "SHA-256 (raw)" ;;
        [a-f0-9]{128}) echo "SHA-512 (raw)" ;;
        *) echo "Unknown" ;;
    esac
}

# Usage
identify_hash_type "\$6\$rounds=5000\$..."
```

**Web application hash extraction:**

```bash
# MySQL user hashes (from dumped data or logs)
grep -E "mysql\.user|authentication_string" /var/log/mysql/*.log | \
    grep -oP "\*[A-F0-9]{40}" | sort -u

# PostgreSQL MD5 hashes
grep "md5" /var/log/postgresql/*.log | \
    grep -oP "md5[a-f0-9]{32}" | sort -u

# Extract from SQL dumps
grep "INSERT INTO.*users" database_dump.sql | \
    grep -oP "password['\"]?\s*[,=]\s*['\"]?\K[a-f0-9]{32,128}" | \
    sort -u
```

**WordPress/Joomla/CMS hashes:**

```bash
# WordPress (phpass)
# Format: $P$B... or $H$9...
grep -oP '\$[PH]\$[a-zA-Z0-9./]{31}' /var/log/apache2/access.log

# Extract from database dumps
grep "INSERT INTO.*wp_users" wp_backup.sql | \
    grep -oP "user_pass['\"]?\s*[,=]\s*['\"]?\K\$[PH]\$[a-zA-Z0-9./]{31}"

# Joomla (bcrypt or MD5)
grep "INSERT INTO.*users" joomla_backup.sql | \
    grep -oP "password['\"]?\s*[,=]\s*['\"]?\K[a-f0-9:]{32,}"
```

**NTLM hash extraction:**

```bash
# Windows NTLM hashes from logs or dumps
# Format: username:RID:LM_hash:NTLM_hash:::

# Extract from mimikatz-style output
grep -oP "NTLM\s*:\s*\K[a-f0-9]{32}" mimikatz.log

# Extract from Responder logs
grep "NTLMv2-SSP Hash" responder.log | \
    grep -oP ":\K[^:]+$"

# pwdump format
grep -E ":[0-9]+:[A-F0-9]{32}:[A-F0-9]{32}:::" ntlm_dump.txt
```

**Application-specific hash patterns:**

```bash
# MD5 hashes (32 hex characters)
grep -roP "\b[a-f0-9]{32}\b" /var/log/ | \
    grep -v "session\|cookie\|JSESSIONID" | \
    sort -u > md5_hashes.txt

# SHA-1 hashes (40 hex characters)
grep -roP "\b[a-f0-9]{40}\b" /var/log/ | \
    sort -u > sha1_hashes.txt

# SHA-256 hashes (64 hex characters)
grep -roP "\b[a-f0-9]{64}\b" /var/log/ | \
    sort -u > sha256_hashes.txt

# Base64-encoded hashes
grep -oP "{SHA}[A-Za-z0-9+/=]{28}" /var/log/*.log | \
    while read hash; do
        echo -n "SHA: "
        echo "${hash#\{SHA\}}" | base64 -d | xxd -p
    done
```

**Hash extraction from authentication headers:**

```bash
# Digest authentication (RFC 2617)
grep "Authorization: Digest" /var/log/apache2/access.log | \
    grep -oP 'response="?\K[a-f0-9]{32}'

# Extract complete Digest auth parameters
grep "Authorization: Digest" /var/log/apache2/access.log | \
    grep -oP "Authorization: Digest \K.*" | \
    python3 -c "
import sys, re
for line in sys.stdin:
    response = re.search(r'response=\"([a-f0-9]{32})\"', line)
    username = re.search(r'username=\"([^\"]+)\"', line)
    realm = re.search(r'realm=\"([^\"]+)\"', line)
    if response and username:
        print(f'{username.group(1)}:{response.group(1)}')
"
```

**Comprehensive hash extraction script:**

```bash
#!/bin/bash
# extract_hashes.sh - Multi-format hash extraction

OUTPUT_DIR="hashes_$(date +%Y%m%d_%H%M%S)"
mkdir -p "$OUTPUT_DIR"

echo "[*] Extracting password hashes..."

# System hashes (requires root)
if [ "$EUID" -eq 0 ]; then
    echo "[+] Extracting /etc/shadow hashes..."
    grep -v "^#\|^$\|:\*:\|:!:" /etc/shadow | \
        awk -F: '{print $1":"$2}' > "$OUTPUT_DIR/shadow_hashes.txt"
else
    echo "[!] Not root - skipping /etc/shadow"
fi

# MD5 hashes
echo "[+] Extracting MD5 hashes..."
grep -roP "\b[a-f0-9]{32}\b" /var/log/ 2>/dev/null | \
    cut -d: -f2 | sort -u | head -1000 > "$OUTPUT_DIR/md5_candidates.txt"

# SHA-1 hashes
echo "[+] Extracting SHA-1 hashes..."
grep -roP "\b[a-f0-9]{40}\b" /var/log/ 2>/dev/null | \
    cut -d: -f2 | sort -u | head -1000 > "$OUTPUT_DIR/sha1_candidates.txt"

# SHA-256 hashes
echo "[+] Extracting SHA-256 hashes..."
grep -roP "\b[a-f0-9]{64}\b" /var/log/ 2>/dev/null | \
    cut -d: -f2 | sort -u | head -1000 > "$OUTPUT_DIR/sha256_candidates.txt"

# MySQL hashes
echo "[+] Extracting MySQL hashes..."
grep -roP "\*[A-F0-9]{40}" /var/log/ 2>/dev/null | \
    cut -d: -f2 | sort -u > "$OUTPUT_DIR/mysql_hashes.txt"

# NTLM hashes
echo "[+] Extracting NTLM hashes..."
grep -roP "\b[a-f0-9]{32}:[a-f0-9]{32}\b" /var/log/ 2>/dev/null | \
    cut -d: -f2- | sort -u > "$OUTPUT_DIR/ntlm_hashes.txt"

# WordPress/CMS hashes
echo "[+] Extracting CMS hashes..."
grep -roP '\$[PH]\$[a-zA-Z0-9./]{31}' /var/log/ 2>/dev/null | \
    cut -d: -f2- | sort -u > "$OUTPUT_DIR/cms_hashes.txt"

# Create hashcat format files
echo "[+] Creating Hashcat-compatible formats..."

# MD5
cat "$OUTPUT_DIR/md5_candidates.txt" > "$OUTPUT_DIR/hashcat_md5.txt"

# SHA-512 crypt (hashcat mode 1800)
grep '^\w*:\$6\$' "$OUTPUT_DIR/shadow_hashes.txt" 2>/dev/null \
    > "$OUTPUT_DIR/hashcat_sha512crypt.txt"

# Summary
echo ""
echo "=== Extraction Summary ==="
for file in "$OUTPUT_DIR"/*.txt; do
    count=$(wc -l < "$file" 2>/dev/null)
    [ $count -gt 0 ] && echo "$(basename $file): $count hashes"
done

echo ""
echo "[+] Results saved to: $OUTPUT_DIR"
```

**Hash extraction from network traffic captures:**

```bash
# Extract hashes from pcap files (requires tshark/wireshark)
# HTTP Digest authentication
tshark -r capture.pcap -Y "http.authz" -T fields \
    -e http.authz | grep -oP 'response="?\K[a-f0-9]{32}'

# NTLM authentication (SMB/HTTP)
tshark -r capture.pcap -Y "ntlmssp.auth" -T fields \
    -e ntlmssp.auth.username -e ntlmssp.auth.ntresponse | \
    awk '{print $1":"$2}'

# Kerberos tickets
tshark -r capture.pcap -Y "kerberos.cipher" -T fields \
    -e kerberos.cipher | head -20
```

**Hash extraction from memory dumps:**

```bash
# Extract hashes from process memory dumps
strings memory.dmp | grep -oP "\b[a-f0-9]{32}\b" | sort -u > memory_md5.txt
strings memory.dmp | grep -oP "\b[a-f0-9]{64}\b" | sort -u > memory_sha256.txt

# Extract Windows NTLM hashes from memory (mimikatz-style patterns)
strings -e l memory.dmp | grep -oP "[a-f0-9]{32}" | \
    awk 'length($0)==32' | sort -u > memory_ntlm.txt

# Look for cleartext credentials in memory
strings memory.dmp | grep -i "password" | head -50
```

**Hash cracking preparation:**

```bash
# Format hashes for John the Ripper
format_for_john() {
    local input="$1"
    
    # Shadow format already compatible
    if grep -q '^\w*:\$' "$input"; then
        cp "$input" "${input%.txt}.john"
    else
        # Add username prefix if missing
        awk '{print "user"NR":"$0}' "$input" > "${input%.txt}.john"
    fi
}

# Format hashes for Hashcat
format_for_hashcat() {
    local input="$1"
    local mode="$2"
    
    case "$mode" in
        "ntlm")
            # NTLM format: hash or username:hash
            awk -F: '{print ($2 ? $2 : $1)}' "$input" > "${input%.txt}.hashcat"
            ;;
        "md5")
            # Raw MD5
            cat "$input" > "${input%.txt}.hashcat"
            ;;
        *)
            cp "$input" "${input%.txt}.hashcat"
            ;;
    esac
}

# Identify hashcat mode
identify_hashcat_mode() {
    local hash="$1"
    
    case "$hash" in
        \$1\$*) echo "500" ;;   # MD5 crypt
        \$2a\$*|\$2b\$*|\$2y\$*) echo "3200" ;; # bcrypt
        \$5\$*) echo "7400" ;;  # SHA-256 crypt
        \$6\$*) echo "1800" ;;  # SHA-512 crypt
        \$y\$*) echo "yescrypt - check hashcat version" ;;
        \$apr1\$*) echo "1600" ;; # Apache MD5
        [a-f0-9]{32}) echo "0" ;; # MD5 raw
        [a-f0-9]{40}) echo "100" ;; # SHA-1 raw
        [a-f0-9]{64}) echo "1400" ;; # SHA-256 raw
        \*[A-F0-9]{40}) echo "300" ;; # MySQL4+
        *) echo "Unknown" ;;
    esac
}
```

**Hash validation and deduplication:**

```bash
# Remove duplicate hashes
deduplicate_hashes() {
    local input="$1"
    local output="${input%.txt}_unique.txt"
    
    # Sort and deduplicate
    sort -u "$input" > "$output"
    
    # Remove invalid entries
    grep -v "^$\|^#\|example\|test123" "$output" > "$output.tmp"
    mv "$output.tmp" "$output"
    
    echo "[+] Original: $(wc -l < $input) | Unique: $(wc -l < $output)"
}

# Validate hash format
validate_hash_format() {
    local hash="$1"
    local type="$2"
    
    case "$type" in
        "md5")
            [[ "$hash" =~ ^[a-f0-9]{32}$ ]] && return 0 ;;
        "sha1")
            [[ "$hash" =~ ^[a-f0-9]{40}$ ]] && return 0 ;;
        "sha256")
            [[ "$hash" =~ ^[a-f0-9]{64}$ ]] && return 0 ;;
        "ntlm")
            [[ "$hash" =~ ^[a-f0-9]{32}$ ]] && return 0 ;;
        "sha512crypt")
            [[ "$hash" =~ ^\$6\$ ]] && return 0 ;;
    esac
    
    return 1
}

# Filter valid hashes
cat hashes.txt | while read hash; do
    if validate_hash_format "$hash" "md5"; then
        echo "$hash"
    fi
done > valid_hashes.txt
```

**Automated hash cracking setup:**

```bash
#!/bin/bash
# crack_hashes.sh - Prepare and crack extracted hashes

HASH_FILE="$1"
WORDLIST="${2:-/usr/share/wordlists/rockyou.txt}"

if [ ! -f "$HASH_FILE" ]; then
    echo "[!] Usage: $0 <hash_file> [wordlist]"
    exit 1
fi

echo "[*] Analyzing hash file: $HASH_FILE"

# Identify hash type from first hash
FIRST_HASH=$(grep -v "^#\|^$" "$HASH_FILE" | head -1 | cut -d: -f2-)
HASHCAT_MODE=$(identify_hashcat_mode "$FIRST_HASH")

echo "[+] Detected hash type: $HASHCAT_MODE"

# Run hashcat if available
if command -v hashcat &> /dev/null; then
    echo "[*] Starting Hashcat..."
    hashcat -m "$HASHCAT_MODE" -a 0 "$HASH_FILE" "$WORDLIST" \
        -o "${HASH_FILE%.txt}_cracked.txt" \
        --username \
        --show
else
    echo "[!] Hashcat not found, trying John the Ripper..."
    
    # Run John the Ripper
    if command -v john &> /dev/null; then
        john --wordlist="$WORDLIST" "$HASH_FILE"
        john --show "$HASH_FILE" > "${HASH_FILE%.txt}_cracked.txt"
    else
        echo "[!] No cracking tools available"
    fi
fi

echo "[+] Cracked passwords saved to: ${HASH_FILE%.txt}_cracked.txt"
```

**Hashcat quick reference:**

```bash
# Common hashcat modes
# 0     = MD5
# 100   = SHA1
# 1400  = SHA256
# 1700  = SHA512
# 1800  = SHA-512 crypt
# 3200  = bcrypt
# 5600  = NetNTLMv2
# 13100 = Kerberos 5 TGS-REP

# Basic dictionary attack
hashcat -m 0 -a 0 hashes.txt /usr/share/wordlists/rockyou.txt

# Dictionary + rules
hashcat -m 1800 -a 0 shadow_hashes.txt wordlist.txt -r /usr/share/hashcat/rules/best64.rule

# Mask attack (brute force with pattern)
# ?l = lowercase, ?u = uppercase, ?d = digit, ?s = special
hashcat -m 0 -a 3 hashes.txt ?l?l?l?l?d?d?d?d

# Combination attack
hashcat -m 0 -a 1 hashes.txt wordlist1.txt wordlist2.txt

# Show cracked hashes
hashcat -m 0 hashes.txt --show

# Resume session
hashcat -m 0 hashes.txt --session=mysession --restore
```

**John the Ripper quick reference:**

```bash
# Auto-detect hash format
john hashes.txt

# Specify format
john --format=sha512crypt shadow_hashes.txt

# Use wordlist
john --wordlist=/usr/share/wordlists/rockyou.txt hashes.txt

# Wordlist with rules
john --wordlist=words.txt --rules=best64 hashes.txt

# Show cracked passwords
john --show hashes.txt

# Incremental mode (brute force)
john --incremental hashes.txt

# Single crack mode (username-based mutations)
john --single hashes.txt

# Export to specific format
john --format=NT ntlm_hashes.txt
```

**Hash extraction from JSON/XML logs:**

```bash
# Extract from JSON logs
jq -r '.password_hash' /var/log/app.log 2>/dev/null | \
    grep -v "^null$" | sort -u

# Extract nested JSON hashes
jq -r '.. | .hash? // empty' /var/log/app.log 2>/dev/null | sort -u

# XML logs
grep -oP '<password_hash>\K[^<]+' /var/log/app.xml | sort -u

# Extract from structured logs with multiple hash fields
jq -r '[.md5_hash, .sha256_hash, .bcrypt_hash] | .[]' app.log 2>/dev/null | \
    grep -v "^null$" | sort -u
```

**Database-specific hash extraction:**

```bash
# PostgreSQL MD5 hash format: md5{hash}
# Extract from pg_authid table dump
grep "INSERT INTO pg_authid" dump.sql | \
    grep -oP "rolpassword\s*=\s*'md5\K[a-f0-9]{32}" | sort -u

# MySQL native password hashing
# Extract from mysql.user table
grep "INSERT INTO.*mysql.user" dump.sql | \
    grep -oP "authentication_string['\"]?\s*=\s*['\"]?\K\*[A-F0-9]{40}" | \
    sort -u

# MongoDB SCRAM-SHA-1/SHA-256
grep "SCRAM-SHA" mongo.log | \
    grep -oP "storedKey\":\s*\"[^\"]+\"|serverKey\":\s*\"[^\"]+\"" | \
    cut -d'"' -f4 | sort -u
```

**Password complexity analysis:**

```bash
# Analyze cracked passwords for patterns
analyze_passwords() {
    local cracked_file="$1"
    
    echo "=== Password Complexity Analysis ==="
    echo ""
    
    # Length distribution
    echo "[+] Length distribution:"
    awk -F: '{print length($NF)}' "$cracked_file" | sort -n | uniq -c | sort -rn
    
    echo ""
    echo "[+] Common patterns:"
    
    # All lowercase
    echo -n "Lowercase only: "
    awk -F: '{pwd=$NF; if(pwd ~ /^[a-z]+$/) print pwd}' "$cracked_file" | wc -l
    
    # All digits
    echo -n "Digits only: "
    awk -F: '{pwd=$NF; if(pwd ~ /^[0-9]+$/) print pwd}' "$cracked_file" | wc -l
    
    # Common patterns
    echo -n "Contains 'password': "
    grep -i "password" "$cracked_file" | wc -l
    
    echo -n "Contains '123': "
    grep "123" "$cracked_file" | wc -l
    
    echo -n "Ends with year (20xx): "
    grep -E "20[0-9]{2}$" "$cracked_file" | wc -l
    
    echo ""
    echo "[+] Top 10 most common:"
    awk -F: '{print $NF}' "$cracked_file" | sort | uniq -c | sort -rn | head -10
}

# Usage
analyze_passwords shadow_hashes_cracked.txt
```

**Advanced hash extraction patterns:**

```bash
# Extract salted hashes with salt values
extract_salted_hashes() {
    local log_file="$1"
    
    # Django format: algorithm$salt$hash
    grep -oP "pbkdf2_sha256\$[0-9]+\$[^$]+\$[A-Za-z0-9+/=]+" "$log_file"
    
    # Custom salt patterns
    grep -oP "salt=[^&\s]+.*hash=[a-f0-9]{32,}" "$log_file" | \
        sed 's/.*salt=\([^&]*\).*hash=\([a-f0-9]*\).*/\1:\2/'
}

# Extract hashes with metadata
extract_hash_with_context() {
    grep "password_hash\|hash\|digest" /var/log/app.log | \
        python3 -c "
import sys, json, re
for line in sys.stdin:
    try:
        data = json.loads(line.strip())
        user = data.get('username', 'unknown')
        hash_val = data.get('password_hash') or data.get('hash')
        if hash_val:
            print(f'{user}:{hash_val}')
    except:
        # Fallback to regex for non-JSON
        match = re.search(r'(\w+).*?([a-f0-9]{32,})', line)
        if match:
            print(f'{match.group(1)}:{match.group(2)}')
"
}
```

**Important considerations:**

1. **Legal/ethical boundaries**: Only extract hashes from systems you have authorization to test
2. **Hash entropy**: Short or patterned hashes (like MD5 of sequential numbers) crack quickly
3. **Salt awareness**: Salted hashes require salt values for successful cracking
4. **Algorithm strength**: Modern algorithms (bcrypt, scrypt, argon2) are intentionally slow to crack
5. **False positives**: Not all 32-character hex strings are MD5 hashes - validate context

**[Inference]** Hash extraction success depends on logging verbosity and application security practices. Many modern applications avoid logging hashes, but debug modes, error logs, and database dumps remain common sources.

**CTF-specific tips:**

- Check for intentionally weak hashes (MD5, SHA1 without salt) as speed-oriented challenges
- Look for custom hash formats that may require reverse engineering the algorithm
- Examine hash lengths carefully - truncated hashes may indicate custom implementations
- Consider rainbow tables for unsalted hashes (MD5, SHA1) when wordlists fail
- Some CTFs embed hints in salt values or use predictable salting schemes

---

# Correlation Analysis

Correlation analysis is the process of linking related events across multiple log sources to reconstruct complete attack chains, user activities, and system behaviors. In CTF scenarios, successful correlation often reveals hidden patterns, identifies pivot points, and uncovers the full scope of compromise that individual logs cannot show in isolation.

## Multi-Source Correlation

Multi-source correlation involves aggregating and analyzing logs from different systems, applications, and network devices to identify relationships between events. This technique is essential for understanding distributed attacks and complex intrusion scenarios.

### Identifying Correlation Points

**Common correlation fields across log sources**:

- **Timestamps**: Synchronize events chronologically
- **IP addresses**: Track network connections across systems
- **Usernames**: Follow user activities across services
- **Session IDs**: Link related transactions
- **Process IDs (PIDs)**: Connect parent-child process relationships
- **File hashes**: Track malware propagation
- **URLs/domains**: Identify C2 communication patterns
- **Transaction IDs**: Correlate database and application logs

**Log source types and their correlation value**:

```bash
# System logs - Authentication, process execution
/var/log/auth.log, /var/log/syslog, /var/log/messages

# Web server logs - HTTP requests, access patterns
/var/log/apache2/access.log, /var/log/nginx/access.log

# Application logs - Business logic, errors
/var/log/application/*.log

# Network logs - Traffic patterns, connections
firewall.log, ids.log, netflow data

# Database logs - Queries, transactions
/var/log/mysql/general.log, postgresql.log

# Security tool logs - Alerts, detections
snort.log, ossec.log, suricata/eve.json
```

### Timestamp Normalization

**Challenge**: Different log sources use different timestamp formats and timezones.

**Common timestamp formats**:

```bash
# Syslog RFC3164
Oct 29 14:23:45

# ISO 8601
2025-10-29T14:23:45Z
2025-10-29T14:23:45+08:00

# Apache Combined Log Format
[29/Oct/2025:14:23:45 +0800]

# Epoch timestamp
1730181825

# Windows Event Log
2025-10-29 14:23:45.123
```

**Normalization using date command**:

```bash
# Convert various formats to epoch for sorting
# Syslog format (add current year)
date -d "Oct 29 14:23:45 2025" +%s

# ISO 8601
date -d "2025-10-29T14:23:45Z" +%s

# Apache format (requires cleanup)
echo "[29/Oct/2025:14:23:45 +0800]" | \
    sed 's/\[//;s/\]//' | \
    date -f - +%s

# Epoch to human-readable
date -d @1730181825
```

**Python timestamp normalization script**:

```python
#!/usr/bin/env python3
import re
from datetime import datetime
import sys

# Common timestamp patterns
PATTERNS = {
    'iso8601': r'\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2}',
    'syslog': r'[A-Z][a-z]{2}\s+\d{1,2}\s+\d{2}:\d{2}:\d{2}',
    'apache': r'\d{2}/[A-Z][a-z]{2}/\d{4}:\d{2}:\d{2}:\d{2}',
    'epoch': r'\b\d{10}\b'
}

def normalize_timestamp(line):
    """Extract and normalize timestamp to ISO 8601 format"""
    
    # ISO 8601 (already normalized)
    match = re.search(PATTERNS['iso8601'], line)
    if match:
        try:
            dt = datetime.fromisoformat(match.group().replace('T', ' '))
            return dt.isoformat(), line
        except:
            pass
    
    # Syslog format (assume current year)
    match = re.search(PATTERNS['syslog'], line)
    if match:
        try:
            dt = datetime.strptime(f"2025 {match.group()}", "%Y %b %d %H:%M:%S")
            return dt.isoformat(), line
        except:
            pass
    
    # Apache format
    match = re.search(PATTERNS['apache'], line)
    if match:
        try:
            dt = datetime.strptime(match.group(), "%d/%b/%Y:%H:%M:%S")
            return dt.isoformat(), line
        except:
            pass
    
    # Epoch timestamp
    match = re.search(PATTERNS['epoch'], line)
    if match:
        try:
            dt = datetime.fromtimestamp(int(match.group()))
            return dt.isoformat(), line
        except:
            pass
    
    return None, line

# Process stdin
for line in sys.stdin:
    timestamp, original = normalize_timestamp(line.strip())
    if timestamp:
        print(f"{timestamp} | {original}")
    else:
        print(f"UNKNOWN_TIME | {original}")
```

Usage:

```bash
cat mixed_logs.txt | python3 normalize_timestamps.py | sort > normalized_timeline.txt
```

### Log Aggregation and Merging

**Simple chronological merge**:

```bash
# Merge multiple log files by timestamp
# Assumes logs have normalized timestamps at start of line

# Using sort
cat /var/log/auth.log /var/log/apache2/access.log /var/log/app.log | \
    sort -k1,1 > merged_timeline.log

# With source identification
for log in /var/log/auth.log /var/log/apache2/access.log; do
    awk -v src="$(basename $log)" '{print $0 " [SOURCE:" src "]"}' "$log"
done | sort > merged_with_source.log
```

**Using awk for complex merging**:

```bash
# Merge with timestamp extraction and source tagging
awk '
{
    # Extract timestamp (adjust regex for your format)
    match($0, /[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}/)
    timestamp = substr($0, RSTART, RLENGTH)
    
    # Add source file and print
    print timestamp, FILENAME, $0
}
' /var/log/*.log | sort -k1,1 > unified_timeline.log
```

**jq for JSON log correlation**:

```bash
# Merge JSON logs from multiple sources
jq -s 'sort_by(.timestamp)' app1.json app2.json app3.json > merged.json

# Add source field during merge
for f in app*.json; do
    jq --arg src "$f" '. + {source: $src}' "$f"
done | jq -s 'sort_by(.timestamp)' > merged_with_source.json

# Merge and filter by specific field
jq -s '[.[] | select(.level == "ERROR")] | sort_by(.timestamp)' *.json
```

### Correlation by IP Address

**Tracking IP across multiple log sources**:

```bash
# Define target IP
TARGET_IP="192.168.1.100"

# Search across all logs
grep -h "$TARGET_IP" /var/log/{auth.log,apache2/access.log,firewall.log} | \
    sort > ip_activity_timeline.txt

# With timestamp normalization and source identification
grep -h "$TARGET_IP" /var/log/auth.log | \
    awk '{print $0 " [SOURCE:auth]"}' > /tmp/ip_correlation.txt
grep -h "$TARGET_IP" /var/log/apache2/access.log | \
    awk '{print $0 " [SOURCE:apache]"}' >> /tmp/ip_correlation.txt
grep -h "$TARGET_IP" /var/log/firewall.log | \
    awk '{print $0 " [SOURCE:firewall]"}' >> /tmp/ip_correlation.txt

sort /tmp/ip_correlation.txt > ip_complete_timeline.txt
```

**IP correlation with pattern recognition**:

```python
#!/usr/bin/env python3
import re
import sys
from collections import defaultdict

# Track events by IP
ip_events = defaultdict(list)

# Process multiple log files
for line in sys.stdin:
    # Extract IP addresses
    ips = re.findall(r'\b(?:\d{1,3}\.){3}\d{1,3}\b', line)
    
    for ip in ips:
        ip_events[ip].append(line.strip())

# Output correlation report
for ip, events in sorted(ip_events.items(), key=lambda x: len(x[1]), reverse=True):
    print(f"\n{'='*80}")
    print(f"IP Address: {ip}")
    print(f"Total Events: {len(events)}")
    print(f"{'='*80}")
    
    for event in events[:10]:  # Show first 10 events
        print(event)
    
    if len(events) > 10:
        print(f"... and {len(events) - 10} more events")
```

Usage:

```bash
cat /var/log/*.log | python3 correlate_by_ip.py > ip_correlation_report.txt
```

### Correlation by Username

**User activity across services**:

```bash
# Track specific user across all logs
USERNAME="admin"

# Authentication logs
grep "$USERNAME" /var/log/auth.log | \
    awk '{print $0 " [AUTH]"}' > /tmp/user_activity.txt

# Web application logs
grep "$USERNAME" /var/log/apache2/access.log | \
    awk '{print $0 " [WEB]"}' >> /tmp/user_activity.txt

# Database logs
grep "$USERNAME" /var/log/mysql/general.log | \
    awk '{print $0 " [DB]"}' >> /tmp/user_activity.txt

# Application logs
grep "$USERNAME" /var/log/app/*.log | \
    awk '{print $0 " [APP]"}' >> /tmp/user_activity.txt

# Sort by timestamp
sort /tmp/user_activity.txt > user_complete_timeline.txt
```

**Multi-user correlation matrix**:

```python
#!/usr/bin/env python3
import re
import sys
from collections import defaultdict
from datetime import datetime

# Structure: {username: {timestamp: [events]}}
user_timeline = defaultdict(lambda: defaultdict(list))

# Common username patterns
USERNAME_PATTERNS = [
    r'user=(\w+)',
    r'username[=:](\w+)',
    r'USER (\w+)',
    r'for (\w+) from',
]

for line in sys.stdin:
    # Extract username
    username = None
    for pattern in USERNAME_PATTERNS:
        match = re.search(pattern, line, re.IGNORECASE)
        if match:
            username = match.group(1)
            break
    
    if not username:
        continue
    
    # Extract timestamp (simplified - adjust for your format)
    ts_match = re.search(r'\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}', line)
    if ts_match:
        timestamp = ts_match.group()
        user_timeline[username][timestamp].append(line.strip())

# Generate correlation report
print("USER ACTIVITY CORRELATION REPORT")
print("=" * 80)

for user in sorted(user_timeline.keys()):
    print(f"\nUser: {user}")
    print(f"Total time periods: {len(user_timeline[user])}")
    print(f"Total events: {sum(len(events) for events in user_timeline[user].values())}")
    
    # Show first and last activity
    timestamps = sorted(user_timeline[user].keys())
    if timestamps:
        print(f"First activity: {timestamps[0]}")
        print(f"Last activity: {timestamps[-1]}")
        print(f"Sample events:")
        for event in list(user_timeline[user][timestamps[0]])[:3]:
            print(f"  {event}")
```

Usage:

```bash
cat /var/log/{auth.log,apache2/access.log,app.log} | \
    python3 user_correlation.py > user_correlation_report.txt
```

### Time Window Correlation

**Finding events within time windows**:

```bash
# Events within 5 seconds of suspicious activity
SUSPICIOUS_TIME="2025-10-29T14:23:45"

# Convert to epoch for range calculation
EPOCH=$(date -d "$SUSPICIOUS_TIME" +%s)
START=$((EPOCH - 5))
END=$((EPOCH + 5))

# Filter logs in time window
awk -v start=$START -v end=$END '
{
    # Extract timestamp and convert to epoch
    match($0, /[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}/)
    ts = substr($0, RSTART, RLENGTH)
    cmd = "date -d \"" ts "\" +%s"
    cmd | getline epoch
    close(cmd)
    
    if (epoch >= start && epoch <= end) {
        print $0
    }
}
' merged_timeline.log > time_window_events.txt
```

**Correlation window analysis**:

```python
#!/usr/bin/env python3
import sys
from datetime import datetime, timedelta
from collections import defaultdict

# Parse events with timestamps
events = []
for line in sys.stdin:
    # Extract ISO timestamp
    import re
    match = re.search(r'\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}', line)
    if match:
        ts = datetime.fromisoformat(match.group())
        events.append((ts, line.strip()))

# Sort by timestamp
events.sort()

# Find clusters of activity (events within 10 seconds)
WINDOW = timedelta(seconds=10)
clusters = []
current_cluster = []

for i, (ts, event) in enumerate(events):
    if not current_cluster:
        current_cluster = [(ts, event)]
    else:
        last_ts = current_cluster[-1][0]
        if ts - last_ts <= WINDOW:
            current_cluster.append((ts, event))
        else:
            if len(current_cluster) >= 3:  # Clusters of 3+ events
                clusters.append(current_cluster)
            current_cluster = [(ts, event)]

# Add final cluster
if len(current_cluster) >= 3:
    clusters.append(current_cluster)

# Report clusters
print("ACTIVITY CLUSTERS (3+ events within 10 seconds)")
print("=" * 80)

for i, cluster in enumerate(clusters, 1):
    print(f"\nCluster {i}:")
    print(f"Time range: {cluster[0][0]} to {cluster[-1][0]}")
    print(f"Duration: {cluster[-1][0] - cluster[0][0]}")
    print(f"Event count: {len(cluster)}")
    print("Events:")
    for ts, event in cluster:
        print(f"  [{ts}] {event}")
```

Usage:

```bash
cat merged_timeline.log | python3 cluster_analysis.py
```

### Cross-Reference by Session/Transaction ID

**Tracking sessions across application layers**:

```bash
# Extract session IDs from web logs
grep -oP 'SESSIONID=\K[A-Za-z0-9]+' /var/log/apache2/access.log | \
    sort -u > session_ids.txt

# For each session, find all related events
while read session_id; do
    echo "=== Session: $session_id ==="
    
    # Web server logs
    grep "$session_id" /var/log/apache2/access.log | \
        awk '{print $0 " [WEB]"}'
    
    # Application logs
    grep "$session_id" /var/log/app/*.log | \
        awk '{print $0 " [APP]"}'
    
    # Database logs
    grep "$session_id" /var/log/mysql/general.log | \
        awk '{print $0 " [DB]"}'
    
    echo ""
done < session_ids.txt | sort > session_correlation.txt
```

**Transaction flow reconstruction**:

```python
#!/usr/bin/env python3
import re
import sys
import json
from collections import defaultdict

# Structure: {transaction_id: [events in chronological order]}
transactions = defaultdict(list)

# Transaction ID patterns (adjust for your logs)
TX_PATTERNS = [
    r'txn[_-]?id[=:]([A-Za-z0-9-]+)',
    r'transaction[=:]([A-Za-z0-9-]+)',
    r'request[_-]?id[=:]([A-Za-z0-9-]+)',
]

for line in sys.stdin:
    # Extract transaction ID
    tx_id = None
    for pattern in TX_PATTERNS:
        match = re.search(pattern, line, re.IGNORECASE)
        if match:
            tx_id = match.group(1)
            break
    
    if tx_id:
        # Extract timestamp for sorting
        ts_match = re.search(r'\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}', line)
        timestamp = ts_match.group() if ts_match else "UNKNOWN"
        
        transactions[tx_id].append({
            'timestamp': timestamp,
            'event': line.strip()
        })

# Output transaction flows
print("TRANSACTION FLOW ANALYSIS")
print("=" * 80)

for tx_id in sorted(transactions.keys()):
    events = sorted(transactions[tx_id], key=lambda x: x['timestamp'])
    
    print(f"\nTransaction ID: {tx_id}")
    print(f"Total events: {len(events)}")
    print(f"Time range: {events[0]['timestamp']} to {events[-1]['timestamp']}")
    print("Flow:")
    
    for i, event_data in enumerate(events, 1):
        print(f"  Step {i} [{event_data['timestamp']}]:")
        print(f"    {event_data['event']}")
```

Usage:

```bash
cat /var/log/{apache2/access.log,app.log,mysql/general.log} | \
    python3 transaction_flow.py > transaction_analysis.txt
```

### Correlation by File/Resource Access

**Tracking file operations across logs**:

```bash
# Target file of interest
TARGET_FILE="/etc/passwd"

# System audit logs
ausearch -f "$TARGET_FILE" 2>/dev/null | \
    awk '{print $0 " [AUDIT]"}' > /tmp/file_access.txt

# Application logs
grep -h "$TARGET_FILE" /var/log/app/*.log | \
    awk '{print $0 " [APP]"}' >> /tmp/file_access.txt

# Web server logs (if accessible via web)
grep -h "$(basename $TARGET_FILE)" /var/log/apache2/access.log | \
    awk '{print $0 " [WEB]"}' >> /tmp/file_access.txt

# System logs (failed access attempts)
grep -h "$TARGET_FILE" /var/log/syslog | \
    awk '{print $0 " [SYSTEM]"}' >> /tmp/file_access.txt

# Sort chronologically
sort /tmp/file_access.txt > file_access_timeline.txt
```

### Network Connection Correlation

**Tracking connections across network and system logs**:

```bash
# Extract connections from multiple sources

# Firewall logs
awk '/ACCEPT|DENY/ {print $1, $2, $SRC, $DST, $SPT, $DPT}' /var/log/firewall.log | \
    awk '{print $0 " [FW]"}' > /tmp/connections.txt

# Web server logs (extract IPs)
awk '{print $1, $7}' /var/log/apache2/access.log | \
    awk '{print $0 " [WEB]"}' >> /tmp/connections.txt

# Authentication logs (SSH connections)
grep "Accepted\|Failed" /var/log/auth.log | \
    awk '{print $0 " [SSH]"}' >> /tmp/connections.txt

# Sort and analyze
sort /tmp/connections.txt > network_correlation.txt
```

### Advanced Correlation Tools

**LogStash configuration example** [Inference]:

```ruby
# Simplified LogStash correlation config
# Install: apt install logstash

input {
  file {
    path => "/var/log/auth.log"
    type => "auth"
  }
  file {
    path => "/var/log/apache2/access.log"
    type => "web"
  }
  file {
    path => "/var/log/app.log"
    type => "application"
  }
}

filter {
  # Extract common fields for correlation
  grok {
    match => {
      "message" => "%{IPORHOST:client_ip}"
    }
  }
  
  # Normalize timestamps
  date {
    match => [ "timestamp", "ISO8601", "yyyy-MM-dd HH:mm:ss" ]
    target => "@timestamp"
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "logs-%{+YYYY.MM.dd}"
  }
}
```

**Splunk-style correlation** (manual approach):

```bash
# Create indexed timeline for fast searching
# Format: timestamp|source|event_type|details

awk '{
    # Extract key fields
    timestamp = $1 " " $2
    source = "auth"
    event_type = $5
    details = $0
    
    print timestamp "|" source "|" event_type "|" details
}' /var/log/auth.log > indexed_auth.log

# Similar processing for other logs...

# Merge and create searchable index
cat indexed_*.log | sort > master_index.log

# Query by multiple criteria
grep "192.168.1.100" master_index.log | grep "admin"
```

## User Activity Tracking

User activity tracking focuses on following individual user actions across systems and time to identify malicious behavior, privilege escalation, data exfiltration, or policy violations.

### Authentication Event Tracking

**SSH authentication analysis**:

```bash
# Successful logins
grep "Accepted" /var/log/auth.log | \
    awk '{print $1, $2, $3, $9, $11, "from", $13}' | \
    column -t

# Failed login attempts
grep "Failed password" /var/log/auth.log | \
    awk '{print $1, $2, $3, "User:", $9, "From:", $11}' | \
    column -t

# Login success rate by user
awk '
    /Failed password/ {failed[$9]++}
    /Accepted/ {success[$9]++}
    END {
        print "User\tSuccess\tFailed\tTotal\tSuccess Rate"
        for (user in success) {
            total = success[user] + failed[user]
            rate = (success[user] / total) * 100
            printf "%s\t%d\t%d\t%d\t%.2f%%\n", user, success[user], failed[user], total, rate
        }
    }
' /var/log/auth.log | column -t
```

**sudo usage tracking**:

```bash
# All sudo commands by user
grep "sudo:" /var/log/auth.log | \
    awk '{
        # Extract user and command
        for(i=1;i<=NF;i++) {
            if($i == "USER=") user=$(i+1);
            if($i == "COMMAND=") {
                cmd="";
                for(j=i+1;j<=NF;j++) cmd=cmd" "$j;
                print user, cmd;
            }
        }
    }' | sort | uniq -c | sort -rn

# Privilege escalation timeline
grep "sudo:" /var/log/auth.log | \
    grep -E "USER=root|sudo -i|su root"
```

**Web application authentication**:

```bash
# Extract login events from Apache logs
# Assuming login POST to /login endpoint

grep "POST /login" /var/log/apache2/access.log | \
    awk '{print $1, $4, $7, $9}' | \
    column -t

# Identify brute force attempts (multiple failures from same IP)
grep "POST /login" /var/log/apache2/access.log | \
    awk '$9 == 401 || $9 == 403 {print $1}' | \
    sort | uniq -c | sort -rn | \
    awk '$1 > 10 {print "Suspicious IP:", $2, "Failed attempts:", $1}'

# Track session cookies
grep "Set-Cookie:" /var/log/apache2/access.log | \
    grep -oP 'SESSIONID=[A-Za-z0-9]+' | \
    sort -u
```

### Command Execution Tracking

**Bash history analysis**:

```bash
# Analyze bash history for suspicious commands
HISTFILE=/mnt/evidence/home/user/.bash_history

# Commands with timestamps (if HISTTIMEFORMAT was set)
cat "$HISTFILE" | \
    awk '/^#[0-9]+/ {ts=$0; getline; print ts, $0}'

# Without timestamps
cat "$HISTFILE" | nl

# Suspicious command patterns
grep -E "wget|curl|nc|netcat|/dev/tcp|base64|eval|chmod \+x" "$HISTFILE"

# Privilege escalation attempts
grep -E "sudo|su |pkexec" "$HISTFILE"

# File system manipulation
grep -E "rm -rf|shred|dd if=/dev/zero" "$HISTFILE"

# Network reconnaissance
grep -E "nmap|masscan|nikto|ping|traceroute" "$HISTFILE"
```

**Process execution from auditd**:

```bash
# Install auditd: apt install auditd

# Search for command executions
ausearch -i -m EXECVE | \
    awk '/^type=EXECVE/ {getline; print}' | \
    grep -v "a0=" | \
    sort -u

# Track specific user's commands
ausearch -i -m EXECVE -ui 1000 | \
    grep "exe=" | \
    sed 's/.*exe="\([^"]*\)".*/\1/' | \
    sort | uniq -c | sort -rn

# Find privilege escalations
ausearch -i -m USER_ROLE_CHANGE,USER_AUTH -sv yes
```

**Syslog process tracking**:

```bash
# Track user process initiation
grep "systemd\[.*\]: Started" /var/log/syslog | \
    grep -v "session-.*scope"

# Cron job executions
grep "CRON" /var/log/syslog | \
    awk '{print $1, $2, $3, $6, $7, $8, $9, $10}'
```

### File Access Tracking

**Tracking file modifications**:

```bash
# Files modified in specific time window
find /mnt/evidence -type f -newermt "2025-10-29 00:00:00" ! -newermt "2025-10-29 23:59:59" \
    -ls | \
    sort -k9

# Recently accessed files (if atime is enabled)
find /mnt/evidence -type f -atime -1 -ls

# Create timeline of file system changes
find /mnt/evidence -type f -printf "%T+ %p\n" | sort > fs_timeline.txt
```

**auditd file access logs**:

```bash
# Watch who accessed specific files
ausearch -f /etc/passwd -i

# Files opened by specific user
ausearch -ui 1000 -m OPEN,OPENAT -i | \
    grep "name=" | \
    sed 's/.*name="\([^"]*\)".*/\1/' | \
    sort | uniq -c | sort -rn

# Track writes to sensitive directories
ausearch -i -m PATH -k sensitive_write | \
    grep "nametype=CREATE\|nametype=DELETE"
```

### Network Activity by User

**Correlating network connections with user sessions**:

```bash
# Netstat output with PIDs
# netstat -tulpn or ss -tulpn

# Find processes and their owners
ss -tulpn | awk 'NR>1 {
    split($7, a, ",");
    split(a[2], b, "=");
    pid = b[2];
    cmd = "ps -o user= -p " pid;
    cmd | getline user;
    close(cmd);
    print user, $1, $5, $6, $7;
}' | column -t

# Track DNS queries by user (from tcpdump or DNS logs)
tcpdump -r capture.pcap -n 'udp port 53' 2>/dev/null | \
    awk '{print $3, $8}' | \
    sort | uniq -c
```

**Web browsing history reconstruction**:

```bash
# From web proxy logs (squid example)
awk '{print $3, $7}' /var/log/squid/access.log | \
    sort | uniq -c | sort -rn

# Browser history from user profile
sqlite3 /mnt/evidence/home/user/.mozilla/firefox/*.default/places.sqlite \
    "SELECT datetime(visit_date/1000000,'unixepoch','localtime'), url 
     FROM moz_places, moz_historyvisits 
     WHERE moz_places.id = moz_historyvisits.place_id 
     ORDER BY visit_date DESC LIMIT 100;"
```

### Data Access and Exfiltration Tracking

**Database query tracking**:

```bash
# MySQL general query log
# Enable: SET GLOBAL general_log = 'ON';

# Parse for SELECT statements by user
awk '
    /Query.*SELECT/ {
        query = $0;
        getline;
        if ($0 ~ /FROM/) {
            table = $2;
            print user, table, query;
        }
    }
    /Connect/ {
        user = $NF;
    }
' /var/log/mysql/general.log

# Large data exports (size-based detection)
grep "SELECT.*FROM" /var/log/mysql/general.log | \
    wc -l  # High count may indicate data scraping
```

**File download/upload tracking**:

```bash
# Large downloads from web server
awk '$10 > 1000000 {print $1, $4, $7, $10}' /var/log/apache2/access.log | \
    column -t

# File uploads (POST requests with Content-Length)
grep "POST" /var/log/apache2/access.log | \
    awk '$10 > 100000 {print $1, $4, $7, $10}' | \
    column -t

# FTP transfers
grep "STOR\|RETR" /var/log/vsftpd.log
```

### User Activity Timeline Generation

**Comprehensive user timeline**:

```python
#!/usr/bin/env python3
import re
import sys
from datetime import datetime
from collections import defaultdict

# Structure: {username: [(timestamp, event_type, details)]}
user_timeline = defaultdict(list)

# Define event extractors
def extract_auth_events(line):
    """Extract authentication events"""
    # SSH login
    match = re.search(r'(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}).*Accepted.*for (\w+)', line)
    if match:
        return match.group(2), match.group(1), "SSH_LOGIN", line.strip()
    
    # Sudo usage
    match = re.search(r'(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}).*sudo.*USER=(\w+).*COMMAND=(.*)', line)
    if match:
        return match.group(2), match.group(1), "SUDO_CMD", match.group(3)
    
    return None


def extract_web_events(line):
    """Extract web access events"""
    # Apache combined log format
    match = re.search(r'(\d+\.\d+\.\d+\.\d+).* \[(.*?)\].*"(GET|POST) (.*?) HTTP', line)
    if match:
        user = match.group(1)  # Default to IP
        timestamp = match.group(2)
        method = match.group(3)
        uri = match.group(4)
        return user, timestamp, f"WEB_{method}", uri
    
    return None


def extract_file_events(line):
    """Extract file access events from audit logs"""
    match = re.search(r'type=OPEN.*time=(\d+\.\d+).*uid=(\d+).*name="(.*?)"', line)
    if match:
        timestamp = datetime.fromtimestamp(float(match.group(1))).isoformat()
        uid = match.group(2)
        filename = match.group(3)
        return uid, timestamp, "FILE_ACCESS", filename
    
    return None


# Process logs
for line in sys.stdin:
    # Try each extractor
    result = extract_auth_events(line) or extract_web_events(line) or extract_file_events(line)
    if result:
        user, timestamp, event_type, details = result
        user_timeline[user].append((timestamp, event_type, details))


# Generate timeline report
print("USER ACTIVITY TIMELINE REPORT")
print("=" * 80)

for user in sorted(user_timeline.keys()):
    events = sorted(user_timeline[user], key=lambda x: x[0])

    print(f"\n{'=' * 80}")
    print(f"User: {user}")
    print(f"Total Events: {len(events)}")

    if events:
        print(f"First Activity: {events[0][0]}")
        print(f"Last Activity: {events[-1][0]}")

        # Event type breakdown
        event_counts = defaultdict(int)
        for _, event_type, _ in events:
            event_counts[event_type] += 1

        print("\nEvent Breakdown:")
        for event_type, count in sorted(event_counts.items(), key=lambda x: x[1], reverse=True):
            print(f"  {event_type}: {count}")

        print("\nTimeline:")
        for timestamp, event_type, details in events:
            print(f"  [{timestamp}] {event_type}: {details[:80]}")

print(f"{'=' * 80}")

````

Usage:
```bash
cat /var/log/{auth.log,apache2/access.log,audit/audit.log} | \
    python3 user_timeline.py > user_activity_report.txt
````

### Behavioral Analysis and Anomaly Detection

**Baseline vs anomalous behavior**:

```bash
# Establish baseline: typical commands per user
awk '
    /USER=/ {
        match($0, /USER=([^ ]+)/, user);
        match($0, /COMMAND=(.*)/, cmd);
        usercommands[user[1]][cmd[1]]++;
    }
    END {
        for (u in usercommands) {
            print "User:", u;
            for (c in usercommands[u]) {
                print "  ", c, ":", usercommands[u][c];
            }
        }
    }
' /var/log/auth.log > baseline.txt

# Detect deviation: commands run only once (unusual)
awk '
    /COMMAND=/ {
        match($0, /USER=([^ ]+)/, user);
        match($0, /COMMAND=(.*)/, cmd);
        usercommands[user[1]][cmd[1]]++;
    }
    END {
        print "ANOMALOUS COMMANDS (executed only once):";
        for (u in usercommands) {
            for (c in usercommands[u]) {
                if (usercommands[u][c] == 1) {
                    print u, ":", c;
                }
            }
        }
    }
' /var/log/auth.log
```

**Time-based anomalies**:

```python
#!/usr/bin/env python3
import re
import sys
from datetime import datetime
from collections import defaultdict

# Track user activity by hour
user_activity_by_hour = defaultdict(lambda: defaultdict(int))

for line in sys.stdin:
    # Extract username
    user_match = re.search(r'(?:user[=:]|for |USER=)(\w+)', line, re.IGNORECASE)
    if not user_match:
        continue
    
    user = user_match.group(1)
    
    # Extract timestamp and hour
    ts_match = re.search(r'(\d{4}-\d{2}-\d{2}T\d{2}):', line)
    if ts_match:
        hour = datetime.fromisoformat(ts_match.group(1)).hour
        user_activity_by_hour[user][hour] += 1

# Analyze patterns
print("USER ACTIVITY PATTERNS BY HOUR")
print("=" * 80)

for user in sorted(user_activity_by_hour.keys()):
    hourly_counts = user_activity_by_hour[user]
    total = sum(hourly_counts.values())
    avg = total / 24
    
    print(f"\nUser: {user}")
    print(f"Total events: {total}")
    print(f"Average per hour: {avg:.2f}")
    
    # Identify anomalous hours (3x average)
    anomalies = []
    for hour, count in hourly_counts.items():
        if count > avg * 3:
            anomalies.append((hour, count))
    
    if anomalies:
        print("Anomalous activity hours:")
        for hour, count in sorted(anomalies):
            print(f"  {hour:02d}:00 - {count} events ({count/avg:.1f}x average)")
    
    # Activity heatmap
    print("Hourly distribution:")
    for hour in range(24):
        count = hourly_counts.get(hour, 0)
        bar = "#" * int(count / avg) if avg > 0 else ""
        print(f"  {hour:02d}:00 [{count:4d}] {bar}")
```

Usage:

```bash
cat /var/log/*.log | python3 time_anomaly_detection.py
```

## Session Reconstruction

Session reconstruction involves piecing together all activities within a logical session to understand the complete sequence of actions, especially useful for analyzing attack chains and understanding attacker methodology.

### HTTP Session Reconstruction

**Extracting and reconstructing web sessions**:

```bash
# Extract all requests for a specific session ID
SESSION_ID="abc123def456"

# Get all requests in chronological order
grep "$SESSION_ID" /var/log/apache2/access.log | \
    sort -k4 | \
    awk '{print $4, $5, $6, $7, $9}' | \
    column -t

# Create session narrative
grep "$SESSION_ID" /var/log/apache2/access.log | \
    awk '{
        print "Time:", $4, $5;
        print "Request:", $6, $7;
        print "Status:", $9;
        print "Size:", $10;
        print "Referrer:", $11;
        print "User-Agent:", $12, $13, $14;
        print "---";
    }'
```

**Multi-user session correlation**:

```python
#!/usr/bin/env python3
import re
import sys
from collections import defaultdict

# Structure: {session_id: [(timestamp, ip, method, uri, status, size)]}
sessions = defaultdict(list)

# Apache combined log pattern
LOG_PATTERN = r'(\S+) \S+ \S+ \[(.*?)\] "(\S+) (\S+) \S+" (\d+) (\S+).*?SESSIONID=(\S+)'

for line in sys.stdin:
    match = re.search(LOG_PATTERN, line)
    if match:
        ip, timestamp, method, uri, status, size, session_id = match.groups()
        sessions[session_id].append({
            'timestamp': timestamp,
            'ip': ip,
            'method': method,
            'uri': uri,
            'status': status,
            'size': size
        })

# Generate session reconstruction report
print("HTTP SESSION RECONSTRUCTION")
print("=" * 80)

for session_id in sorted(sessions.keys()):
    session_data = sorted(sessions[session_id], key=lambda x: x['timestamp'])
    
    print(f"\n{'='*80}")
    print(f"Session ID: {session_id}")
    print(f"Request Count: {len(session_data)}")
    print(f"IP Address: {session_data[0]['ip']}")
    print(f"Start Time: {session_data[0]['timestamp']}")
    print(f"End Time: {session_data[-1]['timestamp']}")
    
    # Calculate session metrics
    total_bytes = sum(int(d['size']) for d in session_data if d['size'] != '-')
    errors = sum(1 for d in session_data if int(d['status']) >= 400)
    
    print(f"Total Bytes Transferred: {total_bytes}")
    print(f"Error Count: {errors}")
    
    # Request flow
    print("\nRequest Flow:")
    for i, req in enumerate(session_data, 1):
        print(f"  {i}. [{req['timestamp']}] {req['method']} {req['uri']} -> {req['status']}")
    
    # Identify suspicious patterns
    suspicious = []
    
    # Path traversal attempts
    if any('../' in req['uri'] or '..\\' in req['uri'] for req in session_data):
        suspicious.append("Path traversal attempts detected")
    
    # SQL injection patterns
    sql_patterns = ['union', 'select', 'or 1=1', 'drop table']
    if any(any(p in req['uri'].lower() for p in sql_patterns) for req in session_data):
        suspicious.append("Possible SQL injection detected")
    
    # High error rate
    if errors > len(session_data) * 0.3:
        suspicious.append(f"High error rate: {errors}/{len(session_data)}")
    
    # Rapid requests (possible automation)
    if len(session_data) > 50:
        suspicious.append(f"High request volume: {len(session_data)} requests")
    
    if suspicious:
        print("\nSUSPICIOUS INDICATORS:")
        for indicator in suspicious:
            print(f"   {indicator}")
    
    print(f"{'='*80}")
```

Usage:

```bash
python3 session_reconstruction.py < /var/log/apache2/access.log > session_report.txt
```

### SSH Session Reconstruction

**Reconstructing SSH sessions**:

```bash
# Track complete SSH session lifecycle
grep "sshd" /var/log/auth.log | \
    awk '
    /session opened/ {
        match($0, /session opened for user ([^ ]+)/, user);
        match($0, /sshd\[([0-9]+)\]/, pid);
        sessions[pid[1]] = user[1];
        print $1, $2, $3, "SESSION_START", "PID:", pid[1], "User:", user[1];
    }
    /Accepted/ {
        match($0, /Accepted .* for ([^ ]+) from ([^ ]+)/, info);
        match($0, /sshd\[([0-9]+)\]/, pid);
        print $1, $2, $3, "AUTH_SUCCESS", "PID:", pid[1], "User:", info[1], "From:", info[2];
    }
    /session closed/ {
        match($0, /sshd\[([0-9]+)\]/, pid);
        if (sessions[pid[1]]) {
            print $1, $2, $3, "SESSION_END", "PID:", pid[1], "User:", sessions[pid[1]];
            delete sessions[pid[1]];
        }
    }
    ' | column -t
```

**Command history within SSH session**:

```bash
# Reconstruct commands executed during specific SSH session
# Requires auditd logging

# Find session PID from auth.log
SESSION_PID=$(grep "session opened for user" /var/log/auth.log | \
    grep "Oct 29 14:23" | \
    grep -oP 'sshd\[\K[0-9]+')

echo "Reconstructing session PID: $SESSION_PID"

# Find all child processes and their commands
ausearch -p $SESSION_PID -i | \
    grep -E "EXECVE|CWD" | \
    awk '/type=EXECVE/ {
        getline; 
        if ($0 ~ /argc=/) {
            cmd = $0;
            getline;
            while ($0 ~ /^a[0-9]+=/) {
                cmd = cmd " " $0;
                getline;
            }
            print cmd;
        }
    }'
```

**PTY session reconstruction** [Inference]:

```python
#!/usr/bin/env python3
import re
import sys
from collections import defaultdict

# Track SSH sessions by PID
ssh_sessions = defaultdict(dict)

for line in sys.stdin:
    # Session start
    match = re.search(r'(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}).*sshd\[(\d+)\].*session opened for user (\w+)', line)
    if match:
        timestamp, pid, user = match.groups()
        ssh_sessions[pid]['user'] = user
        ssh_sessions[pid]['start'] = timestamp
        ssh_sessions[pid]['events'] = []
        continue
    
    # Commands executed
    match = re.search(r'(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}).*ppid=(\d+).*COMMAND=(.*)', line)
    if match:
        timestamp, ppid, command = match.groups()
        if ppid in ssh_sessions:
            ssh_sessions[ppid]['events'].append((timestamp, 'COMMAND', command))
        continue
    
    # Session end
    match = re.search(r'(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}).*sshd\[(\d+)\].*session closed', line)
    if match:
        timestamp, pid = match.groups()
        if pid in ssh_sessions:
            ssh_sessions[pid]['end'] = timestamp

# Generate report
print("SSH SESSION RECONSTRUCTION")
print("=" * 80)

for pid in sorted(ssh_sessions.keys(), key=lambda x: ssh_sessions[x].get('start', '')):
    session = ssh_sessions[pid]
    
    print(f"\n{'='*80}")
    print(f"Session PID: {pid}")
    print(f"User: {session.get('user', 'UNKNOWN')}")
    print(f"Start: {session.get('start', 'UNKNOWN')}")
    print(f"End: {session.get('end', 'Still active')}")
    
    if 'start' in session and 'end' in session:
        # Calculate duration
        from datetime import datetime
        start_dt = datetime.fromisoformat(session['start'])
        end_dt = datetime.fromisoformat(session['end'])
        duration = end_dt - start_dt
        print(f"Duration: {duration}")
    
    print(f"\nCommands Executed ({len(session.get('events', []))}):")
    for timestamp, event_type, details in session.get('events', []):
        print(f"  [{timestamp}] {details}")
    
    print(f"{'='*80}")
```

Usage:

```bash
cat /var/log/{auth.log,audit/audit.log} | python3 ssh_session_recon.py
```

### Database Transaction Session Reconstruction

**MySQL session reconstruction**:

```bash
# Extract sessions from MySQL general log
awk '
    /Connect/ {
        match($0, /Connect.*[^ ]+ on/, conn);
        match($0, /(\d+)/, id);
        sessions[id[1]]["user"] = conn[1];
        sessions[id[1]]["start"] = $1 " " $2;
    }
    /Query/ {
        match($0, /(\d+) Query/, id);
        if (id[1] in sessions) {
            sessions[id[1]]["queries"]++;
            print $1, $2, "SESSION", id[1], "USER", sessions[id[1]]["user"], "QUERY:", $4, $5, $6;
        }
    }
    /Quit/ {
        match($0, /(\d+)/, id);
        if (id[1] in sessions) {
            sessions[id[1]]["end"] = $1 " " $2;
            print "SESSION_END", id[1], "USER", sessions[id[1]]["user"], "QUERIES", sessions[id[1]]["queries"];
            delete sessions[id[1]];
        }
    }
' /var/log/mysql/general.log
```

**Transaction flow analysis**:

```python
#!/usr/bin/env python3
import re
import sys
from collections import defaultdict

# Track database sessions
db_sessions = defaultdict(lambda: {'queries': [], 'transactions': []})

for line in sys.stdin:
    # Connection
    match = re.search(r'(\d+) Connect\s+(\S+)@(\S+)', line)
    if match:
        conn_id, user, host = match.groups()
        db_sessions[conn_id]['user'] = user
        db_sessions[conn_id]['host'] = host
        continue
    
    # Query
    match = re.search(r'(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}).*(\d+) Query\s+(.*)', line)
    if match:
        timestamp, conn_id, query = match.groups()
        
        # Detect transaction boundaries
        if 'BEGIN' in query.upper() or 'START TRANSACTION' in query.upper():
            db_sessions[conn_id]['transactions'].append({
                'start': timestamp,
                'queries': []
            })
        
        db_sessions[conn_id]['queries'].append((timestamp, query.strip()))
        
        # Add to current transaction if one is open
        if db_sessions[conn_id]['transactions']:
            db_sessions[conn_id]['transactions'][-1]['queries'].append((timestamp, query.strip()))
        
        if 'COMMIT' in query.upper() or 'ROLLBACK' in query.upper():
            if db_sessions[conn_id]['transactions']:
                db_sessions[conn_id]['transactions'][-1]['end'] = timestamp
                db_sessions[conn_id]['transactions'][-1]['status'] = 'COMMIT' if 'COMMIT' in query.upper() else 'ROLLBACK'

# Generate report
print("DATABASE SESSION RECONSTRUCTION")
print("=" * 80)

for conn_id in sorted(db_sessions.keys()):
    session = db_sessions[conn_id]
    
    print(f"\n{'='*80}")
    print(f"Connection ID: {conn_id}")
    print(f"User: {session.get('user', 'UNKNOWN')}")
    print(f"Host: {session.get('host', 'UNKNOWN')}")
    print(f"Total Queries: {len(session['queries'])}")
    print(f"Transactions: {len(session['transactions'])}")
    
    # Show transactions
    for i, txn in enumerate(session['transactions'], 1):
        print(f"\n  Transaction {i}:")
        print(f"    Start: {txn.get('start', 'UNKNOWN')}")
        print(f"    End: {txn.get('end', 'NOT COMPLETED')}")
        print(f"    Status: {txn.get('status', 'IN PROGRESS')}")
        print(f"    Query Count: {len(txn['queries'])}")
        
        # Show first few queries
        print("    Queries:")
        for ts, query in txn['queries'][:5]:
            print(f"      [{ts}] {query[:80]}")
        
        if len(txn['queries']) > 5:
            print(f"      ... and {len(txn['queries']) - 5} more queries")
    
    print(f"{'='*80}")
```

Usage:

```bash
python3 db_session_recon.py < /var/log/mysql/general.log > db_session_report.txt
```

### Application Session Reconstruction

**Reconstructing user workflow from application logs**:

```python
#!/usr/bin/env python3
import re
import sys
import json
from collections import defaultdict
from datetime import datetime

# Assuming JSON application logs
sessions = defaultdict(lambda: {
    'user': None,
    'start': None,
    'end': None,
    'actions': [],
    'errors': []
})

for line in sys.stdin:
    try:
        log_entry = json.loads(line.strip())
    except json.JSONDecodeError:
        continue
    
    session_id = log_entry.get('session_id')
    if not session_id:
        continue
    
    timestamp = log_entry.get('timestamp')
    action = log_entry.get('action')
    level = log_entry.get('level', 'INFO')
    
    # Initialize session
    if not sessions[session_id]['user']:
        sessions[session_id]['user'] = log_entry.get('user', 'UNKNOWN')
        sessions[session_id]['start'] = timestamp
    
    # Update end time
    sessions[session_id]['end'] = timestamp
    
    # Record action
    if action:
        sessions[session_id]['actions'].append({
            'timestamp': timestamp,
            'action': action,
            'details': log_entry.get('message', '')
        })
    
    # Track errors
    if level in ['ERROR', 'CRITICAL']:
        sessions[session_id]['errors'].append({
            'timestamp': timestamp,
            'message': log_entry.get('message', ''),
            'details': log_entry
        })

# Generate workflow report
print("APPLICATION SESSION WORKFLOW RECONSTRUCTION")
print("=" * 80)

for session_id in sorted(sessions.keys(), key=lambda x: sessions[x]['start'] or ''):
    session = sessions[session_id]
    
    print(f"\n{'='*80}")
    print(f"Session ID: {session_id}")
    print(f"User: {session['user']}")
    print(f"Start: {session['start']}")
    print(f"End: {session['end']}")
    
    # Calculate duration
    if session['start'] and session['end']:
        try:
            start_dt = datetime.fromisoformat(session['start'].replace('Z', '+00:00'))
            end_dt = datetime.fromisoformat(session['end'].replace('Z', '+00:00'))
            duration = end_dt - start_dt
            print(f"Duration: {duration}")
        except:
            pass
    
    print(f"Actions: {len(session['actions'])}")
    print(f"Errors: {len(session['errors'])}")
    
    # Show workflow
    print("\nWorkflow:")
    for i, action_data in enumerate(session['actions'], 1):
        print(f"  {i}. [{action_data['timestamp']}] {action_data['action']}")
        if action_data['details']:
            print(f"     {action_data['details'][:100]}")
    
    # Show errors if any
    if session['errors']:
        print("\nErrors Encountered:")
        for error in session['errors']:
            print(f"  [{error['timestamp']}] {error['message']}")
    
    # Identify patterns
    action_sequence = [a['action'] for a in session['actions']]
    
    # Check for suspicious patterns
    suspicious = []
    if action_sequence.count('login_attempt') > 5:
        suspicious.append("Multiple login attempts")
    
    if 'admin_access' in action_sequence and session['user'] != 'admin':
        suspicious.append("Non-admin user accessed admin functions")
    
    if len(session['errors']) > len(session['actions']) * 0.5:
        suspicious.append("High error rate")
    
    if suspicious:
        print("\nSUSPICIOUS PATTERNS:")
        for pattern in suspicious:
            print(f"   {pattern}")
    
    print(f"{'='*80}")
```

Usage:

```bash
cat /var/log/app/application.json | python3 app_session_recon.py > app_workflow_report.txt
```

### Network Session Reconstruction from PCAP

**TCP stream reconstruction**:

```bash
# Extract all TCP streams
tshark -r capture.pcap -q -z conv,tcp | tail -n +6

# Reconstruct specific TCP stream
tshark -r capture.pcap -q -z follow,tcp,ascii,0 > stream_0.txt

# Extract all HTTP sessions
tshark -r capture.pcap -Y "http.request or http.response" \
    -T fields -e frame.number -e ip.src -e ip.dst -e http.request.method \
    -e http.request.uri -e http.response.code | \
    column -t

# Session-based extraction
for stream in $(tshark -r capture.pcap -q -z conv,tcp | \
    tail -n +6 | awk '{print $1}' | cut -d'<' -f1); do
    echo "=== Stream $stream ==="
    tshark -r capture.pcap -q -z follow,tcp,ascii,$stream | head -50
done
```

**Reconstructing complete attack chain**:

```python
#!/usr/bin/env python3
import sys
import re
from collections import defaultdict

# Multi-source correlation for attack reconstruction
attack_chain = defaultdict(lambda: {
    'reconnaissance': [],
    'initial_access': [],
    'execution': [],
    'persistence': [],
    'privilege_escalation': [],
    'defense_evasion': [],
    'credential_access': [],
    'discovery': [],
    'lateral_movement': [],
    'collection': [],
    'exfiltration': []
})

# Patterns for MITRE ATT&CK tactics
patterns = {
    'reconnaissance': [r'nmap', r'masscan', r'nikto', r'dirb', r'gobuster'],
    'initial_access': [r'exploit', r'shell\.php', r'webshell', r'sql injection'],
    'execution': [r'bash -c', r'python -c', r'eval\(', r'exec\('],
    'persistence': [r'crontab', r'systemd.*service', r'\.bashrc', r'authorized_keys'],
    'privilege_escalation': [r'sudo', r'su root', r'pkexec', r'CVE-\d{4}-\d+'],
    'defense_evasion': [r'rm.*\.log', r'history -c', r'shred', r'base64'],
    'credential_access': [r'/etc/passwd', r'/etc/shadow', r'mimikatz', r'password'],
    'discovery': [r'whoami', r'ifconfig', r'netstat', r'ps aux', r'find /'],
    'lateral_movement': [r'ssh.*@', r'scp', r'psexec', r'wmic'],
    'collection': [r'tar.*\.tar', r'zip', r'cat.*>', r'grep -r'],
    'exfiltration': [r'curl.*http', r'wget.*http', r'nc.*-e', r'scp.*@']
}

source_ip = None
current_timestamp = None

for line in sys.stdin:
    # Extract timestamp
    ts_match = re.search(r'\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}', line)
    if ts_match:
        current_timestamp = ts_match.group()
    
    # Extract IP address
    ip_match = re.search(r'(?:\d{1,3}\.){3}\d{1,3}', line)
    if ip_match and not source_ip:
        source_ip = ip_match.group()
    
    # Classify by tactic
    for tactic, pattern_list in patterns.items():
        for pattern in pattern_list:
            if re.search(pattern, line, re.IGNORECASE):
                attack_chain[source_ip or 'UNKNOWN'][tactic].append({
                    'timestamp': current_timestamp or 'UNKNOWN',
                    'evidence': line.strip()
                })

# Generate attack chain report
print("ATTACK CHAIN RECONSTRUCTION")
print("=" * 80)
print("MITRE ATT&CK Tactic Mapping\n")

for attacker in sorted(attack_chain.keys()):
    tactics = attack_chain[attacker]
    
    # Count events per tactic
    tactic_counts = {t: len(events) for t, events in tactics.items() if events}
    
    if not tactic_counts:
        continue
    
    print(f"{'='*80}")
    print(f"Attacker: {attacker}")
    print(f"Tactics Observed: {len(tactic_counts)}")
    print(f"{'='*80}\n")
    
    # Display in kill chain order
    for tactic in ['reconnaissance', 'initial_access', 'execution', 'persistence',
                   'privilege_escalation', 'defense_evasion', 'credential_access',
                   'discovery', 'lateral_movement', 'collection', 'exfiltration']:
        
        if tactic in tactic_counts:
            print(f"\n[{tactic.upper().replace('_', ' ')}] - {tactic_counts[tactic]} events")
            print("-" * 80)
            
            for event in tactics[tactic][:5]:  # Show first 5 events
                print(f"  [{event['timestamp']}]")
                print(f"  {event['evidence'][:150]}")
                print()
            
            if len(tactics[tactic]) > 5:
                print(f"  ... and {len(tactics[tactic]) - 5} more events\n")
    
    print(f"{'='*80}\n")
```

Usage:

```bash
cat /var/log/{auth.log,apache2/access.log,syslog} | \
    python3 attack_chain_recon.py > attack_chain_report.txt
```

## Important Related Topics

For comprehensive correlation analysis, consider studying:

- **SIEM Deployment**: Setting up Elastic Stack (ELK), Splunk, or Graylog for centralized log management
- **Timeline Analysis with Plaso**: Creating super-timelines combining file system metadata, logs, and other artifacts
- **Threat Hunting Techniques**: Proactive searching for IOCs and TTPs across correlated log data
- **Behavioral Analytics**: Machine learning approaches to detect anomalies in user and system behavior
- **Graph-Based Analysis**: Visualizing relationships between entities (users, IPs, files) for pattern recognition
- **MITRE ATT&CK Mapping**: Correlating observed activities to known adversary tactics and techniques

---

## Attack Chain Mapping

Attack chain mapping reconstructs the sequence of attacker actions from initial access through objectives, revealing the full scope of compromise.

### Kill Chain phase identification

The Cyber Kill Chain consists of: Reconnaissance, Weaponization, Delivery, Exploitation, Installation, Command & Control (C2), and Actions on Objectives.

**Automated kill chain classifier:**

```python
import re
from datetime import datetime
from collections import defaultdict

def classify_kill_chain_phase(log_entry):
    """
    Classify log entries into kill chain phases based on indicators
    """
    patterns = {
        'reconnaissance': [
            r'nikto', r'nmap', r'dirbuster', r'gobuster', r'wpscan',
            r'/robots\.txt', r'/\.git/', r'/\.env', r'OPTIONS HTTP',
            r'HEAD HTTP', r'\.\./\.\.'
        ],
        'delivery': [
            r'\.exe', r'\.sh', r'\.bat', r'\.ps1', r'\.php\?',
            r'upload', r'POST.*multipart', r'file_put_contents'
        ],
        'exploitation': [
            r'eval\(', r'system\(', r'exec\(', r'shell_exec',
            r'passthru', r'\.\./', r'union.*select', r'1=1',
            r'sleep\(', r'benchmark\(', r'<script', r'cmd\.exe'
        ],
        'installation': [
            r'wget', r'curl.*-o', r'certutil.*download',
            r'powershell.*download', r'bitsadmin', r'\.cron',
            r'schtasks', r'rc\.local', r'startup'
        ],
        'command_control': [
            r':\d{4,5}', r'nc.*-e', r'reverse.*shell',
            r'meterpreter', r'beacon', r'POST.*\/update',
            r'POST.*\/config', r'base64.*POST'
        ],
        'actions_objectives': [
            r'\/etc\/passwd', r'\/etc\/shadow', r'sam\.dat',
            r'\.sql.*dump', r'exfiltrate', r'compress', r'\.zip',
            r'password', r'credit.*card', r'ssn'
        ]
    }
    
    for phase, indicators in patterns.items():
        for pattern in indicators:
            if re.search(pattern, log_entry, re.IGNORECASE):
                return phase
    
    return 'unknown'

def map_attack_chain(log_file):
    """
    Build chronological attack chain from logs
    """
    events = []
    
    with open(log_file, 'r', errors='ignore') as f:
        for line in f:
            # Parse timestamp (adjust for your log format)
            ts_match = re.search(r'\[(\d{2}/\w{3}/\d{4}:\d{2}:\d{2}:\d{2})', line)
            if ts_match:
                try:
                    timestamp = datetime.strptime(ts_match.group(1), '%d/%b/%Y:%H:%M:%S')
                    phase = classify_kill_chain_phase(line)
                    
                    # Extract IP address
                    ip_match = re.search(r'(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})', line)
                    source_ip = ip_match.group(1) if ip_match else 'unknown'
                    
                    events.append({
                        'timestamp': timestamp,
                        'phase': phase,
                        'source_ip': source_ip,
                        'log_entry': line.strip()
                    })
                except:
                    pass
    
    # Sort by timestamp
    events.sort(key=lambda x: x['timestamp'])
    
    # Group by source IP and phase
    chains = defaultdict(lambda: defaultdict(list))
    for event in events:
        chains[event['source_ip']][event['phase']].append(event)
    
    # Display attack chains
    print("=== ATTACK CHAIN MAPPING ===\n")
    for ip, phases in chains.items():
        if ip == 'unknown' or len(phases) <= 1:
            continue
            
        print(f"Source IP: {ip}")
        print(f"Phases detected: {', '.join(phases.keys())}")
        print(f"Timeline:")
        
        for phase in ['reconnaissance', 'delivery', 'exploitation', 
                      'installation', 'command_control', 'actions_objectives']:
            if phase in phases:
                print(f"\n  [{phase.upper()}]")
                for event in phases[phase][:3]:  # Show first 3 events per phase
                    print(f"    {event['timestamp']} - {event['log_entry'][:100]}")
        print("\n" + "="*70 + "\n")

map_attack_chain('access.log')
```

### MITRE ATT&CK mapping

Map observed behaviors to MITRE ATT&CK techniques.

**ATT&CK technique detector:**

```python
import json
import re

def map_to_attack(log_file):
    """
    Map log entries to MITRE ATT&CK techniques
    """
    # Subset of ATT&CK mappings (expand as needed)
    attack_patterns = {
        'T1190': {
            'name': 'Exploit Public-Facing Application',
            'patterns': [r'union.*select', r'\.\./', r'<script', r'eval\(']
        },
        'T1059': {
            'name': 'Command and Scripting Interpreter',
            'patterns': [r'cmd\.exe', r'/bin/bash', r'powershell', r'system\(']
        },
        'T1071': {
            'name': 'Application Layer Protocol',
            'patterns': [r'POST.*\/[a-z]{1,5}$', r'User-Agent: [A-Za-z]{3,10}$']
        },
        'T1083': {
            'name': 'File and Directory Discovery',
            'patterns': [r'ls\s', r'dir\s', r'find\s', r'/etc/passwd']
        },
        'T1003': {
            'name': 'OS Credential Dumping',
            'patterns': [r'mimikatz', r'/etc/shadow', r'sam\.dat', r'lsass']
        },
        'T1048': {
            'name': 'Exfiltration Over Alternative Protocol',
            'patterns': [r'ftp.*STOR', r'scp.*\.zip', r'rsync']
        },
        'T1078': {
            'name': 'Valid Accounts',
            'patterns': [r'Accepted password', r'session opened', r'su:.*success']
        },
        'T1110': {
            'name': 'Brute Force',
            'patterns': [r'Failed password.*([0-9]+)', r'authentication failure.*rhost']
        }
    }
    
    detections = defaultdict(list)
    
    with open(log_file, 'r', errors='ignore') as f:
        for line_num, line in enumerate(f, 1):
            for technique_id, info in attack_patterns.items():
                for pattern in info['patterns']:
                    if re.search(pattern, line, re.IGNORECASE):
                        detections[technique_id].append({
                            'line': line_num,
                            'technique': info['name'],
                            'log': line.strip()[:150]
                        })
                        break
    
    print("=== MITRE ATT&CK TECHNIQUE MAPPING ===\n")
    for technique_id in sorted(detections.keys()):
        events = detections[technique_id]
        print(f"[{technique_id}] {events[0]['technique']}")
        print(f"  Occurrences: {len(events)}")
        print(f"  Sample events:")
        for event in events[:2]:
            print(f"    Line {event['line']}: {event['log']}")
        print()

map_to_attack('access.log')
```

### Timeline reconstruction

**Visual timeline generator:**

```bash
# Generate attack timeline with gnuplot
python3 << 'EOF'
import re
from datetime import datetime
from collections import Counter

def generate_timeline(log_file):
    events = []
    
    with open(log_file, 'r', errors='ignore') as f:
        for line in f:
            ts_match = re.search(r'\[(\d{2}/\w{3}/\d{4}:\d{2}:\d{2}:\d{2})', line)
            if ts_match:
                try:
                    ts = datetime.strptime(ts_match.group(1), '%d/%b/%Y:%H:%M:%S')
                    
                    # Classify event type
                    if re.search(r'GET|POST|PUT', line):
                        event_type = 'HTTP Request'
                    elif re.search(r'Failed password|authentication failure', line):
                        event_type = 'Auth Failure'
                    elif re.search(r'Accepted password|session opened', line):
                        event_type = 'Auth Success'
                    elif re.search(r'union.*select|\.\./', line, re.IGNORECASE):
                        event_type = 'Attack Attempt'
                    else:
                        event_type = 'Other'
                    
                    events.append((ts, event_type))
                except:
                    pass
    
    # Generate timeline data
    events.sort()
    
    # Write data for plotting
    with open('/tmp/timeline.dat', 'w') as f:
        event_types = list(set([e[1] for e in events]))
        for ts, etype in events:
            type_idx = event_types.index(etype)
            f.write(f"{ts.timestamp()} {type_idx} # {etype}\n")
    
    print("Timeline data written to /tmp/timeline.dat")
    print("\nEvent distribution:")
    counter = Counter([e[1] for e in events])
    for etype, count in counter.most_common():
        print(f"  {etype}: {count}")

generate_timeline('access.log')
EOF
```

**Text-based timeline:**

```python
from datetime import datetime
import re

def text_timeline(log_file, time_window_minutes=5):
    """
    Generate ASCII timeline grouped by time windows
    """
    events = []
    
    with open(log_file, 'r', errors='ignore') as f:
        for line in f:
            ts_match = re.search(r'\[(\d{2}/\w{3}/\d{4}:\d{2}:\d{2}:\d{2})', line)
            if ts_match:
                try:
                    ts = datetime.strptime(ts_match.group(1), '%d/%b/%Y:%H:%M:%S')
                    events.append((ts, line.strip()[:100]))
                except:
                    pass
    
    events.sort()
    
    if not events:
        print("No events found")
        return
    
    # Group by time windows
    current_window = events[0][0]
    window_events = []
    
    print("=== ATTACK TIMELINE ===\n")
    
    for ts, log in events:
        if (ts - current_window).total_seconds() > time_window_minutes * 60:
            if window_events:
                print(f"[{current_window.strftime('%Y-%m-%d %H:%M')}] ({len(window_events)} events)")
                for event_ts, event_log in window_events[:5]:
                    print(f"  {event_ts.strftime('%H:%M:%S')} - {event_log}")
                if len(window_events) > 5:
                    print(f"  ... and {len(window_events) - 5} more events")
                print()
            
            current_window = ts
            window_events = []
        
        window_events.append((ts, log))
    
    # Print last window
    if window_events:
        print(f"[{current_window.strftime('%Y-%m-%d %H:%M')}] ({len(window_events)} events)")
        for event_ts, event_log in window_events[:5]:
            print(f"  {event_ts.strftime('%H:%M:%S')} - {event_log}")

text_timeline('access.log', time_window_minutes=5)
```

### Session-based attack reconstruction

**Track complete attack sessions:**

```python
import re
from datetime import datetime, timedelta
from collections import defaultdict

def reconstruct_attack_sessions(log_file, session_timeout_minutes=30):
    """
    Group related events into attack sessions
    """
    # Parse all events with source IPs
    events_by_ip = defaultdict(list)
    
    with open(log_file, 'r', errors='ignore') as f:
        for line in f:
            ip_match = re.search(r'^(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})', line)
            ts_match = re.search(r'\[(\d{2}/\w{3}/\d{4}:\d{2}:\d{2}:\d{2})', line)
            
            if ip_match and ts_match:
                try:
                    ip = ip_match.group(1)
                    ts = datetime.strptime(ts_match.group(1), '%d/%b/%Y:%H:%M:%S')
                    events_by_ip[ip].append((ts, line.strip()))
                except:
                    pass
    
    # Build sessions for each IP
    print("=== ATTACK SESSION RECONSTRUCTION ===\n")
    
    for ip, events in events_by_ip.items():
        if len(events) < 5:  # Filter noise
            continue
        
        events.sort()
        sessions = []
        current_session = [events[0]]
        
        for i in range(1, len(events)):
            time_gap = (events[i][0] - current_session[-1][0]).total_seconds() / 60
            
            if time_gap > session_timeout_minutes:
                sessions.append(current_session)
                current_session = [events[i]]
            else:
                current_session.append(events[i])
        
        sessions.append(current_session)
        
        # Analyze sessions
        if len(sessions) > 1 or len(sessions[0]) > 10:
            print(f"Source IP: {ip}")
            print(f"Total sessions: {len(sessions)}")
            print(f"Total events: {len(events)}")
            
            for idx, session in enumerate(sessions, 1):
                duration = (session[-1][0] - session[0][0]).total_seconds() / 60
                print(f"\n  Session {idx}:")
                print(f"    Start: {session[0][0]}")
                print(f"    Duration: {duration:.1f} minutes")
                print(f"    Events: {len(session)}")
                print(f"    First event: {session[0][1][:100]}")
                print(f"    Last event: {session[-1][1][:100]}")
            
            print("\n" + "="*70 + "\n")

reconstruct_attack_sessions('access.log')
```

### Causal relationship detection

**Identify cause-effect relationships:**

```python
from datetime import timedelta
import re

def detect_causal_chains(log_file, max_gap_seconds=60):
    """
    Find events that likely caused subsequent events
    [Inference: Based on temporal proximity and content patterns]
    """
    events = []
    
    with open(log_file, 'r', errors='ignore') as f:
        for line in f:
            ts_match = re.search(r'\[(\d{2}/\w{3}/\d{4}:\d{2}:\d{2}:\d{2})', line)
            if ts_match:
                try:
                    ts = datetime.strptime(ts_match.group(1), '%d/%b/%Y:%H:%M:%S')
                    
                    # Classify event
                    if re.search(r'\.\./', line):
                        etype = 'path_traversal'
                    elif re.search(r'union.*select', line, re.IGNORECASE):
                        etype = 'sql_injection'
                    elif re.search(r'upload|POST.*multipart', line, re.IGNORECASE):
                        etype = 'file_upload'
                    elif re.search(r'\.php|\.jsp|\.asp', line):
                        etype = 'webshell_access'
                    elif re.search(r'cmd\.exe|/bin/bash', line):
                        etype = 'command_execution'
                    else:
                        etype = 'other'
                    
                    events.append({
                        'timestamp': ts,
                        'type': etype,
                        'content': line.strip()
                    })
                except:
                    pass
    
    events.sort(key=lambda x: x['timestamp'])
    
    # Define likely causal relationships
    causal_patterns = [
        ('path_traversal', 'file_upload'),
        ('file_upload', 'webshell_access'),
        ('webshell_access', 'command_execution'),
        ('sql_injection', 'command_execution')
    ]
    
    print("=== CAUSAL CHAIN DETECTION ===")
    print("[Inference: Based on temporal proximity and event patterns]\n")
    
    for i, event1 in enumerate(events):
        for event2 in events[i+1:]:
            time_gap = (event2['timestamp'] - event1['timestamp']).total_seconds()
            
            if time_gap > max_gap_seconds:
                break
            
            for cause_type, effect_type in causal_patterns:
                if event1['type'] == cause_type and event2['type'] == effect_type:
                    print(f"Possible causal chain (gap: {time_gap:.0f}s):")
                    print(f"  CAUSE [{cause_type}]:")
                    print(f"    {event1['timestamp']} - {event1['content'][:100]}")
                    print(f"  EFFECT [{effect_type}]:")
                    print(f"    {event2['timestamp']} - {event2['content'][:100]}")
                    print()

detect_causal_chains('access.log')
```

## Pivot Point Identification

Pivot points are moments where attackers change tactics, escalate privileges, move laterally, or transition between attack phases.

### Tactic transition detection

**Identify changes in attacker behavior:**

```python
import re
from datetime import datetime
from collections import Counter

def detect_tactic_changes(log_file, window_size=10):
    """
    Detect when attacker changes tactics based on request patterns
    """
    events = []
    
    with open(log_file, 'r', errors='ignore') as f:
        for line in f:
            ts_match = re.search(r'\[(\d{2}/\w{3}/\d{4}:\d{2}:\d{2}:\d{2})', line)
            ip_match = re.search(r'^(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})', line)
            
            if ts_match and ip_match:
                try:
                    ts = datetime.strptime(ts_match.group(1), '%d/%b/%Y:%H:%M:%S')
                    ip = ip_match.group(1)
                    
                    # Classify tactic
                    if re.search(r'nikto|nmap|gobuster', line, re.IGNORECASE):
                        tactic = 'reconnaissance'
                    elif re.search(r'union|sleep\(|1=1', line, re.IGNORECASE):
                        tactic = 'sql_injection'
                    elif re.search(r'<script|javascript:', line, re.IGNORECASE):
                        tactic = 'xss'
                    elif re.search(r'\.\./|\.\.\\', line):
                        tactic = 'path_traversal'
                    elif re.search(r'upload|multipart', line, re.IGNORECASE):
                        tactic = 'file_upload'
                    elif re.search(r'cmd=|exec=|system=', line, re.IGNORECASE):
                        tactic = 'command_injection'
                    else:
                        tactic = 'normal'
                    
                    events.append({
                        'timestamp': ts,
                        'ip': ip,
                        'tactic': tactic,
                        'log': line.strip()
                    })
                except:
                    pass
    
    events.sort(key=lambda x: x['timestamp'])
    
    # Detect tactic transitions using sliding window
    print("=== TACTIC TRANSITION DETECTION ===\n")
    
    for i in range(len(events) - window_size):
        window = events[i:i+window_size]
        
        # Get IP in this window
        ips = [e['ip'] for e in window]
        ip_counter = Counter(ips)
        dominant_ip = ip_counter.most_common(1)[0][0] if ip_counter else None
        
        if not dominant_ip or ip_counter[dominant_ip] < window_size // 2:
            continue
        
        # Filter to dominant IP
        ip_events = [e for e in window if e['ip'] == dominant_ip]
        
        # Check for tactic change
        tactics = [e['tactic'] for e in ip_events]
        tactic_counts = Counter(tactics)
        
        # Multiple tactics in window indicates transition
        if len(tactic_counts) >= 2 and 'normal' not in tactic_counts:
            print(f"Pivot point detected at {window[0]['timestamp']}")
            print(f"IP: {dominant_ip}")
            print(f"Tactic transition: {' -> '.join(tactic_counts.keys())}")
            print("Events in transition window:")
            for event in ip_events:
                print(f"  [{event['tactic']}] {event['log'][:100]}")
            print("\n" + "="*70 + "\n")

detect_tactic_changes('access.log', window_size=10)
```

### Privilege escalation detection

**Identify privilege escalation attempts:**

```bash
# Look for sudo usage, su commands, and privilege changes
python3 << 'EOF'
import re

def detect_privilege_escalation(auth_log):
    """
    Find privilege escalation indicators in authentication logs
    """
    escalation_patterns = [
        (r'sudo:.*COMMAND=', 'sudo_execution'),
        (r'su:.*session opened for user root', 'su_to_root'),
        (r'usermod.*-G.*sudo', 'group_modification'),
        (r'passwd:.*password changed', 'password_change'),
        (r'pam_unix.*session opened.*uid=0', 'root_session'),
        (r'setuid|setgid', 'uid_gid_manipulation')
    ]
    
    events = []
    
    with open(auth_log, 'r', errors='ignore') as f:
        for line_num, line in enumerate(f, 1):
            for pattern, escalation_type in escalation_patterns:
                if re.search(pattern, line, re.IGNORECASE):
                    # Extract user and timestamp
                    user_match = re.search(r'user[= ](\w+)', line)
                    user = user_match.group(1) if user_match else 'unknown'
                    
                    events.append({
                        'line': line_num,
                        'type': escalation_type,
                        'user': user,
                        'log': line.strip()
                    })
                    break
    
    print("=== PRIVILEGE ESCALATION DETECTION ===\n")
    
    if not events:
        print("No privilege escalation events detected")
        return
    
    # Group by user
    from collections import defaultdict
    by_user = defaultdict(list)
    for event in events:
        by_user[event['user']].append(event)
    
    for user, user_events in by_user.items():
        print(f"User: {user}")
        print(f"Escalation attempts: {len(user_events)}")
        print("Events:")
        for event in user_events:
            print(f"  Line {event['line']} [{event['type']}]")
            print(f"    {event['log'][:120]}")
        print()

detect_privilege_escalation('/var/log/auth.log')
EOF
```

### Lateral movement identification

**Detect movement between systems:**

```python
import re
from collections import defaultdict
from datetime import datetime

def detect_lateral_movement(log_files):
    """
    Identify lateral movement across multiple hosts
    Analyzes authentication and network connection logs
    """
    connections = []
    
    for log_file in log_files:
        with open(log_file, 'r', errors='ignore') as f:
            for line in f:
                # SSH connections
                ssh_match = re.search(r'Accepted.*from (\d+\.\d+\.\d+\.\d+).*for (\w+)', line)
                if ssh_match:
                    source_ip = ssh_match.group(1)
                    user = ssh_match.group(2)
                    
                    ts_match = re.search(r'(\w{3}\s+\d+\s+\d{2}:\d{2}:\d{2})', line)
                    if ts_match:
                        connections.append({
                            'timestamp': ts_match.group(1),
                            'source': source_ip,
                            'user': user,
                            'type': 'ssh',
                            'log': line.strip()
                        })
                
                # SMB connections (Windows)
                smb_match = re.search(r'\\\\(\d+\.\d+\.\d+\.\d+).*IPC\$', line)
                if smb_match:
                    connections.append({
                        'timestamp': 'N/A',
                        'source': smb_match.group(1),
                        'user': 'unknown',
                        'type': 'smb',
                        'log': line.strip()
                    })
    
    # Build movement graph
    movement_graph = defaultdict(set)
    
    for conn in connections:
        # Simplified: assume log file host is destination
        movement_graph[conn['source']].add(conn['user'])
    
    print("=== LATERAL MOVEMENT DETECTION ===\n")
    
    # Find IPs connecting as multiple users (suspicious)
    for source, users in movement_graph.items():
        if len(users) > 1:
            print(f"Suspicious lateral movement from {source}")
            print(f"  Connected as users: {', '.join(users)}")
            
            # Show related connections
            related = [c for c in connections if c['source'] == source]
            print(f"  Connection attempts: {len(related)}")
            for conn in related[:3]:
                print(f"    [{conn['type']}] {conn['user']} - {conn['log'][:100]}")
            print()

# Example usage
detect_lateral_movement(['/var/log/auth.log', '/var/log/secure'])
```

### Tool change detection

**Identify when attackers switch tools:**

```python
import re
from collections import Counter

def detect_tool_changes(log_file):
    """
    Identify attacker tool changes based on User-Agent and patterns
    """
    tool_signatures = {
        'nikto': [r'Nikto', r'LibWhisker'],
        'sqlmap': [r'sqlmap', r'User-Agent:.*python'],
        'burp': [r'Burp', r'Collaborator'],
        'metasploit': [r'Metasploit', r'meterpreter', r'msf'],
        'nmap': [r'Nmap.*Scripting Engine'],
        'gobuster': [r'gobuster'],
        'curl': [r'curl/\d'],
        'wget': [r'Wget/\d'],
        'python_requests': [r'python-requests'],
        'custom_script': [r'User-Agent: [A-Z]{1,5}$']
    }
    
    events = []
    
    with open(log_file, 'r', errors='ignore') as f:
        for line in f:
            ip_match = re.search(r'^(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})', line)
            ts_match = re.search(r'\[(\d{2}/\w{3}/\d{4}:\d{2}:\d{2}:\d{2})', line)
            
            if ip_match and ts_match:
                ip = ip_match.group(1)
                timestamp = ts_match.group(1)
                
                detected_tool = 'unknown'
                for tool, patterns in tool_signatures.items():
                    for pattern in patterns:
                        if re.search(pattern, line, re.IGNORECASE):
                            detected_tool = tool
                            break
                    if detected_tool != 'unknown':
                        break
                
                if detected_tool != 'unknown':
                    events.append({
                        'timestamp': timestamp,
                        'ip': ip,
                        'tool': detected_tool,
                        'log': line.strip()
                    })
    
    # Detect tool transitions by IP
    from collections import defaultdict
    tools_by_ip = defaultdict(list)
    
    for event in events:
        tools_by_ip[event['ip']].append(event)
    
    print("=== TOOL CHANGE DETECTION ===\n")
    
    for ip, ip_events in tools_by_ip.items():
        tools_used = [e['tool'] for e in ip_events]
        unique_tools = list(dict.fromkeys(tools_used))  # Preserve order
        
        if len(unique_tools) > 1:
            print(f"IP {ip} used multiple tools:")
            print(f"  Tool sequence: {' -> '.join(unique_tools)}")
            print(f"  Events:")
            
            for i, event in enumerate(ip_events):
                if i > 0 and event['tool'] != ip_events[i-1]['tool']:
                    print(f"    ** TOOL CHANGE to {event['tool']} **")
                print(f"    [{event['tool']}] {event['timestamp']} - {event['log'][:80]}")
            print()

detect_tool_changes('access.log')
```

### Success indicator detection

**Identify when reconnaissance becomes exploitation:**

```python
import re
from datetime import datetime, timedelta
from collections import defaultdict

def detect_success_pivot(log_file, time_window_minutes=10):
    """
    Detect when scanning/probing transitions to successful exploitation
    """
    events = []
    
    with open(log_file, 'r', errors='ignore') as f:
        for line in f:
            ts_match = re.search(r'\[(\d{2}/\w{3}/\d{4}:\d{2}:\d{2}:\d{2})', line)
            ip_match = re.search(r'^(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})', line)
            status_match = re.search(r'\s(200|301|302|401|403|404|500)\s', line)
            
            if ts_match and ip_match and status_match:
                try:
                    ts = datetime.strptime(ts_match.group(1), '%d/%b/%Y:%H:%M:%S')
                    ip = ip_match.group(1)
                    status = int(status_match.group(1))
                    
                    # Classify activity
                    if status == 404:
                        activity = 'scanning'
                    elif status == 403:
                        activity = 'blocked'
                    elif status == 200:
                        if re.search(r'\.php|\.jsp|\.asp', line):
                            activity = 'script_access'
                        elif re.search(r'cmd=|exec=|system=', line):
                            activity = 'exploitation'
                        else:
                            activity = 'normal_access'
                    elif status in [301, 302]:
                        activity = 'redirect'
                    else:
                        activity = 'error'
                    
                    events.append({
                        'timestamp': ts,
                        'ip': ip,
                        'status': status,
                        'activity': activity,
                        'log': line.strip()
                    })
                except:
                    pass

    events.sort(key=lambda x: x['timestamp'])

    # Detect transition from scanning to successful exploitation
    by_ip = defaultdict(list)
    for event in events:
        by_ip[event['ip']].append(event)

    print("=== SUCCESS PIVOT DETECTION ===\n")

    for ip, ip_events in by_ip.items():
        scanning_phase = []
        exploitation_phase = []
        
        for event in ip_events:
            if event['activity'] in ['scanning', 'blocked']:
                scanning_phase.append(event)
            elif event['activity'] in ['script_access', 'exploitation']:
                exploitation_phase.append(event)
        
        # Check if there's a transition from scanning to exploitation
        if scanning_phase and exploitation_phase:
            last_scan = scanning_phase[-1]['timestamp']
            first_exploit = exploitation_phase[0]['timestamp']
            
            time_gap = (first_exploit - last_scan).total_seconds() / 60
            
            if time_gap <= time_window_minutes:
                print(f"PIVOT POINT DETECTED - IP: {ip}")
                print(f"  Scanning phase: {len(scanning_phase)} attempts")
                print(f"  Last scan: {last_scan}")
                print(f"    {scanning_phase[-1]['log'][:100]}")
                print(f"\n  ** SUCCESS ACHIEVED ** (gap: {time_gap:.1f} minutes)")
                print(f"  First exploitation: {first_exploit}")
                print(f"    {exploitation_phase[0]['log'][:100]}")
                
                if len(exploitation_phase) > 1:
                    print(f"\n  Subsequent exploitation: {len(exploitation_phase) - 1} events")
                    for exploit in exploitation_phase[1:3]:
                        print(f"    {exploit['timestamp']} - {exploit['log'][:100]}")
                
                print("\n" + "=" * 70 + "\n")


# Usage
detect_success_pivot('access.log', time_window_minutes=10)
````

## Cross-reference Techniques

Cross-referencing combines data from multiple log sources to build a comprehensive view of security events.

### Multi-log correlation

**Correlate events across different log types:**
```python
import re
from datetime import datetime, timedelta
from collections import defaultdict

def correlate_multiple_logs(log_configs):
    """
    Correlate events across access logs, auth logs, and system logs
    
    log_configs format:
    [
        {'file': 'access.log', 'type': 'apache', 'parser': parse_apache},
        {'file': 'auth.log', 'type': 'auth', 'parser': parse_auth},
        {'file': 'syslog', 'type': 'system', 'parser': parse_syslog}
    ]
    """
    
    def parse_apache(line):
        """Parse Apache/Nginx access log"""
        match = re.search(r'^(\d+\.\d+\.\d+\.\d+).*\[(\d{2}/\w{3}/\d{4}:\d{2}:\d{2}:\d{2})', line)
        if match:
            return {
                'ip': match.group(1),
                'timestamp': datetime.strptime(match.group(2), '%d/%b/%Y:%H:%M:%S'),
                'raw': line.strip()
            }
        return None
    
    def parse_auth(line):
        """Parse authentication log"""
        match = re.search(r'(\w{3}\s+\d+\s+\d{2}:\d{2}:\d{2}).*from (\d+\.\d+\.\d+\.\d+)', line)
        if match:
            # Note: year assumed as current year for auth logs
            try:
                timestamp = datetime.strptime(match.group(1) + ' 2024', '%b %d %H:%M:%S %Y')
                return {
                    'ip': match.group(2),
                    'timestamp': timestamp,
                    'raw': line.strip()
                }
            except:
                pass
        return None
    
    def parse_syslog(line):
        """Parse generic syslog"""
        match = re.search(r'(\w{3}\s+\d+\s+\d{2}:\d{2}:\d{2})', line)
        if match:
            try:
                timestamp = datetime.strptime(match.group(1) + ' 2024', '%b %d %H:%M:%S %Y')
                ip_match = re.search(r'(\d+\.\d+\.\d+\.\d+)', line)
                return {
                    'ip': ip_match.group(1) if ip_match else 'local',
                    'timestamp': timestamp,
                    'raw': line.strip()
                }
            except:
                pass
        return None
    
    # Parse all logs
    all_events = []
    
    parsers = {
        'apache': parse_apache,
        'auth': parse_auth,
        'system': parse_syslog
    }
    
    for config in log_configs:
        log_file = config['file']
        log_type = config['type']
        parser = parsers.get(log_type)
        
        if not parser:
            continue
        
        try:
            with open(log_file, 'r', errors='ignore') as f:
                for line in f:
                    parsed = parser(line)
                    if parsed:
                        parsed['log_type'] = log_type
                        parsed['log_file'] = log_file
                        all_events.append(parsed)
        except FileNotFoundError:
            print(f"Warning: {log_file} not found")
    
    # Sort by timestamp
    all_events.sort(key=lambda x: x['timestamp'])
    
    # Group by IP and find correlated events
    events_by_ip = defaultdict(list)
    for event in all_events:
        events_by_ip[event['ip']].append(event)
    
    print("=== MULTI-LOG CORRELATION ===\n")
    
    for ip, events in events_by_ip.items():
        if ip == 'local' or len(events) < 5:
            continue
        
        # Check if events span multiple log types
        log_types = set(e['log_type'] for e in events)
        
        if len(log_types) > 1:
            print(f"Correlated activity from IP: {ip}")
            print(f"  Log sources: {', '.join(log_types)}")
            print(f"  Total events: {len(events)}")
            print(f"  Time span: {events[0]['timestamp']} to {events[-1]['timestamp']}")
            print(f"\n  Event timeline:")
            
            for event in events[:10]:  # Show first 10
                print(f"    [{event['log_type']:10s}] {event['timestamp']} - {event['raw'][:80]}")
            
            if len(events) > 10:
                print(f"    ... and {len(events)-10} more events")
            
            print("\n" + "="*70 + "\n")

# Example usage
log_configs = [
    {'file': '/var/log/apache2/access.log', 'type': 'apache'},
    {'file': '/var/log/auth.log', 'type': 'auth'},
    {'file': '/var/log/syslog', 'type': 'system'}
]

correlate_multiple_logs(log_configs)
````

### IP reputation cross-reference

**Correlate with threat intelligence:**

```bash
# Extract unique IPs and check against known bad IP lists
python3 << 'EOF'
import re
import requests
from collections import Counter

def check_ip_reputation(log_file, threat_feed_file=None):
    """
    Cross-reference IPs from logs with threat intelligence
    [Unverified: Requires external threat feeds]
    """
    # Extract IPs from log
    ips = []
    with open(log_file, 'r', errors='ignore') as f:
        for line in f:
            ip_match = re.findall(r'\b(?:\d{1,3}\.){3}\d{1,3}\b', line)
            ips.extend(ip_match)
    
    ip_counts = Counter(ips)
    
    # Load local threat feed if provided
    bad_ips = set()
    if threat_feed_file:
        try:
            with open(threat_feed_file, 'r') as f:
                bad_ips = set(line.strip() for line in f if line.strip())
        except:
            pass
    
    print("=== IP REPUTATION ANALYSIS ===")
    print("[Unverified: Results depend on threat feed quality]\n")
    
    print(f"Total unique IPs: {len(ip_counts)}")
    print(f"Threat feed entries: {len(bad_ips)}\n")
    
    # Cross-reference
    matches = []
    for ip, count in ip_counts.most_common():
        if ip in bad_ips:
            matches.append((ip, count))
    
    if matches:
        print(f"MATCHES FOUND: {len(matches)} known malicious IPs\n")
        for ip, count in matches[:10]:
            print(f"  {ip} - {count} occurrences")
    else:
        print("No matches with threat feed")
    
    # Most active IPs
    print("\nMost active IPs (potential investigation targets):")
    for ip, count in ip_counts.most_common(10):
        reputation = "KNOWN BAD" if ip in bad_ips else "unknown"
        print(f"  {ip:15s} - {count:5d} events [{reputation}]")

check_ip_reputation('access.log', 'threatfeed.txt')
EOF
```

**Online IP reputation check (rate-limited):**

```python
import re
import time
import socket
from collections import Counter

def check_ips_online(log_file, max_checks=10):
    """
    Check IPs against online reputation services
    [Unverified: Depends on external API availability]
    Note: Rate-limited, use sparingly in CTF environments
    """
    ips = []
    
    with open(log_file, 'r', errors='ignore') as f:
        for line in f:
            ip_match = re.findall(r'\b(?:\d{1,3}\.){3}\d{1,3}\b', line)
            ips.extend(ip_match)
    
    ip_counts = Counter(ips)
    
    print("=== ONLINE IP REPUTATION CHECK ===")
    print("[Unverified: Results from external APIs]\n")
    
    checked = 0
    for ip, count in ip_counts.most_common():
        if checked >= max_checks:
            break
        
        # Basic validation
        try:
            socket.inet_aton(ip)
        except:
            continue
        
        # Skip private IPs
        if ip.startswith(('10.', '172.', '192.168.', '127.')):
            continue
        
        print(f"Checking {ip} ({count} occurrences)...")
        
        # Example: Check if IP resolves (basic indicator)
        try:
            hostname = socket.gethostbyaddr(ip)[0]
            print(f"  Hostname: {hostname}")
        except:
            print(f"  No reverse DNS (potentially suspicious)")
        
        checked += 1
        time.sleep(1)  # Rate limiting

check_ips_online('access.log', max_checks=5)
```

### Geolocation correlation

**Analyze geographic patterns:**

```python
import re
from collections import Counter, defaultdict

def analyze_geo_patterns(log_file):
    """
    Analyze geographic patterns in access logs
    [Inference: Based on IP ranges and patterns]
    Note: Requires GeoIP database for accurate results
    """
    
    def estimate_geo_from_ip(ip):
        """
        Simplified geographic estimation based on IP ranges
        [Inference: Rough estimation only, not verified]
        """
        octets = ip.split('.')
        if not all(o.isdigit() for o in octets):
            return 'invalid'
        
        first_octet = int(octets[0])
        
        # Very rough approximations for demonstration
        if first_octet in range(1, 3):
            return 'ARIN-US'
        elif first_octet in range(3, 42):
            return 'ARIN-General'
        elif first_octet in range(80, 95):
            return 'RIPE-Europe'
        elif first_octet in range(202, 223):
            return 'APNIC-Asia'
        else:
            return 'Other'
    
    ips = []
    
    with open(log_file, 'r', errors='ignore') as f:
        for line in f:
            ip_match = re.search(r'^(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})', line)
            if ip_match:
                ips.append(ip_match.group(1))
    
    # Analyze geographic distribution
    geo_dist = Counter()
    for ip in ips:
        region = estimate_geo_from_ip(ip)
        geo_dist[region] += 1
    
    print("=== GEOGRAPHIC CORRELATION ANALYSIS ===")
    print("[Inference: Based on IP range estimation]\n")
    
    print("Regional distribution:")
    for region, count in geo_dist.most_common():
        percentage = (count / len(ips)) * 100
        print(f"  {region:20s}: {count:5d} ({percentage:.1f}%)")
    
    # Find IPs from unusual regions
    total = sum(geo_dist.values())
    unusual_threshold = 0.05  # Less than 5% of traffic
    
    print("\nPotentially suspicious (low-frequency regions):")
    for region, count in geo_dist.items():
        if count / total < unusual_threshold and region != 'invalid':
            print(f"  {region}: {count} requests")

analyze_geo_patterns('access.log')
```

### Temporal correlation

**Find events occurring at similar times:**

```python
from datetime import datetime, timedelta
import re

def temporal_correlation(log_files, time_window_seconds=5):
    """
    Find events occurring within same time window across different logs
    """
    all_events = []
    
    for log_file in log_files:
        try:
            with open(log_file, 'r', errors='ignore') as f:
                for line in f:
                    # Try Apache format first
                    ts_match = re.search(r'\[(\d{2}/\w{3}/\d{4}:\d{2}:\d{2}:\d{2})', line)
                    if ts_match:
                        try:
                            ts = datetime.strptime(ts_match.group(1), '%d/%b/%Y:%H:%M:%S')
                            all_events.append({
                                'timestamp': ts,
                                'source': log_file,
                                'content': line.strip()
                            })
                        except:
                            pass
                    else:
                        # Try syslog format
                        ts_match = re.search(r'(\w{3}\s+\d+\s+\d{2}:\d{2}:\d{2})', line)
                        if ts_match:
                            try:
                                ts = datetime.strptime(ts_match.group(1) + ' 2024', '%b %d %H:%M:%S %Y')
                                all_events.append({
                                    'timestamp': ts,
                                    'source': log_file,
                                    'content': line.strip()
                                })
                            except:
                                pass
        except FileNotFoundError:
            continue
    
    all_events.sort(key=lambda x: x['timestamp'])
    
    print("=== TEMPORAL CORRELATION ===\n")
    
    # Find clusters of events within time window
    clusters = []
    current_cluster = []
    
    for event in all_events:
        if not current_cluster:
            current_cluster.append(event)
        else:
            time_diff = (event['timestamp'] - current_cluster[-1]['timestamp']).total_seconds()
            
            if time_diff <= time_window_seconds:
                current_cluster.append(event)
            else:
                if len(current_cluster) > 1:
                    # Check if cluster has events from multiple sources
                    sources = set(e['source'] for e in current_cluster)
                    if len(sources) > 1:
                        clusters.append(current_cluster)
                current_cluster = [event]
    
    # Add last cluster
    if len(current_cluster) > 1:
        sources = set(e['source'] for e in current_cluster)
        if len(sources) > 1:
            clusters.append(current_cluster)
    
    print(f"Found {len(clusters)} correlated event clusters\n")
    
    for idx, cluster in enumerate(clusters[:5], 1):  # Show first 5
        sources = set(e['source'] for e in cluster)
        print(f"Cluster {idx}:")
        print(f"  Time: {cluster[0]['timestamp']}")
        print(f"  Duration: {(cluster[-1]['timestamp'] - cluster[0]['timestamp']).total_seconds():.1f}s")
        print(f"  Sources: {', '.join(sources)}")
        print(f"  Events:")
        for event in cluster:
            print(f"    [{event['source']:20s}] {event['content'][:80]}")
        print()

temporal_correlation(['access.log', 'auth.log', 'syslog'], time_window_seconds=5)
```

### User behavior correlation

**Track user activity across systems:**

```python
import re
from collections import defaultdict
from datetime import datetime

def correlate_user_activity(log_files):
    """
    Track user accounts across multiple systems/logs
    """
    user_events = defaultdict(list)
    
    for log_file in log_files:
        try:
            with open(log_file, 'r', errors='ignore') as f:
                for line in f:
                    # Extract username patterns
                    user_patterns = [
                        r'user[= ](\w+)',
                        r'for (\w+) from',
                        r'USER=(\w+)',
                        r'su: .*to (\w+)'
                    ]
                    
                    username = None
                    for pattern in user_patterns:
                        match = re.search(pattern, line, re.IGNORECASE)
                        if match:
                            username = match.group(1)
                            break
                    
                    if username and username not in ['root', 'SYSTEM', 'LOCAL']:
                        # Extract timestamp
                        ts_match = re.search(r'\[?(\d{2}/\w{3}/\d{4}:\d{2}:\d{2}:\d{2}|\w{3}\s+\d+\s+\d{2}:\d{2}:\d{2})', line)
                        
                        user_events[username].append({
                            'timestamp': ts_match.group(1) if ts_match else 'unknown',
                            'source': log_file,
                            'log': line.strip()
                        })
        except FileNotFoundError:
            continue
    
    print("=== USER ACTIVITY CORRELATION ===\n")
    
    for username, events in sorted(user_events.items(), key=lambda x: len(x[1]), reverse=True):
        if len(events) < 3:  # Filter noise
            continue
        
        sources = set(e['source'] for e in events)
        
        print(f"User: {username}")
        print(f"  Total events: {len(events)}")
        print(f"  Log sources: {len(sources)}")
        
        if len(sources) > 1:
            print(f"  ** CROSS-SYSTEM ACTIVITY DETECTED **")
            print(f"  Sources: {', '.join(sources)}")
        
        print(f"  Sample events:")
        for event in events[:3]:
            print(f"    [{event['source']:20s}] {event['log'][:80]}")
        print()

correlate_user_activity(['access.log', 'auth.log', 'secure'])
```

### Hash correlation

**Cross-reference file hashes and identifiers:**

```bash
# Extract and correlate hashes across logs
python3 << 'EOF'
import re
from collections import defaultdict

def correlate_hashes(log_files):
    """
    Find hash values (MD5, SHA1, SHA256) across multiple logs
    Useful for tracking malware, uploaded files, or artifacts
    """
    hash_patterns = {
        'md5': r'\b[a-fA-F0-9]{32}\b',
        'sha1': r'\b[a-fA-F0-9]{40}\b',
        'sha256': r'\b[a-fA-F0-9]{64}\b'
    }
    
    hash_occurrences = defaultdict(list)
    
    for log_file in log_files:
        try:
            with open(log_file, 'r', errors='ignore') as f:
                for line_num, line in enumerate(f, 1):
                    for hash_type, pattern in hash_patterns.items():
                        matches = re.findall(pattern, line)
                        for hash_value in matches:
                            hash_occurrences[hash_value].append({
                                'type': hash_type,
                                'file': log_file,
                                'line': line_num,
                                'context': line.strip()[:100]
                            })
        except FileNotFoundError:
            continue
    
    print("=== HASH CORRELATION ANALYSIS ===\n")
    
    # Find hashes appearing in multiple logs
    multi_source_hashes = {h: locs for h, locs in hash_occurrences.items() 
                           if len(set(loc['file'] for loc in locs)) > 1}
    
    if multi_source_hashes:
        print(f"Found {len(multi_source_hashes)} hashes across multiple logs:\n")
        
        for hash_value, locations in multi_source_hashes.items():
            files = set(loc['file'] for loc in locations)
            print(f"Hash: {hash_value} ({locations[0]['type']})")
            print(f"  Appears in: {', '.join(files)}")
            print(f"  Total occurrences: {len(locations)}")
            print(f"  Contexts:")
            for loc in locations[:3]:
                print(f"    {loc['file']}:{loc['line']} - {loc['context']}")
            print()
    else:
        print("No hashes found across multiple logs")
    
    # Show most frequent hashes
    print("\nMost frequent hashes:")
    sorted_hashes = sorted(hash_occurrences.items(), key=lambda x: len(x[1]), reverse=True)
    for hash_value, locations in sorted_hashes[:5]:
        print(f"  {hash_value} - {len(locations)} occurrences")

correlate_hashes(['access.log', 'upload.log', 'audit.log'])
EOF
```

### Command sequence correlation

**Reconstruct command execution chains:**

```python
import re
from datetime import datetime

def correlate_command_sequences(log_files):
    """
    Track command execution sequences across logs
    Useful for understanding attacker behavior post-exploitation
    """
    commands = []
    
    command_patterns = [
        r'cmd=([^&\s]+)',
        r'exec\(([^)]+)\)',
        r'system\(([^)]+)\)',
        r'COMMAND=(.+?)(?:\s|$)',
        r'executed: (.+?)(?:\s|$)'
    ]
    
    for log_file in log_files:
        try:
            with open(log_file, 'r', errors='ignore') as f:
                for line in f:
                    for pattern in command_patterns:
                        match = re.search(pattern, line)
                        if match:
                            cmd = match.group(1).strip('"\'')
                            
                            # Extract timestamp
                            ts_match = re.search(r'\[?(\d{2}/\w{3}/\d{4}:\d{2}:\d{2}:\d{2})', line)
                            timestamp = ts_match.group(1) if ts_match else 'unknown'
                            
                            # Extract IP if available
                            ip_match = re.search(r'(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})', line)
                            ip = ip_match.group(1) if ip_match else 'unknown'
                            
                            commands.append({
                                'timestamp': timestamp,
                                'ip': ip,
                                'command': cmd,
                                'source': log_file,
                                'raw': line.strip()
                            })
                            break
        except FileNotFoundError:
            continue
    
    print("=== COMMAND SEQUENCE CORRELATION ===\n")
    
    if not commands:
        print("No commands found in logs")
        return
    
    # Group by IP
    from collections import defaultdict
    by_ip = defaultdict(list)
    for cmd in commands:
        by_ip[cmd['ip']].append(cmd)
    
    for ip, ip_commands in by_ip.items():
        if len(ip_commands) < 2:
            continue
        
        print(f"Command sequence from IP: {ip}")
        print(f"  Total commands: {len(ip_commands)}")
        print(f"  Sequence:")
        
        for idx, cmd_event in enumerate(ip_commands, 1):
            print(f"    {idx}. [{cmd_event['timestamp']}] {cmd_event['command']}")
            
            # Analyze command purpose
            if re.search(r'whoami|id|uname', cmd_event['command'], re.IGNORECASE):
                print(f"       ^ Reconnaissance")
            elif re.search(r'wget|curl|download', cmd_event['command'], re.IGNORECASE):
                print(f"       ^ Download/Staging")
            elif re.search(r'nc|netcat|/bin/bash', cmd_event['command'], re.IGNORECASE):
                print(f"       ^ Shell/Backdoor")
            elif re.search(r'passwd|shadow|\.ssh', cmd_event['command'], re.IGNORECASE):
                print(f"       ^ Credential Access")
        
        print()

correlate_command_sequences(['access.log', 'auth.log', 'audit.log'])
```

---

**Important related topics:**

- Graph database techniques for complex relationship mapping
- Behavioral baselining for anomaly detection in correlations
- Packet capture (PCAP) correlation with log data
- Memory forensics correlation with system logs

---

# Performance & Optimization

Efficient log analysis is critical in CTF scenarios where datasets may contain millions of events across gigabytes of data. Performance optimization techniques determine whether analysis completes in minutes or hours.

## Large File Handling

Large log files (>1GB) require specialized approaches to avoid memory exhaustion and excessive processing time.

### Streaming vs. Loading Approaches

**Memory implications:**

- **Full load:** Reads entire file into memory (fast random access, high memory usage)
- **Streaming:** Processes line-by-line (constant memory, sequential access only)
- **Memory mapping:** Maps file to virtual memory (balanced approach)

**Choosing the right approach:**

```bash
# Check file size first
ls -lh /var/log/apache2/access.log
du -h /var/log/apache2/access.log

# Check available memory
free -h
```

### Streaming with Standard Tools

**Using grep for large files:**

```bash
# Standard grep (buffers entire lines)
grep "ERROR" huge.log

# grep with line buffering for real-time output
grep --line-buffered "ERROR" huge.log

# grep with mmap for better performance
grep --mmap "ERROR" huge.log

# Parallel grep (ripgrep - significantly faster)
rg "ERROR" huge.log

# ripgrep with memory map disabled for huge files
rg --no-mmap "ERROR" huge.log
```

**Performance comparison - [Inference] typical results on modern hardware:**

- `grep`: ~50-100 MB/s
- `rg` (ripgrep): ~500-1000 MB/s
- `ag` (silver searcher): ~200-400 MB/s

**Using awk for streaming processing:**

```bash
# Memory-efficient line-by-line processing
awk '/ERROR/ {print $1, $NF}' huge.log

# Process with minimal memory footprint
awk 'BEGIN {count=0} /failed/ {count++} END {print count}' huge.log

# Stream processing with state tracking (limited memory)
awk '{ip[$1]++} END {for (i in ip) if (ip[i] > 100) print i, ip[i]}' huge.log
```

**[Inference] awk maintains hash tables in memory, so unique key tracking for billions of entries may still cause memory issues.**

### Splitting Large Files

**Split by line count:**

```bash
# Split into files of 1 million lines each
split -l 1000000 huge.log chunk_ --additional-suffix=.log

# Split with numeric suffixes
split -l 1000000 -d huge.log chunk_

# Specify suffix length
split -l 1000000 -d -a 4 huge.log chunk_
```

**Split by size:**

```bash
# Split into 500MB chunks
split -b 500M huge.log chunk_

# Split by size with human-readable units
split -b 1G huge.log chunk_
```

**Split by pattern (csplit):**

```bash
# Split on date boundaries
csplit huge.log '/^2025-10-29/' '{*}' --prefix=day_ --suffix-format='%02d.log'

# Split on empty lines (record boundaries)
csplit -z huge.log '/^$/' '{*}'
```

**Time-based splitting for timestamped logs:**

```bash
# Extract logs by hour
awk -F'[: ]' '$2=="10" {print > "hour_10.log"}; $2=="11" {print > "hour_11.log"}' huge.log

# Split by date pattern
awk '/2025-10-29/ {print > "oct29.log"}; /2025-10-30/ {print > "oct30.log"}' huge.log
```

### Memory Mapping Techniques

**Using mmap in Python:**

```python
import mmap
import re

def search_large_file_mmap(filename, pattern):
    with open(filename, 'rb') as f:
        # Memory map the file
        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mmapped_file:
            # Compile pattern once
            regex = re.compile(pattern.encode())
            
            # Search without loading into memory
            for match in regex.finditer(mmapped_file):
                # Get line containing match
                start = mmapped_file.rfind(b'\n', 0, match.start()) + 1
                end = mmapped_file.find(b'\n', match.end())
                line = mmapped_file[start:end].decode('utf-8', errors='ignore')
                print(line)

search_large_file_mmap('huge.log', r'ERROR|CRITICAL')
```

**Memory-mapped file statistics:**

```python
import mmap
import os

def fast_line_count(filename):
    with open(filename, 'rb') as f:
        # Get file size
        f.seek(0, os.SEEK_END)
        size = f.tell()
        f.seek(0)
        
        # Memory map
        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mmapped:
            count = 0
            chunk_size = 1024 * 1024  # 1MB chunks
            
            for i in range(0, size, chunk_size):
                chunk = mmapped[i:min(i + chunk_size, size)]
                count += chunk.count(b'\n')
            
            return count

print(f"Lines: {fast_line_count('huge.log')}")
```

### Index-Based Access

Creating indices for random access to large files.

**Building line offset index:**

```python
def build_line_index(filename, index_file):
    """Create index of line offsets for fast random access"""
    offsets = []
    
    with open(filename, 'rb') as f:
        offset = 0
        offsets.append(offset)
        
        while f.readline():
            offset = f.tell()
            offsets.append(offset)
    
    # Save index
    import pickle
    with open(index_file, 'wb') as idx:
        pickle.dump(offsets, idx)
    
    return len(offsets) - 1

def get_line_by_number(filename, index_file, line_num):
    """Retrieve specific line using index"""
    import pickle
    
    with open(index_file, 'rb') as idx:
        offsets = pickle.load(idx)
    
    if line_num >= len(offsets) - 1:
        return None
    
    with open(filename, 'rb') as f:
        f.seek(offsets[line_num])
        return f.readline().decode('utf-8', errors='ignore')

# Build index once
build_line_index('huge.log', 'huge.log.idx')

# Fast random access
print(get_line_by_number('huge.log', 'huge.log.idx', 1000000))
```

**Time-based index for timestamped logs:**

```python
import re
from datetime import datetime
import bisect

def build_timestamp_index(filename, index_file, timestamp_pattern):
    """Build index mapping timestamps to file offsets"""
    index = []  # List of (timestamp, offset) tuples
    
    pattern = re.compile(timestamp_pattern)
    
    with open(filename, 'rb') as f:
        while True:
            offset = f.tell()
            line = f.readline()
            
            if not line:
                break
            
            match = pattern.search(line.decode('utf-8', errors='ignore'))
            if match:
                try:
                    timestamp = datetime.strptime(match.group(1), '%Y-%m-%d %H:%M:%S')
                    index.append((timestamp, offset))
                except:
                    pass
    
    import pickle
    with open(index_file, 'wb') as idx:
        pickle.dump(index, idx)
    
    return len(index)

def get_logs_by_timerange(filename, index_file, start_time, end_time):
    """Extract logs within time range using index"""
    import pickle
    
    with open(index_file, 'rb') as idx:
        index = pickle.load(idx)
    
    # Binary search for start position
    timestamps = [t for t, _ in index]
    start_idx = bisect.bisect_left(timestamps, start_time)
    end_idx = bisect.bisect_right(timestamps, end_time)
    
    if start_idx >= len(index):
        return []
    
    start_offset = index[start_idx][1]
    end_offset = index[end_idx][1] if end_idx < len(index) else None
    
    results = []
    with open(filename, 'rb') as f:
        f.seek(start_offset)
        
        while True:
            if end_offset and f.tell() >= end_offset:
                break
            
            line = f.readline()
            if not line:
                break
            
            results.append(line.decode('utf-8', errors='ignore'))
    
    return results

# Usage
build_timestamp_index('huge.log', 'huge.log.tsidx', r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})')

start = datetime(2025, 10, 29, 10, 0, 0)
end = datetime(2025, 10, 29, 11, 0, 0)
logs = get_logs_by_timerange('huge.log', 'huge.log.tsidx', start, end)
```

### Compressed File Handling

**Streaming decompression:**

```bash
# Decompress and process without writing intermediate file
zcat large.log.gz | grep "ERROR"
bzcat large.log.bz2 | awk '{print $1}'
xzcat large.log.xz | wc -l

# Multiple compressed files
zcat *.log.gz | grep "failed login"

# Parallel decompression with pigz
pigz -dc large.log.gz | grep "ERROR"
```

**Python streaming decompression:**

```python
import gzip

def stream_gzip(filename, pattern):
    """Stream and search gzipped file without full decompression"""
    with gzip.open(filename, 'rt') as f:
        for line in f:
            if pattern in line:
                print(line.rstrip())

stream_gzip('large.log.gz', 'ERROR')
```

**Memory-efficient multi-format handler:**

```python
import gzip
import bz2
import lzma

def open_smart(filename):
    """Auto-detect compression and return appropriate file handle"""
    if filename.endswith('.gz'):
        return gzip.open(filename, 'rt')
    elif filename.endswith('.bz2'):
        return bz2.open(filename, 'rt')
    elif filename.endswith('.xz') or filename.endswith('.lzma'):
        return lzma.open(filename, 'rt')
    else:
        return open(filename, 'r')

def process_any_format(filename):
    with open_smart(filename) as f:
        for line in f:
            # Process line
            pass
```

## Memory-Efficient Parsing

Techniques to minimize memory footprint during log analysis.

### Generator-Based Processing

**Python generators for streaming:**

```python
def read_large_file(filename):
    """Generator that yields lines without loading entire file"""
    with open(filename, 'r') as f:
        for line in f:
            yield line.rstrip()

def parse_apache_log(filename):
    """Generator-based Apache log parser"""
    import re
    
    pattern = re.compile(
        r'(?P<ip>[\d.]+) - - \[(?P<timestamp>[^\]]+)\] '
        r'"(?P<method>\w+) (?P<url>[^\s]+) HTTP/[^"]*" '
        r'(?P<status>\d+) (?P<size>\d+)'
    )
    
    for line in read_large_file(filename):
        match = pattern.match(line)
        if match:
            yield match.groupdict()

# Memory-efficient usage
for entry in parse_apache_log('huge_access.log'):
    if entry['status'] == '404':
        print(f"{entry['ip']} -> {entry['url']}")
```

**Generator-based aggregation with limited memory:**

```python
from collections import defaultdict

def count_by_field_limited(filename, field, max_unique=10000):
    """Count occurrences with memory limit on unique values"""
    counts = defaultdict(int)
    overflow = 0
    
    for entry in parse_apache_log(filename):
        value = entry.get(field)
        
        if value in counts:
            counts[value] += 1
        elif len(counts) < max_unique:
            counts[value] = 1
        else:
            overflow += 1
    
    return counts, overflow

# Count IPs with memory limit
ip_counts, overflow = count_by_field_limited('huge_access.log', 'ip', 50000)
print(f"Unique IPs: {len(ip_counts)}, Overflow: {overflow}")
```

### Chunked Processing

**Process file in fixed-size chunks:**

```python
def process_in_chunks(filename, chunk_size=1024*1024):
    """Process file in memory-efficient chunks"""
    with open(filename, 'rb') as f:
        buffer = b''
        
        while True:
            chunk = f.read(chunk_size)
            if not chunk:
                # Process remaining buffer
                if buffer:
                    yield buffer
                break
            
            buffer += chunk
            
            # Split on line boundaries
            lines = buffer.split(b'\n')
            
            # Keep incomplete last line in buffer
            buffer = lines[-1]
            
            # Yield complete lines
            for line in lines[:-1]:
                yield line.decode('utf-8', errors='ignore')

# Usage
error_count = 0
for line in process_in_chunks('huge.log'):
    if 'ERROR' in line:
        error_count += 1

print(f"Errors: {error_count}")
```

### Iterative Aggregation Patterns

**Incremental statistics without storing all data:**

```python
class StreamingStats:
    """Calculate statistics without storing all values"""
    def __init__(self):
        self.count = 0
        self.sum = 0
        self.sum_squares = 0
        self.min_val = float('inf')
        self.max_val = float('-inf')
    
    def update(self, value):
        self.count += 1
        self.sum += value
        self.sum_squares += value * value
        self.min_val = min(self.min_val, value)
        self.max_val = max(self.max_val, value)
    
    def mean(self):
        return self.sum / self.count if self.count > 0 else 0
    
    def variance(self):
        if self.count < 2:
            return 0
        mean = self.mean()
        return (self.sum_squares / self.count) - (mean * mean)
    
    def std_dev(self):
        return self.variance() ** 0.5

# Usage: Calculate response time statistics
stats = StreamingStats()

for line in read_large_file('access.log'):
    # Extract response time (example)
    parts = line.split()
    if len(parts) > 10:
        try:
            response_time = float(parts[10])
            stats.update(response_time)
        except:
            pass

print(f"Mean: {stats.mean():.2f}ms")
print(f"Std Dev: {stats.std_dev():.2f}ms")
print(f"Min: {stats.min_val}ms, Max: {stats.max_val}ms")
```

**Top-N tracking with bounded memory:**

```python
import heapq

class TopNTracker:
    """Track top N items without storing all data"""
    def __init__(self, n=10):
        self.n = n
        self.heap = []
    
    def add(self, value, key):
        """Add item with priority key"""
        if len(self.heap) < self.n:
            heapq.heappush(self.heap, (key, value))
        elif key > self.heap[0][0]:
            heapq.heapreplace(self.heap, (key, value))
    
    def get_top(self):
        """Return top N items sorted descending"""
        return sorted(self.heap, reverse=True)

# Track top 100 IPs by request count
from collections import defaultdict

ip_counts = defaultdict(int)
top_ips = TopNTracker(100)

for line in read_large_file('access.log'):
    ip = line.split()[0]
    ip_counts[ip] += 1
    top_ips.add(ip, ip_counts[ip])

print("Top 100 IPs:")
for count, ip in top_ips.get_top():
    print(f"{ip}: {count}")
```

### Sampling Strategies

When complete accuracy isn't required, sampling reduces processing time and memory.

**Random sampling:**

```python
import random

def sample_lines(filename, sample_rate=0.01):
    """Process only a percentage of lines"""
    for line in read_large_file(filename):
        if random.random() < sample_rate:
            yield line

# Analyze 1% sample
sample_count = sum(1 for line in sample_lines('huge.log', 0.01) if 'ERROR' in line)
estimated_total = sample_count * 100
print(f"Estimated errors: {estimated_total}")
```

**Reservoir sampling (fixed sample size):**

```python
import random

def reservoir_sample(filename, k=1000):
    """Select exactly k random lines from file of unknown size"""
    reservoir = []
    
    for i, line in enumerate(read_large_file(filename)):
        if i < k:
            reservoir.append(line)
        else:
            # Replace with decreasing probability
            j = random.randint(0, i)
            if j < k:
                reservoir[j] = line
    
    return reservoir

# Get 1000 random lines
sample = reservoir_sample('huge.log', 1000)
```

**Systematic sampling:**

```bash
# Process every Nth line
awk 'NR % 100 == 0' huge.log  # Every 100th line

# Time-based sampling (one per second)
awk -F'[: ]' 'prev != $3 {print; prev = $3}' huge.log
```

### External Sorting for Large Datasets

When aggregation results exceed memory.

**Sort large file with limited memory:**

```bash
# External sort (disk-backed)
sort -S 500M --parallel=4 huge.log > sorted.log

# Sort by specific field (IP address)
sort -t' ' -k1,1 -S 500M --parallel=4 huge.log > sorted_by_ip.log

# Unique sorted results
sort -u -S 500M huge.log > unique_sorted.log

# Numeric sort
sort -n -S 500M numbers.log > sorted_numbers.log
```

**Python external sort implementation:**

```python
import tempfile
import heapq
import os

def external_sort(filename, output_file, chunk_size=1000000):
    """Sort file larger than memory using external merge sort"""
    temp_files = []
    
    # Phase 1: Sort chunks and write to temp files
    chunk = []
    with open(filename, 'r') as f:
        for line in f:
            chunk.append(line)
            
            if len(chunk) >= chunk_size:
                chunk.sort()
                temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False)
                temp_file.writelines(chunk)
                temp_file.close()
                temp_files.append(temp_file.name)
                chunk = []
        
        # Handle remaining chunk
        if chunk:
            chunk.sort()
            temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False)
            temp_file.writelines(chunk)
            temp_file.close()
            temp_files.append(temp_file.name)
    
    # Phase 2: Merge sorted chunks
    with open(output_file, 'w') as output:
        file_handles = [open(tf, 'r') for tf in temp_files]
        
        # Use heap for efficient merging
        heap = []
        for i, fh in enumerate(file_handles):
            line = fh.readline()
            if line:
                heapq.heappush(heap, (line, i))
        
        while heap:
            line, i = heapq.heappop(heap)
            output.write(line)
            
            next_line = file_handles[i].readline()
            if next_line:
                heapq.heappush(heap, (next_line, i))
        
        # Cleanup
        for fh in file_handles:
            fh.close()
        
        for tf in temp_files:
            os.unlink(tf)

external_sort('huge_unsorted.log', 'huge_sorted.log')
```

### Database-Backed Processing

For complex queries on large datasets, use embedded databases.

**SQLite for aggregation:**

```python
import sqlite3

def log_to_sqlite(log_filename, db_filename):
    """Import large log file into SQLite for efficient querying"""
    conn = sqlite3.connect(db_filename)
    cursor = conn.cursor()
    
    # Create table
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS logs (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            timestamp TEXT,
            ip TEXT,
            method TEXT,
            url TEXT,
            status INTEGER,
            size INTEGER
        )
    ''')
    
    # Create indices
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_ip ON logs(ip)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_status ON logs(status)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_timestamp ON logs(timestamp)')
    
    # Batch insert for efficiency
    batch = []
    batch_size = 10000
    
    for entry in parse_apache_log(log_filename):
        batch.append((
            entry.get('timestamp'),
            entry.get('ip'),
            entry.get('method'),
            entry.get('url'),
            int(entry.get('status', 0)),
            int(entry.get('size', 0))
        ))
        
        if len(batch) >= batch_size:
            cursor.executemany(
                'INSERT INTO logs (timestamp, ip, method, url, status, size) VALUES (?, ?, ?, ?, ?, ?)',
                batch
            )
            conn.commit()
            batch = []
    
    # Insert remaining
    if batch:
        cursor.executemany(
            'INSERT INTO logs (timestamp, ip, method, url, status, size) VALUES (?, ?, ?, ?, ?, ?)',
            batch
        )
        conn.commit()
    
    conn.close()

# Import once
log_to_sqlite('huge_access.log', 'logs.db')

# Efficient queries
conn = sqlite3.connect('logs.db')
cursor = conn.cursor()

# Top IPs by request count
cursor.execute('''
    SELECT ip, COUNT(*) as count
    FROM logs
    GROUP BY ip
    ORDER BY count DESC
    LIMIT 100
''')

for ip, count in cursor.fetchall():
    print(f"{ip}: {count}")

conn.close()
```

## Parallel Processing

Leverage multiple CPU cores to accelerate log analysis.

### GNU Parallel for Shell Scripts

**Basic parallel processing:**

```bash
# Process multiple log files in parallel
parallel gzip ::: *.log

# Process large file in chunks
cat huge.log | parallel --pipe --block 10M grep "ERROR"

# Parallel processing with results collection
parallel "grep -c ERROR {}" ::: *.log > error_counts.txt

# Specify number of jobs
parallel -j 8 "wc -l {}" ::: *.log
```

**Advanced parallel patterns:**

```bash
# Split file and process chunks in parallel
parallel --pipepart --block 100M -a huge.log grep "failed login"

# Parallel processing with progress bar
parallel --progress --bar gzip ::: *.log

# Process with multiple commands
cat huge.log | parallel --pipe --block 50M "grep ERROR | wc -l"

# Parallel with argument substitution
parallel "python3 analyze.py {} > {.}.analysis" ::: *.log

# Collect and merge results
parallel "awk '{sum+=\$NF} END {print sum}' {}" ::: shard_*.log | awk '{total+=\$1} END {print total}'
```

**Parallel log aggregation:**

```bash
# Count unique IPs across multiple files in parallel
parallel "awk '{print \$1}' {}" ::: access.log.* | sort -u | wc -l

# Parallel grep with merged output
parallel --keep-order "grep -h 'ERROR' {}" ::: *.log > all_errors.log

# Distributed processing across hosts
parallel --sshloginfile nodes.txt "grep ERROR /var/log/app.log" > combined_errors.log
```

### Python Multiprocessing

**Basic multiprocessing pattern:**

```python
from multiprocessing import Pool
import os

def process_log_file(filename):
    """Process single log file"""
    error_count = 0
    with open(filename, 'r') as f:
        for line in f:
            if 'ERROR' in line:
                error_count += 1
    return filename, error_count

def parallel_log_analysis(log_files):
    """Process multiple log files in parallel"""
    cpu_count = os.cpu_count()
    
    with Pool(processes=cpu_count) as pool:
        results = pool.map(process_log_file, log_files)
    
    return results

# Usage
import glob
log_files = glob.glob('/var/log/app/*.log')
results = parallel_log_analysis(log_files)

for filename, count in results:
    print(f"{filename}: {count} errors")
```

**Parallel processing of large single file:**

```python
from multiprocessing import Pool
import os

def count_pattern_in_chunk(args):
    """Process a chunk of the file"""
    filename, start, end, pattern = args
    
    count = 0
    with open(filename, 'r') as f:
        f.seek(start)
        
        # Read until end position
        while f.tell() < end:
            line = f.readline()
            if not line:
                break
            if pattern in line:
                count += 1
    
    return count

def parallel_file_search(filename, pattern, num_processes=None):
    """Search large file in parallel"""
    if num_processes is None:
        num_processes = os.cpu_count()
    
    # Get file size
    file_size = os.path.getsize(filename)
    chunk_size = file_size // num_processes
    
    # Calculate chunk boundaries (align to line boundaries)
    chunks = []
    with open(filename, 'rb') as f:
        for i in range(num_processes):
            start = i * chunk_size
            
            if i > 0:
                # Seek to line boundary
                f.seek(start)
                f.readline()
                start = f.tell()
            
            if i == num_processes - 1:
                end = file_size
            else:
                end = (i + 1) * chunk_size
                f.seek(end)
                f.readline()
                end = f.tell()
            
            chunks.append((filename, start, end, pattern))
    
    # Process chunks in parallel
    with Pool(processes=num_processes) as pool:
        results = pool.map(count_pattern_in_chunk, chunks)
    
    return sum(results)

# Usage
total = parallel_file_search('huge.log', 'ERROR', num_processes=8)
print(f"Total matches: {total}")
```

**Parallel aggregation with shared state:**

```python
from multiprocessing import Pool, Manager
from collections import defaultdict

def process_chunk_to_dict(args):
    """Process chunk and return local results"""
    filename, start, end = args
    
    local_counts = defaultdict(int)
    
    with open(filename, 'r') as f:
        f.seek(start)
        
        while f.tell() < end:
            line = f.readline()
            if not line:
                break
            
            ip = line.split()[0]
            local_counts[ip] += 1
    
    return dict(local_counts)

def merge_dicts(dict_list):
    """Merge multiple dictionaries"""
    result = defaultdict(int)
    for d in dict_list:
        for key, value in d.items():
            result[key] += value
    return dict(result)

def parallel_ip_count(filename, num_processes=None):
    """Count IPs in parallel across file chunks"""
    if num_processes is None:
        num_processes = os.cpu_count()
    
    file_size = os.path.getsize(filename)
    chunk_size = file_size // num_processes
    
    chunks = []
    with open(filename, 'rb') as f:
        for i in range(num_processes):
            start = i * chunk_size
            if i > 0:
                f.seek(start)
                f.readline()
                start = f.tell()
            
            end = file_size if i == num_processes - 1 else (i + 1) * chunk_size
            if i < num_processes - 1:
                f.seek(end)
                f.readline()
                end = f.tell()
            
            chunks.append((filename, start, end))
    
    with Pool(processes=num_processes) as pool:
        results = pool.map(process_chunk_to_dict, chunks)
    
    return merge_dicts(results)

# Usage
ip_counts = parallel_ip_count('huge_access.log', num_processes=8)
sorted_ips = sorted(ip_counts.items(), key=lambda x: x[1], reverse=True)

print("Top 10 IPs:")
for ip, count in sorted_ips[:10]:
    print(f"{ip}: {count}")
```

### Parallel Processing with xargs

**Basic parallel execution:**

```bash
# Process files in parallel with xargs
ls *.log | xargs -P 8 -I {} gzip {}

# Parallel grep across multiple files
find /var/log -name "*.log" | xargs -P 4 -I {} grep -l "ERROR" {}

# Extract data with parallel processing
ls shard_*.log | xargs -P 8 -I {} sh -c 'grep "ERROR" {} | wc -l' > counts.txt
```

**Parallel pipeline processing:**

```bash
# Split large file, process in parallel, merge results
split -n l/8 huge.log chunk_
ls chunk_* | xargs -P 8 -I {} sh -c 'awk "{count++} END {print count}" {} > {}.count'
awk '{sum+=$1} END {print sum}' chunk_*.count
rm chunk_* chunk_*.count
```

### MapReduce Pattern for Logs

**Implementing MapReduce for log analysis:**

```python
from multiprocessing import Pool
from collections import defaultdict
import glob
import os

def read_large_file(filename):
    """Helper generator to read large files line by line"""
    with open(filename, 'r', errors='ignore') as f:
        for line in f:
            yield line

def map_function(filename):
    """Map: Extract key-value pairs from log file"""
    results = []
    
    for line in read_large_file(filename):
        parts = line.split()
        if len(parts) >= 9:
            ip = parts[0]
            status = parts[8]
            results.append(((ip, status), 1))
    
    return results


def reduce_function(key_value_list):
    """Reduce: Aggregate values for same key"""
    key = key_value_list[0][0]
    values = [v for _, v in key_value_list]
    return key, sum(values)


def mapreduce_logs(log_files, num_processes=None):
    """Execute MapReduce on log files"""
    if num_processes is None:
        num_processes = os.cpu_count()

    # Map phase: process files in parallel
    with Pool(processes=num_processes) as pool:
        map_results = pool.map(map_function, log_files)

    # Flatten and group by key
    grouped = defaultdict(list)
    for result_list in map_results:
        for key, value in result_list:
            grouped[key].append((key, value))

    # Reduce phase: aggregate in parallel
    with Pool(processes=num_processes) as pool:
        reduce_results = pool.map(reduce_function, grouped.values())

    return dict(reduce_results)


# Usage
log_files = glob.glob('/var/log/apache2/access.log.*')
results = mapreduce_logs(log_files, num_processes=8)

# Analyze results
for (ip, status), count in sorted(results.items(), key=lambda x: x[1], reverse=True)[:20]:
    print(f"IP: {ip}, Status: {status}, Count: {count}")

````

### Parallel Processing with Ray

Ray provides distributed computing capabilities for advanced parallel processing.

**Installation and basic usage:**

```bash
# Install Ray
pip3 install ray
````

**Ray-based parallel log processing:**

```python
import ray
import glob

ray.init()

@ray.remote
def process_log_file_ray(filename):
    """Ray remote function to process log file"""
    error_count = 0
    warning_count = 0
    
    with open(filename, 'r') as f:
        for line in f:
            if 'ERROR' in line:
                error_count += 1
            elif 'WARNING' in line:
                warning_count += 1
    
    return {
        'filename': filename,
        'errors': error_count,
        'warnings': warning_count
    }

def parallel_analysis_ray(log_files):
    """Process logs in parallel using Ray"""
    # Submit all tasks
    futures = [process_log_file_ray.remote(f) for f in log_files]
    
    # Gather results
    results = ray.get(futures)
    
    return results

# Usage
log_files = glob.glob('/var/log/app/*.log')
results = parallel_analysis_ray(log_files)

total_errors = sum(r['errors'] for r in results)
total_warnings = sum(r['warnings'] for r in results)

print(f"Total errors: {total_errors}")
print(f"Total warnings: {total_warnings}")

ray.shutdown()
```

**Ray for large file chunking:**

```python
import ray
import os

ray.init()

@ray.remote
def process_chunk_ray(filename, start, end):
    """Process a file chunk"""
    matches = []
    
    with open(filename, 'rb') as f:
        f.seek(start)
        
        # Align to line boundary
        if start != 0:
            f.readline()
        
        while f.tell() < end:
            line = f.readline()
            if not line:
                break
            
            line_str = line.decode('utf-8', errors='ignore')
            if 'failed' in line_str.lower():
                matches.append(line_str.strip())
    
    return matches

def ray_parallel_search(filename, num_chunks=8):
    """Search large file using Ray"""
    file_size = os.path.getsize(filename)
    chunk_size = file_size // num_chunks
    
    futures = []
    for i in range(num_chunks):
        start = i * chunk_size
        end = file_size if i == num_chunks - 1 else (i + 1) * chunk_size
        
        futures.append(process_chunk_ray.remote(filename, start, end))
    
    # Gather all results
    results = ray.get(futures)
    
    # Flatten results
    all_matches = []
    for chunk_results in results:
        all_matches.extend(chunk_results)
    
    return all_matches

# Usage
matches = ray_parallel_search('huge.log', num_chunks=16)
print(f"Found {len(matches)} matching lines")

ray.shutdown()
```

### Thread-Based Parallelism for I/O-Bound Tasks

**[Inference] For I/O-bound operations like reading multiple files or network requests, threads may be more efficient than processes due to lower overhead.**

```python
from concurrent.futures import ThreadPoolExecutor, as_completed
import glob

def process_log_threaded(filename):
    """Process log file (I/O bound)"""
    try:
        with open(filename, 'r') as f:
            lines = sum(1 for _ in f)
            f.seek(0)
            errors = sum(1 for line in f if 'ERROR' in line)
        
        return filename, lines, errors
    except Exception as e:
        return filename, 0, 0

def parallel_io_processing(log_files, max_workers=20):
    """Process multiple files using threads"""
    results = []
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit all tasks
        future_to_file = {executor.submit(process_log_threaded, f): f for f in log_files}
        
        # Process completed tasks
        for future in as_completed(future_to_file):
            filename = future_to_file[future]
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                print(f"Error processing {filename}: {e}")
    
    return results

# Usage
log_files = glob.glob('/var/log/**/*.log', recursive=True)
results = parallel_io_processing(log_files, max_workers=30)

for filename, lines, errors in results:
    if errors > 0:
        print(f"{filename}: {lines} lines, {errors} errors")
```

### Performance Monitoring and Optimization

**Profiling parallel code:**

```python
import time
from multiprocessing import Pool
import os

def benchmark_parallel(function, args_list, num_processes_list):
    """Benchmark function with different process counts"""
    results = {}
    
    for num_proc in num_processes_list:
        start_time = time.time()
        
        with Pool(processes=num_proc) as pool:
            pool.map(function, args_list)
        
        elapsed = time.time() - start_time
        results[num_proc] = elapsed
        
        print(f"Processes: {num_proc}, Time: {elapsed:.2f}s")
    
    return results

# Example function
def cpu_bound_task(n):
    return sum(i*i for i in range(n))

# Benchmark
test_data = [1000000] * 100
process_counts = [1, 2, 4, 8, 16]

results = benchmark_parallel(cpu_bound_task, test_data, process_counts)

# Find optimal process count
optimal = min(results.items(), key=lambda x: x[1])
print(f"\nOptimal process count: {optimal[0]} ({optimal[1]:.2f}s)")
```

**Resource usage monitoring:**

```python
import psutil
import os
from multiprocessing import Pool

def monitor_resources(func, args):
    """Monitor CPU and memory during parallel execution"""
    process = psutil.Process(os.getpid())
    
    # Initial measurements
    start_cpu = psutil.cpu_percent(interval=1)
    start_mem = process.memory_info().rss / 1024 / 1024  # MB
    
    # Execute function
    result = func(args)
    
    # Final measurements
    end_cpu = psutil.cpu_percent(interval=1)
    end_mem = process.memory_info().rss / 1024 / 1024  # MB
    
    print(f"CPU: {start_cpu:.1f}% -> {end_cpu:.1f}%")
    print(f"Memory: {start_mem:.1f}MB -> {end_mem:.1f}MB ({end_mem-start_mem:.1f}MB)")
    
    return result
```

### Load Balancing Strategies

**Dynamic work distribution:**

```python
from multiprocessing import Pool, Queue, Process
import queue
import os

def worker_with_queue(task_queue, result_queue):
    """Worker that pulls tasks from queue"""
    while True:
        try:
            task = task_queue.get(timeout=1)
            if task is None:
                break
            
            # Process task
            filename = task
            count = sum(1 for line in open(filename) if 'ERROR' in line)
            result_queue.put((filename, count))
            
        except queue.Empty:
            continue

def dynamic_parallel_processing(log_files, num_workers=None):
    """Process files with dynamic load balancing"""
    if num_workers is None:
        num_workers = os.cpu_count()
    
    from multiprocessing import Manager
    manager = Manager()
    task_queue = manager.Queue()
    result_queue = manager.Queue()
    
    # Add tasks to queue
    for log_file in log_files:
        task_queue.put(log_file)
    
    # Add sentinel values
    for _ in range(num_workers):
        task_queue.put(None)
    
    # Start workers
    workers = []
    for _ in range(num_workers):
        p = Process(target=worker_with_queue, args=(task_queue, result_queue))
        p.start()
        workers.append(p)
    
    # Wait for completion
    for p in workers:
        p.join()
    
    # Collect results
    results = []
    while not result_queue.empty():
        results.append(result_queue.get())
    
    return results

# Usage
import glob
log_files = glob.glob('/var/log/app/*.log')
results = dynamic_parallel_processing(log_files, num_workers=8)

for filename, count in results:
    print(f"{filename}: {count} errors")
```

### Optimizing Parallel I/O

**Buffered reading for better throughput:**

```python
import os
from multiprocessing import Pool

def buffered_file_processing(filename, buffer_size=1024*1024):
    """Process file with optimized buffer size"""
    count = 0
    
    with open(filename, 'r', buffering=buffer_size) as f:
        for line in f:
            if 'ERROR' in line:
                count += 1
    
    return filename, count

def optimal_buffer_benchmark(filename):
    """Find optimal buffer size"""
    import time
    
    buffer_sizes = [8192, 65536, 262144, 1048576, 4194304]  # 8KB to 4MB
    results = {}
    
    for buf_size in buffer_sizes:
        start = time.time()
        buffered_file_processing(filename, buf_size)
        elapsed = time.time() - start
        results[buf_size] = elapsed
        
        print(f"Buffer: {buf_size//1024}KB, Time: {elapsed:.3f}s")
    
    return results
```

**Parallel processing with memory-mapped files:**

```python
import mmap
from multiprocessing import Pool
import os

def process_mmap_chunk(args):
    """Process chunk of memory-mapped file"""
    filename, start, end = args
    count = 0
    
    with open(filename, 'rb') as f:
        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mmapped:
            # Find line boundaries
            if start > 0:
                # Move to next newline
                pos = mmapped.find(b'\n', start)
                start = pos + 1 if pos != -1 else start
            
            # Process chunk
            chunk = mmapped[start:end]
            count = chunk.count(b'ERROR')
    
    return count

def parallel_mmap_search(filename, num_processes=None):
    """Search using parallel memory-mapped access"""
    if num_processes is None:
        num_processes = os.cpu_count()
    
    file_size = os.path.getsize(filename)
    chunk_size = file_size // num_processes
    
    chunks = []
    for i in range(num_processes):
        start = i * chunk_size
        end = file_size if i == num_processes - 1 else (i + 1) * chunk_size
        chunks.append((filename, start, end))
    
    with Pool(processes=num_processes) as pool:
        results = pool.map(process_mmap_chunk, chunks)
    
    return sum(results)

# Usage
total = parallel_mmap_search('huge.log', num_processes=8)
print(f"Total matches: {total}")
```

### Handling Edge Cases in Parallel Processing

**Managing file locks and race conditions:**

```python
import fcntl
from multiprocessing import Pool, Lock
import os

def safe_append_to_file(output_file, data, file_lock):
    """Thread-safe file writing"""
    with file_lock:
        with open(output_file, 'a') as f:
            # Acquire exclusive lock
            fcntl.flock(f.fileno(), fcntl.LOCK_EX)
            try:
                f.write(data + '\n')
            finally:
                fcntl.flock(f.fileno(), fcntl.LOCK_UN)

def process_with_safe_output(args):
    """Process file and write results safely"""
    filename, output_file, lock = args
    
    # Process file
    result = f"Processed {filename}: {os.path.getsize(filename)} bytes"
    
    # Safe write
    safe_append_to_file(output_file, result, lock)
    
    return True
```

**Error handling in parallel contexts:**

```python
from multiprocessing import Pool
import traceback

def safe_process_log(filename):
    """Process log with error handling"""
    try:
        with open(filename, 'r') as f:
            count = sum(1 for line in f if 'ERROR' in line)
        return {'filename': filename, 'count': count, 'error': None}
    except Exception as e:
        return {
            'filename': filename,
            'count': 0,
            'error': str(e),
            'traceback': traceback.format_exc()
        }

def robust_parallel_processing(log_files, num_processes=8):
    """Parallel processing with comprehensive error handling"""
    with Pool(processes=num_processes) as pool:
        results = pool.map(safe_process_log, log_files)
    
    # Separate successful and failed results
    successful = [r for r in results if r['error'] is None]
    failed = [r for r in results if r['error'] is not None]
    
    print(f"Successful: {len(successful)}, Failed: {len(failed)}")
    
    for failure in failed:
        print(f"Error in {failure['filename']}: {failure['error']}")
    
    return successful, failed
```

### Combining Techniques for Maximum Performance

**Hybrid approach: Parallel + streaming + memory-mapped:**

```python
import mmap
import os
from multiprocessing import Pool
from collections import defaultdict

def hybrid_log_analysis(filename, num_processes=None):
    """Combine multiple optimization techniques"""
    if num_processes is None:
        num_processes = os.cpu_count()
    
    file_size = os.path.getsize(filename)
    chunk_size = file_size // num_processes
    
    def process_hybrid_chunk(args):
        """Chunk processor using mmap and streaming"""
        filename, start, end = args
        local_stats = defaultdict(int)
        
        with open(filename, 'rb') as f:
            # Memory map the file
            with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mmapped:
                # Align to line boundaries
                if start > 0:
                    pos = mmapped.find(b'\n', start)
                    start = pos + 1 if pos != -1 else start
                
                # Stream process the chunk
                current = start
                while current < end:
                    # Find next newline
                    next_nl = mmapped.find(b'\n', current, end)
                    if next_nl == -1:
                        break
                    
                    line = mmapped[current:next_nl].decode('utf-8', errors='ignore')
                    
                    # Analyze line
                    if line:
                        parts = line.split()
                        if len(parts) > 0:
                            ip = parts[0]
                            local_stats[ip] += 1
                    
                    current = next_nl + 1
        
        return dict(local_stats)
    
    # Prepare chunks
    chunks = []
    for i in range(num_processes):
        start = i * chunk_size
        end = file_size if i == num_processes - 1 else (i + 1) * chunk_size
        chunks.append((filename, start, end))
    
    # Parallel processing
    with Pool(processes=num_processes) as pool:
        results = pool.map(process_hybrid_chunk, chunks)
    
    # Merge results
    final_stats = defaultdict(int)
    for chunk_stats in results:
        for ip, count in chunk_stats.items():
            final_stats[ip] += count
    
    return dict(final_stats)

# Usage
ip_counts = hybrid_log_analysis('huge_access.log', num_processes=8)
top_ips = sorted(ip_counts.items(), key=lambda x: x[1], reverse=True)[:10]

print("Top 10 IPs:")
for ip, count in top_ips:
    print(f"{ip}: {count}")
```

### Performance Comparison Summary

**Typical performance characteristics - [Inference] based on common hardware configurations:**

|Technique|File Size|Processing Time|Memory Usage|Best Use Case|
|---|---|---|---|---|
|Sequential read|10GB|~200s|Low (constant)|Small files, simple operations|
|Parallel (8 cores)|10GB|~30s|Medium (per-process)|CPU-bound analysis|
|Memory-mapped|10GB|~150s|Low (virtual)|Random access patterns|
|Streaming|10GB|~180s|Very low|Limited memory environments|
|Hybrid parallel+mmap|10GB|~25s|Medium|Large files, complex analysis|

**Selecting the right approach:**

```bash
# Decision tree for optimization strategy

# File size < 100MB
     Use standard tools (grep, awk)

# File size 100MB - 1GB
     Sequential streaming with efficient parsing

# File size 1GB - 10GB
     Parallel processing OR memory-mapped access

# File size > 10GB
     Hybrid: Parallel + memory-mapped + streaming
     Consider splitting and indexing

# Multiple files
     GNU Parallel or Python multiprocessing

# Network-distributed logs
     Ray or distributed frameworks

# Real-time processing
     Streaming with bounded buffers
```

---

**Related topics:** GPU acceleration for log pattern matching using CUDA, distributed log processing with Apache Spark, real-time streaming analytics with Apache Flink, database query optimization for log storage (ClickHouse, TimescaleDB).

---

## Indexed Searching

### Core Concepts

Indexed searching dramatically reduces search time by creating data structures that enable rapid lookups without scanning entire log files. For CTF scenarios involving large log datasets (multi-GB files), indexing can reduce search times from minutes to milliseconds.

### File-Based Indexing

#### Line-Based Index Creation

```bash
#!/bin/bash
# Create line offset index for rapid random access

logfile="$1"
indexfile="${logfile}.idx"

echo "[*] Creating line index for $logfile"

awk '{print NR-1, length($0)+1}' "$logfile" | \
    awk '{offset+=$2; print $1, offset-$2}' > "$indexfile"

echo "[+] Index created: $indexfile"
echo "[*] Index size: $(wc -l < "$indexfile") lines"
```

#### Python Line Index Implementation

```python
#!/usr/bin/env python3
import os
import pickle

class LineIndex:
    def __init__(self, logfile):
        self.logfile = logfile
        self.index_file = f"{logfile}.idx"
        self.offsets = []
    
    def build_index(self):
        """Build byte offset index for each line"""
        print(f"[*] Building index for {self.logfile}")
        
        self.offsets = [0]  # First line starts at byte 0
        
        with open(self.logfile, 'rb') as f:
            while f.readline():
                self.offsets.append(f.tell())
        
        # Remove last offset (EOF)
        self.offsets.pop()
        
        # Save index
        with open(self.index_file, 'wb') as f:
            pickle.dump(self.offsets, f)
        
        print(f"[+] Indexed {len(self.offsets)} lines")
        return len(self.offsets)
    
    def load_index(self):
        """Load existing index"""
        if not os.path.exists(self.index_file):
            return False
        
        with open(self.index_file, 'rb') as f:
            self.offsets = pickle.load(f)
        
        print(f"[+] Loaded index: {len(self.offsets)} lines")
        return True
    
    def get_line(self, line_number):
        """Retrieve specific line by number (0-indexed)"""
        if line_number < 0 or line_number >= len(self.offsets):
            return None
        
        with open(self.logfile, 'rb') as f:
            f.seek(self.offsets[line_number])
            return f.readline().decode('utf-8', errors='ignore').strip()
    
    def get_lines(self, start, end):
        """Retrieve range of lines"""
        if start < 0 or end > len(self.offsets):
            return []
        
        lines = []
        with open(self.logfile, 'rb') as f:
            for i in range(start, end):
                f.seek(self.offsets[i])
                lines.append(f.readline().decode('utf-8', errors='ignore').strip())
        
        return lines
    
    def search_with_context(self, pattern, context=5):
        """Search and return results with surrounding context"""
        import re
        regex = re.compile(pattern)
        results = []
        
        with open(self.logfile, 'r', encoding='utf-8', errors='ignore') as f:
            for i, line in enumerate(f):
                if regex.search(line):
                    # Get context lines efficiently
                    before = self.get_lines(max(0, i-context), i)
                    after = self.get_lines(i+1, min(len(self.offsets), i+context+1))
                    
                    results.append({
                        'line_number': i,
                        'match': line.strip(),
                        'before': before,
                        'after': after
                    })
        
        return results

# Usage example
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print(f"Usage: {sys.argv[0]} <logfile> [command]")
        print("\nCommands:")
        print("  build              : Build index")
        print("  get <line_num>     : Get specific line")
        print("  range <start> <end>: Get line range")
        print("  search <pattern>   : Search with context")
        sys.exit(1)
    
    logfile = sys.argv[1]
    index = LineIndex(logfile)
    
    # Load or build index
    if not index.load_index():
        index.build_index()
    
    if len(sys.argv) > 2:
        command = sys.argv[2]
        
        if command == "build":
            index.build_index()
        
        elif command == "get" and len(sys.argv) > 3:
            line = index.get_line(int(sys.argv[3]))
            print(line)
        
        elif command == "range" and len(sys.argv) > 4:
            lines = index.get_lines(int(sys.argv[3]), int(sys.argv[4]))
            for i, line in enumerate(lines, int(sys.argv[3])):
                print(f"{i:6d}: {line}")
        
        elif command == "search" and len(sys.argv) > 3:
            results = index.search_with_context(sys.argv[3])
            for result in results[:10]:  # Show first 10
                print(f"\n[Line {result['line_number']}]")
                print(f"  {result['match']}")
```

### Hash-Based Indexing

#### IP Address Index

```python
#!/usr/bin/env python3
import re
import json
from collections import defaultdict

class IPIndex:
    def __init__(self, logfile):
        self.logfile = logfile
        self.index_file = f"{logfile}.ip_idx.json"
        self.index = defaultdict(list)  # ip -> [line_numbers]
    
    def build_index(self):
        """Build IP-to-line-number index"""
        print(f"[*] Building IP index for {self.logfile}")
        
        ip_pattern = re.compile(r'\b(?:\d{1,3}\.){3}\d{1,3}\b')
        
        with open(self.logfile, 'r', encoding='utf-8', errors='ignore') as f:
            for line_num, line in enumerate(f):
                ips = ip_pattern.findall(line)
                for ip in ips:
                    self.index[ip].append(line_num)
        
        # Save index
        with open(self.index_file, 'w') as f:
            json.dump(self.index, f)
        
        print(f"[+] Indexed {len(self.index)} unique IPs")
        return len(self.index)
    
    def load_index(self):
        """Load existing index"""
        try:
            with open(self.index_file, 'r') as f:
                self.index = json.load(f)
            print(f"[+] Loaded IP index: {len(self.index)} IPs")
            return True
        except FileNotFoundError:
            return False
    
    def search_ip(self, ip):
        """Find all lines containing specific IP"""
        line_numbers = self.index.get(ip, [])
        
        if not line_numbers:
            return []
        
        results = []
        with open(self.logfile, 'r', encoding='utf-8', errors='ignore') as f:
            all_lines = f.readlines()
            for line_num in line_numbers:
                if line_num < len(all_lines):
                    results.append({
                        'line_number': line_num,
                        'content': all_lines[line_num].strip()
                    })
        
        return results
    
    def get_top_ips(self, n=10):
        """Get top N most active IPs"""
        ip_counts = [(ip, len(lines)) for ip, lines in self.index.items()]
        ip_counts.sort(key=lambda x: x[1], reverse=True)
        return ip_counts[:n]

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 3:
        print(f"Usage: {sys.argv[0]} <logfile> <command>")
        print("\nCommands:")
        print("  build          : Build IP index")
        print("  search <ip>    : Find all lines with IP")
        print("  top [n]        : Show top N IPs")
        sys.exit(1)
    
    logfile = sys.argv[1]
    command = sys.argv[2]
    
    index = IPIndex(logfile)
    
    if command == "build":
        index.build_index()
    
    elif command == "search" and len(sys.argv) > 3:
        if not index.load_index():
            print("[!] Index not found. Building...")
            index.build_index()
        
        results = index.search_ip(sys.argv[3])
        print(f"[*] Found {len(results)} lines with IP {sys.argv[3]}")
        for result in results[:20]:  # Show first 20
            print(f"  Line {result['line_number']:6d}: {result['content'][:100]}")
    
    elif command == "top":
        if not index.load_index():
            print("[!] Index not found. Building...")
            index.build_index()
        
        n = int(sys.argv[3]) if len(sys.argv) > 3 else 10
        top_ips = index.get_top_ips(n)
        
        print(f"[*] Top {n} IPs:")
        for i, (ip, count) in enumerate(top_ips, 1):
            print(f"  {i:2d}. {ip:<15} : {count:>6} occurrences")
```

### Timestamp-Based Indexing

```python
#!/usr/bin/env python3
from datetime import datetime
from dateutil import parser as date_parser
import bisect
import pickle

class TimeIndex:
    def __init__(self, logfile):
        self.logfile = logfile
        self.index_file = f"{logfile}.time_idx"
        self.timestamps = []  # Sorted list of (timestamp, line_number)
    
    def parse_timestamp(self, line):
        """Extract and parse timestamp from log line"""
        # [Inference] These patterns work for common log formats
        # but may need adjustment for specific log types
        
        import re
        
        # Apache/Nginx format: [29/Oct/2025:10:15:32 +0000]
        apache_match = re.search(r'\[(\d{2}/\w{3}/\d{4}:\d{2}:\d{2}:\d{2}[^\]]*)\]', line)
        if apache_match:
            try:
                return datetime.strptime(apache_match.group(1), '%d/%b/%Y:%H:%M:%S %z')
            except:
                pass
        
        # Syslog format: Oct 29 10:15:32
        syslog_match = re.search(r'^(\w{3}\s+\d{1,2}\s+\d{2}:\d{2}:\d{2})', line)
        if syslog_match:
            try:
                # Add current year
                ts_str = f"{syslog_match.group(1)} {datetime.now().year}"
                return datetime.strptime(ts_str, '%b %d %H:%M:%S %Y')
            except:
                pass
        
        # ISO 8601: 2025-10-29T10:15:32
        iso_match = re.search(r'(\d{4}-\d{2}-\d{2}[T\s]\d{2}:\d{2}:\d{2})', line)
        if iso_match:
            try:
                return date_parser.parse(iso_match.group(1))
            except:
                pass
        
        return None
    
    def build_index(self):
        """Build timestamp index"""
        print(f"[*] Building timestamp index for {self.logfile}")
        
        with open(self.logfile, 'r', encoding='utf-8', errors='ignore') as f:
            for line_num, line in enumerate(f):
                ts = self.parse_timestamp(line)
                if ts:
                    self.timestamps.append((ts, line_num))
        
        # Sort by timestamp
        self.timestamps.sort(key=lambda x: x[0])
        
        # Save index
        with open(self.index_file, 'wb') as f:
            pickle.dump(self.timestamps, f)
        
        print(f"[+] Indexed {len(self.timestamps)} timestamped lines")
        return len(self.timestamps)
    
    def load_index(self):
        """Load existing index"""
        try:
            with open(self.index_file, 'rb') as f:
                self.timestamps = pickle.load(f)
            print(f"[+] Loaded timestamp index: {len(self.timestamps)} entries")
            return True
        except FileNotFoundError:
            return False
    
    def search_time_range(self, start_time, end_time):
        """Find all lines within time range"""
        if isinstance(start_time, str):
            start_time = date_parser.parse(start_time)
        if isinstance(end_time, str):
            end_time = date_parser.parse(end_time)
        
        # Binary search for start and end positions
        times_only = [ts for ts, _ in self.timestamps]
        start_idx = bisect.bisect_left(times_only, start_time)
        end_idx = bisect.bisect_right(times_only, end_time)
        
        matching_lines = [line_num for _, line_num in self.timestamps[start_idx:end_idx]]
        
        # Retrieve lines
        results = []
        with open(self.logfile, 'r', encoding='utf-8', errors='ignore') as f:
            all_lines = f.readlines()
            for line_num in matching_lines:
                if line_num < len(all_lines):
                    results.append({
                        'line_number': line_num,
                        'timestamp': self.timestamps[start_idx + len(results)][0],
                        'content': all_lines[line_num].strip()
                    })
        
        return results
    
    def get_events_around(self, target_time, window_minutes=5):
        """Get events within time window around target"""
        from datetime import timedelta
        
        if isinstance(target_time, str):
            target_time = date_parser.parse(target_time)
        
        start = target_time - timedelta(minutes=window_minutes)
        end = target_time + timedelta(minutes=window_minutes)
        
        return self.search_time_range(start, end)

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 3:
        print(f"Usage: {sys.argv[0]} <logfile> <command>")
        print("\nCommands:")
        print("  build                           : Build timestamp index")
        print('  range <start_time> <end_time>   : Search time range')
        print('  around <time> [window_minutes]  : Events around time')
        print("\nTime format examples:")
        print('  "2025-10-29 10:00:00"')
        print('  "Oct 29 10:00:00"')
        sys.exit(1)
    
    logfile = sys.argv[1]
    command = sys.argv[2]
    
    index = TimeIndex(logfile)
    
    if command == "build":
        index.build_index()
    
    elif command == "range" and len(sys.argv) > 4:
        if not index.load_index():
            print("[!] Index not found. Building...")
            index.build_index()
        
        results = index.search_time_range(sys.argv[3], sys.argv[4])
        print(f"[*] Found {len(results)} events in time range")
        for result in results:
            print(f"  [{result['timestamp']}] Line {result['line_number']:6d}: {result['content'][:80]}")
    
    elif command == "around" and len(sys.argv) > 3:
        if not index.load_index():
            print("[!] Index not found. Building...")
            index.build_index()
        
        window = int(sys.argv[4]) if len(sys.argv) > 4 else 5
        results = index.get_events_around(sys.argv[3], window)
        
        print(f"[*] Events within {window} minutes of {sys.argv[3]}:")
        for result in results:
            print(f"  [{result['timestamp']}] {result['content'][:80]}")
```

### Full-Text Search with SQLite

```python
#!/usr/bin/env python3
import sqlite3
import re

class SQLiteLogIndex:
    def __init__(self, logfile, db_file=None):
        self.logfile = logfile
        self.db_file = db_file or f"{logfile}.db"
        self.conn = None
    
    def create_database(self):
        """Create SQLite database with FTS5 (Full-Text Search)"""
        print(f"[*] Creating database: {self.db_file}")
        
        self.conn = sqlite3.connect(self.db_file)
        cursor = self.conn.cursor()
        
        # Create FTS5 virtual table for full-text search
        cursor.execute('''
            CREATE VIRTUAL TABLE IF NOT EXISTS logs USING fts5(
                line_number,
                timestamp,
                ip,
                content,
                tokenize='porter'
            )
        ''')
        
        # Create regular table for metadata
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS metadata (
                line_number INTEGER PRIMARY KEY,
                status_code INTEGER,
                method TEXT,
                url TEXT,
                size INTEGER
            )
        ''')
        
        self.conn.commit()
        print("[+] Database created")
    
    def index_apache_log(self):
        """Index Apache/Nginx logs"""
        print(f"[*] Indexing {self.logfile}")
        
        if not self.conn:
            self.create_database()
        
        cursor = self.conn.cursor()
        
        apache_pattern = re.compile(
            r'([\d.]+) \S+ \S+ \[([^\]]+)\] "(\w+) ([^"]+) [^"]+" (\d+) (\d+)'
        )
        
        batch = []
        batch_size = 1000
        
        with open(self.logfile, 'r', encoding='utf-8', errors='ignore') as f:
            for line_num, line in enumerate(f):
                match = apache_pattern.search(line)
                
                if match:
                    ip, timestamp, method, url, status, size = match.groups()
                    
                    # Add to FTS table
                    batch.append((
                        line_num,
                        timestamp,
                        ip,
                        line.strip()
                    ))
                    
                    # Add to metadata table
                    cursor.execute('''
                        INSERT INTO metadata VALUES (?, ?, ?, ?, ?)
                    ''', (line_num, int(status), method, url, int(size)))
                
                if len(batch) >= batch_size:
                    cursor.executemany('''
                        INSERT INTO logs VALUES (?, ?, ?, ?)
                    ''', batch)
                    self.conn.commit()
                    batch = []
                    print(f"  Indexed {line_num + 1} lines...", end='\r')
        
        # Insert remaining
        if batch:
            cursor.executemany('INSERT INTO logs VALUES (?, ?, ?, ?)', batch)
            self.conn.commit()
        
        print(f"\n[+] Indexed {line_num + 1} lines")
    
    def search(self, query, limit=100):
        """Full-text search"""
        if not self.conn:
            self.conn = sqlite3.connect(self.db_file)
        
        cursor = self.conn.cursor()
        
        # FTS5 MATCH query
        cursor.execute('''
            SELECT line_number, timestamp, ip, content
            FROM logs
            WHERE content MATCH ?
            LIMIT ?
        ''', (query, limit))
        
        results = cursor.fetchall()
        return [
            {
                'line_number': row[0],
                'timestamp': row[1],
                'ip': row[2],
                'content': row[3]
            }
            for row in results
        ]
    
    def search_with_metadata(self, text_query=None, status_code=None, 
                            method=None, url_pattern=None, limit=100):
        """Combined full-text and metadata search"""
        if not self.conn:
            self.conn = sqlite3.connect(self.db_file)
        
        cursor = self.conn.cursor()
        
        # Build query
        conditions = []
        params = []
        
        query = '''
            SELECT l.line_number, l.timestamp, l.ip, l.content,
                   m.status_code, m.method, m.url, m.size
            FROM logs l
            LEFT JOIN metadata m ON l.line_number = m.line_number
            WHERE 1=1
        '''
        
        if text_query:
            query += ' AND l.content MATCH ?'
            params.append(text_query)
        
        if status_code:
            query += ' AND m.status_code = ?'
            params.append(status_code)
        
        if method:
            query += ' AND m.method = ?'
            params.append(method)
        
        if url_pattern:
            query += ' AND m.url LIKE ?'
            params.append(f'%{url_pattern}%')
        
        query += f' LIMIT {limit}'
        
        cursor.execute(query, params)
        results = cursor.fetchall()
        
        return [
            {
                'line_number': row[0],
                'timestamp': row[1],
                'ip': row[2],
                'content': row[3],
                'status_code': row[4],
                'method': row[5],
                'url': row[6],
                'size': row[7]
            }
            for row in results
        ]
    
    def close(self):
        """Close database connection"""
        if self.conn:
            self.conn.close()

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 3:
        print(f"Usage: {sys.argv[0]} <logfile> <command>")
        print("\nCommands:")
        print("  index                  : Create and populate database")
        print("  search <query>         : Full-text search")
        print("  filter <options>       : Advanced filtering")
        print("\nFilter options:")
        print("  --text <query>         : Text search")
        print("  --status <code>        : HTTP status code")
        print("  --method <method>      : HTTP method")
        print("  --url <pattern>        : URL pattern")
        sys.exit(1)
    
    logfile = sys.argv[1]
    command = sys.argv[2]
    
    index = SQLiteLogIndex(logfile)
    
    if command == "index":
        index.create_database()
        index.index_apache_log()
    
    elif command == "search" and len(sys.argv) > 3:
        query = sys.argv[3]
        results = index.search(query)
        
        print(f"[*] Found {len(results)} matches for: {query}")
        for result in results[:20]:
            print(f"  Line {result['line_number']:6d} [{result['ip']}]: {result['content'][:80]}")
    
    elif command == "filter":
        # Parse filter options
        filters = {}
        i = 3
        while i < len(sys.argv):
            if sys.argv[i] == '--text' and i + 1 < len(sys.argv):
                filters['text_query'] = sys.argv[i + 1]
                i += 2
            elif sys.argv[i] == '--status' and i + 1 < len(sys.argv):
                filters['status_code'] = int(sys.argv[i + 1])
                i += 2
            elif sys.argv[i] == '--method' and i + 1 < len(sys.argv):
                filters['method'] = sys.argv[i + 1]
                i += 2
            elif sys.argv[i] == '--url' and i + 1 < len(sys.argv):
                filters['url_pattern'] = sys.argv[i + 1]
                i += 2
            else:
                i += 1
        
        results = index.search_with_metadata(**filters)
        
        print(f"[*] Found {len(results)} matches")
        for result in results[:20]:
            print(f"  Line {result['line_number']:6d} [{result['status_code']}] "
                  f"{result['method']} {result['url'][:50]}")
    
    index.close()
```

---

## Caching Strategies

### Memory Caching Basics

#### LRU Cache for Parsed Entries

```python
#!/usr/bin/env python3
from functools import lru_cache
import re

class CachedLogParser:
    def __init__(self, logfile):
        self.logfile = logfile
        self.parse_count = 0
        self.cache_hits = 0
    
    @lru_cache(maxsize=10000)
    def parse_line(self, line):
        """Parse log line with caching"""
        self.parse_count += 1
        
        # Apache log pattern
        pattern = r'([\d.]+) \S+ \S+ \[([^\]]+)\] "(\w+) ([^"]+) [^"]+" (\d+) (\d+)'
        match = re.search(pattern, line)
        
        if match:
            return {
                'ip': match.group(1),
                'timestamp': match.group(2),
                'method': match.group(3),
                'url': match.group(4),
                'status': int(match.group(5)),
                'size': int(match.group(6))
            }
        return None
    
    def get_cache_stats(self):
        """Get cache statistics"""
        cache_info = self.parse_line.cache_info()
        return {
            'hits': cache_info.hits,
            'misses': cache_info.misses,
            'size': cache_info.currsize,
            'maxsize': cache_info.maxsize,
            'hit_rate': cache_info.hits / (cache_info.hits + cache_info.misses) 
                       if (cache_info.hits + cache_info.misses) > 0 else 0
        }

# Usage
if __name__ == "__main__":
    parser = CachedLogParser("access.log")
    
    # First pass
    with open("access.log", 'r') as f:
        for line in f:
            parser.parse_line(line.strip())
    
    stats = parser.get_cache_stats()
    print(f"[*] Cache Statistics:")
    print(f"  Hits: {stats['hits']}")
    print(f"  Misses: {stats['misses']}")
    print(f"  Hit Rate: {stats['hit_rate']:.2%}")
    print(f"  Cache Size: {stats['size']}/{stats['maxsize']}")
```

### File-Level Caching

#### Chunked File Cache

```python
#!/usr/bin/env python3
import os
import hashlib
import pickle
from pathlib import Path

class ChunkedFileCache:
    def __init__(self, cache_dir=".log_cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
    
    def get_file_hash(self, filepath, chunk_size=8192):
        """Get MD5 hash of file (for cache key)"""
        hasher = hashlib.md5()
        with open(filepath, 'rb') as f:
            while chunk := f.read(chunk_size):
                hasher.update(chunk)
        return hasher.hexdigest()
    
    def get_cache_path(self, filepath, operation):
        """Generate cache file path"""
        file_hash = self.get_file_hash(filepath)
        cache_name = f"{file_hash}_{operation}.cache"
        return self.cache_dir / cache_name
    
    def is_cached(self, filepath, operation):
        """Check if cached result exists and is fresh"""
        cache_path = self.get_cache_path(filepath, operation)
        
        if not cache_path.exists():
            return False
        
        # Check if source file is newer than cache
        source_mtime = os.path.getmtime(filepath)
        cache_mtime = os.path.getmtime(cache_path)
        
        return cache_mtime >= source_mtime
    
    def save(self, filepath, operation, data):
        """Save data to cache"""
        cache_path = self.get_cache_path(filepath, operation)
        
        with open(cache_path, 'wb') as f:
            pickle.dump(data, f)
        
        print(f"[+] Cached result saved: {cache_path}")
    
    def load(self, filepath, operation):
        """Load data from cache"""
        cache_path = self.get_cache_path(filepath, operation)
        
        with open(cache_path, 'rb') as f:
            data = pickle.load(f)
        
        print(f"[+] Loaded from cache: {cache_path}")
        return data
    
    def clear(self):
        """Clear all cache files"""
        for cache_file in self.cache_dir.glob("*.cache"):
            cache_file.unlink()
        print(f"[+] Cache cleared")

# Example: Cached IP analysis
class CachedIPAnalyzer:
    def __init__(self, cache_dir=".log_cache"):
        self.cache = ChunkedFileCache(cache_dir)
    
    def analyze_ips(self, logfile):
        """Analyze IPs with caching"""
        operation = "ip_analysis"
        
        # Check cache
        if self.cache.is_cached(logfile, operation):
            return self.cache.load(logfile, operation)
        
        # Perform analysis
        print(f"[*] Analyzing {logfile}...")
        from collections import Counter
        import re
        
        ip_pattern = re.compile(r'\b(?:\d{1,3}\.){3}\d{1,3}\b')
        ip_counts = Counter()
        
        with open(logfile, 'r', encoding='utf-8', errors='ignore') as f:
            for line in f:
                ips = ip_pattern.findall(line)
                ip_counts.update(ips)
        
        result = {
            'total_ips': len(ip_counts),
            'top_ips': ip_counts.most_common(10),
            'total_requests': sum(ip_counts.values())
        }
        
        # Save to cache
        self.cache.save(logfile, operation, result)
        
        return result

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2: 
		    print(f"Usage: {sys.argv[0]} <logfile> [--clear-cache]")
			sys.exit(1)

	logfile = sys.argv[1]
	
	analyzer = CachedIPAnalyzer()
	
	if "--clear-cache" in sys.argv:
	    analyzer.cache.clear()
	
	result = analyzer.analyze_ips(logfile)
	
	print(f"\n[*] IP Analysis Results:")
	print(f"  Total unique IPs: {result['total_ips']}")
	print(f"  Total requests: {result['total_requests']}")
	print(f"\n  Top 10 IPs:")
	for ip, count in result['top_ips']:
	    print(f"    {ip:<15} : {count:>6}")
````

### Redis-Based Distributed Caching

```python
#!/usr/bin/env python3
# [Unverified] Redis functionality requires redis-py library installation
# This example assumes Redis server is running on localhost:6379

import json
import hashlib
from datetime import timedelta

class RedisLogCache:
    """
    Distributed cache using Redis for team CTF scenarios
    [Inference] Useful when multiple analysts work on same logs
    """
    
    def __init__(self, redis_host='localhost', redis_port=6379, ttl=3600):
        try:
            import redis
            self.redis = redis.Redis(
                host=redis_host,
                port=redis_port,
                decode_responses=True
            )
            self.ttl = ttl  # Time to live in seconds
            self.redis.ping()
            print(f"[+] Connected to Redis at {redis_host}:{redis_port}")
        except Exception as e:
            print(f"[!] Redis connection failed: {e}")
            self.redis = None
    
    def generate_key(self, logfile, query):
        """Generate cache key from logfile and query"""
        key_string = f"{logfile}:{query}"
        return f"logcache:{hashlib.md5(key_string.encode()).hexdigest()}"
    
    def get(self, logfile, query):
        """Retrieve cached result"""
        if not self.redis:
            return None
        
        key = self.generate_key(logfile, query)
        cached = self.redis.get(key)
        
        if cached:
            print(f"[+] Cache hit for query: {query}")
            return json.loads(cached)
        
        return None
    
    def set(self, logfile, query, data):
        """Store result in cache"""
        if not self.redis:
            return False
        
        key = self.generate_key(logfile, query)
        
        try:
            self.redis.setex(
                key,
                timedelta(seconds=self.ttl),
                json.dumps(data)
            )
            print(f"[+] Cached result for query: {query}")
            return True
        except Exception as e:
            print(f"[!] Cache save failed: {e}")
            return False
    
    def invalidate(self, logfile, query=None):
        """Invalidate cache entries"""
        if not self.redis:
            return
        
        if query:
            # Invalidate specific query
            key = self.generate_key(logfile, query)
            self.redis.delete(key)
        else:
            # Invalidate all entries for logfile
            pattern = f"logcache:*"
            for key in self.redis.scan_iter(pattern):
                self.redis.delete(key)
        
        print(f"[+] Cache invalidated")
    
    def get_stats(self):
        """Get cache statistics"""
        if not self.redis:
            return {}
        
        info = self.redis.info()
        return {
            'keys': self.redis.dbsize(),
            'memory_used': info.get('used_memory_human', 'N/A'),
            'hits': info.get('keyspace_hits', 0),
            'misses': info.get('keyspace_misses', 0)
        }

# Example usage with search
class CachedLogSearcher:
    def __init__(self, cache=None):
        self.cache = cache or RedisLogCache()
    
    def search(self, logfile, pattern):
        """Search with caching"""
        # Try cache first
        cached_result = self.cache.get(logfile, pattern)
        if cached_result:
            return cached_result
        
        # Perform search
        print(f"[*] Searching for: {pattern}")
        import re
        regex = re.compile(pattern)
        matches = []
        
        with open(logfile, 'r', encoding='utf-8', errors='ignore') as f:
            for line_num, line in enumerate(f):
                if regex.search(line):
                    matches.append({
                        'line_number': line_num,
                        'content': line.strip()
                    })
        
        result = {
            'pattern': pattern,
            'count': len(matches),
            'matches': matches[:100]  # Limit cached matches
        }
        
        # Cache result
        self.cache.set(logfile, pattern, result)
        
        return result

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 3:
        print(f"Usage: {sys.argv[0]} <logfile> <pattern> [--no-cache]")
        sys.exit(1)
    
    logfile = sys.argv[1]
    pattern = sys.argv[2]
    use_cache = "--no-cache" not in sys.argv
    
    cache = RedisLogCache() if use_cache else None
    searcher = CachedLogSearcher(cache)
    
    result = searcher.search(logfile, pattern)
    
    print(f"\n[*] Search Results:")
    print(f"  Pattern: {result['pattern']}")
    print(f"  Matches: {result['count']}")
    print(f"\n  Sample matches:")
    for match in result['matches'][:10]:
        print(f"    Line {match['line_number']:6d}: {match['content'][:80]}")
````

### Result Set Caching

```python
#!/usr/bin/env python3
import json
from pathlib import Path
from datetime import datetime, timedelta

class ResultCache:
    """Cache search results to disk with expiration"""
    
    def __init__(self, cache_dir=".result_cache", ttl_hours=24):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.ttl = timedelta(hours=ttl_hours)
    
    def cache_key(self, **kwargs):
        """Generate cache filename from parameters"""
        key_str = "_".join(f"{k}={v}" for k, v in sorted(kwargs.items()))
        # Sanitize filename
        key_str = "".join(c if c.isalnum() or c in "._-" else "_" for c in key_str)
        return self.cache_dir / f"{key_str}.json"
    
    def save(self, result, **kwargs):
        """Save result with metadata"""
        cache_file = self.cache_key(**kwargs)
        
        cached_data = {
            'timestamp': datetime.now().isoformat(),
            'params': kwargs,
            'result': result
        }
        
        with open(cache_file, 'w') as f:
            json.dump(cached_data, f, indent=2)
        
        print(f"[+] Cached to: {cache_file}")
    
    def load(self, **kwargs):
        """Load cached result if valid"""
        cache_file = self.cache_key(**kwargs)
        
        if not cache_file.exists():
            return None
        
        with open(cache_file, 'r') as f:
            cached_data = json.load(f)
        
        # Check expiration
        cached_time = datetime.fromisoformat(cached_data['timestamp'])
        if datetime.now() - cached_time > self.ttl:
            print(f"[!] Cache expired: {cache_file}")
            cache_file.unlink()
            return None
        
        print(f"[+] Loaded from cache: {cache_file}")
        return cached_data['result']
    
    def clear_expired(self):
        """Remove expired cache entries"""
        removed = 0
        for cache_file in self.cache_dir.glob("*.json"):
            try:
                with open(cache_file, 'r') as f:
                    cached_data = json.load(f)
                
                cached_time = datetime.fromisoformat(cached_data['timestamp'])
                if datetime.now() - cached_time > self.ttl:
                    cache_file.unlink()
                    removed += 1
            except:
                continue
        
        print(f"[+] Removed {removed} expired cache entries")
        return removed
    
    def clear_all(self):
        """Remove all cache entries"""
        removed = 0
        for cache_file in self.cache_dir.glob("*.json"):
            cache_file.unlink()
            removed += 1
        
        print(f"[+] Removed {removed} cache entries")
        return removed
```

---

## Stream Processing

### Line-by-Line Processing

#### Basic Stream Processor

```bash
#!/bin/bash
# Real-time log monitoring with pattern matching

LOGFILE="$1"
PATTERN="$2"

if [[ -z "$LOGFILE" || -z "$PATTERN" ]]; then
    echo "Usage: $0 <logfile> <pattern>"
    exit 1
fi

echo "[*] Monitoring $LOGFILE for pattern: $PATTERN"
echo "[*] Press Ctrl+C to stop"

tail -f "$LOGFILE" | while IFS= read -r line; do
    if echo "$line" | grep -q "$PATTERN"; then
        timestamp=$(date '+%Y-%m-%d %H:%M:%S')
        echo "[$timestamp] MATCH: $line"
        
        # Optional: trigger alert
        # notify-send "Pattern Match" "$line"
    fi
done
```

#### Python Stream Processor

```python
#!/usr/bin/env python3
import sys
import time
import re
from collections import deque

class StreamProcessor:
    def __init__(self, buffer_size=100):
        self.buffer = deque(maxlen=buffer_size)
        self.stats = {
            'processed': 0,
            'matched': 0,
            'errors': 0
        }
    
    def process_line(self, line):
        """Process single line (override in subclass)"""
        self.stats['processed'] += 1
        return line
    
    def process_stream(self, stream=sys.stdin):
        """Process input stream line by line"""
        try:
            for line in stream:
                line = line.rstrip('\n')
                result = self.process_line(line)
                
                if result:
                    self.buffer.append(result)
                    print(result)
                    sys.stdout.flush()
        
        except KeyboardInterrupt:
            print(f"\n[*] Stream processing stopped", file=sys.stderr)
            self.print_stats()
        except Exception as e:
            print(f"[!] Error: {e}", file=sys.stderr)
            self.stats['errors'] += 1
    
    def print_stats(self):
        """Print processing statistics"""
        print(f"\n[*] Statistics:", file=sys.stderr)
        print(f"  Processed: {self.stats['processed']}", file=sys.stderr)
        print(f"  Matched: {self.stats['matched']}", file=sys.stderr)
        print(f"  Errors: {self.stats['errors']}", file=sys.stderr)

class PatternMatcher(StreamProcessor):
    """Stream processor with pattern matching"""
    
    def __init__(self, patterns, buffer_size=100):
        super().__init__(buffer_size)
        self.patterns = [re.compile(p) for p in patterns]
    
    def process_line(self, line):
        """Match line against patterns"""
        self.stats['processed'] += 1
        
        for pattern in self.patterns:
            if pattern.search(line):
                self.stats['matched'] += 1
                return f"[MATCH] {line}"
        
        return None

class AttackDetector(StreamProcessor):
    """Real-time attack detection"""
    
    def __init__(self, buffer_size=100):
        super().__init__(buffer_size)
        
        self.attack_patterns = {
            'sqli': re.compile(r'(?i)(union.*select|insert.*into|drop.*table)'),
            'xss': re.compile(r'(?i)(<script|javascript:|onerror\s*=)'),
            'lfi': re.compile(r'(\.\.[\\/]|/etc/passwd)'),
            'rce': re.compile(r'[;&|`]\s*(?:cat|ls|whoami|wget)'),
        }
        
        self.attack_counts = {k: 0 for k in self.attack_patterns.keys()}
    
    def process_line(self, line):
        """Detect attacks in log line"""
        self.stats['processed'] += 1
        
        detected = []
        for attack_type, pattern in self.attack_patterns.items():
            if pattern.search(line):
                detected.append(attack_type)
                self.attack_counts[attack_type] += 1
        
        if detected:
            self.stats['matched'] += 1
            attacks = ', '.join(detected)
            return f"[ATTACK: {attacks}] {line}"
        
        return None
    
    def print_stats(self):
        """Print detection statistics"""
        super().print_stats()
        print(f"\n[*] Attack Breakdown:", file=sys.stderr)
        for attack_type, count in self.attack_counts.items():
            if count > 0:
                print(f"  {attack_type.upper()}: {count}", file=sys.stderr)

class RateLimitDetector(StreamProcessor):
    """Detect potential rate-based attacks"""
    
    def __init__(self, window_seconds=60, threshold=100, buffer_size=1000):
        super().__init__(buffer_size)
        self.window = window_seconds
        self.threshold = threshold
        self.ip_requests = {}  # ip -> [(timestamp, line), ...]
    
    def extract_ip(self, line):
        """Extract IP from log line"""
        match = re.search(r'\b(?:\d{1,3}\.){3}\d{1,3}\b', line)
        return match.group() if match else None
    
    def process_line(self, line):
        """Track request rates per IP"""
        self.stats['processed'] += 1
        
        ip = self.extract_ip(line)
        if not ip:
            return None
        
        current_time = time.time()
        
        # Initialize or clean old entries
        if ip not in self.ip_requests:
            self.ip_requests[ip] = []
        
        # Remove requests outside time window
        self.ip_requests[ip] = [
            (ts, l) for ts, l in self.ip_requests[ip]
            if current_time - ts <= self.window
        ]
        
        # Add current request
        self.ip_requests[ip].append((current_time, line))
        
        # Check if threshold exceeded
        request_count = len(self.ip_requests[ip])
        if request_count > self.threshold:
            self.stats['matched'] += 1
            return f"[RATE LIMIT: {ip} - {request_count} req/{self.window}s] {line}"
        
        return None

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Stream log processor")
    parser.add_argument('mode', choices=['pattern', 'attack', 'rate'],
                       help="Processing mode")
    parser.add_argument('--pattern', '-p', action='append',
                       help="Pattern to match (for pattern mode)")
    parser.add_argument('--window', type=int, default=60,
                       help="Time window in seconds (for rate mode)")
    parser.add_argument('--threshold', type=int, default=100,
                       help="Request threshold (for rate mode)")
    parser.add_argument('--file', '-f', help="Log file to process (default: stdin)")
    
    args = parser.parse_args()
    
    # Create processor based on mode
    if args.mode == 'pattern':
        if not args.pattern:
            print("Error: --pattern required for pattern mode")
            sys.exit(1)
        processor = PatternMatcher(args.pattern)
    
    elif args.mode == 'attack':
        processor = AttackDetector()
    
    elif args.mode == 'rate':
        processor = RateLimitDetector(
            window_seconds=args.window,
            threshold=args.threshold
        )
    
    # Process stream
    if args.file:
        with open(args.file, 'r') as f:
            processor.process_stream(f)
    else:
        processor.process_stream(sys.stdin)
```

### Chunked Processing

```python
#!/usr/bin/env python3
import sys
from pathlib import Path

class ChunkedProcessor:
    """Process large files in chunks to manage memory"""
    
    def __init__(self, chunk_size=1024*1024):  # 1MB chunks
        self.chunk_size = chunk_size
        self.stats = {
            'chunks': 0,
            'lines': 0,
            'bytes': 0
        }
    
    def process_chunk(self, chunk):
        """Process chunk of data (override in subclass)"""
        return chunk
    
    def process_file(self, filepath):
        """Process file in chunks"""
        file_size = Path(filepath).stat().st_size
        print(f"[*] Processing {filepath} ({file_size:,} bytes)", file=sys.stderr)
        
        with open(filepath, 'rb') as f:
            while chunk := f.read(self.chunk_size):
                self.stats['chunks'] += 1
                self.stats['bytes'] += len(chunk)
                
                # Process chunk
                try:
                    result = self.process_chunk(chunk.decode('utf-8', errors='ignore'))
                    if result:
                        print(result, end='')
                except Exception as e:
                    print(f"[!] Chunk {self.stats['chunks']} error: {e}", 
                          file=sys.stderr)
                
                # Progress indicator
                progress = (self.stats['bytes'] / file_size) * 100
                print(f"\rProgress: {progress:.1f}%", end='', file=sys.stderr)
        
        print(f"\n[+] Processed {self.stats['chunks']} chunks", file=sys.stderr)

class ChunkedGrep(ChunkedProcessor):
    """Grep implementation using chunked processing"""
    
    def __init__(self, pattern, chunk_size=1024*1024):
        super().__init__(chunk_size)
        import re
        self.pattern = re.compile(pattern)
        self.partial_line = ""
        self.matches = 0
    
    def process_chunk(self, chunk):
        """Search for pattern in chunk"""
        # Prepend any partial line from previous chunk
        chunk = self.partial_line + chunk
        
        lines = chunk.split('\n')
        
        # Save last partial line
        self.partial_line = lines[-1]
        lines = lines[:-1]
        
        # Search complete lines
        results = []
        for line in lines:
            self.stats['lines'] += 1
            if self.pattern.search(line):
                self.matches += 1
                results.append(line + '\n')
        
        return ''.join(results)
    
    def finalize(self):
        """Process any remaining partial line"""
        if self.partial_line and self.pattern.search(self.partial_line):
            self.matches += 1
            print(self.partial_line)
        
        print(f"[*] Found {self.matches} matches in {self.stats['lines']} lines",
              file=sys.stderr)

if __name__ == "__main__":
    if len(sys.argv) < 3:
        print(f"Usage: {sys.argv[0]} <pattern> <file> [chunk_size_mb]")
        sys.exit(1)
    
    pattern = sys.argv[1]
    filepath = sys.argv[2]
    chunk_size = int(sys.argv[3]) * 1024 * 1024 if len(sys.argv) > 3 else 1024*1024
    
    processor = ChunkedGrep(pattern, chunk_size)
    processor.process_file(filepath)
    processor.finalize()
```

### Parallel Stream Processing

```python
#!/usr/bin/env python3
import sys
import multiprocessing as mp
from queue import Empty
import time

class ParallelStreamProcessor:
    """Process log streams using multiple worker processes"""
    
    def __init__(self, num_workers=None):
        self.num_workers = num_workers or mp.cpu_count()
        self.input_queue = mp.Queue(maxsize=1000)
        self.output_queue = mp.Queue(maxsize=1000)
        self.workers = []
        self.stats = mp.Manager().dict({
            'processed': 0,
            'matched': 0
        })
    
    def worker_process(self, worker_id):
        """Worker process function"""
        while True:
            try:
                task = self.input_queue.get(timeout=1)
                
                if task is None:  # Poison pill
                    break
                
                line_num, line = task
                result = self.process_line(line)
                
                self.stats['processed'] += 1
                
                if result:
                    self.stats['matched'] += 1
                    self.output_queue.put((line_num, result))
            
            except Empty:
                continue
            except Exception as e:
                print(f"[!] Worker {worker_id} error: {e}", file=sys.stderr)
    
    def process_line(self, line):
        """Override this method in subclass"""
        return line
    
    def start_workers(self):
        """Start worker processes"""
        print(f"[*] Starting {self.num_workers} workers", file=sys.stderr)
        
        for i in range(self.num_workers):
            worker = mp.Process(target=self.worker_process, args=(i,))
            worker.start()
            self.workers.append(worker)
    
    def stop_workers(self):
        """Stop all workers"""
        # Send poison pills
        for _ in range(self.num_workers):
            self.input_queue.put(None)
        
        # Wait for workers to finish
        for worker in self.workers:
            worker.join()
        
        print(f"\n[*] All workers stopped", file=sys.stderr)
    
    def process_file(self, filepath):
        """Process file using parallel workers"""
        self.start_workers()
        
        # Start output collector thread
        import threading
        results = []
        output_done = threading.Event()
        
        def collect_output():
            while not output_done.is_set() or not self.output_queue.empty():
                try:
                    result = self.output_queue.get(timeout=0.1)
                    results.append(result)
                except Empty:
                    continue
        
        collector = threading.Thread(target=collect_output)
        collector.start()
        
        # Feed lines to workers
        try:
            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                for line_num, line in enumerate(f):
                    self.input_queue.put((line_num, line.strip()))
                    
                    if line_num % 10000 == 0:
                        print(f"\rQueued {line_num:,} lines...", 
                              end='', file=sys.stderr)
        
        except KeyboardInterrupt:
            print(f"\n[!] Interrupted", file=sys.stderr)
        
        # Stop workers and collector
        self.stop_workers()
        output_done.set()
        collector.join()
        
        # Sort and output results
        results.sort(key=lambda x: x[0])  # Sort by line number
        for _, result in results:
            print(result)
        
        print(f"[*] Processed: {self.stats['processed']}", file=sys.stderr)
        print(f"[*] Matched: {self.stats['matched']}", file=sys.stderr)

class ParallelAttackDetector(ParallelStreamProcessor):
    """Parallel attack detection"""
    
    def __init__(self, num_workers=None):
        super().__init__(num_workers)
        
        import re
        self.attack_patterns = {
            'sqli': re.compile(r'(?i)(union.*select|insert.*into|drop.*table)'),
            'xss': re.compile(r'(?i)(<script|javascript:|onerror\s*=)'),
            'lfi': re.compile(r'(\.\.[\\/]|/etc/passwd)'),
            'rce': re.compile(r'[;&|`]\s*(?:cat|ls|whoami|wget)'),
        }
    
    def process_line(self, line):
        """Detect attacks"""
        detected = []
        
        for attack_type, pattern in self.attack_patterns.items():
            if pattern.search(line):
                detected.append(attack_type)
        
        if detected:
            attacks = ', '.join(detected)
            return f"[ATTACK: {attacks}] {line}"
        
        return None

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print(f"Usage: {sys.argv[0]} <logfile> [num_workers]")
        sys.exit(1)
    
    logfile = sys.argv[1]
    num_workers = int(sys.argv[2]) if len(sys.argv) > 2 else None
    
    processor = ParallelAttackDetector(num_workers)
    
    start_time = time.time()
    processor.process_file(logfile)
    elapsed = time.time() - start_time
    
    print(f"[*] Total time: {elapsed:.2f}s", file=sys.stderr)
```

### Memory-Mapped File Processing

```python
#!/usr/bin/env python3
import mmap
import re
import sys

class MmapProcessor:
    """Memory-mapped file processing for large files"""
    
    def __init__(self, filepath):
        self.filepath = filepath
        self.mmap_obj = None
        self.file_obj = None
    
    def __enter__(self):
        """Context manager entry"""
        self.file_obj = open(self.filepath, 'r+b')
        self.mmap_obj = mmap.mmap(
            self.file_obj.fileno(),
            0,
            access=mmap.ACCESS_READ
        )
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit"""
        if self.mmap_obj:
            self.mmap_obj.close()
        if self.file_obj:
            self.file_obj.close()
    
    def find_all(self, pattern):
        """Find all occurrences of pattern"""
        if isinstance(pattern, str):
            pattern = pattern.encode('utf-8')
        
        matches = []
        pos = 0
        
        while True:
            pos = self.mmap_obj.find(pattern, pos)
            if pos == -1:
                break
            
            # Find line boundaries
            line_start = self.mmap_obj.rfind(b'\n', 0, pos) + 1
            line_end = self.mmap_obj.find(b'\n', pos)
            if line_end == -1:
                line_end = len(self.mmap_obj)
            
            line = self.mmap_obj[line_start:line_end].decode('utf-8', errors='ignore')
            matches.append({
                'offset': pos,
                'line_start': line_start,
                'content': line
            })
            
            pos = line_end + 1
        
        return matches
    
    def search_regex(self, pattern):
        """Search using regex pattern"""
        regex = re.compile(pattern.encode('utf-8'))
        matches = []
        
        # Process in chunks to avoid memory issues
        chunk_size = 10 * 1024 * 1024  # 10MB
        overlap = 10000  # Overlap to catch patterns spanning chunks
        
        pos = 0
        while pos < len(self.mmap_obj):
            end = min(pos + chunk_size, len(self.mmap_obj))
            chunk = self.mmap_obj[pos:end]
            
            for match in regex.finditer(chunk):
                match_pos = pos + match.start()
                
                # Find line boundaries
                line_start = self.mmap_obj.rfind(b'\n', 0, match_pos) + 1
                line_end = self.mmap_obj.find(b'\n', match_pos)
                if line_end == -1:
                    line_end = len(self.mmap_obj)
                
                line = self.mmap_obj[line_start:line_end].decode('utf-8', errors='ignore')
                matches.append({
                    'offset': match_pos,
                    'match': match.group().decode('utf-8', errors='ignore'),
                    'content': line
                })
            
            pos = end - overlap
        
        return matches

# Example usage
if __name__ == "__main__":
    if len(sys.argv) < 3:
        print(f"Usage: {sys.argv[0]} <logfile> <pattern>")
        sys.exit(1)
    
    logfile = sys.argv[1]
    pattern = sys.argv[2]
    
    print(f"[*] Searching {logfile} for pattern: {pattern}")
    
    with MmapProcessor(logfile) as processor:
        matches = processor.search_regex(pattern)
        
        print(f"[*] Found {len(matches)} matches\n")
        
        for i, match in enumerate(matches[:20], 1):
            print(f"{i:3d}. [Offset: {match['offset']:,}]")
            print(f"     Match: {match.get('match', '')}")
            print(f"     Line: {match['content'][:100]}")
            print()
```

---

## Performance Benchmarking

```python
#!/usr/bin/env python3
import time
import sys
from pathlib import Path

class PerformanceBenchmark:
    """Benchmark different log processing approaches"""
    
    def __init__(self, logfile):
        self.logfile = logfile
        self.file_size = Path(logfile).stat().st_size
        self.results = {}
    
    def benchmark(self, name, func, *args, **kwargs):
        """Run and time a function"""
        print(f"[*] Benchmarking: {name}...", end=' ', flush=True)
        
        start = time.time()
        result = func(*args, **kwargs)
        elapsed = time.time() - start
        
        throughput = self.file_size / elapsed / (1024 * 1024)  # MB/s
        
        self.results[name] = {
            'time': elapsed,
            'throughput': throughput,
            'result': result
        }
        
        print(f"{elapsed:.2f}s ({throughput:.2f} MB/s)")
        return result
    
    def grep_baseline(self, pattern):
        """Baseline: simple grep"""
        import re
        regex = re.compile(pattern)
        count = 0
        
        with open(self.logfile, 'r', encoding='utf-8', errors='ignore') as f:
            for line in f:
                if regex.search(line):
                    count += 1
        
        return count
    
    def grep_with_index(self, pattern):
        """Grep using line index"""
        from functools import lru_cache
        import re
        
        regex = re.compile(pattern)
        count = 0
        
        # Simulate cached line parsing
        @lru_cache(maxsize=10000)
        def parse_cached(line):
            return regex.search(line)
        
        with open(self.logfile, 'r', encoding='utf-8', errors='ignore') as f:
            for line in f:
                if parse_cached(line):
                    count += 1
        
        return count
    
    def grep_mmap(self, pattern):
        """Grep using memory mapping"""
        import mmap
        import re
        
        count = 0
        with open(self.logfile, 'r+b') as f:
            with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
                pattern_bytes = pattern.encode('utf-8')
                pos = 0
                while True:
                    pos = mm.find(pattern_bytes, pos)
                    if pos == -1:
                        break
                    count += 1
                    pos += len(pattern_bytes)
        
        return count
    
    def grep_chunked(self, pattern, chunk_size=1024*1024):
        """Grep using chunked reading"""
        import re
        regex = re.compile(pattern)
        count = 0
        partial = ""
        
        with open(self.logfile, 'rb') as f:
            while chunk := f.read(chunk_size):
                chunk_str = partial + chunk.decode('utf-8', errors='ignore')
                lines = chunk_str.split('\n')
                partial = lines[-1]
                
                for line in lines[:-1]:
                    if regex.search(line):
                        count += 1
        
        if partial and regex.search(partial):
            count += 1
        
        return count
    
    def run_all(self, pattern):
        """Run all benchmarks"""
        print(f"\n{'='*60}")
        print(f"Performance Benchmark")
        print(f"{'='*60}")
        print(f"File: {self.logfile}")
        print(f"Size: {self.file_size:,} bytes ({self.file_size/(1024*1024):.2f} MB)")
        print(f"Pattern: {pattern}")
        print(f"{'='*60}\n")
        
        # Run benchmarks
        self.benchmark("Baseline (simple grep)", self.grep_baseline, pattern)
        self.benchmark("Cached parsing", self.grep_with_index, pattern)
        self.benchmark("Memory-mapped", self.grep_mmap, pattern)
        self.benchmark("Chunked (1MB)", self.grep_chunked, pattern, 1024*1024)
        self.benchmark("Chunked (10MB)", self.grep_chunked, pattern, 10*1024*1024)
        
        # Results summary
        print(f"\n{'='*60}")
        print("Results Summary")
        print(f"{'='*60}\n")
        
        sorted_results = sorted(self.results.items(), key=lambda x: x[1]['time'])
        
        print(f"{'Method':<25} {'Time (s)':<12} {'Throughput':<15} {'Matches'}")
        print("-" * 70)
        
        for name, data in sorted_results:
            print(f"{name:<25} {data['time']:>10.2f}s  {data['throughput']:>10.2f} MB/s  {data['result']:>8,}")
        
        # Speedup comparison
        baseline_time = self.results['Baseline (simple grep)']['time']
        print(f"\n{'Method':<25} {'Speedup'}")
        print("-" * 40)
        
        for name, data in sorted_results:
            speedup = baseline_time / data['time']
            print(f"{name:<25} {speedup:>6.2f}x")

if __name__ == "__main__":
    if len(sys.argv) < 3:
        print(f"Usage: {sys.argv[0]} <logfile> <pattern>")
        sys.exit(1)
    
    benchmark = PerformanceBenchmark(sys.argv[1])
    benchmark.run_all(sys.argv[2])
```

### Advanced Stream Pipeline

```python
#!/usr/bin/env python3
import sys
from abc import ABC, abstractmethod
from collections import deque
import time

class PipelineStage(ABC):
    """Abstract base class for pipeline stages"""
    
    def __init__(self, name):
        self.name = name
        self.processed = 0
        self.passed = 0
    
    @abstractmethod
    def process(self, item):
        """Process item, return None to filter out"""
        pass
    
    def stats(self):
        """Return processing statistics"""
        pass_rate = (self.passed / self.processed * 100) if self.processed > 0 else 0
        return {
            'stage': self.name,
            'processed': self.processed,
            'passed': self.passed,
            'pass_rate': pass_rate
        }

class FilterStage(PipelineStage):
    """Filter stage using predicate function"""
    
    def __init__(self, name, predicate):
        super().__init__(name)
        self.predicate = predicate
    
    def process(self, item):
        self.processed += 1
        if self.predicate(item):
            self.passed += 1
            return item
        return None

class TransformStage(PipelineStage):
    """Transform stage using mapping function"""
    
    def __init__(self, name, transform_func):
        super().__init__(name)
        self.transform_func = transform_func
    
    def process(self, item):
        self.processed += 1
        result = self.transform_func(item)
        if result is not None:
            self.passed += 1
        return result

class AggregateStage(PipelineStage):
    """Aggregate stage that accumulates results"""
    
    def __init__(self, name, aggregate_func, initial=None):
        super().__init__(name)
        self.aggregate_func = aggregate_func
        self.accumulator = initial
    
    def process(self, item):
        self.processed += 1
        self.accumulator = self.aggregate_func(self.accumulator, item)
        self.passed += 1
        return item  # Pass through
    
    def get_result(self):
        return self.accumulator

class Pipeline:
    """Processing pipeline for log streams"""
    
    def __init__(self, stages=None):
        self.stages = stages or []
        self.total_input = 0
        self.total_output = 0
    
    def add_stage(self, stage):
        """Add processing stage"""
        self.stages.append(stage)
        return self
    
    def process(self, stream):
        """Process stream through all stages"""
        for item in stream:
            self.total_input += 1
            
            # Pass through all stages
            current = item
            for stage in self.stages:
                if current is None:
                    break
                current = stage.process(current)
            
            # Yield if item survived all stages
            if current is not None:
                self.total_output += 1
                yield current
    
    def print_stats(self):
        """Print pipeline statistics"""
        print(f"\n{'='*60}", file=sys.stderr)
        print("Pipeline Statistics", file=sys.stderr)
        print(f"{'='*60}", file=sys.stderr)
        print(f"Total Input: {self.total_input:,}", file=sys.stderr)
        print(f"Total Output: {self.total_output:,}", file=sys.stderr)
        print(f"Overall Pass Rate: {(self.total_output/self.total_input*100):.2f}%\n" 
              if self.total_input > 0 else "N/A\n", file=sys.stderr)
        
        print(f"{'Stage':<25} {'Processed':<12} {'Passed':<12} {'Pass Rate'}", 
              file=sys.stderr)
        print("-" * 65, file=sys.stderr)
        
        for stage in self.stages:
            stats = stage.stats()
            print(f"{stats['stage']:<25} {stats['processed']:>10,}  "
                  f"{stats['passed']:>10,}  {stats['pass_rate']:>8.2f}%",
                  file=sys.stderr)

# Example: Log Analysis Pipeline
class LogAnalysisPipeline:
    """Pre-configured pipeline for log analysis"""
    
    @staticmethod
    def create_attack_detection_pipeline():
        """Pipeline for detecting attacks"""
        import re
        
        pipeline = Pipeline()
        
        # Stage 1: Filter non-error lines
        def is_error(line):
            return '404' in line or '403' in line or '500' in line
        
        pipeline.add_stage(FilterStage("Error Filter", is_error))
        
        # Stage 2: Parse log entry
        def parse_apache(line):
            pattern = r'([\d.]+) .* \[([^\]]+)\] "(\w+) ([^"]+)" (\d+)'
            match = re.search(pattern, line)
            if match:
                return {
                    'ip': match.group(1),
                    'timestamp': match.group(2),
                    'method': match.group(3),
                    'url': match.group(4),
                    'status': int(match.group(5)),
                    'raw': line
                }
            return None
        
        pipeline.add_stage(TransformStage("Parser", parse_apache))
        
        # Stage 3: Detect attack patterns
        attack_patterns = {
            'sqli': re.compile(r'(?i)(union.*select|insert.*into)'),
            'xss': re.compile(r'(?i)(<script|javascript:)'),
            'lfi': re.compile(r'(\.\.[\\/]|/etc/passwd)'),
        }
        
        def detect_attacks(entry):
            if not entry:
                return None
            
            attacks = []
            url = entry['url']
            
            for attack_type, pattern in attack_patterns.items():
                if pattern.search(url):
                    attacks.append(attack_type)
            
            if attacks:
                entry['attacks'] = attacks
                return entry
            return None
        
        pipeline.add_stage(TransformStage("Attack Detector", detect_attacks))
        
        # Stage 4: Format output
        def format_output(entry):
            if not entry:
                return None
            
            attacks = ', '.join(entry['attacks'])
            return f"[{attacks.upper()}] {entry['ip']} -> {entry['url']}"
        
        pipeline.add_stage(TransformStage("Formatter", format_output))
        
        return pipeline
    
    @staticmethod
    def create_rate_analysis_pipeline(window=60, threshold=100):
        """Pipeline for rate limiting analysis"""
        from collections import defaultdict
        
        pipeline = Pipeline()
        
        # Stage 1: Parse IP addresses
        def extract_ip(line):
            import re
            match = re.search(r'\b((?:\d{1,3}\.){3}\d{1,3})\b', line)
            if match:
                return {
                    'ip': match.group(1),
                    'timestamp': time.time(),
                    'raw': line
                }
            return None
        
        pipeline.add_stage(TransformStage("IP Extractor", extract_ip))
        
        # Stage 2: Aggregate by IP with sliding window
        ip_tracker = defaultdict(list)
        
        def track_requests(entry):
            if not entry:
                return None
            
            ip = entry['ip']
            current_time = entry['timestamp']
            
            # Add current request
            ip_tracker[ip].append(current_time)
            
            # Remove old requests outside window
            ip_tracker[ip] = [
                ts for ts in ip_tracker[ip]
                if current_time - ts <= window
            ]
            
            entry['request_count'] = len(ip_tracker[ip])
            return entry
        
        pipeline.add_stage(TransformStage("Rate Tracker", track_requests))
        
        # Stage 3: Filter IPs exceeding threshold
        def exceeds_threshold(entry):
            return entry and entry['request_count'] > threshold
        
        pipeline.add_stage(FilterStage("Threshold Filter", exceeds_threshold))
        
        # Stage 4: Format output
        def format_rate_alert(entry):
            if not entry:
                return None
            
            return (f"[RATE LIMIT] {entry['ip']} - "
                   f"{entry['request_count']} requests in {window}s")
        
        pipeline.add_stage(TransformStage("Formatter", format_rate_alert))
        
        return pipeline

# CLI Interface
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Stream processing pipeline")
    parser.add_argument('mode', choices=['attack', 'rate', 'custom'],
                       help="Pipeline mode")
    parser.add_argument('--file', '-f', help="Log file (default: stdin)")
    parser.add_argument('--window', type=int, default=60,
                       help="Time window for rate analysis (seconds)")
    parser.add_argument('--threshold', type=int, default=100,
                       help="Request threshold for rate analysis")
    parser.add_argument('--stats', action='store_true',
                       help="Show pipeline statistics")
    
    args = parser.parse_args()
    
    # Create pipeline
    if args.mode == 'attack':
        pipeline = LogAnalysisPipeline.create_attack_detection_pipeline()
    elif args.mode == 'rate':
        pipeline = LogAnalysisPipeline.create_rate_analysis_pipeline(
            window=args.window,
            threshold=args.threshold
        )
    else:
        print("[!] Custom mode not implemented", file=sys.stderr)
        sys.exit(1)
    
    # Process stream
    try:
        if args.file:
            with open(args.file, 'r', encoding='utf-8', errors='ignore') as f:
                for result in pipeline.process(f):
                    print(result)
        else:
            for result in pipeline.process(sys.stdin):
                print(result)
    
    except KeyboardInterrupt:
        print("\n[*] Interrupted", file=sys.stderr)
    
    # Print statistics
    if args.stats:
        pipeline.print_stats()
```

### Real-Time Dashboard Monitor

```python
#!/usr/bin/env python3
import sys
import time
import curses
from collections import deque, Counter
from datetime import datetime

class LogDashboard:
    """Real-time log monitoring dashboard"""
    
    def __init__(self, stdscr, max_history=1000):
        self.stdscr = stdscr
        self.max_history = max_history
        
        # Data structures
        self.recent_lines = deque(maxlen=20)
        self.ip_counter = Counter()
        self.status_counter = Counter()
        self.attack_counter = Counter()
        self.total_lines = 0
        self.start_time = time.time()
        
        # Setup curses
        curses.curs_set(0)  # Hide cursor
        self.stdscr.nodelay(1)  # Non-blocking input
        curses.init_pair(1, curses.COLOR_GREEN, curses.COLOR_BLACK)
        curses.init_pair(2, curses.COLOR_YELLOW, curses.COLOR_BLACK)
        curses.init_pair(3, curses.COLOR_RED, curses.COLOR_BLACK)
        curses.init_pair(4, curses.COLOR_CYAN, curses.COLOR_BLACK)
    
    def parse_line(self, line):
        """Parse log line and extract metrics"""
        import re
        
        self.total_lines += 1
        self.recent_lines.append(line[:80])
        
        # Extract IP
        ip_match = re.search(r'\b((?:\d{1,3}\.){3}\d{1,3})\b', line)
        if ip_match:
            self.ip_counter[ip_match.group(1)] += 1
        
        # Extract status code
        status_match = re.search(r'\b([1-5]\d{2})\b', line)
        if status_match:
            self.status_counter[status_match.group(1)] += 1
        
        # Detect attacks
        if re.search(r'(?i)(union.*select|<script)', line):
            if 'sqli' in line.lower() or 'union' in line.lower():
                self.attack_counter['SQL Injection'] += 1
            if 'script' in line.lower():
                self.attack_counter['XSS'] += 1
    
    def draw(self):
        """Draw dashboard"""
        self.stdscr.clear()
        height, width = self.stdscr.getmaxyx()
        
        # Header
        uptime = time.time() - self.start_time
        header = f"Log Monitor Dashboard - Uptime: {uptime:.0f}s - Lines: {self.total_lines:,}"
        self.stdscr.addstr(0, 0, header[:width-1], curses.A_BOLD)
        self.stdscr.addstr(1, 0, "=" * (width-1))
        
        row = 3
        
        # Recent lines
        self.stdscr.addstr(row, 0, "Recent Activity:", curses.color_pair(4) | curses.A_BOLD)
        row += 1
        for i, line in enumerate(list(self.recent_lines)[-10:]):
            if row >= height - 2:
                break
            self.stdscr.addstr(row, 2, line[:width-3])
            row += 1
        
        row += 1
        
        # Top IPs
        if row < height - 10:
            self.stdscr.addstr(row, 0, "Top IPs:", curses.color_pair(4) | curses.A_BOLD)
            row += 1
            for ip, count in self.ip_counter.most_common(5):
                if row >= height - 2:
                    break
                self.stdscr.addstr(row, 2, f"{ip:<15} : {count:>6}", curses.color_pair(1))
                row += 1
        
        row += 1
        
        # Status codes
        if row < height - 10:
            self.stdscr.addstr(row, 0, "Status Codes:", curses.color_pair(4) | curses.A_BOLD)
            row += 1
            for status, count in sorted(self.status_counter.items()):
                if row >= height - 2:
                    break
                
                # Color code by status
                color = curses.color_pair(1)  # Green for 2xx
                if status.startswith('4'):
                    color = curses.color_pair(2)  # Yellow for 4xx
                elif status.startswith('5'):
                    color = curses.color_pair(3)  # Red for 5xx
                
                self.stdscr.addstr(row, 2, f"{status} : {count:>6}", color)
                row += 1
        
        row += 1
        
        # Attacks
        if self.attack_counter and row < height - 5:
            self.stdscr.addstr(row, 0, "Attacks Detected:", 
                             curses.color_pair(3) | curses.A_BOLD)
            row += 1
            for attack, count in self.attack_counter.most_common():
                if row >= height - 2:
                    break
                self.stdscr.addstr(row, 2, f"{attack:<20} : {count:>4}", 
                                 curses.color_pair(3))
                row += 1
        
        # Footer
        self.stdscr.addstr(height-1, 0, "Press 'q' to quit", curses.A_BOLD)
        
        self.stdscr.refresh()
    
    def run(self, stream):
        """Run dashboard with input stream"""
        try:
            for line in stream:
                # Check for quit
                key = self.stdscr.getch()
                if key == ord('q'):
                    break
                
                # Process line
                self.parse_line(line.strip())
                
                # Update display
                self.draw()
                
                # Brief sleep to reduce CPU
                time.sleep(0.01)
        
        except KeyboardInterrupt:
            pass
        
        finally:
            # Show final stats
            self.stdscr.clear()
            self.stdscr.addstr(0, 0, "Final Statistics:", curses.A_BOLD)
            self.stdscr.addstr(2, 0, f"Total Lines Processed: {self.total_lines:,}")
            self.stdscr.addstr(3, 0, f"Total Uptime: {time.time() - self.start_time:.1f}s")
            self.stdscr.addstr(4, 0, f"Lines/sec: {self.total_lines/(time.time()-self.start_time):.1f}")
            self.stdscr.addstr(6, 0, "Press any key to exit...")
            self.stdscr.refresh()
            self.stdscr.nodelay(0)
            self.stdscr.getch()

def main(stdscr, filepath=None):
    """Main function for curses"""
    dashboard = LogDashboard(stdscr)
    
    if filepath:
        # For testing with file
        import subprocess
        proc = subprocess.Popen(['tail', '-f', filepath], 
                               stdout=subprocess.PIPE,
                               universal_newlines=True)
        dashboard.run(proc.stdout)
    else:
        # Read from stdin
        dashboard.run(sys.stdin)

if __name__ == "__main__":
    if len(sys.argv) > 1:
        curses.wrapper(main, sys.argv[1])
    else:
        print("Usage: tail -f /var/log/apache2/access.log | python3 dashboard.py")
        print("   or: python3 dashboard.py /var/log/apache2/access.log")
```

---

## Performance Optimization Best Practices

### Summary of Techniques

**1. Indexed Searching**

- Build line offset indexes for random access
- Create hash-based indexes for specific fields (IP, timestamp)
- Use SQLite FTS5 for complex full-text searches
- [Inference] Indexes add overhead during creation but dramatically improve repeated queries

**2. Caching Strategies**

- Use LRU cache for parsed entries (functools.lru_cache)
- Implement file-level caching for analysis results
- Consider Redis for distributed team environments
- Always validate cache freshness against source file modification time

**3. Stream Processing**

- Process logs line-by-line to minimize memory usage
- Use chunked reading for large files (1-10MB chunks optimal)
- Implement parallel processing for CPU-intensive tasks
- Memory-map files for pattern searching in very large files

**4. Optimization Guidelines**

- Compile regex patterns once and reuse
- Use appropriate data structures (Counter, defaultdict)
- Batch database operations when using SQLite
- Consider trade-offs: speed vs memory vs accuracy

---

## Important Related Topics

For comprehensive performance optimization in CTF log analysis:

1. **Database Optimization** - Index strategies, query optimization, connection pooling
2. **Distributed Processing** - Apache Spark, Hadoop for massive datasets
3. **Compression Techniques** - Working with compressed logs (gzip, bzip2) without decompression
4. **Profiling & Monitoring** - cProfile, memory_profiler, identifying bottlenecks
5. **Hardware Optimization** - SSD vs HDD impact, RAM disk usage, CPU core utilization

---

# Output & Reporting

Effective output and reporting are critical in CTF scenarios for documenting findings, presenting evidence to judges, and creating actionable intelligence from log analysis. This section covers techniques for generating comprehensive summaries, formatting professional reports, and presenting evidence in a clear, verifiable manner.

## Summary Generation

Summary generation involves aggregating raw log data into meaningful, concise insights that highlight security events, attack patterns, and key findings. Proper summarization reduces cognitive load and enables rapid decision-making.

### Command-Line Summary Techniques

**Basic log summarization with standard tools**:

```bash
# Count events by type
cat auth.log | awk '{print $5}' | sort | uniq -c | sort -rn

# Summarize by date
cat auth.log | awk '{print $1, $2, $3}' | sort | uniq -c

# Extract and count unique IPs
grep -oE '\b([0-9]{1,3}\.){3}[0-9]{1,3}\b' auth.log | sort | uniq -c | sort -rn

# Failed login summary
grep "Failed password" auth.log | awk '{print $(NF-3)}' | sort | uniq -c | sort -rn

# Time-based distribution
cat auth.log | awk '{print $3}' | cut -d: -f1 | sort | uniq -c

# User activity summary
grep "session opened" auth.log | awk '{print $11}' | sort | uniq -c | sort -rn
```

**Cloud log summarization**:

```bash
# AWS CloudTrail event summary
cat cloudtrail.json | jq -r '.Records[].eventName' | sort | uniq -c | sort -rn

# Top users by activity
cat cloudtrail.json | jq -r '.Records[].userIdentity.userName' | sort | uniq -c | sort -rn

# Error summary
cat cloudtrail.json | jq -r '.Records[] | select(.errorCode) | .errorCode' | sort | uniq -c

# Azure Activity Logs summary
cat activity-logs.json | jq -r '.[].operationName.value' | sort | uniq -c | sort -rn

# GCP operations by service
cat admin-activity.json | jq -r '.[] | .protoPayload.serviceName + " : " + .protoPayload.methodName' | sort | uniq -c
```

### Automated Summary Scripts

**Comprehensive log summary script**:

```bash
#!/bin/bash
# log-summarizer.sh - Generate comprehensive log summaries

LOGFILE=$1
OUTPUT=${2:-summary-report.txt}

if [ -z "$LOGFILE" ]; then
    echo "Usage: $0 <logfile> [output-file]"
    exit 1
fi

{
    echo ""
    echo "         LOG ANALYSIS SUMMARY REPORT"
    echo ""
    echo "Generated: $(date)"
    echo "Source File: $LOGFILE"
    echo "Total Lines: $(wc -l < "$LOGFILE")"
    echo ""

    echo ""
    echo "TOP 10 SOURCE IPs"
    echo ""
    grep -oE '\b([0-9]{1,3}\.){3}[0-9]{1,3}\b' "$LOGFILE" | \
        sort | uniq -c | sort -rn | head -10 | \
        awk '{printf "%-15s : %d occurrences\n", $2, $1}'
    echo ""

    echo ""
    echo "FAILED AUTHENTICATION ATTEMPTS"
    echo ""
    grep -i "failed\|failure\|invalid" "$LOGFILE" | wc -l | \
        awk '{print "Total Failed Attempts: " $1}'
    echo ""
    echo "Top Failed Login Sources:"
    grep -i "failed password" "$LOGFILE" | \
        grep -oE '\b([0-9]{1,3}\.){3}[0-9]{1,3}\b' | \
        sort | uniq -c | sort -rn | head -5 | \
        awk '{printf "  %s : %d attempts\n", $2, $1}'
    echo ""

    echo ""
    echo "SUCCESSFUL AUTHENTICATIONS"
    echo ""
    grep -i "accepted\|success" "$LOGFILE" | wc -l | \
        awk '{print "Total Successful Logins: " $1}'
    echo ""

    echo ""
    echo "PRIVILEGED OPERATIONS"
    echo ""
    grep -i "sudo\|su \|root" "$LOGFILE" | wc -l | \
        awk '{print "Total Privilege Operations: " $1}'
    echo ""

    echo ""
    echo "HOURLY ACTIVITY DISTRIBUTION"
    echo ""
    awk '{print $3}' "$LOGFILE" | cut -d: -f1 | sort | uniq -c | \
        awk '{printf "%02d:00 : %s\n", $2, $1}' | sort
    echo ""

    echo ""
    echo "CRITICAL EVENTS"
    echo ""
    grep -iE "critical|alert|emergency|panic" "$LOGFILE" | head -10
    echo ""

    echo ""
    echo "END OF REPORT"
    echo ""

} > "$OUTPUT"

echo "Summary report generated: $OUTPUT"
cat "$OUTPUT"
```

```bash
# Usage
chmod +x log-summarizer.sh
./log-summarizer.sh /var/log/auth.log auth-summary.txt
```

**Cloud-specific summary generator**:

```bash
#!/bin/bash
# cloud-log-summarizer.sh - AWS CloudTrail summary

CLOUDTRAIL_JSON=$1
OUTPUT=${2:-cloudtrail-summary.txt}

{
    echo ""
    echo "      AWS CLOUDTRAIL ANALYSIS SUMMARY"
    echo ""
    echo "Generated: $(date)"
    echo "Source: $CLOUDTRAIL_JSON"
    echo ""

    echo ""
    echo "TOP 10 API CALLS"
    echo ""
    cat "$CLOUDTRAIL_JSON" | jq -r '.Records[].eventName' | \
        sort | uniq -c | sort -rn | head -10 | \
        awk '{printf "%-40s : %d calls\n", $2, $1}'
    echo ""

    echo ""
    echo "UNIQUE USERS"
    echo ""
    cat "$CLOUDTRAIL_JSON" | jq -r '.Records[].userIdentity.userName // .Records[].userIdentity.principalId' | \
        sort -u | head -20
    echo ""

    echo ""
    echo "ERROR EVENTS"
    echo ""
    cat "$CLOUDTRAIL_JSON" | jq -r '.Records[] | select(.errorCode) | "\(.eventTime) | \(.eventName) | \(.errorCode) | \(.errorMessage)"' | \
        head -10
    echo ""

    echo ""
    echo "PRIVILEGE ESCALATION INDICATORS"
    echo ""
    cat "$CLOUDTRAIL_JSON" | jq -r '.Records[] | select(.eventName | test("PutUser|PutRole|Attach.*Policy|CreateAccessKey|CreateUser")) | "\(.eventTime) | \(.eventName) | \(.userIdentity.principalId)"'
    echo ""

    echo ""
    echo "SOURCE IP SUMMARY"
    echo ""
    cat "$CLOUDTRAIL_JSON" | jq -r '.Records[].sourceIPAddress' | \
        sort | uniq -c | sort -rn | head -10 | \
        awk '{printf "%-15s : %d requests\n", $2, $1}'
    echo ""

    echo ""

} > "$OUTPUT"

echo "CloudTrail summary generated: $OUTPUT"
cat "$OUTPUT"
```

### Python-Based Summary Generation

**Advanced log analysis with statistics**:

```python
#!/usr/bin/env python3
# advanced-log-analyzer.py - Statistical log analysis

import sys
import re
import json
from collections import Counter, defaultdict
from datetime import datetime
import argparse

def analyze_auth_log(logfile):
    """Analyze authentication logs with statistics"""
    
    ip_pattern = r'\b(?:\d{1,3}\.){3}\d{1,3}\b'
    failed_logins = []
    successful_logins = []
    ip_counter = Counter()
    user_counter = Counter()
    hourly_distribution = Counter()
    
    with open(logfile, 'r', errors='ignore') as f:
        for line in f:
            # Extract timestamp
            try:
                parts = line.split()
                if len(parts) >= 3:
                    hour = parts[2].split(':')[0]
                    hourly_distribution[hour] += 1
            except:
                pass
            
            # Extract IPs
            ips = re.findall(ip_pattern, line)
            ip_counter.update(ips)
            
            # Analyze authentication events
            if 'Failed password' in line:
                failed_logins.append(line)
                # Extract user
                user_match = re.search(r'Failed password for (?:invalid user )?(\S+)', line)
                if user_match:
                    user_counter[user_match.group(1)] += 1
            
            elif 'Accepted password' in line or 'Accepted publickey' in line:
                successful_logins.append(line)
    
    # Generate report
    report = []
    report.append("="*70)
    report.append("STATISTICAL LOG ANALYSIS REPORT")
    report.append("="*70)
    report.append(f"Analysis Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append(f"Log File: {logfile}")
    report.append("")
    
    report.append("-"*70)
    report.append("AUTHENTICATION SUMMARY")
    report.append("-"*70)
    report.append(f"Total Failed Login Attempts: {len(failed_logins)}")
    report.append(f"Total Successful Logins: {len(successful_logins)}")
    report.append(f"Success Rate: {len(successful_logins)/(len(failed_logins)+len(successful_logins))*100:.2f}%" if (len(failed_logins)+len(successful_logins)) > 0 else "N/A")
    report.append("")
    
    report.append("-"*70)
    report.append("TOP 10 SOURCE IPs BY FREQUENCY")
    report.append("-"*70)
    for ip, count in ip_counter.most_common(10):
        report.append(f"  {ip:<15} : {count:>6} occurrences")
    report.append("")
    
    report.append("-"*70)
    report.append("TOP 10 TARGETED USERNAMES")
    report.append("-"*70)
    for user, count in user_counter.most_common(10):
        report.append(f"  {user:<20} : {count:>6} attempts")
    report.append("")
    
    report.append("-"*70)
    report.append("HOURLY ACTIVITY DISTRIBUTION")
    report.append("-"*70)
    for hour in sorted(hourly_distribution.keys()):
        bar = '' * (hourly_distribution[hour] // 10)
        report.append(f"  {hour}:00 | {bar} ({hourly_distribution[hour]})")
    report.append("")
    
    # Identify anomalies
    report.append("-"*70)
    report.append("ANOMALY DETECTION")
    report.append("-"*70)
    
    # High failure rate IPs
    for ip, count in ip_counter.most_common(5):
        if count > 50:
            report.append(f"  [!] HIGH ACTIVITY: {ip} with {count} occurrences")
    
    # Off-hours activity (assuming 22:00-06:00 is off-hours)
    off_hours = sum(hourly_distribution.get(str(h).zfill(2), 0) for h in list(range(0, 6)) + list(range(22, 24)))
    if off_hours > 0:
        report.append(f"  [!] OFF-HOURS ACTIVITY: {off_hours} events between 22:00-06:00")
    
    report.append("")
    report.append("="*70)
    
    return "\n".join(report)

def analyze_cloudtrail(jsonfile):
    """Analyze AWS CloudTrail logs"""
    
    with open(jsonfile, 'r') as f:
        data = json.load(f)
    
    events = data.get('Records', [])
    
    event_counter = Counter()
    user_counter = Counter()
    ip_counter = Counter()
    error_events = []
    privilege_events = []
    
    privilege_patterns = ['PutUser', 'PutRole', 'AttachPolicy', 'CreateAccessKey', 'CreateUser']
    
    for event in events:
        event_name = event.get('eventName', 'Unknown')
        event_counter[event_name] += 1
        
        # User tracking
        user_identity = event.get('userIdentity', {})
        principal = user_identity.get('userName') or user_identity.get('principalId', 'Unknown')
        user_counter[principal] += 1
        
        # IP tracking
        source_ip = event.get('sourceIPAddress', 'Unknown')
        ip_counter[source_ip] += 1
        
        # Error tracking
        if event.get('errorCode'):
            error_events.append({
                'time': event.get('eventTime'),
                'event': event_name,
                'error': event.get('errorCode'),
                'user': principal
            })
        
        # Privilege escalation detection
        if any(pattern in event_name for pattern in privilege_patterns):
            privilege_events.append({
                'time': event.get('eventTime'),
                'event': event_name,
                'user': principal,
                'ip': source_ip
            })
    
    # Generate report
    report = []
    report.append("="*70)
    report.append("AWS CLOUDTRAIL ANALYSIS REPORT")
    report.append("="*70)
    report.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append(f"Total Events: {len(events)}")
    report.append("")
    
    report.append("-"*70)
    report.append("TOP 15 API CALLS")
    report.append("-"*70)
    for event, count in event_counter.most_common(15):
        report.append(f"  {event:<50} : {count:>5}")
    report.append("")
    
    report.append("-"*70)
    report.append("TOP 10 ACTIVE PRINCIPALS")
    report.append("-"*70)
    for user, count in user_counter.most_common(10):
        report.append(f"  {user:<50} : {count:>5} actions")
    report.append("")
    
    report.append("-"*70)
    report.append("ERROR SUMMARY")
    report.append("-"*70)
    report.append(f"Total Errors: {len(error_events)}")
    for err in error_events[:10]:
        report.append(f"  {err['time']} | {err['event']} | {err['error']}")
    report.append("")
    
    if privilege_events:
        report.append("-"*70)
        report.append("PRIVILEGE ESCALATION INDICATORS")
        report.append("-"*70)
        for priv in privilege_events:
            report.append(f"  [!] {priv['time']} | {priv['event']} | User: {priv['user']} | IP: {priv['ip']}")
        report.append("")
    
    report.append("="*70)
    
    return "\n".join(report)

def main():
    parser = argparse.ArgumentParser(description='Advanced Log Analyzer')
    parser.add_argument('logfile', help='Log file to analyze')
    parser.add_argument('-t', '--type', choices=['auth', 'cloudtrail'], default='auth',
                       help='Log type (auth or cloudtrail)')
    parser.add_argument('-o', '--output', help='Output file (default: stdout)')
    
    args = parser.parse_args()
    
    if args.type == 'auth':
        report = analyze_auth_log(args.logfile)
    elif args.type == 'cloudtrail':
        report = analyze_cloudtrail(args.logfile)
    
    if args.output:
        with open(args.output, 'w') as f:
            f.write(report)
        print(f"Report saved to: {args.output}")
    else:
        print(report)

if __name__ == '__main__':
    main()
```

```bash
# Usage
chmod +x advanced-log-analyzer.py

# Analyze authentication logs
./advanced-log-analyzer.py /var/log/auth.log -o auth-analysis.txt

# Analyze CloudTrail logs
./advanced-log-analyzer.py cloudtrail.json -t cloudtrail -o cloudtrail-analysis.txt
```

### Timeline Generation

**Creating chronological event timelines**:

```bash
#!/bin/bash
# timeline-generator.sh - Create chronological timeline from multiple sources

OUTPUT=${1:-timeline.txt}

{
    echo ""
    echo "         INCIDENT TIMELINE REPORT"
    echo ""
    echo "Generated: $(date)"
    echo ""

    # Combine and sort logs chronologically
    (
        # Auth logs
        awk '{print $1" "$2" "$3" [AUTH] "$0}' /var/log/auth.log 2>/dev/null
        
        # Apache access logs
        awk '{print $4" "$5" [WEB] "$0}' /var/log/apache2/access.log 2>/dev/null | \
            sed 's/\[//;s/\]//' 
        
        # Syslog
        awk '{print $1" "$2" "$3" [SYS] "$0}' /var/log/syslog 2>/dev/null
    ) | sort -k1,1M -k2,2n -k3,3 | head -1000

    echo ""
    echo ""

} > "$OUTPUT"

echo "Timeline generated: $OUTPUT"
```

**Python timeline generator with filtering**:

```python
#!/usr/bin/env python3
# timeline-builder.py - Advanced timeline construction

import sys
import re
from datetime import datetime
from collections import namedtuple

Event = namedtuple('Event', ['timestamp', 'source', 'category', 'description'])

def parse_timestamp(date_str, time_str):
    """Parse various timestamp formats"""
    try:
        # Try standard syslog format
        dt = datetime.strptime(f"{datetime.now().year} {date_str} {time_str}", "%Y %b %d %H:%M:%S")
        return dt
    except:
        return None

def parse_auth_log(filepath):
    """Parse authentication logs"""
    events = []
    with open(filepath, 'r', errors='ignore') as f:
        for line in f:
            parts = line.split()
            if len(parts) < 5:
                continue
            
            ts = parse_timestamp(f"{parts[0]} {parts[1]}", parts[2])
            if not ts:
                continue
            
            category = "AUTH"
            if "Failed password" in line:
                category = "AUTH_FAIL"
            elif "Accepted password" in line:
                category = "AUTH_SUCCESS"
            elif "sudo" in line:
                category = "PRIVILEGE"
            
            events.append(Event(ts, "auth.log", category, line.strip()))
    
    return events

def generate_timeline(event_list, output_file=None):
    """Generate formatted timeline"""
    # Sort by timestamp
    sorted_events = sorted(event_list, key=lambda x: x.timestamp)
    
    lines = []
    lines.append("="*100)
    lines.append("CHRONOLOGICAL EVENT TIMELINE")
    lines.append("="*100)
    lines.append(f"Total Events: {len(sorted_events)}")
    lines.append("")
    
    current_date = None
    for event in sorted_events:
        event_date = event.timestamp.date()
        
        # Print date header if changed
        if event_date != current_date:
            lines.append("")
            lines.append(f"{''*100}")
            lines.append(f"DATE: {event_date}")
            lines.append(f"{''*100}")
            current_date = event_date
        
        time_str = event.timestamp.strftime("%H:%M:%S")
        lines.append(f"{time_str} | {event.category:<15} | {event.source:<20} | {event.description[:100]}")
    
    lines.append("")
    lines.append("="*100)
    
    output = "\n".join(lines)
    
    if output_file:
        with open(output_file, 'w') as f:
            f.write(output)
        print(f"Timeline saved to: {output_file}")
    else:
        print(output)

if __name__ == '__main__':
    if len(sys.argv) < 2:
        print("Usage: timeline-builder.py <auth-log> [output-file]")
        sys.exit(1)
    
    events = parse_auth_log(sys.argv[1])
    output = sys.argv[2] if len(sys.argv) > 2 else None
    generate_timeline(events, output)
```

### Summary Best Practices for CTF

1. **Include key metrics**: Total events, unique IPs, failed vs successful attempts, time ranges
2. **Highlight anomalies**: Unusual times, high-frequency events, privilege escalation indicators
3. **Maintain context**: Don't lose critical details in over-summarization
4. **Use visualization**: ASCII charts, graphs, histograms for clarity
5. **Version control**: Timestamp all summaries, track analysis progression

## Report Formatting

Professional report formatting ensures findings are clear, reproducible, and suitable for presentation to technical and non-technical audiences.

### Report Structure Standards

**Standard CTF forensics report structure**:

```
1. Executive Summary
   - High-level findings
   - Impact assessment
   - Key recommendations

2. Incident Overview
   - Timeline of events
   - Affected systems/services
   - Attack vectors identified

3. Technical Analysis
   - Detailed log analysis
   - Evidence artifacts
   - Attack methodology

4. Indicators of Compromise (IOCs)
   - IP addresses
   - User accounts
   - File hashes
   - URLs/domains

5. Recommendations
   - Immediate actions
   - Long-term mitigations
   - Security improvements

6. Appendices
   - Raw log excerpts
   - Command outputs
   - Tool configurations
```

### Markdown Report Templates

**Comprehensive CTF analysis report template**:

```markdown
# CTF Log Analysis Report

**Challenge Name:** [Challenge Name]  
**Team:** [Team Name]  
**Analyst:** [Your Name]  
**Date:** [Analysis Date]  
**Report Version:** 1.0

---

## Executive Summary

### Key Findings
- [Finding 1]
- [Finding 2]
- [Finding 3]

### Impact Assessment
**Severity:** [High/Medium/Low]  
**Confidence:** [High/Medium/Low]

[Brief description of impact]

### Recommendations
1. [Priority 1 recommendation]
2. [Priority 2 recommendation]

---

## Incident Timeline

| Time (UTC) | Event | Source | Category |
|------------|-------|--------|----------|
| 2024-10-29 14:23:15 | Initial reconnaissance | 192.168.1.100 | Scanning |
| 2024-10-29 14:45:32 | Exploitation attempt | 192.168.1.100 | Attack |
| 2024-10-29 15:12:08 | Successful compromise | 192.168.1.100 | Breach |

---

## Technical Analysis

### Attack Vector

[Detailed description of how the attack was conducted]

### Evidence Artifacts

#### Log Source 1: Authentication Logs

```

Oct 29 14:45:32 server sshd[12345]: Failed password for admin from 192.168.1.100 port 52341 ssh2 Oct 29 14:45:35 server sshd[12345]: Failed password for admin from 192.168.1.100 port 52341 ssh2 Oct 29 14:45:38 server sshd[12345]: Accepted password for admin from 192.168.1.100 port 52341 ssh2

```

**Analysis:** Brute force attack followed by successful authentication

#### Log Source 2: Web Server Logs

```

192.168.1.100 - - [29/Oct/2024:14:23:15 +0000] "GET /admin/../../etc/passwd HTTP/1.1" 200 2431

```

**Analysis:** Directory traversal vulnerability exploitation

### Attack Methodology

1. **Reconnaissance Phase**
   - Port scanning detected at 14:23:15
   - Service enumeration on ports 22, 80, 443
   
2. **Exploitation Phase**
   - Directory traversal on web application
   - SSH brute force attack
   
3. **Post-Exploitation**
   - Privilege escalation via sudo misconfiguration
   - Credential harvesting

---

## Indicators of Compromise (IOCs)

### Network Indicators

| Type | Value | First Seen | Last Seen | Context |
|------|-------|------------|-----------|---------|
| IPv4 | 192.168.1.100 | 2024-10-29 14:23 | 2024-10-29 15:30 | Attacker source |
| IPv4 | 10.0.0.50 | 2024-10-29 15:15 | 2024-10-29 15:30 | Lateral movement |

### Account Indicators

| Account | Activity | Confidence |
|---------|----------|------------|
| admin | Compromised | High |
| backup-svc | Created by attacker | High |

### File Indicators

| Path | Hash (SHA256) | Type |
|------|---------------|------|
| /tmp/.hidden | abc123... | Backdoor |
| /var/www/shell.php | def456... | Web shell |

---

## Detailed Log Analysis

### Authentication Analysis

**Total Events:** 1,247  
**Failed Logins:** 156  
**Successful Logins:** 3  
**Suspicious Success Rate:** 1.9%

#### Top Failed Login Sources

```

192.168.1.100 : 142 attempts 10.0.0.25 : 8 attempts 172.16.0.50 : 6 attempts

````

### Privilege Escalation Analysis

```bash
Oct 29 15:12:45 server sudo: admin : TTY=pts/0 ; PWD=/home/admin ; USER=root ; COMMAND=/bin/bash
Oct 29 15:13:02 server sudo: admin : TTY=pts/0 ; PWD=/root ; USER=root ; COMMAND=/usr/bin/cat /etc/shadow
````

**Analysis:** Attacker gained root access and accessed credential files

---

## Attack Flow Diagram

```
[Attacker: 192.168.1.100]
         |
         v
    Port Scan (14:23)
         |
         v
    Web App Scan (14:25)
         |
         v
    Directory Traversal (14:30)
         |
         v
    SSH Brute Force (14:45)
         |
         v
    Successful Login (14:45)
         |
         v
    Privilege Escalation (15:12)
         |
         v
    Credential Harvesting (15:13)
         |
         v
    Lateral Movement (15:15)
```

---

## Recommendations

### Immediate Actions (0-24 hours)

1. **Block attacker IP:** `iptables -A INPUT -s 192.168.1.100 -j DROP`
2. **Reset compromised credentials:** Force password reset for 'admin' account
3. **Remove backdoors:** Delete `/tmp/.hidden` and `/var/www/shell.php`
4. **Review sudo configuration:** Restrict `/bin/bash` access

### Short-term Actions (1-7 days)

1. Implement fail2ban for SSH brute force protection
2. Enable multi-factor authentication for all administrative accounts
3. Conduct full system audit for additional compromise indicators
4. Review and harden web application input validation

### Long-term Actions (1-30 days)

1. Implement centralized logging and SIEM solution
2. Deploy intrusion detection system (IDS)
3. Conduct security awareness training
4. Perform penetration testing to identify additional vulnerabilities

---

## Appendix A: Analysis Commands

### Commands Used

```bash
# Extract failed login attempts
grep "Failed password" /var/log/auth.log | awk '{print $(NF-3)}' | sort | uniq -c

# Analyze web logs
cat /var/log/apache2/access.log | grep "192.168.1.100"

# Check sudo history
grep "sudo" /var/log/auth.log | tail -20
```

### Tools Used

- **grep/awk/sed:** Pattern matching and text processing
- **jq:** JSON log parsing
- **Python 3:** Custom analysis scripts
- **multitail:** Real-time log monitoring

---

## Appendix B: Raw Log Excerpts

### Critical Event Logs

[Include relevant raw log entries with timestamps]

---

**Report Classification:** [PUBLIC/CONFIDENTIAL/INTERNAL]  
**Distribution:** [Intended audience]  
**Contact:** [analyst@team.com]

````
### HTML Report Generation

**Convert markdown to professional HTML**:

```bash
#!/bin/bash
# generate-html-report.sh - Convert markdown to styled HTML report

MARKDOWN_FILE=$1
OUTPUT_HTML=${2:-report.html}

if [ -z "$MARKDOWN_FILE" ]; then
    echo "Usage: $0 <markdown-file> [output.html]"
    exit 1
fi

# Check for pandoc
if ! command -v pandoc &> /dev/null; then
    echo "Installing pandoc..."
    sudo apt install pandoc -y
fi

# Create CSS file
cat > /tmp/report-style.css << 'EOF' 
body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.6; color: #333; max-width: 1200px; margin: 0 auto; padding: 20px; background-color: #f5f5f5; }

.container { background-color: white; padding: 40px; box-shadow: 0 0 10px rgba(0,0,0,0.1); border-radius: 5px; }

h1 { color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; font-size: 2.5em; }

h2 { color: #34495e; border-bottom: 2px solid #95a5a6; padding-bottom: 8px; margin-top: 30px; font-size: 1.8em; }

h3 { color: #7f8c8d; margin-top: 20px; font-size: 1.4em; }

table { border-collapse: collapse; width: 100%; margin: 20px 0; box-shadow: 0 2px 3px rgba(0,0,0,0.1); }

th { background-color: #3498db; color: white; padding: 12px; text-align: left; font-weight: bold; }

td { padding: 10px; border-bottom: 1px solid #ddd; }

tr:hover { background-color: #f5f5f5; }

pre { background-color: #2c3e50; color: #ecf0f1; padding: 15px; border-radius: 5px; overflow-x: auto; font-family: 'Courier New', monospace; font-size: 0.9em; line-height: 1.4; }

code { background-color: #ecf0f1; color: #e74c3c; padding: 2px 6px; border-radius: 3px; font-family: 'Courier New', monospace; font-size: 0.9em; }

pre code { background-color: transparent; color: #ecf0f1; padding: 0; }

.executive-summary { background-color: #e8f4f8; border-left: 4px solid #3498db; padding: 20px; margin: 20px 0; }

.critical { color: #e74c3c; font-weight: bold; }

.warning { color: #f39c12; font-weight: bold; }

.success { color: #27ae60; font-weight: bold; }

blockquote { border-left: 4px solid #3498db; padding-left: 20px; margin: 20px 0; font-style: italic; color: #7f8c8d; }

ul, ol { margin: 15px 0; padding-left: 30px; }

li { margin: 8px 0; }

.metadata { background-color: #ecf0f1; padding: 15px; border-radius: 5px; margin-bottom: 30px; }

.metadata strong { color: #2c3e50; }

.timestamp { color: #7f8c8d; font-family: monospace; font-size: 0.9em; }

hr { border: none; border-top: 2px solid #bdc3c7; margin: 30px 0; }

.ioc-table td:first-child { font-family: monospace; background-color: #f8f9fa; }

.severity-high { background-color: #fee; }

.severity-medium { background-color: #ffeaa7; }

.severity-low { background-color: #dfe6e9; }

@media print { body { background-color: white; } .container { box-shadow: none; } pre { border: 1px solid #ddd; } }

.footer { margin-top: 50px; padding-top: 20px; border-top: 2px solid #bdc3c7; text-align: center; color: #7f8c8d; font-size: 0.9em; } EOF

# Convert markdown to HTML with CSS

pandoc "$MARKDOWN_FILE"  
-f markdown  
-t html5  
--standalone  
--css=/tmp/report-style.css  
--metadata title="CTF Log Analysis Report"  
--toc  
--toc-depth=3  
-o "$OUTPUT_HTML"

# Embed CSS directly into HTML (for portability)
sed -i 's|<link rel="stylesheet" href="/tmp/report-style.css" />|<style>'"$(cat /tmp/report-style.css)"'</style>|' "$OUTPUT_HTML"

echo "HTML report generated: $OUTPUT_HTML"
````

```bash
# Usage
chmod +x generate-html-report.sh
./generate-html-report.sh analysis-report.md analysis-report.html
````

### PDF Report Generation

**Generate professional PDF reports**:

```bash
#!/bin/bash
# generate-pdf-report.sh - Create PDF from markdown

MARKDOWN_FILE=$1
OUTPUT_PDF=${2:-report.pdf}

if [ -z "$MARKDOWN_FILE" ]; then
    echo "Usage: $0 <markdown-file> [output.pdf]"
    exit 1
fi

# Install dependencies
if ! command -v pandoc &> /dev/null; then
    sudo apt install pandoc -y
fi

if ! command -v pdflatex &> /dev/null; then
    sudo apt install texlive-latex-base texlive-fonts-recommended texlive-latex-extra -y
fi

# Generate PDF with custom styling
pandoc "$MARKDOWN_FILE" \
    -f markdown \
    -t pdf \
    --pdf-engine=pdflatex \
    -V geometry:margin=1in \
    -V fontsize=11pt \
    -V documentclass=article \
    -V colorlinks=true \
    -V linkcolor=blue \
    -V urlcolor=blue \
    -V toccolor=black \
    --toc \
    --toc-depth=3 \
    --number-sections \
    --highlight-style=tango \
    -o "$OUTPUT_PDF"

echo "PDF report generated: $OUTPUT_PDF"
```

```bash
# Usage
./generate-pdf-report.sh analysis-report.md analysis-report.pdf
```

### JSON Report Format for Tool Integration

**Structured JSON report for automation**:

```python
#!/usr/bin/env python3
# json-report-generator.py - Generate structured JSON reports

import json
import sys
from datetime import datetime

def generate_json_report(findings):
    """Generate structured JSON report"""
    
    report = {
        "metadata": {
            "report_version": "1.0",
            "generated_at": datetime.now().isoformat(),
            "analyst": "CTF Team",
            "report_type": "log_analysis"
        },
        "executive_summary": {
            "severity": "HIGH",
            "confidence": "HIGH",
            "key_findings": findings.get('key_findings', []),
            "total_events_analyzed": findings.get('total_events', 0),
            "threat_detected": findings.get('threat_detected', False)
        },
        "timeline": findings.get('timeline', []),
        "indicators": {
            "ip_addresses": findings.get('ip_addresses', []),
            "user_accounts": findings.get('user_accounts', []),
            "file_paths": findings.get('file_paths', []),
            "urls": findings.get('urls', [])
        },
        "technical_details": {
            "attack_vectors": findings.get('attack_vectors', []),
            "exploited_vulnerabilities": findings.get('vulnerabilities', []),
            "tools_detected": findings.get('tools', [])
        },
        "recommendations": {
            "immediate": findings.get('immediate_actions', []),
            "short_term": findings.get('short_term_actions', []),
            "long_term": findings.get('long_term_actions', [])
        },
        "evidence": {
            "log_sources": findings.get('log_sources', []),
            "artifacts": findings.get('artifacts', [])
        }
    }
    
    return report

def main():
    # Example usage with sample findings
    findings = {
        "key_findings": [
            "SSH brute force attack detected from 192.168.1.100",
            "Successful privilege escalation to root",
            "Credential harvesting activity observed"
        ],
        "total_events": 1247,
        "threat_detected": True,
        "timeline": [
            {
                "timestamp": "2024-10-29T14:23:15Z",
                "event": "Port scanning activity",
                "source": "192.168.1.100",
                "severity": "MEDIUM"
            },
            {
                "timestamp": "2024-10-29T14:45:38Z",
                "event": "Successful SSH authentication",
                "source": "192.168.1.100",
                "severity": "HIGH"
            }
        ],
        "ip_addresses": [
            {"ip": "192.168.1.100", "first_seen": "2024-10-29T14:23:15Z", "threat_level": "HIGH"},
            {"ip": "10.0.0.50", "first_seen": "2024-10-29T15:15:00Z", "threat_level": "MEDIUM"}
        ],
        "user_accounts": [
            {"username": "admin", "status": "compromised"},
            {"username": "backup-svc", "status": "created_by_attacker"}
        ],
        "attack_vectors": ["SSH Brute Force", "Privilege Escalation"],
        "immediate_actions": [
            "Block IP 192.168.1.100",
            "Reset admin credentials",
            "Review sudo configuration"
        ]
    }
    
    report = generate_json_report(findings)
    
    # Output to file or stdout
    output_file = sys.argv[1] if len(sys.argv) > 1 else None
    
    if output_file:
        with open(output_file, 'w') as f:
            json.dump(report, f, indent=2)
        print(f"JSON report generated: {output_file}")
    else:
        print(json.dumps(report, indent=2))

if __name__ == '__main__':
    main()
```

```bash
# Usage
python3 json-report-generator.py findings-report.json
```

### CSV Export for Data Analysis

**Export findings to CSV for spreadsheet analysis**:

```python
#!/usr/bin/env python3
# csv-export.py - Export log analysis to CSV

import csv
import sys
from datetime import datetime

def export_timeline_csv(events, output_file):
    """Export timeline events to CSV"""
    
    with open(output_file, 'w', newline='') as f:
        writer = csv.writer(f)
        
        # Header
        writer.writerow(['Timestamp', 'Event Type', 'Source IP', 'Destination', 'Severity', 'Description'])
        
        # Data rows
        for event in events:
            writer.writerow([
                event.get('timestamp', ''),
                event.get('event_type', ''),
                event.get('source_ip', ''),
                event.get('destination', ''),
                event.get('severity', ''),
                event.get('description', '')
            ])
    
    print(f"Timeline exported to: {output_file}")

def export_ioc_csv(iocs, output_file):
    """Export IOCs to CSV"""
    
    with open(output_file, 'w', newline='') as f:
        writer = csv.writer(f)
        
        # Header
        writer.writerow(['Type', 'Value', 'First Seen', 'Last Seen', 'Confidence', 'Context'])
        
        # Data rows
        for ioc in iocs:
            writer.writerow([
                ioc.get('type', ''),
                ioc.get('value', ''),
                ioc.get('first_seen', ''),
                ioc.get('last_seen', ''),
                ioc.get('confidence', ''),
                ioc.get('context', '')
            ])
    
    print(f"IOCs exported to: {output_file}")

# Example usage
if __name__ == '__main__':
    # Sample timeline data
    timeline_events = [
        {
            'timestamp': '2024-10-29 14:23:15',
            'event_type': 'Port Scan',
            'source_ip': '192.168.1.100',
            'destination': '10.0.0.1',
            'severity': 'MEDIUM',
            'description': 'SYN scan on ports 22, 80, 443'
        },
        {
            'timestamp': '2024-10-29 14:45:38',
            'event_type': 'Authentication',
            'source_ip': '192.168.1.100',
            'destination': '10.0.0.1',
            'severity': 'HIGH',
            'description': 'Successful SSH login as admin'
        }
    ]
    
    # Sample IOC data
    ioc_data = [
        {
            'type': 'IPv4',
            'value': '192.168.1.100',
            'first_seen': '2024-10-29 14:23:15',
            'last_seen': '2024-10-29 15:30:00',
            'confidence': 'HIGH',
            'context': 'Attacker source IP'
        },
        {
            'type': 'Account',
            'value': 'backup-svc',
            'first_seen': '2024-10-29 15:20:00',
            'last_seen': '2024-10-29 15:20:00',
            'confidence': 'HIGH',
            'context': 'Persistence account created by attacker'
        }
    ]
    
    export_timeline_csv(timeline_events, 'timeline-export.csv')
    export_ioc_csv(ioc_data, 'ioc-export.csv')
```

### Report Formatting Best Practices

1. **Consistency**: Use standardized templates across all reports
2. **Clarity**: Use clear headings, bullet points, and visual separators
3. **Completeness**: Include all relevant metadata, timestamps, and context
4. **Traceability**: Reference specific log lines, file paths, and commands
5. **Reproducibility**: Provide exact commands used for analysis
6. **Version Control**: Maintain report versioning for updates

## Evidence Presentation

Evidence presentation involves organizing, validating, and displaying forensic artifacts in a manner that supports findings and withstands scrutiny.

### Chain of Custody Documentation

**Evidence tracking template**:

````markdown
# Chain of Custody Log

## Evidence Item: [EVIDENCE-001]

| Field | Value |
|-------|-------|
| Evidence ID | EVIDENCE-001 |
| Description | Authentication log file |
| Source Path | /var/log/auth.log |
| Collection Date | 2024-10-29 16:30:00 UTC |
| Collected By | [Analyst Name] |
| File Size | 2,451,234 bytes |
| MD5 Hash | abc123def456... |
| SHA256 Hash | 789ghi012jkl... |

## Custody Timeline

| Date/Time | Action | Performed By | Location | Notes |
|-----------|--------|--------------|----------|-------|
| 2024-10-29 16:30 | Collection | Alice | Server-01 | Original file copied |
| 2024-10-29 16:35 | Transfer | Alice | Forensics Lab | Moved to analysis workstation |
| 2024-10-29 16:40 | Analysis Started | Bob | Forensics Lab | Initial examination |
| 2024-10-29 18:15 | Analysis Complete | Bob | Forensics Lab | Findings documented |

## Integrity Verification

```bash
# Original hash
md5sum auth.log.original
abc123def456... auth.log.original

# Working copy hash (verified identical)
md5sum auth.log.working
abc123def456... auth.log.working
````

## Access Log

|Date/Time|Person|Purpose|Duration|
|---|---|---|---|
|2024-10-29 16:40|Bob|Initial analysis|1h 35m|
|2024-10-29 19:00|Carol|Peer review|30m|

````

### Evidence Collection Script

```bash
#!/bin/bash
# evidence-collector.sh - Collect and hash evidence with chain of custody

EVIDENCE_DIR="/forensics/evidence/$(date +%Y%m%d-%H%M%S)"
LOG_FILE="$EVIDENCE_DIR/collection.log"
MANIFEST="$EVIDENCE_DIR/manifest.txt"

# Create evidence directory
mkdir -p "$EVIDENCE_DIR"

# Logging function
log_action() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_FILE"
}

# Collect file with hashing
collect_evidence() {
    local source_file=$1
    local evidence_id=$2
    
    if [ ! -f "$source_file" ]; then
        log_action "ERROR: Source file not found: $source_file"
        return 1
    fi
    
    local basename=$(basename "$source_file")
    local dest_file="$EVIDENCE_DIR/${evidence_id}_${basename}"
    
    log_action "Collecting: $source_file -> $dest_file"
    
    # Copy with preservation of timestamps
    cp -p "$source_file" "$dest_file"
    
    # Generate hashes
    local md5_hash=$(md5sum "$dest_file" | awk '{print $1}')
    local sha256_hash=$(sha256sum "$dest_file" | awk '{print $1}')
    local file_size=$(stat -c%s "$dest_file")
    
    # Record in manifest
    cat >> "$MANIFEST" << EOF

Evidence ID: $evidence_id
Source Path: $source_file
Collection Time: $(date '+%Y-%m-%d %H:%M:%S %Z')
File Size: $file_size bytes
MD5: $md5_hash
SHA256: $sha256_hash
Collected By: $(whoami)
Hostname: $(hostname)


EOF
    
    log_action "Successfully collected $evidence_id"
    log_action "  MD5: $md5_hash"
    log_action "  SHA256: $sha256_hash"
}

# Main collection process
log_action "=== Evidence Collection Started ==="
log_action "Analyst: $(whoami)"
log_action "System: $(hostname)"
log_action "Evidence Directory: $EVIDENCE_DIR"

# Collect system logs
collect_evidence "/var/log/auth.log" "EVIDENCE-001"
collect_evidence "/var/log/syslog" "EVIDENCE-002"
collect_evidence "/var/log/apache2/access.log" "EVIDENCE-003"
collect_evidence "/var/log/apache2/error.log" "EVIDENCE-004"

# Collect system information
log_action "Collecting system information..."
uname -a > "$EVIDENCE_DIR/system-info.txt"
date >> "$EVIDENCE_DIR/system-info.txt"
uptime >> "$EVIDENCE_DIR/system-info.txt"

# Create compressed archive
log_action "Creating evidence archive..."
tar -czf "$EVIDENCE_DIR.tar.gz" -C "$(dirname $EVIDENCE_DIR)" "$(basename $EVIDENCE_DIR)"

# Hash the archive
archive_hash=$(sha256sum "$EVIDENCE_DIR.tar.gz" | awk '{print $1}')
log_action "Archive created: $EVIDENCE_DIR.tar.gz"
log_action "Archive SHA256: $archive_hash"

log_action "=== Evidence Collection Complete ==="

echo ""
echo "Evidence collected to: $EVIDENCE_DIR"
echo "Archive: $EVIDENCE_DIR.tar.gz"
echo "Manifest: $MANIFEST"
echo "Collection log: $LOG_FILE"
````

```bash
# Usage
sudo ./evidence-collector.sh
```

### Annotated Log Evidence

**Creating annotated evidence exhibits**:

```bash
#!/bin/bash
# annotate-evidence.sh - Create annotated log excerpts for evidence presentation

INPUT_LOG=$1
OUTPUT_FILE=${2:-annotated-evidence.txt}

if [ -z "$INPUT_LOG" ]; then
    echo "Usage: $0 <input-log> [output-file]"
    exit 1
fi

{
    echo ""
    echo "           ANNOTATED EVIDENCE EXHIBIT"
    echo ""
    echo "Evidence ID: EXHIBIT-A"
    echo "Source: $INPUT_LOG"
    echo "Date Prepared: $(date)"
    echo "Prepared By: $(whoami)"
    echo ""
    echo "File Hash (SHA256): $(sha256sum "$INPUT_LOG" | awk '{print $1}')"
    echo ""
    echo ""

    echo ""
    echo "EVIDENCE ITEM 1: Initial Reconnaissance Activity"
    echo ""
    echo "Timestamp: 2024-10-29 14:23:15"
    echo "Significance: First indicator of attacker presence"
    echo ""
    echo "Log Entry:"
    grep "14:23:15" "$INPUT_LOG" | head -5 | while read line; do
        echo "  $line"
    done
    echo ""
    echo "Analysis: Port scanning activity detected from 192.168.1.100"
    echo ""

    echo ""
    echo "EVIDENCE ITEM 2: Brute Force Attack"
    echo ""
    echo "Timestamp Range: 2024-10-29 14:45:00 - 14:45:38"
    echo "Significance: Multiple failed authentication attempts"
    echo ""
    echo "Log Entries:"
    grep "Failed password" "$INPUT_LOG" | tail -5 | while read line; do
        echo "  $line"
    done
    echo ""
    echo "Analysis: 142 failed login attempts for 'admin' account from 192.168.1.100"
    echo ""

    echo ""
    echo "EVIDENCE ITEM 3: Successful Compromise"
    echo ""
    echo "Timestamp: 2024-10-29 14:45:38"
    echo "Significance: Successful authentication after brute force"
    echo ""
    echo "Log Entry:"
    grep "Accepted password" "$INPUT_LOG" | grep "14:45:38" | while read line; do
        echo "  >>> $line <<<"
    done
    echo ""
    echo "Analysis: Attacker successfully authenticated as 'admin' from 192.168.1.100"
    echo ""

    echo ""
    echo "EVIDENCE CORRELATION SUMMARY"
    echo ""
    echo ""
    echo "Attack Timeline:"
    echo "  14:23:15 - Reconnaissance begins"
    echo "  14:45:00 - Brute force attack starts"
    echo "  14:45:38 - Successful authentication"
    echo "  15:12:45 - Privilege escalation"
    echo ""
    echo "Source IP: 192.168.1.100 (consistent across all events)"
    echo "Target Account: admin"
    echo "Attack Duration: ~49 minutes"
    echo ""
    echo ""

} > "$OUTPUT_FILE"

echo "Annotated evidence created: $OUTPUT_FILE"
```

### Visual Evidence Presentation

**Generate timeline visualizations**:

```python
#!/usr/bin/env python3
# timeline-visualizer.py - Create ASCII timeline visualization

from datetime import datetime
from collections import defaultdict

def create_timeline_visual(events, output_file=None):
    """Create ASCII timeline visualization"""
    
    # Sort events by time
    sorted_events = sorted(events, key=lambda x: x['timestamp'])
    
    lines = []
    lines.append("="*100)
    lines.append("VISUAL TIMELINE - ATTACK PROGRESSION")
    lines.append("="*100)
    lines.append("")
    
    # Create timeline
    for i, event in enumerate(sorted_events):
        time_str = event['timestamp'].strftime("%H:%M:%S")
        event_type = event['type']
        description = event['description']
        severity = event.get('severity', 'INFO')
        
        # Visual markers based on severity
        if severity == 'CRITICAL':
            marker = "[!!!]"
            connector = ">"
        elif severity == 'HIGH':
            marker = "[!!]"
            connector = ">"
        elif severity == 'MEDIUM':
            marker = "[!]"
            connector = ">"
        else:
            marker = "[]"
            connector = ">"
        
        lines.append(f"{time_str} {marker} {event_type}")
        lines.append(f"         {connector} {description}")
        
        if i < len(sorted_events) - 1:
            lines.append("         ")
            lines.append("         ")
        
    lines.append("")
    lines.append("="*100)
    lines.append("Legend: [!!!] CRITICAL  [!!] HIGH  [!] MEDIUM  [] INFO")
    lines.append("="*100)
    
    output = "\n".join(lines)
    
    if output_file:
        with open(output_file, 'w') as f:
            f.write(output)
        print(f"Timeline visualization saved to: {output_file}")
    else:
        print(output)

# Example usage
if __name__ == '__main__':
    sample_events = [
        {
            'timestamp': datetime(2024, 10, 29, 14, 23, 15),
            'type': 'Reconnaissance',
            'description': 'Port scanning from 192.168.1.100',
            'severity': 'MEDIUM'
        },
        {
            'timestamp': datetime(2024, 10, 29, 14, 45, 0),
            'type': 'Attack',
            'description': 'SSH brute force attack initiated',
            'severity': 'HIGH'
        },
        {
            'timestamp': datetime(2024, 10, 29, 14, 45, 38),
            'type': 'Compromise',
            'description': 'Successful authentication as admin',
            'severity': 'CRITICAL'
        },
        {
            'timestamp': datetime(2024, 10, 29, 15, 12, 45),
            'type': 'Privilege Escalation',
            'description': 'Sudo to root shell',
            'severity': 'CRITICAL'
        },
        {
            'timestamp': datetime(2024, 10, 29, 15, 13, 2),
            'type': 'Data Exfiltration',
            'description': 'Access to /etc/shadow',
            'severity': 'CRITICAL'
        }
    ]
    
    create_timeline_visual(sample_events, 'timeline-visual.txt')
```

### Interactive HTML Evidence Viewer

**Create interactive evidence browser**:

```python
#!/usr/bin/env python3
# evidence-viewer-generator.py - Generate interactive HTML evidence viewer

import json
import html


def generate_evidence_viewer(evidence_items, output_file='evidence-viewer.html'):
    """Generate interactive HTML evidence viewer"""
    
    html_content = '''
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CTF Evidence Viewer</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background-color: #1e1e1e;
            color: #d4d4d4;
            padding: 20px;
        }
        
        .container {
            max-width: 1400px;
            margin: 0 auto;
        }
        
        h1 {
            color: #4ec9b0;
            border-bottom: 2px solid #4ec9b0;
            padding-bottom: 10px;
            margin-bottom: 30px;
        }
        
        .evidence-list {
            display: grid;
            grid-template-columns: 300px 1fr;
            gap: 20px;
        }
        
        .evidence-nav {
            background-color: #252526;
            padding: 20px;
            border-radius: 5px;
            height: fit-content;
            position: sticky;
            top: 20px;
        }
        
        .evidence-item-link {
            display: block;
            padding: 12px;
            margin-bottom: 10px;
            background-color: #2d2d30;
            border-left: 3px solid transparent;
            cursor: pointer;
            transition: all 0.3s;
            border-radius: 3px;
        }
        
        .evidence-item-link:hover {
            background-color: #3e3e42;
            border-left-color: #4ec9b0;
        }
        
        .evidence-item-link.active {
            background-color: #094771;
            border-left-color: #4ec9b0;
        }
        
        .severity-badge {
            display: inline-block;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 0.8em;
            font-weight: bold;
            margin-left: 8px;
        }
        
        .severity-critical { background-color: #f48771; color: #1e1e1e; }
        .severity-high { background-color: #ce9178; color: #1e1e1e; }
        .severity-medium { background-color: #dcdcaa; color: #1e1e1e; }
        .severity-low { background-color: #4ec9b0; color: #1e1e1e; }
        
        .evidence-content {
            background-color: #252526;
            padding: 30px;
            border-radius: 5px;
        }
        
        .evidence-detail {
            display: none;
        }
        
        .evidence-detail.active {
            display: block;
        }
        
        .evidence-header {
            border-bottom: 2px solid #3e3e42;
            padding-bottom: 15px;
            margin-bottom: 20px;
        }
        
        .evidence-id {
            color: #4ec9b0;
            font-size: 1.5em;
            font-weight: bold;
        }
        
        .metadata-grid {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 15px;
            margin: 20px 0;
            background-color: #2d2d30;
            padding: 20px;
            border-radius: 5px;
        }
        
        .metadata-item {
            display: flex;
            flex-direction: column;
        }
        
        .metadata-label {
            color: #9cdcfe;
            font-size: 0.9em;
            margin-bottom: 5px;
        }
        
        .metadata-value {
            color: #ce9178;
            font-family: 'Courier New', monospace;
        }
        
        .log-content {
            background-color: #1e1e1e;
            padding: 20px;
            border-radius: 5px;
            overflow-x: auto;
            margin: 20px 0;
        }
        
        .log-line {
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            line-height: 1.6;
            color: #d4d4d4;
            padding: 5px;
            border-left: 3px solid transparent;
        }
        
        .log-line.highlight {
            background-color: #1a3a52;
            border-left-color: #4ec9b0;
        }
        
        .log-line .timestamp {
            color: #4ec9b0;
        }
        
        .log-line .ip {
            color: #ce9178;
        }
        
        .log-line .keyword {
            color: #f48771;
            font-weight: bold;
        }
        
        .analysis-section {
            background-color: #2d2d30;
            padding: 20px;
            border-radius: 5px;
            margin-top: 20px;
        }
        
        .analysis-title {
            color: #9cdcfe;
            font-size: 1.2em;
            margin-bottom: 10px;
        }
        
        .analysis-text {
            line-height: 1.6;
        }
        
        .hash-display {
            background-color: #1e1e1e;
            padding: 10px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 0.85em;
            word-break: break-all;
            color: #4ec9b0;
        }
        
        .filter-box {
            margin-bottom: 20px;
            padding: 15px;
            background-color: #2d2d30;
            border-radius: 5px;
        }
        
        .filter-box input {
            width: 100%;
            padding: 10px;
            background-color: #1e1e1e;
            border: 1px solid #3e3e42;
            border-radius: 3px;
            color: #d4d4d4;
            font-size: 1em;
        }
        
        .filter-box input:focus {
            outline: none;
            border-color: #4ec9b0;
        }
        
        .tag {
            display: inline-block;
            padding: 4px 10px;
            background-color: #094771;
            color: #4ec9b0;
            border-radius: 3px;
            margin-right: 8px;
            margin-top: 5px;
            font-size: 0.85em;
        }
        
        .timeline-connector {
            text-align: center;
            color: #3e3e42;
            margin: 10px 0;
            font-size: 1.2em;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1> CTF Evidence Viewer</h1>
        
        <div class="evidence-list">
            <div class="evidence-nav">
                <div class="filter-box">
                    <input type="text" id="searchBox" placeholder="Search evidence..." onkeyup="filterEvidence()">
                </div>
                <div id="evidenceNav">
                    <!-- Evidence navigation items will be inserted here -->
                </div>
            </div>
            
            <div class="evidence-content">
                <div id="evidenceDetails">
                    <!-- Evidence details will be displayed here -->
                </div>
            </div>
        </div>
    </div>
    
    <script>
        const evidenceData = ''' + json.dumps(evidence_items) + ''';
        
        function initializeViewer() {
            const nav = document.getElementById('evidenceNav');
            const details = document.getElementById('evidenceDetails');
            
            // Create navigation items
            evidenceData.forEach((item, index) => {
                const navItem = document.createElement('div');
                navItem.className = 'evidence-item-link';
                navItem.setAttribute('data-id', item.id);
                navItem.innerHTML = `
                    <div>${item.id}</div>
                    <div style="font-size: 0.9em; color: #9cdcfe; margin-top: 5px;">${item.type}</div>
                    <span class="severity-badge severity-${item.severity.toLowerCase()}">${item.severity}</span>
                `;
                navItem.onclick = () => showEvidence(item.id);
                nav.appendChild(navItem);
                
                // Create detail section
                const detailDiv = document.createElement('div');
                detailDiv.className = 'evidence-detail';
                detailDiv.id = `detail-${item.id}`;
                detailDiv.innerHTML = createEvidenceDetail(item);
                details.appendChild(detailDiv);
            });
            
            // Show first evidence by default
            if (evidenceData.length > 0) {
                showEvidence(evidenceData[0].id);
            }
        }
        
        function createEvidenceDetail(item) {
            let html = `
                <div class="evidence-header">
                    <div class="evidence-id">${item.id}</div>
                    <div style="color: #9cdcfe; margin-top: 10px;">${item.type}</div>
                    <span class="severity-badge severity-${item.severity.toLowerCase()}">${item.severity}</span>
                </div>
                
                <div class="metadata-grid">
                    <div class="metadata-item">
                        <div class="metadata-label">Timestamp</div>
                        <div class="metadata-value">${item.timestamp}</div>
                    </div>
                    <div class="metadata-item">
                        <div class="metadata-label">Source</div>
                        <div class="metadata-value">${item.source}</div>
                    </div>
                    <div class="metadata-item">
                        <div class="metadata-label">Source IP</div>
                        <div class="metadata-value">${item.source_ip || 'N/A'}</div>
                    </div>
                    <div class="metadata-item">
                        <div class="metadata-label">Event Count</div>
                        <div class="metadata-value">${item.event_count || 1}</div>
                    </div>
                </div>
            `;
            
            if (item.tags && item.tags.length > 0) {
                html += '<div style="margin: 15px 0;">';
                item.tags.forEach(tag => {
                    html += `<span class="tag">${tag}</span>`;
                });
                html += '</div>';
            }
            
            if (item.log_entries && item.log_entries.length > 0) {
                html += `
                    <div class="log-content">
                        <div style="color: #9cdcfe; margin-bottom: 10px; font-weight: bold;">Log Entries:</div>
                `;
                item.log_entries.forEach((entry, idx) => {
                    const highlight = entry.highlight ? 'highlight' : '';
                    const formattedEntry = formatLogEntry(entry.content);
                    html += `<div class="log-line ${highlight}">${formattedEntry}</div>`;
                });
                html += '</div>';
            }
            
            if (item.analysis) {
                html += `
                    <div class="analysis-section">
                        <div class="analysis-title"> Analysis</div>
                        <div class="analysis-text">${item.analysis}</div>
                    </div>
                `;
            }
            
            if (item.hash) {
                html += `
                    <div class="analysis-section">
                        <div class="analysis-title"> Evidence Hash (SHA256)</div>
                        <div class="hash-display">${item.hash}</div>
                    </div>
                `;
            }
            
            return html;
        }
        
        function formatLogEntry(content) {
            // Highlight timestamps
            content = content.replace(/(\\d{2}:\\d{2}:\\d{2})/g, '<span class="timestamp">$1</span>');
            
            // Highlight IP addresses
            content = content.replace(/(\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b)/g, '<span class="ip">$1</span>');
            
            // Highlight keywords
            const keywords = ['Failed', 'Accepted', 'ERROR', 'CRITICAL', 'sudo', 'root', 'denied'];
            keywords.forEach(keyword => {
                const regex = new RegExp(`\\\\b${keyword}\\\\b`, 'gi');
                content = content.replace(regex, '<span class="keyword">$&</span>');
            });
            
            return content;
        }
        
        function showEvidence(id) {
            // Hide all details
            document.querySelectorAll('.evidence-detail').forEach(el => {
                el.classList.remove('active');
            });
            
            // Remove active from all nav items
            document.querySelectorAll('.evidence-item-link').forEach(el => {
                el.classList.remove('active');
            });
            
            // Show selected detail
            document.getElementById(`detail-${id}`).classList.add('active');
            
            // Mark nav item as active
            document.querySelector(`[data-id="${id}"]`).classList.add('active');
        }
        
        function filterEvidence() {
            const searchTerm = document.getElementById('searchBox').value.toLowerCase();
            const navItems = document.querySelectorAll('.evidence-item-link');
            
            navItems.forEach(item => {
                const id = item.getAttribute('data-id');
                const evidence = evidenceData.find(e => e.id === id);
                const searchableText = `${evidence.id} ${evidence.type} ${evidence.analysis || ''}`.toLowerCase();
                
                if (searchableText.includes(searchTerm)) {
                    item.style.display = 'block';
                } else {
                    item.style.display = 'none';
                }
            });
        }
        
        // Initialize on page load
        window.onload = initializeViewer;
    </script>
</body>
</html>
    '''
    
    with open(output_file, 'w') as f:
        f.write(html_content)
    
    print(f"Interactive evidence viewer generated: {output_file}")


# Example usage
if __name__ == '__main__':
    sample_evidence = [
        {
            "id": "EVIDENCE-001",
            "type": "Authentication Logs",
            "timestamp": "2024-10-29 14:23:15 UTC",
            "source": "/var/log/auth.log",
            "source_ip": "192.168.1.100",
            "severity": "CRITICAL",
            "event_count": 142,
            "tags": ["SSH", "Brute Force", "Authentication"],
            "log_entries": [
                {
                    "content": "Oct 29 14:45:30 server sshd[12340]: Failed password for admin from 192.168.1.100 port 52341 ssh2",
                    "highlight": False
                },
                {
                    "content": "Oct 29 14:45:32 server sshd[12341]: Failed password for admin from 192.168.1.100 port 52342 ssh2",
                    "highlight": False
                },
                {
                    "content": "Oct 29 14:45:38 server sshd[12345]: Accepted password for admin from 192.168.1.100 port 52345 ssh2",
                    "highlight": True
                }
            ],
            "analysis": "Brute force attack detected with 142 failed login attempts over 38 seconds, followed by successful authentication. This indicates either a successful password guess or credential compromise. The attack originated from IP 192.168.1.100 and targeted the 'admin' account.",
            "hash": "a1b2c3d4e5f6789012345678901234567890123456789012345678901234567890"
        },
        {
            "id": "EVIDENCE-002",
            "type": "Privilege Escalation",
            "timestamp": "2024-10-29 15:12:45 UTC",
            "source": "/var/log/auth.log",
            "source_ip": "192.168.1.100",
            "severity": "CRITICAL",
            "event_count": 1,
            "tags": ["Privilege Escalation", "Sudo", "Root Access"],
            "log_entries": [
                {
                    "content": "Oct 29 15:12:45 server sudo: admin : TTY=pts/0 ; PWD=/home/admin ; USER=root ; COMMAND=/bin/bash",
                    "highlight": True
                }
            ],
            "analysis": "The compromised 'admin' account successfully escalated privileges to root using sudo. This gave the attacker full system access. The sudo command was used to spawn a root shell (/bin/bash), which is a common technique for maintaining elevated privileges throughout an attack session.",
            "hash": "b2c3d4e5f6789012345678901234567890123456789012345678901234567890ab"
        },
        {
            "id": "EVIDENCE-003",
            "type": "Data Exfiltration",
            "timestamp": "2024-10-29 15:13:02 UTC",
            "source": "/var/log/auth.log",
            "source_ip": "192.168.1.100",
            "severity": "CRITICAL",
            "event_count": 5,
            "tags": ["Data Access", "Credential Theft", "Root"],
            "log_entries": [
                {
                    "content": "Oct 29 15:13:02 server sudo: admin : TTY=pts/0 ; PWD=/root ; USER=root ; COMMAND=/usr/bin/cat /etc/shadow",
                    "highlight": True
                },
                {
                    "content": "Oct 29 15:13:15 server sudo: admin : TTY=pts/0 ; PWD=/root ; USER=root ; COMMAND=/usr/bin/cat /etc/passwd",
                    "highlight": True
                }
            ],
            "analysis": "Attacker accessed sensitive credential files (/etc/shadow and /etc/passwd) using root privileges. These files contain password hashes and user account information that can be used for offline password cracking or lateral movement to other systems.",
            "hash": "c3d4e5f6789012345678901234567890123456789012345678901234567890abc1"
        },
        {
            "id": "EVIDENCE-004",
            "type": "Web Application Attack",
            "timestamp": "2024-10-29 14:30:22 UTC",
            "source": "/var/log/apache2/access.log",
            "source_ip": "192.168.1.100",
            "severity": "HIGH",
            "event_count": 15,
            "tags": ["Directory Traversal", "Web Attack", "Path Manipulation"],
            "log_entries": [
                {
                    "content": "192.168.1.100 - - [29/Oct/2024:14:30:22 +0000] \"GET /admin/../../etc/passwd HTTP/1.1\" 200 2431",
                    "highlight": True
                },
                {
                    "content": "192.168.1.100 - - [29/Oct/2024:14:30:25 +0000] \"GET /admin/../../etc/shadow HTTP/1.1\" 403 285",
                    "highlight": False
                }
            ],
            "analysis": "Directory traversal vulnerability exploited to access system files through the web application. The attacker successfully retrieved /etc/passwd (HTTP 200) but was denied access to /etc/shadow (HTTP 403). This indicates inadequate input validation on file path parameters.",
            "hash": "d4e5f6789012345678901234567890123456789012345678901234567890abc12"
        }
    ]
    
    generate_evidence_viewer(sample_evidence, 'evidence-viewer.html')
````

### Evidence Package Creation

**Complete evidence package script**:

```bash
#!/bin/bash
# create-evidence-package.sh - Create complete evidence package with all artifacts

CASE_ID=${1:-"CASE-$(date +%Y%m%d-%H%M%S)"}
PACKAGE_DIR="/forensics/packages/$CASE_ID"
ANALYST=$(whoami)

# Create directory structure
mkdir -p "$PACKAGE_DIR"/{logs,reports,evidence,tools,hashes}

echo "Creating evidence package: $CASE_ID"
echo "Analyst: $ANALYST"
echo "Date: $(date)"

# Package metadata
cat > "$PACKAGE_DIR/README.txt" << EOF

              EVIDENCE PACKAGE

Case ID: $CASE_ID
Created: $(date)
Analyst: $ANALYST
System: $(hostname)

CONTENTS:
  /logs/          - Original log files
  /reports/       - Analysis reports
  /evidence/      - Annotated evidence items
  /tools/         - Analysis scripts used
  /hashes/        - File integrity hashes

INTEGRITY:
  See hashes/manifest.sha256 for file verification


EOF

# Copy log files with hashing
echo "Collecting log files..."
for log in /var/log/auth.log /var/log/syslog /var/log/apache2/*.log; do
    if [ -f "$log" ]; then
        cp -p "$log" "$PACKAGE_DIR/logs/"
        sha256sum "$log" >> "$PACKAGE_DIR/hashes/source-hashes.txt"
    fi
done

# Generate analysis reports
echo "Generating reports..."
./log-summarizer.sh "$PACKAGE_DIR/logs/auth.log" "$PACKAGE_DIR/reports/auth-summary.txt" 2>/dev/null
./timeline-generator.sh "$PACKAGE_DIR/reports/timeline.txt" 2>/dev/null

# Create annotated evidence
echo "Creating annotated evidence..."
./annotate-evidence.sh "$PACKAGE_DIR/logs/auth.log" "$PACKAGE_DIR/evidence/annotated-auth.txt" 2>/dev/null

# Copy analysis tools
echo "Archiving analysis tools..."
cp log-summarizer.sh timeline-generator.sh annotate-evidence.sh "$PACKAGE_DIR/tools/" 2>/dev/null

# Generate manifest with hashes
echo "Generating integrity manifest..."
cd "$PACKAGE_DIR"
find . -type f -exec sha256sum {} \; > hashes/manifest.sha256

# Create compressed archive
echo "Creating compressed archive..."
cd ..
tar -czf "${CASE_ID}.tar.gz" "$CASE_ID"
sha256sum "${CASE_ID}.tar.gz" > "${CASE_ID}.tar.gz.sha256"

# Generate final report
cat > "${CASE_ID}-REPORT.txt" << EOF

         EVIDENCE PACKAGE SUMMARY

Case ID: $CASE_ID
Package Created: $(date)
Created By: $ANALYST

PACKAGE DETAILS:
  Location: $PACKAGE_DIR
  Archive: ${CASE_ID}.tar.gz
  Archive Size: $(du -h "${CASE_ID}.tar.gz" | cut -f1)
  Archive Hash: $(cat "${CASE_ID}.tar.gz.sha256" | awk '{print $1}')

CONTENTS SUMMARY:
  Log Files: $(find "$CASE_ID/logs" -type f | wc -l)
  Reports: $(find "$CASE_ID/reports" -type f | wc -l)
  Evidence Items: $(find "$CASE_ID/evidence" -type f | wc -l)
  Total Files: $(find "$CASE_ID" -type f | wc -l)

VERIFICATION:
  To verify package integrity:
    sha256sum -c ${CASE_ID}.tar.gz.sha256
  
  To verify individual files:
    cd $CASE_ID
    sha256sum -c hashes/manifest.sha256

CHAIN OF CUSTODY:
  Collected By: $ANALYST
  Collection Time: $(date)
  Collection System: $(hostname)


EOF

cat "${CASE_ID}-REPORT.txt"

echo ""
echo "Evidence package created successfully!"
echo "Archive: ${CASE_ID}.tar.gz"
echo "Report: ${CASE_ID}-REPORT.txt"
echo ""
echo "To extract: tar -xzf ${CASE_ID}.tar.gz"
echo "To verify: sha256sum -c ${CASE_ID}.tar.gz.sha256"
````

```bash
# Usage
chmod +x create-evidence-package.sh
sudo ./create-evidence-package.sh CASE-2024-001
```

### Presentation Best Practices

1. **Clear Attribution**: Every evidence item must reference its source with exact timestamps
2. **Integrity Verification**: Include hashes for all evidence files
3. **Context Preservation**: Maintain surrounding log entries for context
4. **Visual Clarity**: Use highlighting, annotations, and formatting to emphasize key findings
5. **Reproducibility**: Document exact commands and methods used for analysis
6. **Chain of Custody**: Track all handling of evidence from collection to presentation
7. **Multiple Formats**: Provide evidence in multiple formats (text, HTML, PDF) for different audiences

### Evidence Validation Checklist

```markdown
## Evidence Validation Checklist

### Collection Phase
- [ ] Source system identified and documented
- [ ] Collection timestamp recorded
- [ ] Collector identity documented
- [ ] Original file hashes computed (MD5, SHA256)
- [ ] Files copied with preservation of metadata
- [ ] Collection method documented

### Integrity Phase
- [ ] Hashes verified after transfer
- [ ] Working copies created
- [ ] Original evidence secured
- [ ] Access control implemented
- [ ] Backup copies created

### Analysis Phase
- [ ] Analysis methods documented
- [ ] Tools and versions recorded
- [ ] Commands executed logged
- [ ] Findings cross-referenced to source
- [ ] Peer review conducted

### Presentation Phase
- [ ] Evidence clearly labeled
- [ ] Source attribution provided
- [ ] Timestamps in consistent format
- [ ] Context maintained
- [ ] Visual aids enhance clarity
- [ ] Technical accuracy verified
- [ ] Findings supported by evidence
```

---

## Comprehensive Reporting Workflow

**End-to-end CTF reporting pipeline**:

```bash
#!/bin/bash
# ctf-reporting-pipeline.sh - Complete analysis to report workflow

CHALLENGE_NAME=$1
EVIDENCE_DIR=$2
OUTPUT_DIR=${3:-"./reports"}

if [ -z "$CHALLENGE_NAME" ] || [ -z "$EVIDENCE_DIR" ]; then
    echo "Usage: $0 <challenge-name> <evidence-directory> [output-directory]"
    exit 1
fi

mkdir -p "$OUTPUT_DIR"

echo ""
echo "CTF REPORTING PIPELINE"
echo ""
echo "Challenge: $CHALLENGE_NAME"
echo "Evidence: $EVIDENCE_DIR"
echo "Output: $OUTPUT_DIR"
echo ""

# Step 1: Generate summaries
echo "[1/6] Generating log summaries..."
for log in "$EVIDENCE_DIR"/*.log; do
    if [ -f "$log" ]; then
        ./log-summarizer.sh "$log" "$OUTPUT_DIR/summary-$(basename $log).txt"
    fi
done

# Step 2: Create timeline
echo "[2/6] Building event timeline..."
./timeline-generator.sh "$OUTPUT_DIR/timeline.txt"

# Step 3: Extract IOCs
echo "[3/6] Extracting indicators of compromise..."
cat "$EVIDENCE_DIR"/*.log | grep -oE '\b([0-9]{1,3}\.){3}[0-9]{1,3}\b' | sort -u > "$OUTPUT_DIR/iocs-ips.txt"

# Step 4: Generate annotated evidence
echo "[4/6] Creating annotated evidence..."
for log in "$EVIDENCE_DIR"/*.log; do
    if [ -f "$log" ]; then
        ./annotate-evidence.sh "$log" "$OUTPUT_DIR/evidence-$(basename $log).txt"
    fi
done

# Step 5: Create HTML report
echo "[5/6] Generating HTML report..."
./generate-html-report.sh "$OUTPUT_DIR/summary-auth.log.txt" "$OUTPUT_DIR/$CHALLENGE_NAME-report.html"

# Step 6: Create evidence package
echo "[6/6] Creating evidence package..."
./create-evidence-package.sh "$CHALLENGE_NAME-$(date +%Y%m%d)"

echo ""
echo ""
echo "REPORTING COMPLETE"
echo ""
echo "Reports location: $OUTPUT_DIR"
echo ""
echo "Generated files:"
ls -lh "$OUTPUT_DIR"
```

This comprehensive guide covers all aspects of output and reporting for CTF log analysis, providing practical tools and templates for generating professional, verifiable, and presentation-ready findings documentation.

---

## Key Finding Extraction

Key findings are the critical insights derived from log analysisevidence of compromise, attack patterns, misconfigurations, or flags in CTF challenges.

### Structured Finding Documentation

**Basic finding template:**

```bash
cat << 'EOF' > finding_template.txt
===========================================
FINDING: [Brief Title]
===========================================
Severity: [Critical/High/Medium/Low]
Category: [Exploitation/Reconnaissance/Lateral Movement/Data Exfiltration/etc.]
Timestamp: [When discovered/occurred]
Source: [Log file or system]

DESCRIPTION:
[Detailed explanation of what was found]

EVIDENCE:
[Specific log entries, commands, or artifacts]

IMPACT:
[What this means for the system/challenge]

RECOMMENDATION:
[Remediation steps or next actions]

REFERENCES:
[Related CVEs, documentation, or techniques]
===========================================
EOF
```

**Example finding:**

```bash
cat << 'EOF' > finding_001.txt
===========================================
FINDING: Unauthorized Root Access via SSH
===========================================
Severity: Critical
Category: Initial Access / Privilege Escalation
Timestamp: 2024-10-29 14:23:17 UTC
Source: /var/log/auth.log

DESCRIPTION:
Successful SSH authentication as root user from external IP 
address 203.0.113.45 after multiple failed attempts. This IP
is not part of authorized administrator ranges.

EVIDENCE:
Oct 29 14:22:45 server sshd[12345]: Failed password for root from 203.0.113.45 port 52341 ssh2
Oct 29 14:22:58 server sshd[12345]: Failed password for root from 203.0.113.45 port 52341 ssh2
Oct 29 14:23:17 server sshd[12389]: Accepted password for root from 203.0.113.45 port 52341 ssh2
Oct 29 14:23:18 server sshd[12389]: pam_unix(sshd:session): session opened for user root by (uid=0)

IMPACT:
Attacker gained full administrative access to the system.
All subsequent actions performed with root privileges.

RECOMMENDATION:
1. Immediately revoke root SSH access
2. Implement SSH key-based authentication only
3. Enable fail2ban or similar brute-force protection
4. Audit all actions performed during compromised session

REFERENCES:
- MITRE ATT&CK: T1078.001 (Valid Accounts: Default Accounts)
- MITRE ATT&CK: T1110 (Brute Force)
===========================================
EOF
```

### Automated Finding Extraction

**Script to extract potential findings:**

```bash
#!/bin/bash
# extract_findings.sh

LOGFILE="$1"
OUTPUT_DIR="findings"

mkdir -p "$OUTPUT_DIR"

echo "[*] Extracting key findings from $LOGFILE..."

# Finding 1: Failed authentication followed by success
echo "[+] Checking for brute force attacks..."
awk '/Failed password/ {user=$9; ip=$11; failed[ip,user]++} 
     /Accepted password/ {ip=$11; user=$9; if (failed[ip,user] > 0) 
     print "FINDING: Successful login after failures - User:", user, "IP:", ip, "Failed attempts:", failed[ip,user]}' \
     "$LOGFILE" > "$OUTPUT_DIR/brute_force_findings.txt"

# Finding 2: Suspicious commands in sudo logs
echo "[+] Checking for privilege escalation..."
grep -E "sudo.*COMMAND" "$LOGFILE" | \
grep -E "(nc|netcat|bash|sh|python|perl|ruby|wget|curl|chmod|chown)" \
> "$OUTPUT_DIR/suspicious_sudo_commands.txt"

# Finding 3: Web exploitation attempts
echo "[+] Checking for web attacks..."
grep -E "(union.*select|\.\.\/|%00|<script|javascript:|eval\(|system\()" "$LOGFILE" \
> "$OUTPUT_DIR/web_exploitation_attempts.txt"

# Finding 4: Unusual network connections
echo "[+] Checking for unusual network activity..."
grep -E "(ESTABLISHED|CONNECTED)" "$LOGFILE" | \
grep -vE "(80|443|22|53)" | \
awk '{print $0}' > "$OUTPUT_DIR/unusual_connections.txt"

# Finding 5: Data exfiltration indicators
echo "[+] Checking for data exfiltration..."
grep -iE "(download|export|backup|dump|archive)" "$LOGFILE" | \
grep -E "\.(tar|zip|gz|sql|csv|json)" \
> "$OUTPUT_DIR/potential_exfiltration.txt"

# Generate summary
echo "[*] Generating summary..."
cat << EOF > "$OUTPUT_DIR/SUMMARY.txt"
Log Analysis Summary
====================
Log File: $LOGFILE
Analysis Date: $(date)

Findings:
1. Brute Force Attacks: $(wc -l < "$OUTPUT_DIR/brute_force_findings.txt") incidents
2. Suspicious Sudo Commands: $(wc -l < "$OUTPUT_DIR/suspicious_sudo_commands.txt") incidents
3. Web Exploitation Attempts: $(wc -l < "$OUTPUT_DIR/web_exploitation_attempts.txt") incidents
4. Unusual Network Connections: $(wc -l < "$OUTPUT_DIR/unusual_connections.txt") connections
5. Potential Data Exfiltration: $(wc -l < "$OUTPUT_DIR/potential_exfiltration.txt") incidents

Review individual files in the $OUTPUT_DIR directory for details.
EOF

cat "$OUTPUT_DIR/SUMMARY.txt"
echo "[*] Findings saved to $OUTPUT_DIR/"
```

**Usage:**

```bash
chmod +x extract_findings.sh
./extract_findings.sh /var/log/auth.log
```

### CTF Flag Extraction

**Automated flag finder:**

```bash
#!/bin/bash
# flag_finder.sh

LOGDIR="${1:-.}"
OUTPUT="flags_found.txt"

echo "[*] Searching for flags in $LOGDIR..."
echo "Flag Search Results - $(date)" > "$OUTPUT"
echo "================================" >> "$OUTPUT"

# Common CTF flag formats
PATTERNS=(
    'flag\{[^}]+\}'
    'CTF\{[^}]+\}'
    'FLAG\{[^}]+\}'
    '[A-Z0-9]{32}'  # MD5-like
    '[a-f0-9]{40}'  # SHA1-like
    'picoCTF\{[^}]+\}'
    'HTB\{[^}]+\}'
)

for pattern in "${PATTERNS[@]}"; do
    echo -e "\n[+] Pattern: $pattern" >> "$OUTPUT"
    
    if [ -d "$LOGDIR" ]; then
        grep -rE "$pattern" "$LOGDIR" 2>/dev/null | while read -r line; do
            echo "  $line" >> "$OUTPUT"
        done
    else
        grep -E "$pattern" "$LOGDIR" 2>/dev/null | while read -r line; do
            echo "  $line" >> "$OUTPUT"
        done
    fi
done

# Check for base64 encoded flags
echo -e "\n[+] Checking for base64 encoded content..." >> "$OUTPUT"
if [ -d "$LOGDIR" ]; then
    grep -roE '[A-Za-z0-9+/]{40,}={0,2}' "$LOGDIR" 2>/dev/null | while read -r encoded; do
        decoded=$(echo "$encoded" | base64 -d 2>/dev/null)
        if echo "$decoded" | grep -qE 'flag|CTF|HTB'; then
            echo "  Encoded: $encoded" >> "$OUTPUT"
            echo "  Decoded: $decoded" >> "$OUTPUT"
        fi
    done
fi

# Check for ROT13
echo -e "\n[+] Checking for ROT13 encoded flags..." >> "$OUTPUT"
if [ -d "$LOGDIR" ]; then
    grep -roE '[a-zA-Z]{20,}' "$LOGDIR" 2>/dev/null | tr 'A-Za-z' 'N-ZA-Mn-za-m' | \
    grep -E 'flag|CTF' | head -10 >> "$OUTPUT"
fi

echo "[*] Results saved to $OUTPUT"
cat "$OUTPUT"
```

**Python advanced flag extractor:**

```python
#!/usr/bin/env python3
import re
import os
import sys
import base64
from pathlib import Path

class FlagExtractor:
    def __init__(self):
        self.flag_patterns = [
            r'flag\{[^}]+\}',
            r'CTF\{[^}]+\}',
            r'FLAG\{[^}]+\}',
            r'picoCTF\{[^}]+\}',
            r'HTB\{[^}]+\}',
            r'CHTB\{[^}]+\}',
            r'[A-Z0-9]{32}',  # MD5-like
            r'[a-f0-9]{40}',  # SHA1-like
        ]
        
    def search_files(self, path):
        """Search files for flags"""
        findings = []
        
        if os.path.isfile(path):
            findings.extend(self._search_file(path))
        elif os.path.isdir(path):
            for root, dirs, files in os.walk(path):
                for filename in files:
                    filepath = os.path.join(root, filename)
                    try:
                        findings.extend(self._search_file(filepath))
                    except Exception as e:
                        continue
        
        return findings
    
    def _search_file(self, filepath):
        """Search individual file"""
        findings = []
        
        try:
            with open(filepath, 'r', errors='ignore') as f:
                content = f.read()
                
                # Direct pattern matching
                for pattern in self.flag_patterns:
                    matches = re.finditer(pattern, content, re.IGNORECASE)
                    for match in matches:
                        findings.append({
                            'type': 'direct',
                            'file': filepath,
                            'pattern': pattern,
                            'flag': match.group(0),
                            'context': self._get_context(content, match.start(), match.end())
                        })
                
                # Base64 encoded flags
                b64_matches = re.finditer(r'[A-Za-z0-9+/]{20,}={0,2}', content)
                for match in b64_matches:
                    try:
                        decoded = base64.b64decode(match.group(0)).decode('utf-8', errors='ignore')
                        if any(re.search(pattern, decoded, re.IGNORECASE) for pattern in self.flag_patterns):
                            findings.append({
                                'type': 'base64',
                                'file': filepath,
                                'encoded': match.group(0),
                                'flag': decoded,
                                'context': self._get_context(content, match.start(), match.end())
                            })
                    except:
                        continue
                
                # ROT13
                rot13_content = content.translate(str.maketrans(
                    'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz',
                    'NOPQRSTUVWXYZABCDEFGHIJKLMnopqrstuvwxyzabcdefghijklm'
                ))
                for pattern in self.flag_patterns:
                    matches = re.finditer(pattern, rot13_content, re.IGNORECASE)
                    for match in matches:
                        findings.append({
                            'type': 'rot13',
                            'file': filepath,
                            'flag': match.group(0),
                            'context': 'ROT13 decoded'
                        })
        
        except Exception as e:
            pass
        
        return findings
    
    def _get_context(self, content, start, end, window=50):
        """Get context around match"""
        context_start = max(0, start - window)
        context_end = min(len(content), end + window)
        return content[context_start:context_end].replace('\n', ' ')
    
    def generate_report(self, findings):
        """Generate formatted report"""
        print("=" * 70)
        print("FLAG EXTRACTION REPORT")
        print("=" * 70)
        print(f"\nTotal flags found: {len(findings)}\n")
        
        # Group by type
        by_type = {}
        for finding in findings:
            ftype = finding['type']
            if ftype not in by_type:
                by_type[ftype] = []
            by_type[ftype].append(finding)
        
        for ftype, items in by_type.items():
            print(f"\n[+] {ftype.upper()} FLAGS: {len(items)}")
            print("-" * 70)
            
            for i, item in enumerate(items, 1):
                print(f"\n  [{i}] File: {item['file']}")
                if ftype == 'base64':
                    print(f"      Encoded: {item['encoded'][:60]}...")
                print(f"      Flag: {item['flag']}")
                if 'context' in item and len(item['context']) > 0:
                    print(f"      Context: ...{item['context']}...")
        
        # Export to file
        with open('flags_report.txt', 'w') as f:
            f.write("FLAG EXTRACTION REPORT\n")
            f.write("=" * 70 + "\n\n")
            for finding in findings:
                f.write(f"Type: {finding['type']}\n")
                f.write(f"File: {finding['file']}\n")
                f.write(f"Flag: {finding['flag']}\n")
                if 'encoded' in finding:
                    f.write(f"Encoded: {finding['encoded']}\n")
                f.write(f"Context: {finding.get('context', 'N/A')}\n")
                f.write("-" * 70 + "\n")
        
        print(f"\n[*] Report saved to flags_report.txt")

if __name__ == '__main__':
    if len(sys.argv) < 2:
        print(f"Usage: {sys.argv[0]} <file_or_directory>")
        sys.exit(1)
    
    extractor = FlagExtractor()
    findings = extractor.search_files(sys.argv[1])
    extractor.generate_report(findings)
```

**Usage:**

```bash
chmod +x flag_extractor.py
./flag_extractor.py /var/log/
./flag_extractor.py challenge.log
```

## IOC (Indicator of Compromise) Lists

IOC lists document artifacts of malicious activityIP addresses, domains, file hashes, URLs, and other indicators used for detection and correlation.

### IOC List Structure

**Standard IOC format:**

```bash
cat << 'EOF' > ioc_template.txt
================================================================================
INDICATOR OF COMPROMISE (IOC) LIST
================================================================================
Case ID: [Unique identifier]
Analysis Date: [Date]
Analyst: [Name]
Confidence Level: [High/Medium/Low]

SUMMARY:
[Brief description of the incident/campaign]

================================================================================
IP ADDRESSES
================================================================================
Type: IPv4/IPv6
Purpose: [C2/Scanner/Attacker/Exfiltration Target]

IP Address          | First Seen          | Last Seen           | Activity
--------------------|---------------------|---------------------|------------------
192.0.2.1           | 2024-10-29 10:00:00 | 2024-10-29 15:30:00 | C2 Communication
203.0.113.45        | 2024-10-29 14:22:00 | 2024-10-29 14:25:00 | Brute Force SSH

================================================================================
DOMAINS & URLs
================================================================================
Domain/URL                          | Category      | First Seen
------------------------------------|---------------|-------------------
malicious.example.com               | C2 Server     | 2024-10-29 11:00:00
http://evil.com/payload.sh          | Malware DL    | 2024-10-29 12:00:00

================================================================================
FILE HASHES
================================================================================
Algorithm | Hash                                                             | Filename
----------|------------------------------------------------------------------|------------------
MD5       | 5d41402abc4b2a76b9719d911017c592                                 | backdoor.sh
SHA-256   | e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 | exploit.py

================================================================================
USER ACCOUNTS
================================================================================
Username    | Type      | Activity
------------|-----------|--------------------------------------------------
backdoor    | Created   | Unauthorized account created 2024-10-29 13:00:00
admin       | Compromised| Multiple failed then successful login

================================================================================
NETWORK ARTIFACTS
================================================================================
Protocol | Port | Description
---------|------|-----------------------------------------------------------
TCP      | 4444 | Reverse shell listener
HTTP     | 8080 | Unauthorized web service

================================================================================
RECOMMENDATIONS
================================================================================
1. Block listed IP addresses at firewall
2. Add domains to DNS blacklist
3. Scan systems for file hashes
4. Disable/audit compromised accounts
5. Close unauthorized network services

================================================================================
REFERENCES
================================================================================
- MITRE ATT&CK TTP: [Technique IDs]
- Related CVEs: [CVE numbers]
- External Reports: [URLs]
================================================================================
EOF
```

### Automated IOC Generation

**Shell script for IOC extraction:**

```bash
#!/bin/bash
# generate_ioc_list.sh

LOGFILE="$1"
OUTPUT="ioc_list_$(date +%Y%m%d_%H%M%S).txt"

cat << EOF > "$OUTPUT"
================================================================================
INDICATOR OF COMPROMISE (IOC) LIST
================================================================================
Case ID: $(uuidgen 2>/dev/null || echo "$(date +%s)")
Analysis Date: $(date)
Source: $LOGFILE
Analyst: $(whoami)@$(hostname)
Confidence Level: Medium (Automated Extraction)

================================================================================
IP ADDRESSES (Unique)
================================================================================
EOF

echo "[*] Extracting IP addresses..."
grep -oE '\b([0-9]{1,3}\.){3}[0-9]{1,3}\b' "$LOGFILE" | sort -u | \
while read ip; do
    first_seen=$(grep "$ip" "$LOGFILE" | head -1 | awk '{print $1, $2, $3}')
    last_seen=$(grep "$ip" "$LOGFILE" | tail -1 | awk '{print $1, $2, $3}')
    count=$(grep -c "$ip" "$LOGFILE")
    printf "%-16s | %-20s | %-20s | Seen %d times\n" "$ip" "$first_seen" "$last_seen" "$count" >> "$OUTPUT"
done

cat << EOF >> "$OUTPUT"

================================================================================
DOMAINS & URLs
================================================================================
EOF

echo "[*] Extracting domains and URLs..."
grep -oE 'https?://[^[:space:]"]+' "$LOGFILE" | sort -u | head -50 | \
while read url; do
    first_seen=$(grep -F "$url" "$LOGFILE" | head -1 | awk '{print $1, $2, $3}')
    printf "%-50s | %-20s\n" "$url" "$first_seen" >> "$OUTPUT"
done

cat << EOF >> "$OUTPUT"

================================================================================
EMAIL ADDRESSES
================================================================================
EOF

echo "[*] Extracting email addresses..."
grep -oE '\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b' "$LOGFILE" | sort -u | \
while read email; do
    count=$(grep -c "$email" "$LOGFILE")
    printf "%-40s | Seen %d times\n" "$email" "$count" >> "$OUTPUT"
done

cat << EOF >> "$OUTPUT"

================================================================================
SUSPICIOUS PATTERNS DETECTED
================================================================================
EOF

echo "[*] Checking for suspicious patterns..."

# SQL Injection
if grep -qiE "(union.*select|' or |1=1)" "$LOGFILE"; then
    echo "- SQL Injection attempts detected" >> "$OUTPUT"
    grep -iE "(union.*select|' or |1=1)" "$LOGFILE" | head -5 >> "$OUTPUT"
fi

# XSS
if grep -qiE "(<script|javascript:|onerror=)" "$LOGFILE"; then
    echo "- XSS attempts detected" >> "$OUTPUT"
    grep -iE "(<script|javascript:|onerror=)" "$LOGFILE" | head -5 >> "$OUTPUT"
fi

# Path Traversal
if grep -qE '(\.\./|%2e%2e)' "$LOGFILE"; then
    echo "- Path traversal attempts detected" >> "$OUTPUT"
    grep -E '(\.\./|%2e%2e)' "$LOGFILE" | head -5 >> "$OUTPUT"
fi

# Command Injection
if grep -qE '(;|\||&&|`|\$\()' "$LOGFILE"; then
    echo "- Potential command injection detected" >> "$OUTPUT"
    grep -E '(;|\||&&|`|\$\()' "$LOGFILE" | head -5 >> "$OUTPUT"
fi

cat << EOF >> "$OUTPUT"

================================================================================
RECOMMENDATIONS
================================================================================
1. Review and validate all extracted IOCs
2. Block malicious IP addresses at network perimeter
3. Add suspicious domains to DNS blacklist/sinkhole
4. Scan endpoints for additional compromise indicators
5. Review authentication logs for compromised accounts
6. Implement enhanced monitoring for detected attack patterns

================================================================================
EOF

echo "[*] IOC list generated: $OUTPUT"
cat "$OUTPUT"
```

**Usage:**

```bash
chmod +x generate_ioc_list.sh
./generate_ioc_list.sh /var/log/apache2/access.log
```

### Advanced IOC Extraction with Python

```python
#!/usr/bin/env python3
import re
import sys
import json
from datetime import datetime
from collections import defaultdict, Counter
from pathlib import Path


class IOCExtractor:
    def __init__(self):
        self.iocs = {
            'ips': defaultdict(lambda: {'first_seen': None, 'last_seen': None, 'count': 0, 'lines': []}),
            'domains': defaultdict(lambda: {'first_seen': None, 'last_seen': None, 'count': 0}),
            'urls': defaultdict(lambda: {'first_seen': None, 'last_seen': None, 'count': 0}),
            'emails': defaultdict(lambda: {'first_seen': None, 'last_seen': None, 'count': 0}),
            'hashes': defaultdict(lambda: {'algorithm': None, 'first_seen': None, 'count': 0}),
            'suspicious_patterns': defaultdict(list)
        }
        
        self.patterns = {
            'ip': re.compile(r'\b(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\b'),
            'url': re.compile(r'https?://[^\s<>"{}|\\^`\[\]]+', re.IGNORECASE),
            'email': re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b'),
            'md5': re.compile(r'\b[a-fA-F0-9]{32}\b'),
            'sha1': re.compile(r'\b[a-fA-F0-9]{40}\b'),
            'sha256': re.compile(r'\b[a-fA-F0-9]{64}\b'),
        }
        
        self.attack_patterns = {
            'sqli': re.compile(r"(union.*select|' or |1=1|sleep\(|benchmark\()", re.IGNORECASE),
            'xss': re.compile(r'(<script|javascript:|onerror=|onload=|<iframe)', re.IGNORECASE),
            'lfi': re.compile(r'(\.\./|%2e%2e|/etc/passwd|/proc/)', re.IGNORECASE),
            'rce': re.compile(r'(;.*\||&&|`|\$\(|%0a)', re.IGNORECASE),
        }
    
    def extract_timestamp(self, line):
        """Extract timestamp from log line"""
        # Common log timestamp patterns
        timestamp_patterns = [
            r'\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2}',
            r'\d{2}/\w{3}/\d{4}:\d{2}:\d{2}:\d{2}',
            r'\w{3}\s+\d{1,2}\s+\d{2}:\d{2}:\d{2}',
        ]
        
        for pattern in timestamp_patterns:
            match = re.search(pattern, line)
            if match:
                return match.group(0)
        return datetime.now().isoformat()
    
    def process_log(self, logfile):
        """Process log file and extract IOCs"""
        print(f"[*] Processing {logfile}...")
        
        with open(logfile, 'r', errors='ignore') as f:
            for line_num, line in enumerate(f, 1):
                timestamp = self.extract_timestamp(line)
                
                # Extract IPs
                for ip in self.patterns['ip'].findall(line):
                    ioc = self.iocs['ips'][ip]
                    if not ioc['first_seen']:
                        ioc['first_seen'] = timestamp
                    ioc['last_seen'] = timestamp
                    ioc['count'] += 1
                    if len(ioc['lines']) < 5:  # Store first 5 occurrences
                        ioc['lines'].append(f"Line {line_num}: {line.strip()[:100]}")
                
                # Extract URLs and domains
                for url in self.patterns['url'].findall(line):
                    ioc = self.iocs['urls'][url]
                    if not ioc['first_seen']:
                        ioc['first_seen'] = timestamp
                    ioc['last_seen'] = timestamp
                    ioc['count'] += 1
                    
                    # Extract domain from URL
                    domain_match = re.search(r'https?://([^/:]+)', url)
                    if domain_match:
                        domain = domain_match.group(1)
                        domain_ioc = self.iocs['domains'][domain]
                        if not domain_ioc['first_seen']:
                            domain_ioc['first_seen'] = timestamp
                        domain_ioc['last_seen'] = timestamp
                        domain_ioc['count'] += 1
                
                # Extract emails
                for email in self.patterns['email'].findall(line):
                    ioc = self.iocs['emails'][email]
                    if not ioc['first_seen']:
                        ioc['first_seen'] = timestamp
                    ioc['last_seen'] = timestamp
                    ioc['count'] += 1
                
                # Extract hashes
                for md5 in self.patterns['md5'].findall(line):
                    ioc = self.iocs['hashes'][md5]
                    ioc['algorithm'] = 'MD5'
                    if not ioc['first_seen']:
                        ioc['first_seen'] = timestamp
                    ioc['count'] += 1
                
                for sha1 in self.patterns['sha1'].findall(line):
                    ioc = self.iocs['hashes'][sha1]
                    ioc['algorithm'] = 'SHA-1'
                    if not ioc['first_seen']:
                        ioc['first_seen'] = timestamp
                    ioc['count'] += 1
                
                for sha256 in self.patterns['sha256'].findall(line):
                    ioc = self.iocs['hashes'][sha256]
                    ioc['algorithm'] = 'SHA-256'
                    if not ioc['first_seen']:
                        ioc['first_seen'] = timestamp
                    ioc['count'] += 1
                
                # Check for attack patterns
                for attack_type, pattern in self.attack_patterns.items():
                    if pattern.search(line):
                        self.iocs['suspicious_patterns'][attack_type].append({
                            'line_num': line_num,
                            'timestamp': timestamp,
                            'content': line.strip()[:200]
                        })
    
    def generate_text_report(self, output_file='ioc_report.txt'):
        """Generate human-readable text report"""
        with open(output_file, 'w') as f:
            f.write("=" * 80 + "\n")
            f.write("INDICATOR OF COMPROMISE (IOC) REPORT\n")
            f.write("=" * 80 + "\n")
            f.write(f"Generated: {datetime.now().isoformat()}\n")
            f.write(f"Analyst: Automated IOC Extractor\n\n")
            
            # IP Addresses
            f.write("=" * 80 + "\n")
            f.write(f"IP ADDRESSES: {len(self.iocs['ips'])} unique\n")
            f.write("=" * 80 + "\n")
            sorted_ips = sorted(self.iocs['ips'].items(), key=lambda x: x[1]['count'], reverse=True)
            for ip, data in sorted_ips[:50]:  # Top 50
                f.write(f"\nIP: {ip}\n")
                f.write(f"  First Seen: {data['first_seen']}\n")
                f.write(f"  Last Seen: {data['last_seen']}\n")
                f.write(f"  Occurrences: {data['count']}\n")
                if data['lines']:
                    f.write(f"  Sample entries:\n")
                    for line in data['lines'][:3]:
                        f.write(f"    {line}\n")
            
            # Domains
            f.write("\n" + "=" * 80 + "\n")
            f.write(f"DOMAINS: {len(self.iocs['domains'])} unique\n")
            f.write("=" * 80 + "\n")
            sorted_domains = sorted(self.iocs['domains'].items(), key=lambda x: x[1]['count'], reverse=True)
            for domain, data in sorted_domains[:30]:
                f.write(f"{domain:50s} | First: {data['first_seen']} | Count: {data['count']}\n")
            
            # URLs
            f.write("\n" + "=" * 80 + "\n")
            f.write(f"URLS: {len(self.iocs['urls'])} unique\n")
            f.write("=" * 80 + "\n")
            sorted_urls = sorted(self.iocs['urls'].items(), key=lambda x: x[1]['count'], reverse=True)
            for url, data in sorted_urls[:30]:
                f.write(f"\n{url}\n")
                f.write(f"  First: {data['first_seen']} | Last: {data['last_seen']} | Count: {data['count']}\n")
            
            # Emails
            f.write("\n" + "=" * 80 + "\n")
            f.write(f"EMAIL ADDRESSES: {len(self.iocs['emails'])} unique\n")
            f.write("=" * 80 + "\n")
            sorted_emails = sorted(self.iocs['emails'].items(), key=lambda x: x[1]['count'], reverse=True)
            for email, data in sorted_emails:
                f.write(f"{email:40s} | First: {data['first_seen']} | Count: {data['count']}\n")
            
            # File Hashes
            f.write("\n" + "=" * 80 + "\n")
            f.write(f"FILE HASHES: {len(self.iocs['hashes'])} unique\n")
            f.write("=" * 80 + "\n")
            for hash_val, data in self.iocs['hashes'].items():
                f.write(f"{data['algorithm']:8s} | {hash_val} | First: {data['first_seen']}\n")
            
            # Suspicious Patterns
            f.write("\n" + "=" * 80 + "\n")
            f.write("SUSPICIOUS PATTERNS DETECTED\n")
            f.write("=" * 80 + "\n")
            for attack_type, occurrences in self.iocs['suspicious_patterns'].items():
                f.write(f"\n[!] {attack_type.upper()}: {len(occurrences)} occurrences\n")
                for occ in occurrences[:10]:  # First 10 of each type
                    f.write(f"  Line {occ['line_num']} [{occ['timestamp']}]:\n")
                    f.write(f"    {occ['content']}\n")
            
            f.write("\n" + "=" * 80 + "\n")
            f.write("END OF REPORT\n")
            f.write("=" * 80 + "\n")
        
        print(f"[+] Text report saved to {output_file}")
    
    def generate_json_report(self, output_file='ioc_report.json'):
        """Generate machine-readable JSON report"""
        json_data = {
            'metadata': {
                'generated': datetime.now().isoformat(),
                'version': '1.0',
                'analyst': 'Automated IOC Extractor'
            },
            'statistics': {
                'total_ips': len(self.iocs['ips']),
                'total_domains': len(self.iocs['domains']),
                'total_urls': len(self.iocs['urls']),
                'total_emails': len(self.iocs['emails']),
                'total_hashes': len(self.iocs['hashes']),
                'total_suspicious_patterns': sum(len(v) for v in self.iocs['suspicious_patterns'].values())
            },
            'iocs': {
                'ips': dict(self.iocs['ips']),
                'domains': dict(self.iocs['domains']),
                'urls': dict(self.iocs['urls']),
                'emails': dict(self.iocs['emails']),
                'hashes': dict(self.iocs['hashes']),
                'suspicious_patterns': dict(self.iocs['suspicious_patterns'])
            }
        }
        
        with open(output_file, 'w') as f:
            json.dump(json_data, f, indent=2, default=str)
        
        print(f"[+] JSON report saved to {output_file}")
    
    def generate_csv_reports(self, prefix='ioc'):
        """Generate CSV files for each IOC type"""
        import csv
        
        # IPs CSV
        with open(f'{prefix}_ips.csv', 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['IP Address', 'First Seen', 'Last Seen', 'Count'])
            for ip, data in sorted(self.iocs['ips'].items(), key=lambda x: x[1]['count'], reverse=True):
                writer.writerow([ip, data['first_seen'], data['last_seen'], data['count']])
        
        # Domains CSV
        with open(f'{prefix}_domains.csv', 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['Domain', 'First Seen', 'Last Seen', 'Count'])
            for domain, data in sorted(self.iocs['domains'].items(), key=lambda x: x[1]['count'], reverse=True):
                writer.writerow([domain, data['first_seen'], data['last_seen'], data['count']])
        
        # URLs CSV
        with open(f'{prefix}_urls.csv', 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['URL', 'First Seen', 'Last Seen', 'Count'])
            for url, data in sorted(self.iocs['urls'].items(), key=lambda x: x[1]['count'], reverse=True):
                writer.writerow([url, data['first_seen'], data['last_seen'], data['count']])
        
        print(f"[+] CSV reports saved with prefix '{prefix}_'")
    
    def generate_stix_bundle(self, output_file='ioc_stix.json'):
        """Generate STIX 2.1 bundle"""
        stix_bundle = {
            "type": "bundle",
            "id": f"bundle--{datetime.now().strftime('%Y%m%d-%H%M%S')}",
            "objects": []
        }
        
        # Add IP indicators
        for ip, data in list(self.iocs['ips'].items())[:100]:  # Limit for practicality
            indicator = {
                "type": "indicator",
                "spec_version": "2.1",
                "id": f"indicator--ip-{ip.replace('.', '-')}",
                "created": datetime.utcnow().isoformat() + "Z",
                "modified": datetime.utcnow().isoformat() + "Z",
                "name": f"Malicious IP: {ip}",
                "description": f"Observed {data['count']} times. First seen: {data['first_seen']}",
                "pattern": f"[ipv4-addr:value = '{ip}']",
                "pattern_type": "stix",
                "valid_from": data['first_seen'] if isinstance(data['first_seen'], str) else datetime.utcnow().isoformat() + "Z"
            }
            stix_bundle["objects"].append(indicator)
        
        # Add domain indicators
        for domain, data in list(self.iocs['domains'].items())[:100]:
            indicator = {
                "type": "indicator",
                "spec_version": "2.1",
                "id": f"indicator--domain-{hash(domain)}",
                "created": datetime.utcnow().isoformat() + "Z",
                "modified": datetime.utcnow().isoformat() + "Z",
                "name": f"Malicious Domain: {domain}",
                "description": f"Observed {data['count']} times",
                "pattern": f"[domain-name:value = '{domain}']",
                "pattern_type": "stix",
                "valid_from": data['first_seen'] if isinstance(data['first_seen'], str) else datetime.utcnow().isoformat() + "Z"
            }
            stix_bundle["objects"].append(indicator)
        
        with open(output_file, 'w') as f:
            json.dump(stix_bundle, f, indent=2)
        
        print(f"[+] STIX bundle saved to {output_file}")


if __name__ == '__main__':
    if len(sys.argv) < 2:
        print(f"Usage: {sys.argv[0]} <logfile>")
        sys.exit(1)
    
    extractor = IOCExtractor()
    extractor.process_log(sys.argv[1])
    
    print("\n[*] Generating reports...")
    extractor.generate_text_report()
    extractor.generate_json_report()
    extractor.generate_csv_reports()
    extractor.generate_stix_bundle()
    
    print("\n[*] IOC extraction complete!")
````

**Usage:**
```bash
chmod +x ioc_extractor.py
./ioc_extractor.py /var/log/apache2/access.log
````

### OpenIOC Format Export

**Generate OpenIOC XML format (used by many security tools):**

```python
#!/usr/bin/env python3
import sys
from datetime import datetime
from xml.etree.ElementTree import Element, SubElement, tostring
from xml.dom import minidom

def create_openioc(iocs, output_file='indicators.ioc'):
    """Create OpenIOC XML format"""
    
    # Root element
    ioc = Element('ioc', {
        'xmlns': 'http://schemas.mandiant.com/2010/ioc',
        'xmlns:xsi': 'http://www.w3.org/2001/XMLSchema-instance',
        'id': f'ioc-{datetime.now().strftime("%Y%m%d-%H%M%S")}',
        'last-modified': datetime.utcnow().isoformat() + 'Z'
    })
    
    # Metadata
    metadata = SubElement(ioc, 'metadata')
    SubElement(metadata, 'short_description').text = 'Automated IOC Extraction'
    SubElement(metadata, 'description').text = 'IOCs extracted from log analysis'
    SubElement(metadata, 'authored_by').text = 'IOC Extractor Tool'
    SubElement(metadata, 'authored_date').text = datetime.utcnow().isoformat() + 'Z'
    
    # Definition
    definition = SubElement(ioc, 'definition')
    indicator = SubElement(definition, 'Indicator', {'operator': 'OR'})
    
    # Add IP indicators
    for ip in iocs.get('ips', [])[:50]:
        indicator_item = SubElement(indicator, 'IndicatorItem', {'condition': 'is'})
        SubElement(indicator_item, 'Context', {'document': 'Network', 'search': 'Network/IP'})
        SubElement(indicator_item, 'Content', {'type': 'IP'}).text = ip
    
    # Add domain indicators
    for domain in iocs.get('domains', [])[:50]:
        indicator_item = SubElement(indicator, 'IndicatorItem', {'condition': 'contains'})
        SubElement(indicator_item, 'Context', {'document': 'Network', 'search': 'Network/DNS'})
        SubElement(indicator_item, 'Content', {'type': 'string'}).text = domain
    
    # Add URL indicators
    for url in iocs.get('urls', [])[:50]:
        indicator_item = SubElement(indicator, 'IndicatorItem', {'condition': 'contains'})
        SubElement(indicator_item, 'Context', {'document': 'Network', 'search': 'Network/URI'})
        SubElement(indicator_item, 'Content', {'type': 'string'}).text = url
    
    # Pretty print
    xml_str = minidom.parseString(tostring(ioc)).toprettyxml(indent="  ")
    
    with open(output_file, 'w') as f:
        f.write(xml_str)
    
    print(f"[+] OpenIOC file saved to {output_file}")

if __name__ == '__main__':
    # Example usage
    iocs = {
        'ips': ['192.0.2.1', '203.0.113.45'],
        'domains': ['malicious.example.com', 'evil.com'],
        'urls': ['http://malicious.example.com/payload.sh']
    }
    create_openioc(iocs)
```

## Timeline Documentation

Timeline documentation provides chronological reconstruction of events, critical for understanding attack progression, cause-effect relationships, and incident response.

### Timeline Structure

**Standard timeline format:**

```bash
cat << 'EOF' > timeline_template.txt
================================================================================
INCIDENT TIMELINE
================================================================================
Case ID: [Unique identifier]
Incident Type: [Intrusion/Data Breach/Malware/etc.]
Analysis Date: [Date]
Analyst: [Name]
Timezone: UTC (All times in UTC unless specified)

================================================================================
EXECUTIVE SUMMARY
================================================================================
[Brief overview of the incident timeline]

Key Dates:
- Initial Compromise: [Date/Time]
- Detection: [Date/Time]
- Containment: [Date/Time]
- Total Dwell Time: [Duration]

================================================================================
DETAILED TIMELINE
================================================================================

[YYYY-MM-DD HH:MM:SS UTC] - EVENT DESCRIPTION
Source: [Log source]
Severity: [Critical/High/Medium/Low/Info]
Category: [Initial Access/Execution/Persistence/Privilege Escalation/etc.]
Details: [Detailed description]
Evidence: [Specific log entries or artifacts]
---

[Next event...]

================================================================================
ATTACK CHAIN SUMMARY
================================================================================
1. Initial Access: [Method and time]
2. Execution: [What was executed]
3. Persistence: [How attacker maintained access]
4. Privilege Escalation: [How escalation occurred]
5. Defense Evasion: [Evasion techniques used]
6. Credential Access: [Credentials obtained]
7. Discovery: [Reconnaissance performed]
8. Lateral Movement: [Systems accessed]
9. Collection: [Data collected]
10. Exfiltration: [Data exfiltrated]
11. Impact: [Damage caused]

================================================================================
ARTIFACTS & EVIDENCE
================================================================================
[List of log files, screenshots, memory dumps, etc. that support timeline]

================================================================================
EOF
```

### Automated Timeline Generation

**Basic timeline extractor:**

```bash
#!/bin/bash
# generate_timeline.sh

LOGFILE="$1"
OUTPUT="timeline_$(date +%Y%m%d_%H%M%S).txt"

cat << EOF > "$OUTPUT"
================================================================================
INCIDENT TIMELINE
================================================================================
Generated: $(date)
Source: $LOGFILE
Timezone: $(date +%Z)

================================================================================
DETAILED TIMELINE
================================================================================

EOF

# Extract events with timestamps
echo "[*] Extracting timeline events..."

# Parse log lines with timestamps and categorize
while IFS= read -r line; do
    # Extract timestamp (adjust regex based on log format)
    if [[ $line =~ ([A-Z][a-z]{2}[[:space:]]+[0-9]{1,2}[[:space:]]+[0-9]{2}:[0-9]{2}:[0-9]{2}) ]]; then
        timestamp="${BASH_REMATCH[1]}"
        
        # Categorize events
        category="INFO"
        if echo "$line" | grep -qi "failed\|error\|denied"; then
            category="WARNING"
        fi
        if echo "$line" | grep -qi "accepted\|success\|established"; then
            category="SUCCESS"
        fi
        if echo "$line" | grep -qi "root\|sudo\|admin"; then
            category="PRIVILEGED"
        fi
        
        echo "[$timestamp] [$category]" >> "$OUTPUT"
        echo "  $line" >> "$OUTPUT"
        echo "---" >> "$OUTPUT"
    fi
done < "$LOGFILE"

echo "[*] Timeline generated: $OUTPUT"
```

**Advanced Python timeline generator:**

```python
#!/usr/bin/env python3
import re
import sys
from datetime import datetime
from collections import defaultdict

class TimelineGenerator:
    def __init__(self):
        self.events = []
        self.timestamp_patterns = [
            # ISO 8601
            (r'(\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2}(?:\.\d+)?(?:Z|[+-]\d{2}:\d{2})?)', '%Y-%m-%dT%H:%M:%S'),
            # Apache/nginx
            (r'(\d{2}/\w{3}/\d{4}:\d{2}:\d{2}:\d{2} [+-]\d{4})', '%d/%b/%Y:%H:%M:%S %z'),
            # Syslog
            (r'(\w{3}\s+\d{1,2}\s+\d{2}:\d{2}:\d{2})', '%b %d %H:%M:%S'),
            # Windows Event Log
            (r'(\d{1,2}/\d{1,2}/\d{4}\s+\d{1,2}:\d{2}:\d{2}\s+[AP]M)', '%m/%d/%Y %I:%M:%S %p'),
        ]
        
        self.event_categories = {
            'authentication': [
                (r'failed password|authentication failure|login failed', 'AUTH_FAIL'),
                (r'accepted password|login successful|authenticated', 'AUTH_SUCCESS'),
                (r'session opened|logged in', 'SESSION_START'),
                (r'session closed|logged out', 'SESSION_END'),
            ],
            'privilege': [
                (r'sudo|privilege|escalation|root', 'PRIVILEGE_USE'),
            ],
            'network': [
                (r'connection (established|accepted)|ESTABLISHED', 'CONN_ESTABLISHED'),
                (r'connection (closed|terminated)|CLOSE', 'CONN_CLOSED'),
            ],
            'file': [
                (r'created|wrote|uploaded', 'FILE_CREATE'),
                (r'deleted|removed', 'FILE_DELETE'),
                (r'modified|changed', 'FILE_MODIFY'),
                (r'accessed|read|opened', 'FILE_ACCESS'),
            ],
            'web': [
                (r'" (200|201|204) ', 'HTTP_SUCCESS'),
                (r'" (301|302|303|307|308) ', 'HTTP_REDIRECT'),
                (r'" (400|401|403|404) ', 'HTTP_CLIENT_ERROR'),
                (r'" (500|501|502|503) ', 'HTTP_SERVER_ERROR'),
            ],
            'attack': [
                (r'union.*select|sql injection', 'ATTACK_SQLI'),
                (r'<script|javascript:|xss', 'ATTACK_XSS'),
                (r'\.\./|path traversal', 'ATTACK_LFI'),
                (r';.*\||command injection', 'ATTACK_RCE'),
            ]
        }
    
    def parse_timestamp(self, line):
        """Extract and parse timestamp from log line"""
        for pattern, fmt in self.timestamp_patterns:
            match = re.search(pattern, line)
            if match:
                timestamp_str = match.group(1)
                try:
                    # Handle various formats
                    if 'T' in timestamp_str or ' ' in timestamp_str[:10]:
                        # ISO format
                        timestamp_str = timestamp_str.replace('T', ' ').split('.')[0].split('+')[0].split('Z')[0]
                        return datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')
                    elif '/' in timestamp_str and ':' in timestamp_str:
                        # Apache format
                        return datetime.strptime(timestamp_str.split()[0], '%d/%b/%Y:%H:%M:%S')
                    else:
                        # Try other formats
                        return datetime.strptime(timestamp_str, fmt.split()[0])
                except:
                    continue
        return None
    
    def categorize_event(self, line):
        """Categorize event based on content"""
        categories = []
        severity = 'INFO'
        
        for category_name, patterns in self.event_categories.items():
            for pattern, event_type in patterns:
                if re.search(pattern, line, re.IGNORECASE):
                    categories.append(event_type)
                    
                    # Determine severity
                    if 'ATTACK' in event_type or 'FAIL' in event_type:
                        severity = 'HIGH'
                    elif 'ERROR' in event_type:
                        severity = 'MEDIUM'
        
        return categories if categories else ['UNKNOWN'], severity
    
    def process_log(self, logfile):
        """Process log file and build timeline"""
        print(f"[*] Processing {logfile}...")
        
        with open(logfile, 'r', errors='ignore') as f:
            for line_num, line in enumerate(f, 1):
                timestamp = self.parse_timestamp(line)
                if not timestamp:
                    continue
                
                categories, severity = self.categorize_event(line)
                
                event = {
                    'timestamp': timestamp,
                    'line_num': line_num,
                    'categories': categories,
                    'severity': severity,
                    'content': line.strip()
                }
                
                self.events.append(event)
        
        # Sort events by timestamp
        self.events.sort(key=lambda x: x['timestamp'])
        print(f"[+] Extracted {len(self.events)} timeline events")
    
    def generate_text_timeline(self, output_file='timeline.txt'):
        """Generate human-readable timeline"""
        with open(output_file, 'w') as f:
            f.write("=" * 100 + "\n")
            f.write("INCIDENT TIMELINE\n")
            f.write("=" * 100 + "\n")
            f.write(f"Generated: {datetime.now().isoformat()}\n")
            f.write(f"Total Events: {len(self.events)}\n")
            
            if self.events:
                f.write(f"First Event: {self.events[0]['timestamp']}\n")
                f.write(f"Last Event: {self.events[-1]['timestamp']}\n")
                duration = self.events[-1]['timestamp'] - self.events[0]['timestamp']
                f.write(f"Timeline Duration: {duration}\n")
            
            f.write("\n" + "=" * 100 + "\n")
            f.write("DETAILED TIMELINE\n")
            f.write("=" * 100 + "\n\n")
            
            for event in self.events:
                f.write(f"[{event['timestamp'].isoformat()}] ")
                f.write(f"[{event['severity']}] ")
                f.write(f"[{', '.join(event['categories'])}]\n")
                f.write(f"  Line {event['line_num']}: {event['content'][:150]}\n")
                f.write("  " + "-" * 96 + "\n\n")
        
        print(f"[+] Timeline saved to {output_file}")
    
    def generate_attack_chain(self, output_file='attack_chain.txt'):
        """Generate MITRE ATT&CK chain visualization"""
        # Group events by category
        chain = defaultdict(list)
        
        mitre_mapping = {
            'AUTH_FAIL': 'Initial Access (T1078)',
            'AUTH_SUCCESS': 'Initial Access (T1078)',
            'PRIVILEGE_USE': 'Privilege Escalation (T1548)',
            'FILE_CREATE': 'Execution (T1059)',
            'CONN_ESTABLISHED': 'Command and Control (T1071)',
            'ATTACK_SQLI': 'Initial Access (T1190)',
            'ATTACK_RCE': 'Execution (T1059)',
        }
        
        for event in self.events:
            for category in event['categories']:
                if category in mitre_mapping:
                    chain[mitre_mapping[category]].append(event)
        
        with open(output_file, 'w') as f:
            f.write("=" * 100 + "\n")
            f.write("ATTACK CHAIN ANALYSIS (MITRE ATT&CK)\n")
            f.write("=" * 100 + "\n\n")
            
            for tactic, events in sorted(chain.items()):
                f.write(f"\n{tactic}\n")
                f.write("-" * 100 + "\n")
                f.write(f"Events: {len(events)}\n")
                f.write(f"First Occurrence: {events[0]['timestamp']}\n")
                f.write(f"Last Occurrence: {events[-1]['timestamp']}\n\n")
                
                for event in events[:5]:  # Show first 5
                    f.write(f"  [{event['timestamp']}] {event['content'][:100]}\n")
                
                if len(events) > 5:
                    f.write(f"  ... and {len(events) - 5} more\n")
        
        print(f"[+] Attack chain saved to {output_file}")
    
    def generate_visual_timeline(self, output_file='timeline.html'):
        """Generate HTML visual timeline"""
        html_template = """
<!DOCTYPE html>
<html>
<head>
    <title>Incident Timeline</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; background: #f5f5f5; }
        .timeline { max-width: 1200px; margin: 0 auto; background: white; padding: 20px; }
        .event { margin: 20px 0; padding: 15px; border-left: 4px solid #ccc; background: #fafafa; }
        .event.HIGH { border-left-color: #d32f2f; }
        .event.MEDIUM { border-left-color: #f57c00; }
        .event.INFO { border-left-color: #1976d2; }
        .timestamp { font-weight: bold; color: #555; }
        .categories { display: inline-block; padding: 3px 8px; background: #e0e0e0; 
                      border-radius: 3px; font-size: 12px; margin-right: 5px; }
        .content { margin-top: 10px; font-family: monospace; font-size: 13px; }
        h1 { color: #333; }
        .stats { background: #e3f2fd; padding: 15px; border-radius: 5px; margin-bottom: 20px; }
    </style>
</head>
<body>
    <div class="timeline">
        <h1>Incident Timeline</h1>
        <div class="stats">
            <strong>Total Events:</strong> {total_events}<br>
            <strong>Timeline Duration:</strong> {duration}<br>
            <strong>Generated:</strong> {generated}
        </div>
        {events_html}
    </div>
</body>
</html>
"""
        
        events_html = ""
        for event in self.events:
            categories_html = "".join([f'<span class="categories">{cat}</span>' for cat in event['categories']])
            events_html += f"""
        <div class="event {event['severity']}">
            <div class="timestamp">{event['timestamp'].isoformat()}</div>
            <div>{categories_html}</div>
            <div class="content">{event['content'][:200]}</div>
        </div>
"""
        
        duration = "N/A"
        if len(self.events) > 1:
            duration = str(self.events[-1]['timestamp'] - self.events[0]['timestamp'])
        
        html = html_template.format(
            total_events=len(self.events),
            duration=duration,
            generated=datetime.now().isoformat(),
            events_html=events_html
        )
        
        with open(output_file, 'w') as f:
            f.write(html)
        
        print(f"[+] Visual timeline saved to {output_file}")
    
    def export_csv(self, output_file='timeline.csv'):
        """Export timeline to CSV"""
        import csv
        
        with open(output_file, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['Timestamp', 'Severity', 'Categories', 'Line Number', 'Content'])
            
            for event in self.events:
                writer.writerow([
                    event['timestamp'].isoformat(),
                    event['severity'],
                    ', '.join(event['categories']),
                    event['line_num'],
                    event['content']
                ])
        
        print(f"[+] CSV timeline saved to {output_file}")

if __name__ == '__main__':
    if len(sys.argv) < 2:
        print(f"Usage: {sys.argv[0]} <logfile>")
        sys.exit(1)
    
    generator = TimelineGenerator()
    generator.process_log(sys.argv[1])
    
    print("\n[*] Generating timeline reports...")
    generator.generate_text_timeline()
    generator.generate_attack_chain()
    generator.generate_visual_timeline()
    generator.export_csv()
    
    print("\n[*] Timeline generation complete!")
```

**Usage:**

```bash
chmod +x timeline_generator.py
./timeline_generator.py /var/log/auth.log
```

### Correlation Timeline

**Multi-source timeline correlation:**

```bash
#!/bin/bash
# correlate_logs.sh - Merge multiple log sources into single timeline

OUTPUT="correlated_timeline.txt"

echo "Correlating multiple log sources..." > "$OUTPUT"
echo "Generated: $(date)" >> "$OUTPUT"
echo "========================================" >> "$OUTPUT"

# Extract timestamped events from multiple sources
{
    # Web logs
    awk '{print $4, $5, "WEB:", $0}' /var/log/apache2/access.log 2>/dev/null
    
    # Auth logs
    awk '{print $1, $2, $3, "AUTH:", $0}' /var/log/auth.log 2>/dev/null
    
    # Syslog
    awk '{print $1, $2, $3, "SYS:", $0}' /var/log/syslog 2>/dev/null
    
    # Application logs
    grep -E '^\d{4}-\d{2}-\d{2}' /var/log/app.log 2>/dev/null | awk '{print $1, $2, "APP:", $0}'
    
} | sort >> "$OUTPUT"

echo "[*] Correlated timeline saved to $OUTPUT"
```

---

**Important related topics:**

- Graphical timeline tools (Timesketch, Plaso)
- Log correlation engines (SIEM integration)
- Automated report generation with templates
- Export formats for threat intelligence platforms (MISP, TheHive)
- Chain-of-custody documentation for forensic evidence

---

# Log Analysis Frameworks

Log analysis frameworks aggregate, normalize, and visualize log data at scale. Essential for correlating events across multiple sources, detecting attack patterns, and performing temporal analysis in complex CTF environments.

## SIEM Concepts

Security Information and Event Management (SIEM) systems centralize log collection, perform real-time analysis, correlate events, and generate alerts. Understanding SIEM architecture is critical for enterprise-level log analysis scenarios.

**Core SIEM components:**

```

                  SIEM Architecture                   

  Log Sources  Collection  Normalization          
  Correlation  Alerting  Storage  Visualization   

```

**Log collection methods:**

```bash
# Agent-based collection (installed on source systems)
# - Syslog agents (rsyslog, syslog-ng)
# - Proprietary agents (Splunk Forwarder, Elastic Agent)

# Agentless collection
# - Syslog forwarding
# - API polling
# - File sharing (NFS, CIFS)

# Configure rsyslog forwarding to SIEM
cat >> /etc/rsyslog.conf << 'EOF'
# Forward all logs to SIEM at 192.168.1.100:514
*.* @192.168.1.100:514

# UDP forwarding: @ip:port
# TCP forwarding: @@ip:port
# TLS forwarding: @@(o)ip:port with TLS configuration
EOF

systemctl restart rsyslog

# Test syslog forwarding
logger -p local0.info "Test message to SIEM"
```

**Event normalization concepts:**

```bash
# Raw log formats vary significantly
# Apache: 192.168.1.10 - - [29/Oct/2025:10:15:30 +0000] "GET /admin HTTP/1.1" 403
# Syslog: Oct 29 10:15:30 server sshd[1234]: Failed password for admin from 192.168.1.10
# Windows: EventID 4625, Logon Type 3, Account: admin, Source: 192.168.1.10

# Normalized format (Common Event Format example)
# CEF:0|Vendor|Product|Version|EventID|Name|Severity|Extension
# CEF:0|Apache|WebServer|2.4|403|Access Denied|5|src=192.168.1.10 dst=192.168.1.1

# Parse to normalized format
normalize_log() {
    local raw_log="$1"
    
    # Extract common fields
    local timestamp=$(echo "$raw_log" | grep -oP '\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}')
    local src_ip=$(echo "$raw_log" | grep -oP '\b\d{1,3}(\.\d{1,3}){3}\b' | head -1)
    local event_type=$(echo "$raw_log" | grep -oP '(Failed|Accepted|Denied|Error)')
    
    # Output in normalized format
    echo "timestamp=$timestamp|src_ip=$src_ip|event_type=$event_type|raw=$raw_log"
}
```

**Event correlation principles:**

```bash
# Correlation Rule Example: Brute Force Detection
# IF: 5+ failed logins from same source IP within 60 seconds
# THEN: Generate "Brute Force Attack" alert

# Simple correlation implementation
detect_brute_force() {
    local log_file="$1"
    local threshold=5
    local timewindow=60
    
    # Extract failed login timestamps and source IPs
    grep "Failed password" "$log_file" | \
        awk '{print $1" "$2" "$3, $11}' | \
        python3 << 'EOF'
import sys
from datetime import datetime
from collections import defaultdict

events = defaultdict(list)

for line in sys.stdin:
    parts = line.strip().split()
    timestamp_str = ' '.join(parts[:3])
    src_ip = parts[3]
    
    try:
        # Parse timestamp (simplified)
        timestamp = datetime.strptime(timestamp_str, "%b %d %H:%M:%S")
        events[src_ip].append(timestamp)
    except:
        continue

# Check for threshold violations
threshold = 5
timewindow = 60

for ip, timestamps in events.items():
    timestamps.sort()
    for i in range(len(timestamps) - threshold + 1):
        window_start = timestamps[i]
        window_end = timestamps[i + threshold - 1]
        
        if (window_end - window_start).seconds <= timewindow:
            print(f"[ALERT] Brute force detected from {ip}")
            print(f"  {threshold} attempts in {(window_end - window_start).seconds}s")
            print(f"  First: {window_start}, Last: {window_end}")
            break
EOF
}

# Usage
detect_brute_force /var/log/auth.log
```

**Multi-stage attack correlation:**

```bash
# Correlation: Reconnaissance  Exploitation  Post-Exploitation
# Stage 1: Port scan (multiple connection attempts)
# Stage 2: Vulnerability exploit (SQL injection, RCE)
# Stage 3: Privilege escalation (sudo attempts, file access)

# Correlation script
correlate_attack_chain() {
    local start_time="$1"  # e.g., "2025-10-29 10:00:00"
    local end_time="$2"
    
    echo "[*] Analyzing attack chains between $start_time and $end_time"
    
    # Stage 1: Identify port scans
    echo "[+] Stage 1: Port Scan Detection"
    awk -v start="$start_time" -v end="$end_time" '
        $0 >= start && $0 <= end && /SYN.*port/ {
            print $11  # Source IP
        }
    ' /var/log/syslog | sort | uniq -c | awk '$1 > 10 {print $2}' > suspicious_ips.txt
    
    # Stage 2: Check for exploitation attempts from those IPs
    echo "[+] Stage 2: Exploitation Attempts"
    while read ip; do
        grep "$ip" /var/log/apache2/access.log | \
            grep -E "(SELECT|UNION|\.\.\/|exec\(|eval\()" && \
            echo "[ALERT] Exploitation attempt from $ip"
    done < suspicious_ips.txt
    
    # Stage 3: Check for post-exploitation activity
    echo "[+] Stage 3: Post-Exploitation"
    while read ip; do
        grep "$ip" /var/log/auth.log | \
            grep -E "(sudo|su |Accepted)" && \
            echo "[ALERT] Post-exploitation activity from $ip"
    done < suspicious_ips.txt
}

# Usage
correlate_attack_chain "2025-10-29 10:00:00" "2025-10-29 12:00:00"
```

**SIEM query concepts (vendor-agnostic):**

```bash
# Time-based queries
# Search last 24 hours: timestamp:[now-24h TO now]
# Search specific range: timestamp:[2025-10-29T00:00:00 TO 2025-10-29T23:59:59]

# Field-based filtering
# source_ip:192.168.1.10
# event_type:failed_login
# user:admin AND event_type:privilege_escalation

# Aggregation concepts
# count by source_ip: | stats count by source_ip
# top 10 users: | stats count by user | sort -count | head 10
# time-based bucketing: | bucket timestamp span=1h | stats count by timestamp
```

**Alerting rule examples:**

```bash
# Alert rule structure (pseudo-code)
# RULE: Multiple Failed Logins
# CONDITION: count(event_type=failed_login) > 5 WHERE time_window=5m GROUP BY source_ip
# ACTIONS: send_email(soc@example.com), create_ticket(severity=medium)

# Implementation using simple bash tracking
#!/bin/bash
# alert_monitor.sh

ALERT_THRESHOLD=5
TIME_WINDOW=300  # 5 minutes in seconds
ALERT_LOG="/var/log/siem_alerts.log"

declare -A ip_failures
declare -A ip_timestamps

tail -F /var/log/auth.log | while read line; do
    if echo "$line" | grep -q "Failed password"; then
        timestamp=$(date +%s)
        src_ip=$(echo "$line" | awk '{print $11}')
        
        # Clean old entries outside time window
        for ip in "${!ip_timestamps[@]}"; do
            age=$((timestamp - ip_timestamps[$ip]))
            if [ $age -gt $TIME_WINDOW ]; then
                unset ip_failures[$ip]
                unset ip_timestamps[$ip]
            fi
        done
        
        # Increment failure count
        ip_failures[$src_ip]=$((${ip_failures[$src_ip]:-0} + 1))
        ip_timestamps[$src_ip]=$timestamp
        
        # Check threshold
        if [ ${ip_failures[$src_ip]} -ge $ALERT_THRESHOLD ]; then
            alert_msg="[$(date)] ALERT: Brute force from $src_ip (${ip_failures[$src_ip]} attempts)"
            echo "$alert_msg" | tee -a "$ALERT_LOG"
            
            # Reset counter to prevent alert spam
            ip_failures[$src_ip]=0
        fi
    fi
done
```

**SIEM use case examples:**

```bash
# Use Case 1: Detect privilege escalation
# Look for: sudo usage by non-admin users, especially to root shell
grep "sudo:" /var/log/auth.log | \
    grep -v "admin\|sysadmin" | \
    grep -E "sudo: .* : TTY=.* ; PWD=.* ; USER=root" | \
    awk '{print $1, $2, $3, $5}'

# Use Case 2: Detect data exfiltration
# Look for: Large outbound transfers, unusual hours, external destinations
awk '$9 > 10000000 {print $4, $1, $7, $9, $10}' /var/log/apache2/access.log | \
    column -t

# Use Case 3: Detect lateral movement
# Look for: SSH connections from internal hosts to other internal hosts
grep "Accepted password" /var/log/auth.log | \
    awk '{print $11}' | \
    while read ip; do
        if [[ $ip =~ ^192\.168\. ]] || [[ $ip =~ ^10\. ]]; then
            echo "[!] Internal SSH login from: $ip"
        fi
    done

# Use Case 4: Detect credential stuffing
# Look for: Many different usernames from same IP
grep "Failed password" /var/log/auth.log | \
    awk '{print $11, $9}' | \
    sort | uniq | \
    awk '{count[$1]++; users[$1]=users[$1]","$2} 
         END {for (ip in count) if (count[ip] > 10) 
              print ip, count[ip], "users:", users[ip]}'
```

## ELK Stack (Elasticsearch, Logstash, Kibana)

The ELK Stack provides open-source log aggregation, search, and visualization. Elasticsearch stores data, Logstash processes logs, Kibana visualizes results.

**ELK Stack architecture:**

```
          
   Log Source    Logstash   Elasticsearch 
  (Filebeat)         (Parser)            (Storage)  
          
                                                   
                                                   
                                           
                                               Kibana    
                                           (Visualization)
                                           
```

**Elasticsearch basics:**

```bash
# Check Elasticsearch status
curl -X GET "localhost:9200/_cat/health?v"

# List all indices
curl -X GET "localhost:9200/_cat/indices?v"

# Create index with mapping
curl -X PUT "localhost:9200/logs-2025.10.29" -H 'Content-Type: application/json' -d'
{
  "mappings": {
    "properties": {
      "timestamp": {"type": "date"},
      "source_ip": {"type": "ip"},
      "message": {"type": "text"},
      "severity": {"type": "keyword"}
    }
  }
}'

# Index a document
curl -X POST "localhost:9200/logs-2025.10.29/_doc" -H 'Content-Type: application/json' -d'
{
  "timestamp": "2025-10-29T10:15:30Z",
  "source_ip": "192.168.1.10",
  "message": "Failed login attempt",
  "severity": "warning"
}'

# Search documents
curl -X GET "localhost:9200/logs-2025.10.29/_search" -H 'Content-Type: application/json' -d'
{
  "query": {
    "match": {
      "message": "failed login"
    }
  }
}'
```

**Elasticsearch Query DSL examples:**

```bash
# Match query (full-text search)
curl -X GET "localhost:9200/logs-*/_search" -H 'Content-Type: application/json' -d'
{
  "query": {
    "match": {
      "message": "authentication failed"
    }
  }
}'

# Term query (exact match on keyword fields)
curl -X GET "localhost:9200/logs-*/_search" -H 'Content-Type: application/json' -d'
{
  "query": {
    "term": {
      "severity.keyword": "critical"
    }
  }
}'

# Range query (time-based)
curl -X GET "localhost:9200/logs-*/_search" -H 'Content-Type: application/json' -d'
{
  "query": {
    "range": {
      "timestamp": {
        "gte": "now-1h",
        "lte": "now"
      }
    }
  }
}'

# Boolean query (combine multiple conditions)
curl -X GET "localhost:9200/logs-*/_search" -H 'Content-Type: application/json' -d'
{
  "query": {
    "bool": {
      "must": [
        {"match": {"message": "failed"}},
        {"range": {"timestamp": {"gte": "now-24h"}}}
      ],
      "must_not": [
        {"term": {"source_ip": "127.0.0.1"}}
      ],
      "filter": [
        {"term": {"severity.keyword": "high"}}
      ]
    }
  }
}'

# Aggregation (count by field)
curl -X GET "localhost:9200/logs-*/_search" -H 'Content-Type: application/json' -d'
{
  "size": 0,
  "aggs": {
    "top_ips": {
      "terms": {
        "field": "source_ip",
        "size": 10
      }
    }
  }
}'

# Time-based histogram
curl -X GET "localhost:9200/logs-*/_search" -H 'Content-Type: application/json' -d'
{
  "size": 0,
  "aggs": {
    "events_over_time": {
      "date_histogram": {
        "field": "timestamp",
        "calendar_interval": "1h"
      }
    }
  }
}'
```

**Logstash configuration:**

```bash
# Logstash pipeline configuration
# /etc/logstash/conf.d/apache-logs.conf

input {
  file {
    path => "/var/log/apache2/access.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
  }
}

filter {
  grok {
    match => { 
      "message" => "%{COMBINEDAPACHELOG}"
    }
  }
  
  date {
    match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
    target => "@timestamp"
  }
  
  geoip {
    source => "clientip"
  }
  
  useragent {
    source => "agent"
    target => "user_agent"
  }
  
  # Add custom fields
  mutate {
    add_field => { "log_type" => "apache_access" }
    remove_field => [ "message" ]
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "apache-logs-%{+YYYY.MM.dd}"
  }
  
  # Debug output
  stdout { codec => rubydebug }
}
```

**Custom Grok patterns:**

```bash
# Grok patterns for custom log formats
# /etc/logstash/patterns/custom_patterns

# SSH authentication logs
SSHAUTH %{SYSLOGTIMESTAMP:timestamp} %{HOSTNAME:hostname} sshd\[%{POSINT:pid}\]: %{DATA:auth_status} %{DATA:auth_method} for %{USER:username} from %{IP:source_ip}

# Custom application log
APPLOG \[%{TIMESTAMP_ISO8601:timestamp}\] %{LOGLEVEL:severity} %{GREEDYDATA:message}

# SQL query log
SQLLOG %{TIMESTAMP_ISO8601:timestamp} \| User: %{USER:username} \| Query: %{GREEDYDATA:query}

# Use in Logstash filter
filter {
  grok {
    patterns_dir => ["/etc/logstash/patterns"]
    match => { 
      "message" => "%{SSHAUTH}"
    }
  }
}
```

**Logstash filter examples:**

```ruby
# Parse JSON logs
filter {
  json {
    source => "message"
  }
}

# Extract fields with regex
filter {
  grok {
    match => { 
      "message" => "User %{USER:username} from %{IP:source_ip} attempted action: %{WORD:action}"
    }
  }
}

# Conditional processing
filter {
  if [source_ip] =~ /^10\./ {
    mutate {
      add_tag => [ "internal" ]
    }
  }
  
  if [severity] == "critical" {
    mutate {
      add_field => { "alert_required" => "true" }
    }
  }
}

# Enrich with threat intelligence
filter {
  translate {
    field => "source_ip"
    destination => "threat_level"
    dictionary_path => "/etc/logstash/threat_intel.yml"
    fallback => "unknown"
  }
}

# Drop unwanted events
filter {
  if [source_ip] == "127.0.0.1" {
    drop { }
  }
}

# Anonymize sensitive data
filter {
  mutate {
    gsub => [
      "message", "\d{3}-\d{2}-\d{4}", "XXX-XX-XXXX",  # SSN
      "message", "\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b", "[email]"  # Email
    ]
  }
}
```

**Filebeat configuration (lightweight log shipper):**

```yaml
# /etc/filebeat/filebeat.yml

filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/auth.log
    - /var/log/syslog
  fields:
    log_type: system
    environment: production

- type: log
  enabled: true
  paths:
    - /var/log/apache2/*.log
  fields:
    log_type: apache
  multiline.pattern: '^[0-9]{4}-[0-9]{2}-[0-9]{2}'
  multiline.negate: true
  multiline.match: after

# Output to Logstash
output.logstash:
  hosts: ["localhost:5044"]

# Or output directly to Elasticsearch
output.elasticsearch:
  hosts: ["localhost:9200"]
  index: "filebeat-%{+yyyy.MM.dd}"

# Processing
processors:
  - drop_fields:
      fields: ["agent", "ecs", "host"]
  - add_host_metadata:
      when.not.contains.tags: forwarded
```

**Kibana query language (KQL):**

```bash
# Simple field match
source_ip: "192.168.1.10"

# Wildcard search
message: *failed*

# Boolean operators
message: "failed" AND severity: "high"
message: "error" OR message: "critical"
source_ip: "192.168.1.10" AND NOT user: "admin"

# Range queries
timestamp >= "2025-10-29T00:00:00" AND timestamp < "2025-10-29T23:59:59"
response_size > 1000000

# Exists check
user: *  # user field exists
NOT user: *  # user field does not exist

# Nested field access
user_agent.os.name: "Windows"

# IP range (CIDR)
source_ip: "192.168.1.0/24"

# Multiple values
severity: ("high" OR "critical")
user: (admin OR root OR system)
```

**Kibana visualization types:**

```bash
# Common visualization purposes:

# 1. Line chart - Events over time
# X-axis: @timestamp (date histogram)
# Y-axis: Count of events

# 2. Bar chart - Top values
# X-axis: source_ip (terms aggregation)
# Y-axis: Count

# 3. Pie chart - Distribution
# Slice: severity (terms)
# Size: Count

# 4. Data table - Detailed records
# Columns: timestamp, source_ip, user, message
# Sorted by timestamp descending

# 5. Metric - Single value
# Aggregation: Count of critical events
# Display: Large number

# 6. Heatmap - Correlation
# X-axis: Hour of day
# Y-axis: Day of week
# Color: Count of events
```

**Kibana dashboard for CTF scenario:**

```bash
# Dashboard: "Security Monitoring"

# Panel 1: Failed Login Attempts Over Time
# Visualization: Line chart
# Query: event_type: "failed_login"
# X-axis: @timestamp (1-hour intervals)
# Y-axis: Count

# Panel 2: Top Attack Source IPs
# Visualization: Bar chart
# Query: severity: "high" OR severity: "critical"
# Terms: source_ip (top 10)

# Panel 3: Attack Type Distribution
# Visualization: Pie chart
# Query: attack_detected: true
# Terms: attack_type

# Panel 4: Geographic Source Map
# Visualization: Coordinate map
# Query: *
# Geospatial field: source_ip (with geoip)

# Panel 5: Recent Critical Events
# Visualization: Data table
# Query: severity: "critical"
# Columns: @timestamp, source_ip, user, message
# Sort: @timestamp descending
# Size: 20
```

**Elasticsearch API scripting:**

```python
#!/usr/bin/env python3
# elk_query.py - Query Elasticsearch programmatically

from elasticsearch import Elasticsearch
from datetime import datetime, timedelta
import json

# Connect to Elasticsearch
es = Elasticsearch(['http://localhost:9200'])

# Search for failed logins in last hour
def search_failed_logins():
    query = {
        "query": {
            "bool": {
                "must": [
                    {"match": {"message": "failed"}},
                    {"range": {
                        "@timestamp": {
                            "gte": "now-1h",
                            "lte": "now"
                        }
                    }}
                ]
            }
        },
        "sort": [{"@timestamp": "desc"}],
        "size": 100
    }
    
    result = es.search(index="logs-*", body=query)
    
    print(f"[+] Found {result['hits']['total']['value']} failed logins")
    for hit in result['hits']['hits']:
        source = hit['_source']
        print(f"  {source.get('@timestamp')} | {source.get('source_ip')} | {source.get('user')}")

# Aggregate by source IP
def top_attackers():
    query = {
        "size": 0,
        "query": {
            "match": {"event_type": "attack"}
        },
        "aggs": {
            "top_ips": {
                "terms": {
                    "field": "source_ip",
                    "size": 10
                }
            }
        }
    }
    
    result = es.search(index="logs-*", body=query)
    
    print("[+] Top 10 attack source IPs:")
    for bucket in result['aggregations']['top_ips']['buckets']:
        print(f"  {bucket['key']}: {bucket['doc_count']} events")

# Scroll API for large result sets
def export_all_logs(query, output_file):
    """Export large result sets using scroll API"""
    
    # Initial search
    result = es.search(
        index="logs-*",
        body=query,
        scroll='2m',
        size=1000
    )
    
    scroll_id = result['_scroll_id']
    hits = result['hits']['hits']
    
    with open(output_file, 'w') as f:
        while hits:
            for hit in hits:
                f.write(json.dumps(hit['_source']) + '\n')
            
            # Get next batch
            result = es.scroll(scroll_id=scroll_id, scroll='2m')
            scroll_id = result['_scroll_id']
            hits = result['hits']['hits']
    
    # Clean up scroll
    es.clear_scroll(scroll_id=scroll_id)
    print(f"[+] Exported to {output_file}")

if __name__ == "__main__":
    search_failed_logins()
    top_attackers()
```

**ELK Stack troubleshooting:**

```bash
# Check Elasticsearch cluster health
curl -X GET "localhost:9200/_cluster/health?pretty"

# Check Logstash pipeline stats
curl -X GET "localhost:9600/_node/stats/pipelines?pretty"

# Test Logstash configuration
/usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/test.conf --config.test_and_exit

# Check Filebeat connection
filebeat test output -c /etc/filebeat/filebeat.yml

# View Elasticsearch logs
tail -f /var/log/elasticsearch/elasticsearch.log

# View Logstash logs
tail -f /var/log/logstash/logstash-plain.log

# Check index statistics
curl -X GET "localhost:9200/_cat/indices?v&s=store.size:desc"

# Delete old indices (free up space)
curl -X DELETE "localhost:9200/logs-2025.10.01"

# Reindex data
curl -X POST "localhost:9200/_reindex" -H 'Content-Type: application/json' -d'
{
  "source": {"index": "old-index"},
  "dest": {"index": "new-index"}
}'
```

## Splunk Basics

Splunk is a commercial SIEM platform with powerful search processing language (SPL). Common in enterprise environments and CTF scenarios mimicking corporate infrastructure.

**Splunk architecture:**

```
          
   Forwarder     Indexer   Search Head  
 (Data Input)       (Storage)        (Query/UI)   
          
```

**Splunk Universal Forwarder configuration:**

```bash
# Install Universal Forwarder (example)
# wget -O splunkforwarder.tgz 'https://download.splunk.com/...'
# tar xvzf splunkforwarder.tgz -C /opt

# Configure inputs
cat > /opt/splunkforwarder/etc/system/local/inputs.conf << 'EOF'
[monitor:///var/log/auth.log]
disabled = false
sourcetype = linux_secure
index = security

[monitor:///var/log/apache2/access.log]
disabled = false
sourcetype = access_combined
index = web

[monitor:///var/log/syslog]
disabled = false
sourcetype = syslog
index = main
EOF

# Configure outputs (forward to indexer)
cat > /opt/splunkforwarder/etc/system/local/outputs.conf << 'EOF'
[tcpout]
defaultGroup = splunk_indexers

[tcpout:splunk_indexers]
server = 192.168.1.100:9997
compressed = true
EOF

# Start forwarder
/opt/splunkforwarder/bin/splunk start
```

**Splunk Search Processing Language (SPL):**

```bash
# Basic search
index=security "failed password"

# Time range
index=security "failed password" earliest=-24h latest=now

# Field extraction
index=security sourcetype=linux_secure | 
    rex field=_raw "from (?<src_ip>\d+\.\d+\.\d+\.\d+)"

# Statistical operations
index=security "failed password" | 
    stats count by src_ip | 
    sort -count | 
    head 10

# Time chart
index=web sourcetype=access_combined status=404 | 
    timechart span=1h count

# Transaction (group related events)
index=security | 
    transaction user maxspan=5m | 
    search eventcount>5

# Subsearch (use results of one search in another)
index=security src_ip=[search index=threats | 
    fields malicious_ip | 
    rename malicious_ip as src_ip]
```

**SPL commands reference:**

```bash
# Search commands
search    # Primary search (implicit, rarely typed)
where     # Filter results (post-search)
regex     # Filter using regular expressions

# Reporting commands
stats     # Calculate statistics
chart     # Create data visualizations
timechart # Time-based statistics
top       # Find most common values
rare      # Find least common values

# Transforming commands
eval      # Calculate expressions
rename    # Rename fields
table     # Format output as table
fields    # Include/exclude fields
dedup     # Remove duplicates
sort      # Sort results

# Examples:
index=security | 
    eval failed_attempts=if(status="failed", 1, 0) | 
    stats sum(failed_attempts) as total_failures by user

index=web | 
    eval response_time_category=case(
        response_time<100, "fast",
        response_time<500, "medium",
        response_time>=500, "slow"
    ) | 
    chart count by response_time_category
```

**Splunk field extraction:**

```bash
# Automatic field extraction (if not recognized)
# Settings > Fields > Field Extractions > New Field Extraction

# Rex command (regex extraction)
index=security sourcetype=ssh_logs | 
    rex field=_raw "user=(?<username>\w+) ip=(?<source_ip>\S+)" | 
    table _time username source_ip

# Extract with named groups
index=web | 
    rex field=uri_path "/user/(?<user_id>\d+)/action/(?<action>\w+)" | 
    stats count by user_id, action

# Multiple extractions
index=security | 
    rex field=_raw "from (?<src_ip>\d+\.\d+\.\d+\.\d+)" | 
    rex field=_raw "user (?<username>\w+)" | 
    rex field=_raw "port (?<port>\d+)" | 
    table _time src_ip username port

# Extract key-value pairs
index=app | 
    rex field=_raw mode=sed "s/([a-z_]+)=([^,\s]+)/\1::\2/g" | 
    kv

# Extract delimited fields
index=csv_data | 
    rex field=_raw "^(?<field1>[^,]*),(?<field2>[^,]*),(?<field3>[^,]*)"
```

**Splunk correlation searches:**

```bash
# Correlation 1: Failed login followed by success (compromised account)
index=security sourcetype=linux_secure 
    [ search index=security sourcetype=linux_secure "Failed password" | 
      stats earliest(_time) as fail_time by src_ip, user | 
      return src_ip, user, fail_time ] | 
search "Accepted password" | 
eval success_time=_time | 
where success_time > fail_time AND success_time < (fail_time + 300) | 
table _time src_ip user fail_time success_time

# Correlation 2: Port scan followed by exploit attempt
index=network action=blocked 
    [ search index=network action=blocked | 
      stats count by src_ip | 
      where count > 50 | 
      fields src_ip ] | 
join type=inner src_ip 
    [ search index=web status=500 OR status=404 | 
      fields src_ip, uri_path, status ]

# Correlation 3: Data exfiltration detection
index=web | 
stats sum(bytes_out) as total_bytes by src_ip | 
where total_bytes > 10000000 | 
join src_ip 
    [ search index=security "authentication successful" | 
      fields src_ip, user ]

# Correlation 4: Privilege escalation chain
index=security 
    (sourcetype=linux_secure "sudo:" OR sourcetype=linux_secure "su:") | 
transaction src_ip user maxspan=10m | 
where eventcount > 3 | 
table _time src_ip user eventcount
```

**Splunk lookup tables (enrichment):**

```bash
# Create lookup file: /opt/splunk/etc/apps/search/lookups/threat_intel.csv
# Format: ip,threat_level,description
# 192.168.1.100,high,Known botnet
# 10.0.0.50,medium,Suspicious activity

# Configure lookup in Splunk
# Settings > Lookups > Lookup definitions > Add new

# Use lookup in search
index=security | 
    lookup threat_intel.csv ip as src_ip OUTPUT threat_level, description | 
    where threat_level="high" | 
    table _time src_ip user threat_level description

# Automatic lookup (apply to all searches)
# Settings > Lookups > Automatic lookups > Add new
# Apply to: sourcetype=firewall
# Lookup table: threat_intel.csv
# Lookup input: src_ip
# Lookup output: threat_level, description

# Dynamic lookup (update from search results)
index=threats | 
    table ip threat_level | 
    outputlookup threat_intel.csv
```

**Splunk alerting:**

```bash
# Alert search example: Brute force detection
index=security sourcetype=linux_secure "Failed password" | 
    stats count by src_ip, user | 
    where count > 5 | 
    eval alert_message="Brute force detected: ".count." attempts from ".src_ip." for user ".user

# Alert configuration (via UI or savedsearches.conf)
# Settings > Searches, reports, and alerts > New Alert

# Example alert configuration:
# Search: index=security "Failed password" | stats count by src_ip | where count > 5
# Time range: Last 15 minutes
# Schedule: Run every 5 minutes
# Trigger condition: Number of results > 0
# Actions: Send email, Run script, Webhook

# Alert actions via CLI
cat > /opt/splunk/etc/apps/search/local/savedsearches.conf << 'EOF'
[Brute Force Alert]
search = index=security "Failed password" | stats count by src_ip | where count > 5
cron_schedule = */5 * * * *
enableSched = 1
action.email = 1
action.email.to = soc@example.com
action.email.subject = Brute Force Attack Detected
dispatch.earliest_time = -15m
dispatch.latest_time = now
EOF
```

**Splunk macros (reusable search fragments):**

```bash
# Define macro in Settings > Advanced search > Search macros
# Or in macros.conf

# Example macro: failed_logins
# Definition: sourcetype=linux_secure "Failed password"

# Use in searches
index=security `failed_logins` | stats count by src_ip

# Macro with arguments
# Name: failed_logins_by_time(1)
# Definition: sourcetype=linux_secure "Failed password" earliest=-$time$
# Arguments: time

# Usage
index=security `failed_logins_by_time(1h)` | stats count by user

# Complex macro example
# Name: suspicious_activity
# Definition: (status=403 OR status=404 OR status=500) AND (uri_path="*admin*" OR uri_path="*config*" OR uri_path="*..*")

index=web `suspicious_activity` | 
    stats count by src_ip, uri_path | 
    sort -count
```

**Splunk data models and pivots:**

```bash
# Data models define structured data for faster searches
# Settings > Data models > New data model

# Example: Authentication data model
# Objects:
#   - Authentication
#     Fields: user, src_ip, dest, action, result
#   - Failed_Authentication (child of Authentication)
#     Additional constraint: result="failed"

# Use data model in search (accelerated)
| datamodel Authentication Authentication search | 
    search result="failed" | 
    stats count by src_ip

# Pivot (GUI-based reporting on data models)
# Pivot > Authentication > Failed_Authentication
# Split rows by: src_ip
# Split columns by: _time (hourly)
# Column values: Count of events
```

**Splunk CTF detection use cases:**

```bash
# Use Case 1: SQL Injection Detection
index=web | 
    regex uri_path="(?i)(union|select|insert|update|delete|drop|exec|script)" | 
    rex field=uri_path "(?<sql_keyword>union|select|insert|update|delete|drop)" | 
    stats count by src_ip, sql_keyword, uri_path | 
    where count > 1

# Use Case 2: Command Injection Detection
index=web | 
    regex uri_path="(?i)(;|\||&|`|\$\(|<|>)" | 
    regex uri_path="(?i)(cat|ls|wget|curl|nc|bash|sh|cmd)" | 
    table _time src_ip uri_path user_agent

# Use Case 3: Directory Traversal
index=web | 
    regex uri_path="\.\./" | 
    eval traversal_attempts=mvcount(split(uri_path, "../")) - 1 | 
    where traversal_attempts > 2 | 
    stats count by src_ip, uri_path

# Use Case 4: Credential Stuffing
index=security sourcetype=web_access uri_path="/login" method=POST | 
    stats dc(username) as unique_users count by src_ip | 
    where unique_users > 20 AND count > 50 | 
    eval attack_type="credential_stuffing"

# Use Case 5: Unusual Data Transfer
index=web | 
    stats sum(bytes_out) as total_out, avg(bytes_out) as avg_out by src_ip, user | 
    where total_out > 100000000 | 
    eval MB_transferred=round(total_out/1024/1024, 2) | 
    table src_ip user MB_transferred avg_out

# Use Case 6: Time-based Anomaly Detection
index=security sourcetype=linux_secure "Accepted password" | 
    eval hour=strftime(_time, "%H") | 
    eval is_afterhours=if(hour<6 OR hour>22, 1, 0) | 
    where is_afterhours=1 | 
    stats count by user, src_ip, hour

# Use Case 7: Beaconing Detection (C2 Communication)
index=network | 
    bin _time span=1m | 
    stats count by _time, src_ip, dest_ip | 
    streamstats current=f last(count) as prev_count by src_ip, dest_ip | 
    eval diff=abs(count-prev_count) | 
    stats avg(diff) as variance, avg(count) as avg_connections by src_ip, dest_ip | 
    where variance < 2 AND avg_connections > 5
```

**Splunk performance optimization:**

```bash
# Use indexed fields when possible
index=web status=404  # Fast (indexed field)
# vs
index=web | search status=404  # Slower (search-time field)

# Limit time range
index=security earliest=-1h latest=now

# Use summary indexing for frequently-run reports
# Create summary index
index=security sourcetype=linux_secure | 
    stats count by src_ip, user | 
    collect index=summary_security

# Query summary index (much faster)
index=summary_security | stats sum(count) by src_ip

# Use tstats for data model searches (fastest)
| tstats count from datamodel=Authentication where Authentication.result="failed" by Authentication.src_ip

# Filter early in search pipeline
index=web status!=200 | stats count by uri_path
# vs (slower)
index=web | stats count by uri_path | where status!=200

# Use fields command to reduce data
index=security | fields _time, src_ip, user, action
```

**Splunk REST API usage:**

```bash
# Authentication
SPLUNK_HOST="https://localhost:8089"
SPLUNK_USER="admin"
SPLUNK_PASS="changeme"

# Get auth token
curl -k -u $SPLUNK_USER:$SPLUNK_PASS $SPLUNK_HOST/services/auth/login \
    -d username=$SPLUNK_USER -d password=$SPLUNK_PASS | \
    grep -oP '<sessionKey>\K[^<]+'

# Run search via API
SEARCH='search index=security "failed password" | stats count by src_ip'

curl -k -u $SPLUNK_USER:$SPLUNK_PASS $SPLUNK_HOST/services/search/jobs \
    -d search="$SEARCH" \
    -d earliest_time="-1h" \
    -d latest_time="now"

# Get search results (replace {sid} with search ID from previous command)
SID="1234567890.12345"
curl -k -u $SPLUNK_USER:$SPLUNK_PASS \
    $SPLUNK_HOST/services/search/jobs/$SID/results \
    --get -d output_mode=json | jq .

# Export search results
curl -k -u $SPLUNK_USER:$SPLUNK_PASS \
    $SPLUNK_HOST/services/search/jobs/$SID/results \
    --get -d output_mode=csv > results.csv
```

**Splunk Python SDK:**

```python
#!/usr/bin/env python3
# splunk_query.py - Query Splunk programmatically

import splunklib.client as client
import splunklib.results as results
import json

# Connect to Splunk
service = client.connect(
    host='localhost',
    port=8089,
    username='admin',
    password='changeme'
)

# Run search
search_query = 'search index=security "failed password" | stats count by src_ip | sort -count'

# Create search job
job = service.jobs.create(
    search_query,
    earliest_time='-1h',
    latest_time='now'
)

# Wait for job to complete
while not job.is_done():
    pass

# Get results
result_stream = job.results(output_mode='json')
reader = results.JSONResultsReader(result_stream)

print("[+] Top failed login sources:")
for result in reader:
    if isinstance(result, dict):
        print(f"  {result.get('src_ip')}: {result.get('count')} attempts")

# Export to file
with open('splunk_results.json', 'w') as f:
    job = service.jobs.create(search_query, earliest_time='-24h', latest_time='now')
    while not job.is_done():
        pass
    
    result_stream = job.results(output_mode='json')
    f.write(result_stream.read().decode('utf-8'))

print("[+] Results exported to splunk_results.json")

# Cleanup
job.cancel()
```

**Splunk Enterprise Security (ES) content:**

```bash
# Notable Events (high-priority alerts)
index=notable | 
    table _time, rule_name, src, dest, user, severity | 
    sort -_time

# Risk Analysis (accumulated risk scores)
index=risk | 
    stats sum(risk_score) as total_risk by risk_object | 
    sort -total_risk | 
    head 20

# Threat Activity (correlation searches)
| from datamodel:"Intrusion_Detection"."IDS_Attacks" | 
    stats count by src, signature | 
    sort -count

# Asset Investigation
| inputlookup asset_lookup_by_str | 
    search ip="192.168.1.10" | 
    table ip, dns, mac, nt_host, category, priority
```

**Splunk vs ELK comparison:**

```
Feature              | Splunk           | ELK Stack
---------------------|------------------|------------------
Cost                 | Commercial       | Open Source
Learning Curve       | Moderate         | Steep
Query Language       | SPL (intuitive)  | Query DSL (JSON)
Performance          | Excellent        | Good (tuning req)
Visualization        | Built-in         | Kibana
Scalability          | Excellent        | Good
Enterprise Support   | Yes              | Elastic (paid)
Resource Usage       | Heavy            | Moderate-Heavy
CTF Prevalence       | Common           | Very Common
```

**Framework selection guidance:**

```bash
# Use ELK Stack when:
# - Budget constraints (open source)
# - Full control over infrastructure needed
# - Integration with existing Elastic ecosystem
# - Learning modern DevOps tools
# - CTF environment (most common)

# Use Splunk when:
# - Enterprise environment simulation
# - Budget available (or free license for <500MB/day)
# - Rapid deployment needed
# - Advanced correlation required
# - Enterprise support needed

# Use custom SIEM/scripts when:
# - Lightweight analysis needed
# - Specific log format not well-supported
# - Limited resources
# - Learning log analysis fundamentals
```

**Comprehensive log analysis workflow:**

```bash
#!/bin/bash
# comprehensive_analysis.sh - Multi-framework approach

LOGS_DIR="/var/log"
OUTPUT_DIR="analysis_$(date +%Y%m%d_%H%M%S)"
mkdir -p "$OUTPUT_DIR"

echo "[*] Starting comprehensive log analysis..."

# 1. Basic statistics
echo "[+] Phase 1: Basic Statistics"
{
    echo "=== Log Volume ==="
    du -sh $LOGS_DIR/*
    echo ""
    echo "=== Most Active Log Files ==="
    find $LOGS_DIR -type f -exec wc -l {} \; | sort -rn | head -10
} > "$OUTPUT_DIR/01_statistics.txt"

# 2. Timeline analysis
echo "[+] Phase 2: Timeline Analysis"
{
    echo "=== Event Timeline ==="
    find $LOGS_DIR -type f -name "*.log" -exec grep -h "$(date -d '1 hour ago' '+%Y-%m-%d %H')" {} \; | \
        sort | head -100
} > "$OUTPUT_DIR/02_timeline.txt"

# 3. Threat detection
echo "[+] Phase 3: Threat Detection"
{
    echo "=== Failed Authentication ==="
    grep -h "Failed\|failed" $LOGS_DIR/auth.log* 2>/dev/null | tail -50
    echo ""
    echo "=== Suspicious Web Activity ==="
    grep -Eh "(\.\.\/|union|select|exec|drop)" $LOGS_DIR/apache2/*.log 2>/dev/null | tail -20
} > "$OUTPUT_DIR/03_threats.txt"

# 4. User activity
echo "[+] Phase 4: User Activity"
{
    echo "=== Unique Users ==="
    grep -h "Accepted" $LOGS_DIR/auth.log* 2>/dev/null | awk '{print $9}' | sort -u
    echo ""
    echo "=== Sudo Usage ==="
    grep -h "sudo:" $LOGS_DIR/auth.log* 2>/dev/null | tail -20
} > "$OUTPUT_DIR/04_users.txt"

# 5. Network connections
echo "[+] Phase 5: Network Analysis"
{
    echo "=== Unique Source IPs ==="
    grep -rh -oE "\b([0-9]{1,3}\.){3}[0-9]{1,3}\b" $LOGS_DIR/*.log 2>/dev/null | \
        grep -v "127.0.0.1\|0.0.0.0" | sort -u | head -50
} > "$OUTPUT_DIR/05_network.txt"

# 6. Generate report
echo "[+] Phase 6: Generating Report"
{
    echo "=== Log Analysis Report ==="
    echo "Generated: $(date)"
    echo "Analyzed: $LOGS_DIR"
    echo ""
    for file in "$OUTPUT_DIR"/*.txt; do
        echo "=== $(basename $file) ==="
        cat "$file"
        echo ""
    done
} > "$OUTPUT_DIR/REPORT.txt"

echo "[+] Analysis complete: $OUTPUT_DIR/REPORT.txt"
```

**Important considerations:**

1. **Index management**: Both ELK and Splunk require proper index lifecycle management to avoid storage exhaustion
2. **Field naming**: Consistent field naming across sources improves correlation accuracy
3. **Time synchronization**: Ensure NTP is configured on all log sources for accurate correlation
4. **Sampling**: In high-volume environments, consider sampling to reduce storage costs
5. **Retention policies**: Define clear retention policies based on compliance requirements

**[Inference]** Framework choice often depends on organizational preferences and existing infrastructure. ELK Stack dominates CTF scenarios due to open-source accessibility, while Splunk appears in enterprise-focused challenges.

**CTF-specific tips:**

- Look for pre-configured dashboards or searches that might contain hints
- Check saved searches for investigation starting points
- Examine field extractions - custom extractions may indicate important log fields
- Review correlation rules for multi-stage attack patterns
- Use time-based filtering to focus on relevant event windows
- Export results to files for offline analysis with traditional tools

---

## Graylog

Graylog is an open-source log management platform that provides real-time log analysis, search capabilities, and alerting through a web-based interface. It uses MongoDB for metadata storage and Elasticsearch for log indexing.

### Installation and Setup

**System requirements**:

- RAM: Minimum 4GB (8GB+ recommended for production)
- Java: OpenJDK 11 or later
- MongoDB: 4.x or later
- Elasticsearch: 7.x (OpenSearch also supported)

**Installation on Ubuntu/Debian**:

```bash
# Update system
sudo apt update && sudo apt upgrade -y

# Install prerequisites
sudo apt install -y apt-transport-https openjdk-11-jre-headless uuid-runtime pwgen

# Install MongoDB
wget -qO - https://www.mongodb.org/static/pgp/server-4.4.asc | sudo apt-key add -
echo "deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu focal/mongodb-org/4.4 multiverse" | \
    sudo tee /etc/apt/sources.list.d/mongodb-org-4.4.list
sudo apt update
sudo apt install -y mongodb-org

# Start MongoDB
sudo systemctl enable mongod
sudo systemctl start mongod

# Install Elasticsearch
wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
echo "deb https://artifacts.elastic.co/packages/7.x/apt stable main" | \
    sudo tee /etc/apt/sources.list.d/elastic-7.x.list
sudo apt update
sudo apt install -y elasticsearch

# Configure Elasticsearch for Graylog
sudo tee -a /etc/elasticsearch/elasticsearch.yml <<EOF
cluster.name: graylog
action.auto_create_index: false
EOF

# Start Elasticsearch
sudo systemctl enable elasticsearch
sudo systemctl start elasticsearch

# Verify Elasticsearch is running
curl -X GET "localhost:9200/"

# Install Graylog
wget https://packages.graylog2.org/repo/packages/graylog-4.3-repository_latest.deb
sudo dpkg -i graylog-4.3-repository_latest.deb
sudo apt update
sudo apt install -y graylog-server

# Generate password secret
pwgen -N 1 -s 96 | sudo tee -a /etc/graylog/server/server.conf

# Generate admin password hash (replace 'yourpassword' with actual password)
echo -n "yourpassword" | sha256sum | awk '{print $1}' | \
    sudo tee -a /etc/graylog/server/server.conf

# Configure Graylog
sudo nano /etc/graylog/server/server.conf
```

**Essential Graylog configuration** (`/etc/graylog/server/server.conf`):

```bash
# Generate password_secret (96 characters)
password_secret = <generated_secret>

# Set admin password (SHA256 hash)
root_password_sha2 = <sha256_hash>

# HTTP bind address (0.0.0.0 for all interfaces)
http_bind_address = 0.0.0.0:9000

# Elasticsearch hosts
elasticsearch_hosts = http://127.0.0.1:9200

# MongoDB connection
mongodb_uri = mongodb://localhost/graylog

# Timezone
root_timezone = UTC

# Email configuration (for alerts)
transport_email_enabled = true
transport_email_hostname = smtp.example.com
transport_email_port = 587
transport_email_use_auth = true
transport_email_auth_username = alerts@example.com
transport_email_auth_password = password
transport_email_from_email = graylog@example.com
```

**Start Graylog**:

```bash
sudo systemctl enable graylog-server
sudo systemctl start graylog-server

# Check status
sudo systemctl status graylog-server

# View logs
sudo tail -f /var/log/graylog-server/server.log

# Access web interface
# Navigate to: http://your-server-ip:9000
# Default credentials: admin / yourpassword
```

### Input Configuration

Graylog receives logs through configured inputs that listen on specific ports or pull from sources.

**Syslog UDP Input** (most common):

```bash
# Via Web UI:
# System/Inputs  Select "Syslog UDP"  Launch new input

# Configuration:
Title: Syslog UDP
Bind address: 0.0.0.0
Port: 514

# Or via API:
curl -u admin:password -H 'Content-Type: application/json' \
    -X POST 'http://localhost:9000/api/system/inputs' \
    -d '{
  "title": "Syslog UDP",
  "type": "org.graylog2.inputs.syslog.udp.SyslogUDPInput",
  "configuration": {
    "bind_address": "0.0.0.0",
    "port": 514,
    "recv_buffer_size": 262144
  },
  "global": true
}'
```

**GELF (Graylog Extended Log Format) Input**:

```bash
# GELF TCP Input
Title: GELF TCP
Bind address: 0.0.0.0
Port: 12201

# GELF HTTP Input (for web applications)
Title: GELF HTTP
Bind address: 0.0.0.0
Port: 12202
```

**Raw/Plain Text Input**:

```bash
# For simple TCP/UDP text logs
Title: Raw TCP
Bind address: 0.0.0.0
Port: 5555
```

**Beats Input** (for Filebeat, Winlogbeat):

```bash
Title: Beats
Bind address: 0.0.0.0
Port: 5044
```

### Sending Logs to Graylog

**Configure rsyslog to forward logs**:

```bash
# Edit rsyslog configuration
sudo nano /etc/rsyslog.d/90-graylog.conf

# Add forwarding rule
*.* @graylog-server:514;RSYSLOG_SyslogProtocol23Format

# For TCP (more reliable)
*.* @@graylog-server:514;RSYSLOG_SyslogProtocol23Format

# Restart rsyslog
sudo systemctl restart rsyslog
```

**Send logs using netcat**:

```bash
# Single message
echo "Test log message" | nc -u graylog-server 514

# From file
cat /var/log/auth.log | nc -u graylog-server 514

# With timestamp
logger -n graylog-server -P 514 -t test "Test message from logger"
```

**Python GELF example**:

```python
#!/usr/bin/env python3
import graypy
import logging

# Setup logging handler
logger = logging.getLogger('test_logger')
logger.setLevel(logging.INFO)

# Add GELF handler
handler = graypy.GELFTCPHandler('graylog-server', 12201)
logger.addHandler(handler)

# Send logs
logger.info('Application started', extra={'user': 'admin', 'ip': '192.168.1.100'})
logger.error('Database connection failed', extra={'db': 'production'})
```

Install: `pip3 install graypy`

**Filebeat configuration** (for file-based logs):

```yaml
# /etc/filebeat/filebeat.yml
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/apache2/access.log
    - /var/log/auth.log
  fields:
    log_type: system
    environment: production

output.logstash:
  hosts: ["graylog-server:5044"]

# Start Filebeat
# sudo systemctl start filebeat
```

### Search and Query Language

**Basic search syntax**:

```bash
# Simple text search
error

# Field search
source:webserver

# Phrase search
"failed login"

# Field with value
level:ERROR

# Wildcard
user:admin*

# Range queries
http_response_code:[400 TO 499]
timestamp:[2025-10-29T00:00:00.000Z TO 2025-10-29T23:59:59.999Z]

# Boolean operators
error AND database
(failed OR error) AND NOT test
source:webserver AND http_response_code:500

# Field existence
_exists_:error_message

# Regular expressions
message:/.*failed.*/
```

**Advanced query examples**:

```bash
# Find failed SSH logins
message:"Failed password" AND source:auth

# Web application errors from specific IP
source:apache AND http_response_code:>=500 AND client_ip:192.168.1.100

# Sudo commands executed
message:"sudo" AND message:"COMMAND"

# Database errors in last hour
level:ERROR AND application:database AND timestamp:[now-1h TO now]

# Unusual login times (outside business hours)
event_type:login AND NOT (hour:[8 TO 17])

# High-volume requests from single IP (potential DDoS)
# Use aggregation in search: Terms on client_ip, count >1000

# Privilege escalation indicators
(message:"su root" OR message:"sudo su" OR message:"pkexec") AND success:true
```

**Query via API**:

```bash
# Search API
curl -u admin:password -X GET \
    'http://localhost:9000/api/search/universal/relative?query=error&range=3600&limit=100' | \
    jq '.'

# Export search results
curl -u admin:password -X GET \
    'http://localhost:9000/api/search/universal/relative/export?query=error&range=3600' \
    > search_results.json
```

### Streams and Rules

Streams route messages to specific categories based on rules, enabling focused analysis and alerting.

**Creating a stream via UI**:

```
Streams  Create Stream

Title: SSH Authentication
Description: All SSH authentication events
Remove matches from default stream: No (unless exclusive processing needed)

Add Stream Rules:
- Field: source, Type: match exactly, Value: auth
- Field: message, Type: match regular expression, Value: .*sshd.*
```

**Stream rules via API**:

```bash
# Create stream
curl -u admin:password -H 'Content-Type: application/json' \
    -X POST 'http://localhost:9000/api/streams' \
    -d '{
  "title": "Web Server Errors",
  "description": "HTTP 500 errors from web servers",
  "matching_type": "AND",
  "remove_matches_from_default_stream": false
}'

# Get stream ID from response, then add rules
STREAM_ID="<stream_id>"

curl -u admin:password -H 'Content-Type: application/json' \
    -X POST "http://localhost:9000/api/streams/$STREAM_ID/rules" \
    -d '{
  "field": "http_response_code",
  "type": 5,
  "value": "500",
  "inverted": false,
  "description": "HTTP 500 status"
}'
```

**Stream rule types**:

- Type 1: Match exactly
- Type 2: Match regular expression
- Type 3: Greater than
- Type 4: Smaller than
- Type 5: Field presence
- Type 6: Contain
- Type 7: Always match

### Alerts and Notifications

**Event Definition (Alert Configuration)**:

```bash
# Via UI:
Alerts  Event Definitions  Create Event Definition

Title: Failed SSH Login Attempts
Description: Alert on 5+ failed SSH logins in 5 minutes

Condition:
- Type: Aggregation
- Filter: message:"Failed password" AND source:auth
- Group By: client_ip
- Aggregation Function: count()
- Threshold: > 5
- Time Range: 5 minutes

Notifications:
- Type: Email
- Recipients: security@example.com
- Subject: [ALERT] Multiple Failed SSH Login Attempts
- Body Template:
  Alert: ${event_definition_title}
  Time: ${event.timestamp}
  Source IP: ${event.fields.client_ip}
  Attempts: ${event.count}
  
  Recent messages:
  ${foreach event.backlog message}
  - ${message.message}
  ${end}
```

**Alert conditions examples**:

**Brute force detection**:

```
Filter: message:"Failed password"
Group By: client_ip
Aggregation: count()
Threshold: > 10
Time Range: 5 minutes
```

**Privilege escalation monitoring**:

```
Filter: message:"sudo" AND message:"COMMAND" AND user:NOT(admin OR root)
Aggregation: count()
Threshold: > 1
Time Range: 1 minute
Execute immediately: Yes
```

**Data exfiltration detection**:

```
Filter: direction:outbound
Group By: source_ip
Aggregation: sum(bytes_out)
Threshold: > 10000000  (10GB)
Time Range: 1 hour
```

**Webhook notification** (for integration with other tools):

```bash
# Notification Type: HTTP Notification
URL: http://your-webhook-endpoint/alerts
Method: POST
Content-Type: application/json

Body:
{
  "alert": "${event_definition_title}",
  "severity": "high",
  "timestamp": "${event.timestamp}",
  "message": "${event.message}",
  "source": "${event.source}"
}
```

### Extractors and Parsing

Extractors parse structured data from raw log messages.

**Grok pattern extractor** (Apache access log):

```bash
# Input example:
# 192.168.1.100 - - [29/Oct/2025:14:23:45 +0000] "GET /admin HTTP/1.1" 200 1234

# Grok pattern:
%{IPORHOST:client_ip} %{USER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] "(?:%{WORD:http_method} %{NOTSPACE:http_request}(?: HTTP/%{NUMBER:http_version})?|%{DATA:rawrequest})" %{NUMBER:http_response_code} (?:%{NUMBER:bytes}|-)

# Create extractor via UI:
# System/Inputs  Select Input  Manage Extractors  Load Message
# Select message  Create Extractor  Grok Pattern
# Paste pattern above  Try  Create extractor
```

**Regular expression extractor** (custom log format):

```bash
# Input: [ERROR] 2025-10-29 14:23:45 Database connection failed: timeout
# Regex: ^\[(?<log_level>\w+)\] (?<timestamp>[\d-]+ [\d:]+) (?<message>.*)$

# Fields extracted:
# - log_level: ERROR
# - timestamp: 2025-10-29 14:23:45
# - message: Database connection failed: timeout
```

**JSON extractor** (for JSON-formatted logs):

```bash
# Input: {"level":"ERROR","timestamp":"2025-10-29T14:23:45Z","user":"admin","action":"delete_user"}

# Extractor: JSON
# Key: (leave empty to extract all fields)
# Flatten: Yes (to extract nested JSON)
```

**Split and index extractor** (for delimited data):

```bash
# Input: admin|192.168.1.100|login|success
# Split by: |
# Index: 0, 1, 2, 3
# Field names: user, ip, action, status
```

### Dashboards and Visualization

**Creating a dashboard**:

```bash
# Via UI:
Dashboards  Create Dashboard

Dashboard Title: Security Overview

Widgets:
1. Message Count (last 24h)
   - Type: Quick Values
   - Field: source
   - Show data table: Yes

2. Failed Login Attempts
   - Type: Chart
   - Query: message:"Failed password"
   - Chart Type: Line chart
   - Interval: 5 minutes

3. Top Source IPs
   - Type: Quick Values
   - Field: client_ip
   - Limit: 10
   - Show pie chart: Yes

4. HTTP Status Codes
   - Type: Quick Values
   - Field: http_response_code
   - Query: source:apache

5. Geographic Map
   - Type: World Map
   - Field: client_ip_geolocation
   - (Requires GeoIP plugin)
```

**Export dashboard**:

```bash
# Export via API
curl -u admin:password -X GET \
    'http://localhost:9000/api/dashboards/<dashboard_id>' \
    > dashboard_export.json

# Import dashboard
curl -u admin:password -H 'Content-Type: application/json' \
    -X POST 'http://localhost:9000/api/dashboards' \
    -d @dashboard_export.json
```

### CTF-Specific Usage

**Analyzing uploaded log files**:

```bash
# Create file-based input for CTF logs
# Use Raw/Plain text input or GELF file input

# Send logs to Graylog
while read line; do
    echo "$line" | nc -u localhost 514
done < /path/to/ctf/logs.txt

# Or use graylog-sidecar with file input configuration
```

**Quick incident investigation workflow**:

```bash
# 1. Identify suspicious IPs
# Search: source:apache
# Widget: Quick Values on client_ip
# Identify IPs with high request counts

# 2. Investigate suspicious IP
# Search: client_ip:192.168.1.100
# Review all activities from this IP

# 3. Check for attack patterns
# Search: client_ip:192.168.1.100 AND (message:".." OR message:"union" OR message:"select")

# 4. Find lateral movement
# Search: source_ip:192.168.1.100 AND (message:"ssh" OR message:"rdp")

# 5. Export findings
# Use export button to download results for reporting
```

## OSSEC

OSSEC (Open Source Security Event Correlator) is a host-based intrusion detection system (HIDS) that performs log analysis, file integrity monitoring, rootkit detection, and active response.

### Installation and Architecture

**OSSEC architecture**:

- **Manager/Server**: Central analysis and management server
- **Agent**: Installed on monitored hosts, sends logs to manager
- **Agentless**: SSH-based monitoring without installing agents

**Installation on Ubuntu/Debian (Manager)**:

```bash
# Install dependencies
sudo apt install -y build-essential gcc make libevent-dev zlib1g-dev libssl-dev \
    libpcre2-dev wget tar

# Download OSSEC
cd /tmp
wget https://github.com/ossec/ossec-hids/archive/3.7.0.tar.gz
tar -xzf 3.7.0.tar.gz
cd ossec-hids-3.7.0

# Run installation script
sudo ./install.sh

# Installation prompts:
# 1. Language: en
# 2. Installation type: server
# 3. Choose where to install: /var/ossec (default)
# 4. Email notification: yes
# 5. Email address: admin@example.com
# 6. SMTP server: localhost
# 7. Enable integrity check daemon: yes
# 8. Enable rootkit detection: yes
# 9. Enable active response: yes
# 10. Enable firewall-drop response: yes (blocks attacking IPs)

# Start OSSEC
sudo /var/ossec/bin/ossec-control start
```

**Installation (Agent)**:

```bash
# Download and extract
cd /tmp
wget https://github.com/ossec/ossec-hids/archive/3.7.0.tar.gz
tar -xzf 3.7.0.tar.gz
cd ossec-hids-3.7.0

# Install as agent
sudo ./install.sh

# Installation prompts:
# 1. Installation type: agent
# 2. Server IP: <manager_ip_address>
# 3. Install directory: /var/ossec (default)

# Note: Agent needs to be registered with manager before starting
```

**Agent registration**:

```bash
# On Manager:
sudo /var/ossec/bin/manage_agents

# Select option: A (Add agent)
# Agent name: webserver01
# IP Address: 192.168.1.10 (or any for dynamic IPs)
# Confirm
# Extract key (copy the key)

# On Agent:
sudo /var/ossec/bin/manage_agents

# Select option: I (Import key)
# Paste the key from manager
# Confirm

# Start agent
sudo /var/ossec/bin/ossec-control start
```

### Configuration

**Main configuration** (`/var/ossec/etc/ossec.conf`):

```xml
<ossec_config>
  <!-- Global settings -->
  <global>
    <email_notification>yes</email_notification>
    <email_to>admin@example.com</email_to>
    <smtp_server>localhost</smtp_server>
    <email_from>ossec@example.com</email_from>
    <email_maxperhour>12</email_maxperhour>
    
    <!-- Alert levels to email (0-16, default 7) -->
    <email_alert_level>7</email_alert_level>
    
    <!-- White list (IPs that won't trigger alerts) -->
    <white_list>127.0.0.1</white_list>
    <white_list>192.168.1.1</white_list>
  </global>

  <!-- Alert output settings -->
  <alerts>
    <log_alert_level>1</log_alert_level>
    <email_alert_level>7</email_alert_level>
  </alerts>

  <!-- Remote syslog output -->
  <syslog_output>
    <level>9</level>
    <server>192.168.1.100</server>
    <port>514</port>
  </syslog_output>

  <!-- File integrity monitoring -->
  <syscheck>
    <frequency>79200</frequency> <!-- 22 hours -->
    
    <!-- Directories to monitor -->
    <directories check_all="yes">/etc,/usr/bin,/usr/sbin</directories>
    <directories check_all="yes">/bin,/sbin,/boot</directories>
    
    <!-- What to monitor -->
    <directories check_all="yes" realtime="yes">/var/www/html</directories>
    
    <!-- Ignore files -->
    <ignore>/etc/mtab</ignore>
    <ignore>/etc/hosts.deny</ignore>
    <ignore>/etc/random-seed</ignore>
  </syscheck>

  <!-- Rootkit detection -->
  <rootcheck>
    <frequency>36000</frequency> <!-- Every 10 hours -->
    <rootkit_files>/var/ossec/etc/shared/rootkit_files.txt</rootkit_files>
    <rootkit_trojans>/var/ossec/etc/shared/rootkit_trojans.txt</rootkit_trojans>
    <system_audit>/var/ossec/etc/shared/system_audit_rcl.txt</system_audit>
  </rootcheck>

  <!-- Active response -->
  <active-response>
    <command>firewall-drop</command>
    <location>local</location>
    <level>6</level>
    <timeout>600</timeout> <!-- Block for 10 minutes -->
  </active-response>

  <!-- Log analysis -->
  <localfile>
    <log_format>syslog</log_format>
    <location>/var/log/auth.log</location>
  </localfile>

  <localfile>
    <log_format>syslog</log_format>
    <location>/var/log/syslog</location>
  </localfile>

  <localfile>
    <log_format>apache</log_format>
    <location>/var/log/apache2/access.log</location>
  </localfile>

  <localfile>
    <log_format>apache</log_format>
    <location>/var/log/apache2/error.log</location>
  </localfile>
  
  <!-- Custom JSON logs -->
  <localfile>
    <log_format>json</log_format>
    <location>/var/log/app/application.json</location>
  </localfile>

  <!-- Command output monitoring -->
  <localfile>
    <log_format>full_command</log_format>
    <command>netstat -tulpn | grep LISTEN</command>
    <frequency>300</frequency>
  </localfile>

  <!-- Remote connection -->
  <remote>
    <connection>secure</connection>
    <port>1514</port>
    <protocol>tcp</protocol>
  </remote>
</ossec_config>
```

**Apply configuration changes**:

```bash
# Verify configuration
sudo /var/ossec/bin/ossec-control enable debug
sudo /var/ossec/bin/ossec-control start

# Check logs for errors
sudo tail -f /var/ossec/logs/ossec.log

# Restart after configuration changes
sudo /var/ossec/bin/ossec-control restart
```

### Rules and Decoders

**Understanding rule structure** (`/var/ossec/rules/`):

```xml
<!-- Example rule: /var/ossec/rules/local_rules.xml -->
<group name="local,syslog,">
  
  <!-- Rule: SSH brute force detection -->
  <rule id="100001" level="10">
    <if_sid>5710</if_sid> <!-- Base SSH failed login rule -->
    <same_source_ip />
    <description>Multiple SSH failed login attempts (brute force)</description>
    <group>authentication_failures,pci_dss_10.2.4,pci_dss_10.2.5,</group>
  </rule>

  <!-- Rule: Web shell detection -->
  <rule id="100002" level="12">
    <if_sid>31100</if_sid> <!-- Base apache access rule -->
    <url>.php|.jsp|.asp</url>
    <regex>cmd=|exec\(|system\(|passthru\(</regex>
    <description>Possible web shell detected in HTTP request</description>
    <group>web,attack,</group>
  </rule>

  <!-- Rule: Privilege escalation -->
  <rule id="100003" level="8">
    <if_sid>5402</if_sid> <!-- Base sudo rule -->
    <match>sudo</match>
    <user>^(?!root$|admin$)</user> <!-- Not root or admin -->
    <description>Non-privileged user executing sudo command</description>
    <group>privilege_escalation,</group>
  </rule>

  <!-- Rule: Data exfiltration (large outbound transfers) -->
  <rule id="100004" level="9">
    <decoded_as>web-accesslog</decoded_as>
    <regex>GET|POST</regex>
    <match> 200 </match>
    <id>^[5-9]\d{6,}$</id> <!-- Response size > 5MB -->
    <description>Large data transfer detected (possible exfiltration)</description>
    <group>data_loss,</group>
  </rule>

  <!-- Rule: Suspicious file access -->
  <rule id="100005" level="7">
    <if_matched_sid>550</if_matched_sid> <!-- File modified -->
    <match>/etc/passwd|/etc/shadow|/root/.ssh</match>
    <description>Sensitive file accessed or modified</description>
    <group>file_integrity,</group>
  </rule>

  <!-- Rule: Port scanning detection -->
  <rule id="100006" level="6" frequency="10" timeframe="60">
    <if_matched_sid>5710</if_matched_sid>
    <same_source_ip />
    <different_dstport />
    <description>Possible port scan (multiple ports from same IP)</description>
    <group>reconnaissance,</group>
  </rule>

</group>
```

**Rule attributes**:

- `level`: Severity (0-16, higher = more severe)
- `if_sid`: Trigger on specific rule ID
- `if_matched_sid`: Chain with previous rule match
- `same_source_ip`: Multiple events from same IP
- `frequency`: Number of times rule must match
- `timeframe`: Time window for frequency
- `decoded_as`: Match specific decoder
- `regex`: Regular expression matching
- `match`: Simple string matching
- `user`: Filter by username

**Custom decoders** (`/var/ossec/etc/decoders/local_decoder.xml`):

```xml
<!-- Example decoder for custom application logs -->
<decoder name="custom-app">
  <program_name>custom_app</program_name>
</decoder>

<decoder name="custom-app-json">
  <parent>custom-app</parent>
  <plugin_decoder>JSON_Decoder</plugin_decoder>
</decoder>

<!-- Decoder for custom syslog format -->
<decoder name="custom-format">
  <prematch>^\[\w+\] \d{4}-\d{2}-\d{2}</prematch>
  <regex>^\[(\w+)\] (\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}) (\S+) (\.+)</regex>
  <order>status, timestamp, user, extra_data</order>
</decoder>
```

**Test decoders and rules**:

```bash
# Test decoder
echo '[ERROR] 2025-10-29T14:23:45 admin Database connection failed' | \
    sudo /var/ossec/bin/ossec-logtest

# Output shows:
# - Decoder used
# - Fields extracted
# - Rules matched
# - Alert level

# Test from file
sudo /var/ossec/bin/ossec-logtest < test_logs.txt
```

### Alert Management

**Alert locations**:

```bash
# All alerts
/var/ossec/logs/alerts/alerts.log

# JSON format alerts
/var/ossec/logs/alerts/alerts.json

# Archived logs (rotated daily)
/var/ossec/logs/archives/archives.log
```

**Alert format** (`alerts.log`):

```
** Alert 1698581825.12345: - syslog,sshd,authentication_success,
2025 Oct 29 14:23:45 webserver01->/var/log/auth.log
Rule: 5715 (level 3) -> 'sshd: authentication success.'
Src IP: 192.168.1.100
User: admin
Oct 29 14:23:45 webserver01 sshd[12345]: Accepted password for admin from 192.168.1.100 port 54321 ssh2
```

**Query alerts**:

```bash
# Search for specific rule ID
grep "Rule: 100001" /var/ossec/logs/alerts/alerts.log

# Filter by source IP
grep "Src IP: 192.168.1.100" /var/ossec/logs/alerts/alerts.log

# Filter by level
grep "level 10\|level 11\|level 12" /var/ossec/logs/alerts/alerts.log

# Count alerts by rule
awk '/^Rule:/ {rules[$2]++} END {for (r in rules) print rules[r], r}' \
    /var/ossec/logs/alerts/alerts.log | sort -rn

# Extract JSON alerts for analysis
jq '.' /var/ossec/logs/alerts/alerts.json | less
```

**Parse alerts with script**:

```python
#!/usr/bin/env python3
import re
import sys
from collections import defaultdict

alerts = defaultdict(list)

with open('/var/ossec/logs/alerts/alerts.log', 'r') as f:
    current_alert = {}
    
    for line in f:
        # Alert header
        if line.startswith('** Alert'):
            if current_alert:
                rule_id = current_alert.get('rule_id', 'unknown')
                alerts[rule_id].append(current_alert)
            current_alert = {}
        
        # Rule line
        match = re.search(r'Rule: (\d+) \(level (\d+)\) -> \'(.*)\'', line)
        if match:
            current_alert['rule_id'] = match.group(1)
            current_alert['level'] = match.group(2)
            current_alert['description'] = match.group(3)
        
        # Source IP
        match = re.search(r'Src IP: (\S+)', line)
        if match:
            current_alert['src_ip'] = match.group(1)
        
        # User
        match = re.search(r'User: (\S+)', line)
        if match:
            current_alert['user'] = match.group(1)
        
        # Log line
        if not line.startswith('**') and not line.startswith('Rule:') and \
           not line.startswith('Src') and not line.startswith('User:'):
            if 'log_line' not in current_alert:
                current_alert['log_line'] = line.strip()

# Add last alert
if current_alert:
    rule_id = current_alert.get('rule_id', 'unknown')
    alerts[rule_id].append(current_alert)

# Generate report
print("OSSEC ALERT SUMMARY")
print("=" * 80)

for rule_id in sorted(alerts.keys(), key=lambda x: len(alerts[x]), reverse=True):
    alert_list = alerts[rule_id]
    print(f"\nRule ID: {rule_id}")
    print(f"Count: {len(alert_list)}")
    if alert_list:
        print(f"Description: {alert_list[0].get('description', 'N/A')}")
        print(f"Level: {alert_list[0].get('level', 'N/A')}")
        
        # Show unique source IPs
        src_ips = set(a.get('src_ip') for a in alert_list if a.get('src_ip'))
        if src_ips:
            print(f"Source IPs: {', '.join(list(src_ips)[:5])}")
            if len(src_ips) > 5:
                print(f"  ... and {len(src_ips) - 5} more")
````

Usage:
```bash
python3 ossec_alert_parser.py
````

### Active Response

Active response allows OSSEC to automatically respond to threats by executing commands.

**Active response commands** (`/var/ossec/active-response/`):

**firewall-drop.sh** - Block IP address:

```bash
#!/bin/bash
# Blocks attacking IP using iptables

ACTION=$1
USER=$2
IP=$3

if [ "x${IP}" = "x" ]; then
    exit 1
fi

# Add to iptables
if [ "x${ACTION}" = "xadd" ]; then
    iptables -I INPUT -s ${IP} -j DROP
fi

# Remove from iptables
if [ "x${ACTION}" = "xdelete" ]; then
    iptables -D INPUT -s ${IP} -j DROP
fi

exit 0
```

**Custom active response** - Notify external system:

```bash
#!/bin/bash
# Custom: Send alert to external webhook
# /var/ossec/active-response/webhook-notify.sh

ACTION=$1
USER=$2
IP=$3
ALERT_ID=$4
RULE_ID=$5

WEBHOOK_URL="http://your-webhook-endpoint/ossec-alert"

if [ "x${ACTION}" = "xadd" ]; then
    curl -X POST "${WEBHOOK_URL}" \
        -H "Content-Type: application/json" \
        -d "{
            \"action\": \"${ACTION}\",
            \"user\": \"${USER}\",
            \"src_ip\": \"${IP}\",
            \"alert_id\": \"${ALERT_ID}\",
            \"rule_id\": \"${RULE_ID}\"
        }"
fi
```

**Configure active response** (`/var/ossec/etc/ossec.conf`):

```xml
<ossec_config>
  <!-- Define command -->
  <command>
    <name>webhook-notify</name>
    <executable>webhook-notify.sh</executable>
    <expect>srcip</expect>
    <timeout_allowed>no</timeout_allowed>
  </command>

  <!-- Active response configuration -->
  <active-response>
    <command>webhook-notify</command>
    <location>server</location>
    <rules_id>100001,100002,100003</rules_id> <!-- Specific rules -->
  </active-response>

  <!-- Block attacker IP for SSH brute force -->
  <active-response>
    <command>firewall-drop</command>
    <location>local</location>
    <level>10</level> <!-- All alerts level 10+ -->
    <timeout>600</timeout> <!-- Block for 10 minutes -->
  </active-response>

  <!-- Disable user account on multiple failed logins -->
  <active-response>
    <disabled>no</disabled>
    <command>disable-account</command>
    <location>local</location>
    <rules_id>100001</rules_id>
    <timeout>1800</timeout>
  </active-response>
</ossec_config>
```

**Test active response**:

```bash
# Manually trigger active response
echo '{"action":"add","user":"test","srcip":"192.168.1.100"}' | \
    sudo /var/ossec/active-response/firewall-drop.sh add test 192.168.1.100

# Check if IP is blocked
sudo iptables -L INPUT -n | grep 192.168.1.100

# Remove block
echo '{"action":"delete","user":"test","srcip":"192.168.1.100"}' | \
    sudo /var/ossec/active-response/firewall-drop.sh delete test 192.168.1.100
```

### File Integrity Monitoring

**Syscheck configuration** (`/var/ossec/etc/ossec.conf`):

```xml
<syscheck>
  <frequency>79200</frequency> <!-- Check every 22 hours -->
  
  <!-- Real-time monitoring (Linux inotify) -->
  <directories realtime="yes" check_all="yes">/var/www/html</directories>
  <directories realtime="yes" check_all="yes" report_changes="yes">/etc</directories>
  
  <!-- Standard monitoring -->
  <directories check_all="yes">/usr/bin,/usr/sbin</directories>
  <directories check_all="yes">/bin,/sbin</directories>
  
  <!-- Check specific attributes -->
  <directories check_sum="yes" check_sha256="yes">/opt/app/bin</directories>
  
  <!-- Ignore patterns -->
  <ignore>/etc/mtab</ignore>
  <ignore>/etc/hosts.deny</ignore>
  <ignore type="sregex">.log$|.swp$</ignore>
  
  <!-- Windows-specific (if monitoring Windows agents) -->
  <windows_registry>HKEY_LOCAL_MACHINE\Software\Classes\batfile</windows_registry>
  <windows_registry>HKEY_LOCAL_MACHINE\System\CurrentControlSet\Services</windows_registry>
  
  <!-- Alert on new files -->
  <alert_new_files>yes</alert_new_files>
  
  <!-- Scan on start -->
  <scan_on_start>yes</scan_on_start>
</syscheck>
```

**Trigger manual scan**:

```bash
# Force syscheck scan
sudo /var/ossec/bin/agent_control -r -a

# Check syscheck database
sudo /var/ossec/bin/syscheck_control -l

# View file changes
sudo /var/ossec/bin/syscheck_control -f /etc/passwd

# Clear syscheck database (reset baseline)
sudo /var/ossec/bin/syscheck_control -u all
```

**Syscheck alerts** (`/var/ossec/logs/alerts/alerts.log`):

```
** Alert: File modified
Rule: 550 (level 7) -> 'Integrity checksum changed.'
File: /etc/passwd
Old md5sum: 'd41d8cd98f00b204e9800998ecf8427e'
New md5sum: '098f6bcd4621d373cade4e832627b4f6'
Old sha1sum: 'da39a3ee5e6b4b0d3255bfef95601890afd80709'
New sha1sum: '5baa61e4c9b93f3f0682250b6cf8331b7ee68fd8'
```

### CTF-Specific Usage

**Analyzing CTF logs with OSSEC**:

```bash
# 1. Configure OSSEC to read CTF log files
sudo nano /var/ossec/etc/ossec.conf

# Add localfile entry:
<localfile>
  <log_format>syslog</log_format>
  <location>/ctf/logs/challenge.log</location>
</localfile>

# 2. Create custom rules for CTF scenarios
sudo nano /var/ossec/rules/ctf_rules.xml
```

```xml
<group name="ctf,">
  <!-- Detect flag format -->
  <rule id="110001" level="15">
    <decoded_as>syslog</decoded_as>
    <regex>flag\{[A-Za-z0-9_]+\}</regex>
    <description>CTF flag detected in logs!</description>
    <group>ctf_flag,</group>
  </rule>

  <!-- Suspicious base64 strings -->
  <rule id="110002" level="8">
    <decoded_as>syslog</decoded_as>
    <regex type="pcre2">(?:[A-Za-z0-9+/]{40,}={0,2})</regex>
    <description>Base64-encoded data detected</description>
    <group>encoding,</group>
  </rule>

  <!-- Command injection indicators -->
  <rule id="110003" level="10">
    <decoded_as>web-accesslog</decoded_as>
    <regex>;|`|\$\(|\|\||&&</regex>
    <description>Command injection attempt detected</description>
    <group>web_attack,</group>
  </rule>
</group>
```

**Automated incident reconstruction**:

```bash
# 3. Restart OSSEC
sudo /var/ossec/bin/ossec-control restart

# 4. Monitor alerts
sudo tail -f /var/ossec/logs/alerts/alerts.log | grep "CTF flag detected"

# 5. Extract all high-priority alerts
grep "level 15" /var/ossec/logs/alerts/alerts.log > ctf_findings.txt
```

## Wazuh

Wazuh is a fork and evolution of OSSEC with additional features including Security Information and Event Management (SIEM), compliance management (PCI DSS, GDPR, HIPAA), cloud security monitoring, and container security.

### Architecture and Components

**Wazuh architecture**:

- **Wazuh Manager**: Central server (evolved OSSEC manager)
- **Wazuh Agent**: Endpoint monitoring agent
- **Elastic Stack**: Elasticsearch, Filebeat, Kibana for data storage and visualization
- **Wazuh API**: RESTful API for management

**Advantages over OSSEC**:

- Modern web UI (Kibana integration)
- Enhanced cloud monitoring (AWS, Azure, GCP)
- Container security (Docker, Kubernetes)
- Vulnerability detection
- Regulatory compliance dashboards
- Built-in threat intelligence integration

### Installation

**All-in-one installation** (Manager + Elastic Stack):

```bash
# Install dependencies
sudo apt update
sudo apt install -y curl apt-transport-https lsb-release gnupg2

# Add Wazuh repository
curl -s https://packages.wazuh.com/key/GPG-KEY-WAZUH | \
    sudo apt-key add -
echo "deb https://packages.wazuh.com/4.x/apt/ stable main" | \
    sudo tee /etc/apt/sources.list.d/wazuh.list

sudo apt update

# Install Wazuh manager
sudo apt install -y wazuh-manager

# Start Wazuh manager
sudo systemctl enable wazuh-manager
sudo systemctl start wazuh-manager

# Verify installation
sudo systemctl status wazuh-manager
```

**Install Elasticsearch**:

```bash
# Add Elasticsearch repository
curl -s https://artifacts.elastic.co/GPG-KEY-elasticsearch | \
    sudo apt-key add -
echo "deb https://artifacts.elastic.co/packages/7.x/apt stable main" | \
    sudo tee /etc/apt/sources.list.d/elastic-7.x.list

sudo apt update

# Install Elasticsearch
sudo apt install -y elasticsearch=7.17.6

# Configure for Wazuh
sudo curl -so /etc/elasticsearch/elasticsearch.yml \
    https://packages.wazuh.com/resources/4.3/open-distro/elasticsearch/7.x/elasticsearch.yml

# Start Elasticsearch
sudo systemctl enable elasticsearch
sudo systemctl start elasticsearch

# Verify (may take a minute to start)
curl -k -u admin:admin https://localhost:9200
```

**Install Filebeat**:

```bash
# Install Filebeat
sudo apt install -y filebeat=7.17.6

# Configure Filebeat for Wazuh
sudo curl -so /etc/filebeat/filebeat.yml \
    https://packages.wazuh.com/resources/4.3/open-distro/filebeat/filebeat.yml

# Download Wazuh module
sudo curl -so /etc/filebeat/wazuh-template.json \
    https://raw.githubusercontent.com/wazuh/wazuh/v4.3.10/extensions/elasticsearch/7.x/wazuh-template.json

sudo curl -s https://packages.wazuh.com/4.x/filebeat/wazuh-filebeat-0.2.tar.gz | \
    sudo tar -xvz -C /usr/share/filebeat/module

# Start Filebeat
sudo systemctl enable filebeat
sudo systemctl start filebeat

# Verify
sudo filebeat test output
```

**Install Kibana**:

```bash
# Install Kibana
sudo apt install -y kibana=7.17.6

# Install Wazuh Kibana plugin
cd /usr/share/kibana
sudo -u kibana bin/kibana-plugin install \
    https://packages.wazuh.com/4.x/ui/kibana/wazuh_kibana-4.3.10_7.17.6-1.zip

# Configure Kibana
sudo curl -so /etc/kibana/kibana.yml \
    https://packages.wazuh.com/resources/4.3/open-distro/kibana/7.x/kibana.yml

# Start Kibana
sudo systemctl enable kibana
sudo systemctl start kibana

# Access Kibana
# Navigate to: http://your-server-ip:5601
# Default credentials: admin / admin
```

**Quick installation script** (automated):

```bash
# Download and run installation script
curl -sO https://packages.wazuh.com/4.3/wazuh-install.sh
sudo bash wazuh-install.sh -a

# Script installs:
# - Wazuh manager
# - Elasticsearch
# - Filebeat
# - Kibana

# Credentials will be displayed at end of installation
```

### Agent Installation and Registration

**Install Wazuh agent (Ubuntu/Debian)**:

```bash
# Add Wazuh repository
curl -s https://packages.wazuh.com/key/GPG-KEY-WAZUH | sudo apt-key add -
echo "deb https://packages.wazuh.com/4.x/apt/ stable main" | \
    sudo tee /etc/apt/sources.list.d/wazuh.list

sudo apt update

# Install agent
sudo apt install -y wazuh-agent

# Configure manager IP
echo "WAZUH_MANAGER='192.168.1.100'" | sudo tee /var/ossec/etc/ossec.conf
sudo systemctl daemon-reload

# Start agent
sudo systemctl enable wazuh-agent
sudo systemctl start wazuh-agent
```

**Agent registration (automatic)**:

```bash
# Register agent with authentication
sudo /var/ossec/bin/agent-auth -m 192.168.1.100

# Restart agent after registration
sudo systemctl restart wazuh-agent

# Verify connection
sudo tail -f /var/ossec/logs/ossec.log | grep "Connected to the server"
```

**Manual agent registration**:

```bash
# On Wazuh Manager:
sudo /var/ossec/bin/manage_agents

# Add agent:
# - Choose 'A' to add agent
# - Name: webserver01
# - IP: 192.168.1.10 (or 'any')
# - Extract key

# On Agent:
sudo /var/ossec/bin/manage_agents

# Import key:
# - Choose 'I' to import
# - Paste key from manager
# - Restart agent
sudo systemctl restart wazuh-agent
```

**Bulk agent deployment**:

```bash
# Using API to register multiple agents
curl -k -X POST "https://wazuh-manager:55000/agents" \
    -H "Authorization: Bearer $TOKEN" \
    -H "Content-Type: application/json" \
    -d '{
  "name": "webserver01",
  "ip": "192.168.1.10"
}'

# Deploy agent via configuration management (Ansible example)
---
- name: Deploy Wazuh agents
  hosts: all
  tasks:
    - name: Add Wazuh repository
      apt_repository:
        repo: "deb https://packages.wazuh.com/4.x/apt/ stable main"
        
    - name: Install Wazuh agent
      apt:
        name: wazuh-agent
        
    - name: Configure manager
      lineinfile:
        path: /var/ossec/etc/ossec.conf
        regexp: '<address>'
        line: '    <address>192.168.1.100</address>'
        
    - name: Restart agent
      service:
        name: wazuh-agent
        state: restarted
```

### Configuration

**Wazuh manager configuration** (`/var/ossec/etc/ossec.conf`):

```xml
<ossec_config>
  <global>
    <jsonout_output>yes</jsonout_output>
    <alerts_log>yes</alerts_log>
    <logall>yes</logall>
    <logall_json>yes</logall_json>
    <email_notification>yes</email_notification>
    <email_to>admin@example.com</email_to>
    <smtp_server>localhost</smtp_server>
    <email_from>wazuh@example.com</email_from>
  </global>

  <!-- Remote connection (agent communication) -->
  <remote>
    <connection>secure</connection>
    <port>1514</port>
    <protocol>tcp,udp</protocol>
    <queue_size>131072</queue_size>
  </remote>

  <!-- Alerts configuration -->
  <alerts>
    <log_alert_level>3</log_alert_level>
    <email_alert_level>12</email_alert_level>
  </alerts>

  <!-- Vulnerability detection -->
  <vulnerability-detector>
    <enabled>yes</enabled>
    <interval>5m</interval>
    <min_full_scan_interval>6h</min_full_scan_interval>
    <run_on_start>yes</run_on_start>

    <!-- Ubuntu provider -->
    <provider name="canonical">
      <enabled>yes</enabled>
      <os>trusty</os>
      <os>xenial</os>
      <os>bionic</os>
      <os>focal</os>
      <update_interval>1h</update_interval>
    </provider>

    <!-- Debian provider -->
    <provider name="debian">
      <enabled>yes</enabled>
      <os>buster</os>
      <os>bullseye</os>
      <update_interval>1h</update_interval>
    </provider>

    <!-- Red Hat provider -->
    <provider name="redhat">
      <enabled>yes</enabled>
      <os>rhel7</os>
      <os>rhel8</os>
      <update_interval>1h</update_interval>
    </provider>
  </vulnerability-detector>

  <!-- Cloud security monitoring (AWS) -->
  <wodle name="aws-s3">
    <disabled>no</disabled>
    <interval>10m</interval>
    <run_on_start>yes</run_on_start>
    <skip_on_error>yes</skip_on_error>
    <bucket type="cloudtrail">
      <name>wazuh-cloudtrail</name>
      <access_key>YOUR_ACCESS_KEY</access_key>
      <secret_key>YOUR_SECRET_KEY</secret_key>
    </bucket>
  </wodle>

  <!-- Docker monitoring -->
  <wodle name="docker-listener">
    <disabled>no</disabled>
    <interval>10m</interval>
    <attempts>5</attempts>
    <run_on_start>yes</run_on_start>
  </wodle>

  <!-- Osquery integration -->
  <wodle name="osquery">
    <disabled>no</disabled>
    <run_daemon>yes</run_daemon>
    <log_path>/var/log/osquery/osqueryd.results.log</log_path>
    <config_path>/etc/osquery/osquery.conf</config_path>
    <add_labels>yes</add_labels>
  </wodle>

  <!-- CIS-CAT compliance scanning -->
  <wodle name="cis-cat">
    <disabled>no</disabled>
    <timeout>1800</timeout>
    <interval>1d</interval>
    <scan-on-start>yes</scan-on-start>
    <java_path>/usr/bin/java</java_path>
    <ciscat_path>/var/ossec/wodles/ciscat</ciscat_path>
  </wodle>

  <!-- Syscheck (File Integrity Monitoring) -->
  <syscheck>
    <disabled>no</disabled>
    <frequency>43200</frequency>
    <scan_on_start>yes</scan_on_start>
    
    <directories realtime="yes" check_all="yes" report_changes="yes">/etc</directories>
    <directories realtime="yes" check_all="yes">/var/www</directories>
    <directories check_all="yes">/usr/bin,/usr/sbin</directories>
    
    <ignore>/etc/mtab</ignore>
    <ignore>/etc/hosts.deny</ignore>
    <ignore>/etc/mail/statistics</ignore>
    <ignore type="sregex">.log$|.swp$</ignore>
  </syscheck>

  <!-- Rootcheck -->
  <rootcheck>
    <disabled>no</disabled>
    <check_files>yes</check_files>
    <check_trojans>yes</check_trojans>
    <check_dev>yes</check_dev>
    <check_sys>yes</check_sys>
    <check_pids>yes</check_pids>
    <check_ports>yes</check_ports>
    <check_if>yes</check_if>
    <frequency>36000</frequency>
    <rootkit_files>/var/ossec/etc/shared/rootkit_files.txt</rootkit_files>
    <rootkit_trojans>/var/ossec/etc/shared/rootkit_trojans.txt</rootkit_trojans>
    <skip_nfs>yes</skip_nfs>
  </rootcheck>

  <!-- Active response -->
  <active-response>
    <disabled>no</disabled>
    <command>firewall-drop</command>
    <location>local</location>
    <rules_id>5710,5711,5712</rules_id>
    <timeout>600</timeout>
  </active-response>

  <!-- Log analysis -->
  <localfile>
    <log_format>syslog</log_format>
    <location>/var/log/auth.log</location>
  </localfile>

  <localfile>
    <log_format>syslog</log_format>
    <location>/var/log/syslog</location>
  </localfile>

  <localfile>
    <log_format>apache</log_format>
    <location>/var/log/apache2/access.log</location>
  </localfile>

  <localfile>
    <log_format>json</log_format>
    <location>/var/log/app/*.json</location>
  </localfile>
</ossec_config>
```

**Agent configuration** (`/var/ossec/etc/ossec.conf` on agent):

```xml
<ossec_config>
  <client>
    <server>
      <address>192.168.1.100</address>
      <port>1514</port>
      <protocol>tcp</protocol>
    </server>
    <config-profile>ubuntu, ubuntu20, ubuntu20.04</config-profile>
    <notify_time>10</notify_time>
    <time-reconnect>60</time-reconnect>
    <auto_restart>yes</auto_restart>
  </client>

  <!-- Monitored logs (agent-specific) -->
  <localfile>
    <log_format>syslog</log_format>
    <location>/var/log/auth.log</location>
  </localfile>

  <localfile>
    <log_format>syslog</log_format>
    <location>/var/log/syslog</location>
  </localfile>

  <localfile>
    <log_format>apache</log_format>
    <location>/var/log/nginx/access.log</location>
  </localfile>

  <!-- Command output monitoring -->
  <localfile>
    <log_format>command</log_format>
    <command>df -P</command>
    <frequency>360</frequency>
  </localfile>

  <localfile>
    <log_format>full_command</log_format>
    <command>netstat -tulpn | grep LISTEN</command>
    <alias>netstat listening ports</alias>
    <frequency>180</frequency>
  </localfile>

  <!-- File integrity monitoring -->
  <syscheck>
    <disabled>no</disabled>
    <frequency>43200</frequency>
    <scan_on_start>yes</scan_on_start>
    
    <directories check_all="yes" realtime="yes">/var/www/html</directories>
    <directories check_all="yes">/etc</directories>
    <directories check_all="yes">/usr/bin,/usr/sbin</directories>
    <directories check_all="yes">/bin,/sbin</directories>
    
    <ignore>/etc/mtab</ignore>
    <ignore>/etc/hosts.deny</ignore>
    <ignore type="sregex">.log$|.tmp$</ignore>
  </syscheck>

  <!-- Rootcheck -->
  <rootcheck>
    <disabled>no</disabled>
    <check_files>yes</check_files>
    <check_trojans>yes</check_trojans>
    <check_dev>yes</check_dev>
    <check_sys>yes</check_sys>
    <check_pids>yes</check_pids>
    <check_ports>yes</check_ports>
    <check_if>yes</check_if>
    <frequency>36000</frequency>
  </rootcheck>
</ossec_config>
```

### Rules and Decoders

Wazuh uses the same rule structure as OSSEC but adds enhancements:

**Custom rules** (`/var/ossec/etc/rules/local_rules.xml`):

```xml
<group name="local,wazuh,">

  <!-- Detect cryptocurrency mining -->
  <rule id="100100" level="12">
    <if_matched_sid>2902</if_matched_sid>
    <match>xmrig|minergate|cpuminer|cryptonight</match>
    <description>Cryptocurrency mining software detected</description>
    <mitre>
      <id>T1496</id>
    </mitre>
    <group>crypto_mining,</group>
  </rule>

  <!-- Docker container escape attempt -->
  <rule id="100101" level="15">
    <if_sid>87900</if_sid>
    <field name="docker.Action">^exec_start</field>
    <regex type="pcre2">docker\.Actor\.Attributes\.name.*privileged.*true</regex>
    <description>Possible Docker container escape attempt</description>
    <mitre>
      <id>T1611</id>
    </mitre>
    <group>docker,container_escape,</group>
  </rule>

  <!-- AWS unauthorized API call -->
  <rule id="100102" level="10">
    <if_sid>80200</if_sid>
    <field name="aws.errorCode">UnauthorizedOperation|AccessDenied</field>
    <description>AWS unauthorized API call detected</description>
    <mitre>
      <id>T1078</id>
    </mitre>
    <group>aws,cloud_security,</group>
  </rule>

  <!-- Kubernetes privilege escalation -->
  <rule id="100103" level="12">
    <if_sid>86000</if_sid>
    <field name="objectRef.resource">pods/exec</field>
    <field name="verb">create</field>
    <description>Kubernetes exec into pod detected</description>
    <mitre>
      <id>T1609</id>
    </mitre>
    <group>kubernetes,privilege_escalation,</group>
  </rule>

  <!-- Web shell upload -->
  <rule id="100104" level="15">
    <if_sid>31103</if_sid>
    <match>POST</match>
    <regex type="pcre2">\.php|\.jsp|\.asp</regex>
    <field name="url">upload</field>
    <description>Possible web shell upload</description>
    <mitre>
      <id>T1505.003</id>
    </mitre>
    <group>web_attack,webshell,</group>
  </rule>

  <!-- Suspicious PowerShell execution -->
  <rule id="100105" level="10">
    <if_sid>60009</if_sid>
    <field name="win.eventdata.image" type="pcre2">powershell\.exe</field>
    <field name="win.eventdata.commandLine" type="pcre2">-enc|-encodedcommand|-nop|-w hidden</field>
    <description>Suspicious PowerShell command detected</description>
    <mitre>
      <id>T1059.001</id>
    </mitre>
    <group>windows,powershell,</group>
  </rule>

</group>
```

### Wazuh API

**Authentication**:

```bash
# Get API token
TOKEN=$(curl -u wazuh:wazuh -k -X POST "https://localhost:55000/security/user/authenticate?raw=true")

# Use token in requests
curl -k -X GET "https://localhost:55000/agents" \
    -H "Authorization: Bearer $TOKEN"
```

**Common API operations**:

```bash
# List all agents
curl -k -X GET "https://localhost:55000/agents?pretty=true" \
    -H "Authorization: Bearer $TOKEN"

# Get agent details
curl -k -X GET "https://localhost:55000/agents/001?pretty=true" \
    -H "Authorization: Bearer $TOKEN"

# List active agents
curl -k -X GET "https://localhost:55000/agents?status=active&pretty=true" \
    -H "Authorization: Bearer $TOKEN"

# Restart agent
curl -k -X PUT "https://localhost:55000/agents/001/restart" \
    -H "Authorization: Bearer $TOKEN"

# Get agent configuration
curl -k -X GET "https://localhost:55000/agents/001/config/syscheck/syscheck?pretty=true" \
    -H "Authorization: Bearer $TOKEN"

# Run rootcheck on agent
curl -k -X PUT "https://localhost:55000/rootcheck/001" \-H "Authorization: Bearer $TOKEN"

# Run syscheck scan on agent
curl -k -X PUT "https://localhost:55000/syscheck/001"  
-H "Authorization: Bearer $TOKEN"

# Get alerts
curl -k -X GET "https://localhost:55000/alerts?pretty=true&limit=10"  
-H "Authorization: Bearer $TOKEN"

# Get rules
curl -k -X GET "https://localhost:55000/rules?pretty=true&search=ssh"  
-H "Authorization: Bearer $TOKEN"

# Get decoders
curl -k -X GET "https://localhost:55000/decoders?pretty=true"  
-H "Authorization: Bearer $TOKEN"

# Delete agent
curl -k -X DELETE "https://localhost:55000/agents/001?pretty=true"  
-H "Authorization: Bearer $TOKEN"

# Get manager info
curl -k -X GET "https://localhost:55000/manager/info?pretty=true"  
-H "Authorization: Bearer $TOKEN"

# Get cluster status
curl -k -X GET "https://localhost:55000/cluster/status?pretty=true"  
-H "Authorization: Bearer $TOKEN"
````

**Scripted agent management**:
```python
#!/usr/bin/env python3
import requests
import json
from requests.auth import HTTPBasicAuth

# Disable SSL warnings for self-signed certificates
requests.packages.urllib3.disable_warnings()

# Configuration
WAZUH_URL = "https://localhost:55000"
USERNAME = "wazuh"
PASSWORD = "wazuh"

class WazuhAPI:
    def __init__(self, url, username, password):
        self.url = url
        self.token = self._authenticate(username, password)
    
    def _authenticate(self, username, password):
        """Get authentication token"""
        endpoint = f"{self.url}/security/user/authenticate"
        response = requests.post(
            endpoint,
            auth=HTTPBasicAuth(username, password),
            params={"raw": "true"},
            verify=False
        )
        return response.text
    
    def _request(self, method, endpoint, **kwargs):
        """Make authenticated API request"""
        headers = {"Authorization": f"Bearer {self.token}"}
        url = f"{self.url}{endpoint}"
        response = requests.request(method, url, headers=headers, verify=False, **kwargs)
        return response.json()
    
    def get_agents(self, status=None):
        """List all agents"""
        params = {}
        if status:
            params['status'] = status
        return self._request('GET', '/agents', params=params)
    
    def get_agent(self, agent_id):
        """Get specific agent details"""
        return self._request('GET', f'/agents/{agent_id}')
    
    def restart_agent(self, agent_id):
        """Restart agent"""
        return self._request('PUT', f'/agents/{agent_id}/restart')
    
    def get_alerts(self, limit=100, rule_id=None, level=None):
        """Get alerts"""
        params = {'limit': limit}
        if rule_id:
            params['rule.id'] = rule_id
        if level:
            params['rule.level'] = level
        return self._request('GET', '/alerts', params=params)
    
    def run_syscheck(self, agent_id):
        """Trigger syscheck scan"""
        return self._request('PUT', f'/syscheck/{agent_id}')
    
    def get_syscheck_changes(self, agent_id):
        """Get file integrity changes"""
        return self._request('GET', f'/syscheck/{agent_id}')

# Usage example
if __name__ == "__main__":
    api = WazuhAPI(WAZUH_URL, USERNAME, PASSWORD)
    
    # Get all active agents
    agents = api.get_agents(status='active')
    print(f"Active agents: {agents['data']['total_affected_items']}")
    
    for agent in agents['data']['affected_items']:
        print(f"  - {agent['name']} (ID: {agent['id']}, IP: {agent['ip']})")
    
    # Get high-severity alerts
    alerts = api.get_alerts(limit=10, level="gte:10")
    print(f"\nHigh-severity alerts: {alerts['data']['total_affected_items']}")
    
    for alert in alerts['data']['affected_items']:
        print(f"  - [{alert['rule']['level']}] {alert['rule']['description']}")
        print(f"    Agent: {alert.get('agent', {}).get('name', 'N/A')}")
        print(f"    Time: {alert['timestamp']}")
````

Usage:

```bash
python3 wazuh_api_client.py
```

### Kibana Interface

**Accessing Wazuh Kibana**:

```
URL: http://your-server-ip:5601
Default credentials: admin / admin
```

**Key Kibana sections**:

1. **Overview Dashboard**:
    
    - Security events summary
    - Top agents
    - Alert evolution
    - MITRE ATT&CK framework mapping
2. **Security Events**:
    
    - Real-time event stream
    - Filterable by agent, rule, level
    - Event details and context
3. **Integrity Monitoring**:
    
    - File changes across all agents
    - Real-time file modification alerts
    - File comparison (before/after)
4. **Vulnerability Detection**:
    
    - CVE inventory per agent
    - Severity distribution
    - Affected packages
5. **Regulatory Compliance**:
    
    - PCI DSS dashboard
    - GDPR dashboard
    - HIPAA dashboard
    - NIST 800-53 dashboard
6. **Security Configuration Assessment**:
    
    - CIS benchmark results
    - Security misconfigurations
    - Compliance scoring

**Creating custom dashboards**:

```bash
# Via Kibana UI:
# Stack Management  Saved Objects  Create new visualization

# Or via API:
curl -k -X POST "http://localhost:5601/api/saved_objects/visualization" \
    -H "kbn-xsrf: true" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $TOKEN" \
    -d '{
  "attributes": {
    "title": "Failed SSH Logins by IP",
    "visState": "{...}",
    "uiStateJSON": "{}",
    "description": "",
    "version": 1,
    "kibanaSavedObjectMeta": {
      "searchSourceJSON": "{\"query\":{\"query\":\"rule.id:5710\",\"language\":\"lucene\"}}"
    }
  }
}'
```

**Query syntax in Kibana**:

```
# Lucene query language (default)
rule.level:>=10
agent.name:"webserver01"
rule.groups:"web" AND rule.level:>=8
data.srcip:"192.168.1.100"

# KQL (Kibana Query Language)
rule.level >= 10
agent.name: "webserver01"
rule.groups: "web" and rule.level >= 8
data.srcip: "192.168.1.100"

# Time-based queries
@timestamp >= "2025-10-29T00:00:00" and @timestamp <= "2025-10-29T23:59:59"

# Wildcards
rule.description: *brute*force*

# Exists
_exists_:data.srcip

# Negation
NOT rule.groups:"test"
```

### Advanced Features

**Vulnerability Detection**:

```bash
# View vulnerabilities for specific agent
curl -k -X GET "https://localhost:55000/vulnerability/001?pretty=true" \
    -H "Authorization: Bearer $TOKEN"

# Get vulnerabilities by CVE
curl -k -X GET "https://localhost:55000/vulnerability/001/cves/CVE-2021-44228?pretty=true" \
    -H "Authorization: Bearer $TOKEN"

# Get affected packages
curl -k -X GET "https://localhost:55000/vulnerability/001/packages?pretty=true" \
    -H "Authorization: Bearer $TOKEN"
```

**Cloud Security Monitoring (AWS)**:

```xml
<!-- AWS CloudTrail monitoring -->
<wodle name="aws-s3">
  <disabled>no</disabled>
  <interval>10m</interval>
  <run_on_start>yes</run_on_start>
  
  <!-- CloudTrail bucket -->
  <bucket type="cloudtrail">
    <name>my-cloudtrail-bucket</name>
    <aws_profile>default</aws_profile>
  </bucket>
  
  <!-- VPC Flow Logs -->
  <bucket type="vpcflow">
    <name>my-vpcflow-bucket</name>
    <aws_profile>default</aws_profile>
  </bucket>
  
  <!-- Config logs -->
  <bucket type="config">
    <name>my-config-bucket</name>
    <aws_profile>default</aws_profile>
  </bucket>
  
  <!-- GuardDuty findings -->
  <service type="guardduty">
    <aws_profile>default</aws_profile>
  </service>
  
  <!-- Inspector findings -->
  <service type="inspector">
    <aws_profile>default</aws_profile>
  </service>
</wodle>
```

**Docker/Kubernetes Monitoring**:

```bash
# Docker listener configuration
<wodle name="docker-listener">
  <disabled>no</disabled>
  <interval>10m</interval>
  <attempts>5</attempts>
  <run_on_start>yes</run_on_start>
</wodle>

# View Docker events in Kibana
# Navigate to: Security events  Filter by rule.groups:"docker"

# Query Docker events via API
curl -k -X GET "https://localhost:55000/alerts?q=rule.groups:docker&pretty=true" \
    -H "Authorization: Bearer $TOKEN"
```

**Osquery Integration**:

```json
// /etc/osquery/osquery.conf
{
  "options": {
    "config_plugin": "filesystem",
    "logger_plugin": "filesystem",
    "logger_path": "/var/log/osquery",
    "disable_logging": "false",
    "log_result_events": "true",
    "schedule_splay_percent": "10",
    "pidfile": "/var/osquery/osquery.pidfile",
    "events_expiry": "3600",
    "database_path": "/var/osquery/osquery.db",
    "verbose": "false",
    "worker_threads": "2",
    "enable_monitor": "true",
    "disable_events": "false",
    "disable_audit": "false",
    "audit_allow_config": "true",
    "host_identifier": "hostname",
    "enable_syslog": "true",
    "audit_allow_sockets": "true",
    "schedule_default_interval": "3600"
  },
  
  "schedule": {
    "system_info": {
      "query": "SELECT hostname, cpu_brand, physical_memory FROM system_info;",
      "interval": 3600
    },
    "high_load_average": {
      "query": "SELECT period, average, '70' AS threshold FROM load_average WHERE period = '15m' AND average > '0.7';",
      "interval": 900,
      "description": "Report if load charge is over 70% at 15m"
    },
    "usb_devices": {
      "query": "SELECT * FROM usb_devices;",
      "interval": 300
    },
    "network_connections": {
      "query": "SELECT pid, process.name, local_address, remote_address, family, protocol, local_port, remote_port FROM process_open_sockets JOIN processes AS process USING (pid) WHERE remote_port != 0;",
      "interval": 600
    },
    "ssh_login_events": {
      "query": "SELECT * FROM last WHERE username NOT IN ('reboot', 'shutdown') AND time > (SELECT unix_time FROM time) - 3600;",
      "interval": 3600
    }
  },
  
  "packs": {
    "incident-response": "/usr/share/osquery/packs/incident-response.conf",
    "ossec-rootkit": "/usr/share/osquery/packs/ossec-rootkit.conf"
  }
}
```

**CIS Benchmark Scanning**:

```bash
# Download CIS-CAT
cd /var/ossec/wodles/
wget https://downloads.cisecurity.org/cis-cat-full.zip
unzip cis-cat-full.zip

# Configure in ossec.conf
<wodle name="cis-cat">
  <disabled>no</disabled>
  <timeout>1800</timeout>
  <interval>1d</interval>
  <scan-on-start>yes</scan-on-start>
  
  <java_path>/usr/bin/java</java_path>
  <ciscat_path>/var/ossec/wodles/cis-cat</ciscat_path>
  
  <content type="xccdf" path="benchmarks/CIS_Ubuntu_Linux_20.04_Benchmark_v1.0.0-xccdf.xml">
    <profile>xccdf_org.cisecurity.benchmarks_profile_Level_1_-_Server</profile>
  </content>
</wodle>

# View results in Kibana:
# Modules  Security Configuration Assessment  Dashboard
```

### CTF-Specific Usage

**Analyzing CTF logs with Wazuh**:

**1. Configure input for CTF log files**:

```xml
<!-- Add to agent ossec.conf -->
<localfile>
  <log_format>syslog</log_format>
  <location>/ctf/logs/challenge.log</location>
</localfile>

<localfile>
  <log_format>json</log_format>
  <location>/ctf/logs/app.json</location>
</localfile>
```

**2. Create CTF-specific rules**:

```xml
<!-- /var/ossec/etc/rules/ctf_rules.xml -->
<group name="ctf,">
  
  <!-- Flag detection -->
  <rule id="120001" level="15">
    <decoded_as>syslog</decoded_as>
    <regex type="pcre2">flag\{[A-Za-z0-9_\-]+\}|CTF\{[A-Za-z0-9_\-]+\}</regex>
    <description> CTF FLAG FOUND IN LOGS!</description>
    <group>ctf_flag,</group>
  </rule>

  <!-- Base64 encoded data -->
  <rule id="120002" level="6">
    <decoded_as>syslog</decoded_as>
    <regex type="pcre2">[A-Za-z0-9+/]{40,}={0,2}</regex>
    <description>Base64-encoded string detected (potential hidden data)</description>
    <group>encoding,</group>
  </rule>

  <!-- Hex encoded data -->
  <rule id="120003" level="6">
    <decoded_as>syslog</decoded_as>
    <regex type="pcre2">(?:0x)?[0-9a-fA-F]{32,}</regex>
    <description>Hex-encoded string detected</description>
    <group>encoding,</group>
  </rule>

  <!-- SQL injection attempts -->
  <rule id="120004" level="10">
    <decoded_as>web-accesslog</decoded_as>
    <regex type="pcre2">union.*select|drop.*table|exec\(|' or '1'='1</regex>
    <description>SQL injection attempt detected</description>
    <group>web_attack,sql_injection,</group>
  </rule>

  <!-- Command injection -->
  <rule id="120005" level="10">
    <decoded_as>web-accesslog</decoded_as>
    <regex type="pcre2">;.*cat|;.*ls|\|.*wget|\$\(.*\)|`.*`</regex>
    <description>Command injection attempt detected</description>
    <group>web_attack,command_injection,</group>
  </rule>

  <!-- Suspicious user agents -->
  <rule id="120006" level="7">
    <decoded_as>web-accesslog</decoded_as>
    <regex type="pcre2">sqlmap|nikto|nmap|masscan|burp</regex>
    <description>Suspicious user agent (security tool detected)</description>
    <group>reconnaissance,</group>
  </rule>

  <!-- Directory traversal -->
  <rule id="120007" level="9">
    <decoded_as>web-accesslog</decoded_as>
    <regex type="pcre2">\.\./|\.\./\.\./|\.\.\\|%2e%2e</regex>
    <description>Directory traversal attempt detected</description>
    <group>web_attack,path_traversal,</group>
  </rule>

  <!-- Unusual ports -->
  <rule id="120008" level="5">
    <decoded_as>syslog</decoded_as>
    <regex type="pcre2">:4444|:1337|:31337|:8888</regex>
    <description>Connection to unusual port (potential backdoor)</description>
    <group>backdoor,</group>
  </rule>

</group>
```

**3. Create CTF dashboard**:

```bash
# Query for flags in Kibana
rule.id:120001

# Query for attack patterns
rule.groups:"web_attack"

# Timeline of events
@timestamp:[now-1d TO now] AND rule.level:>=8

# Top attacking IPs
# Aggregation: Terms on data.srcip
```

**4. Automated flag extraction**:

```python
#!/usr/bin/env python3
import requests
import re
import json

# Wazuh API configuration
WAZUH_URL = "https://localhost:55000"
USERNAME = "wazuh"
PASSWORD = "wazuh"

# Authenticate
token_response = requests.post(
    f"{WAZUH_URL}/security/user/authenticate?raw=true",
    auth=(USERNAME, PASSWORD),
    verify=False
)
token = token_response.text

# Get alerts with flag pattern
headers = {"Authorization": f"Bearer {token}"}
alerts = requests.get(
    f"{WAZUH_URL}/alerts",
    headers=headers,
    params={
        "rule.id": "120001",
        "limit": 1000
    },
    verify=False
).json()

# Extract flags
flags = set()
flag_pattern = re.compile(r'(flag\{[A-Za-z0-9_\-]+\}|CTF\{[A-Za-z0-9_\-]+\})', re.IGNORECASE)

for alert in alerts.get('data', {}).get('affected_items', []):
    full_log = alert.get('full_log', '')
    matches = flag_pattern.findall(full_log)
    flags.update(matches)

# Output results
print("CTF FLAGS FOUND:")
print("=" * 60)
for flag in sorted(flags):
    print(flag)

print(f"\nTotal unique flags: {len(flags)}")
```

**5. Incident reconstruction**:

```bash
# Get all events from specific agent during CTF timeframe
curl -k -X GET "https://localhost:55000/alerts" \
    -H "Authorization: Bearer $TOKEN" \
    -G \
    --data-urlencode "q=agent.id:001" \
    --data-urlencode "q=timestamp>2025-10-29T00:00:00" \
    --data-urlencode "q=timestamp<2025-10-29T23:59:59" \
    --data-urlencode "sort=+timestamp" \
    --data-urlencode "limit=10000" \
    > ctf_timeline.json

# Parse timeline
jq -r '.data.affected_items[] | [.timestamp, .rule.description, .data.srcip // "N/A"] | @csv' \
    ctf_timeline.json | \
    column -t -s','
```

## Important Related Topics

For comprehensive log analysis framework mastery, consider studying:

- **Elastic Stack (ELK) Deployment**: Advanced Elasticsearch clustering, index lifecycle management, performance tuning
- **Threat Intelligence Integration**: Feeding IOC lists, STIX/TAXII integration, automated threat hunting
- **SOAR Integration**: Connecting with Security Orchestration, Automation and Response platforms (TheHive, Cortex, Shuffle)
- **Custom Decoder Development**: Creating parsers for proprietary or unusual log formats
- **Distributed Deployment**: Multi-node clusters, load balancing, high availability configurations
- **Machine Learning for Anomaly Detection**: Wazuh ML capabilities, Elasticsearch anomaly detection
- **Compliance Automation**: Automated compliance reporting, audit trail generation, evidence collection

---

# Cryptographic Artifacts in Logs

Cryptographic artifacts in logs reveal encryption handshakes, certificate chains, cipher negotiations, and key exchange mechanisms. In CTF contexts, these artifacts may contain weak configurations, leaked keys, certificate vulnerabilities, or hidden data within cryptographic structures.

## SSL/TLS Handshake Logs

SSL/TLS handshake logs capture the negotiation phase where client and server establish encrypted communication, including protocol versions, cipher suites, and session parameters.

### Identifying Handshake Events

**Common handshake log patterns:**

```bash
# Apache/Nginx SSL handshake logs
grep -E "SSL|TLS|handshake" /var/log/apache2/error.log
grep -E "SSL_do_handshake|SSL_accept" /var/log/nginx/error.log

# OpenSSL debug logs
grep -E "SSL_connect|SSL_accept|ClientHello|ServerHello" /var/log/ssl_debug.log

# Syslog SSL events
grep -E "ssl|tls" /var/log/syslog | grep -iE "handshake|hello|certificate"

# Filter by handshake phases
grep -E "ClientHello|ServerHello|Certificate|ServerKeyExchange|ClientKeyExchange|Finished" ssl.log
```

**Extract specific handshake components:**

```bash
# Protocol versions
grep -oE "TLSv1\.[0-3]|SSLv[23]" ssl.log | sort | uniq -c

# Handshake message types
grep -oE "(Client|Server)Hello|Certificate|ServerKeyExchange|CertificateVerify|Finished" ssl.log | sort | uniq -c

# Session IDs
grep -oE "Session-ID: [A-F0-9]{64}" ssl.log

# Extract client IP addresses from handshakes
awk '/ClientHello/ {print $1}' ssl.log | sort | uniq -c | sort -rn
```

### Parsing TLS Handshake Structure

**Manual handshake packet analysis:**

```bash
# If you have pcap files referenced in logs
tshark -r capture.pcap -Y "ssl.handshake" -T fields \
    -e ip.src -e ip.dst -e ssl.handshake.type \
    -e ssl.handshake.version -e ssl.handshake.ciphersuite

# Extract ClientHello details
tshark -r capture.pcap -Y "ssl.handshake.type == 1" -V | \
    grep -A50 "Secure Sockets Layer"

# Extract ServerHello details
tshark -r capture.pcap -Y "ssl.handshake.type == 2" -V
```

**Python TLS handshake parser:**

```python
#!/usr/bin/env python3
"""
Parse TLS handshake information from logs
"""
import re
from collections import defaultdict

class TLSHandshakeParser:
    def __init__(self, logfile):
        self.logfile = logfile
        self.handshakes = []
        self.statistics = defaultdict(int)
        
    def parse_handshake_log(self):
        """Parse TLS handshake from text logs"""
        with open(self.logfile, 'r', errors='ignore') as f:
            content = f.read()
        
        # Extract ClientHello messages
        client_hellos = re.finditer(
            r'ClientHello.*?(?:Version:|TLS).*?(TLSv1\.[0-3]|SSLv[23])',
            content, re.DOTALL | re.IGNORECASE
        )
        
        for match in client_hellos:
            handshake_data = match.group(0)
            version = match.group(1)
            
            # Extract cipher suites
            ciphers = re.findall(
                r'(TLS_[A-Z0-9_]+|0x[0-9A-Fa-f]{4})',
                handshake_data
            )
            
            # Extract extensions
            extensions = re.findall(
                r'Extension:\s*([a-z_]+)',
                handshake_data,
                re.IGNORECASE
            )
            
            # Extract SNI (Server Name Indication)
            sni_match = re.search(
                r'server_name.*?:\s*([a-z0-9\.-]+)',
                handshake_data,
                re.IGNORECASE
            )
            sni = sni_match.group(1) if sni_match else None
            
            handshake = {
                'type': 'ClientHello',
                'version': version,
                'ciphers': ciphers[:10],  # First 10 ciphers
                'extensions': extensions,
                'sni': sni
            }
            
            self.handshakes.append(handshake)
            self.statistics[version] += 1
    
    def parse_openssl_debug(self):
        """Parse OpenSSL s_client debug output"""
        with open(self.logfile, 'r', errors='ignore') as f:
            lines = f.readlines()
        
        current_handshake = {}
        in_certificate = False
        cert_lines = []
        
        for line in lines:
            # Protocol version
            if 'Protocol' in line:
                match = re.search(r'(TLSv1\.[0-3]|SSLv[23])', line)
                if match:
                    current_handshake['protocol'] = match.group(1)
            
            # Cipher suite
            if 'Cipher' in line:
                match = re.search(r'Cipher\s*:\s*([A-Z0-9-_]+)', line)
                if match:
                    current_handshake['cipher'] = match.group(1)
            
            # Session ID
            if 'Session-ID' in line:
                match = re.search(r'([A-F0-9]{64})', line)
                if match:
                    current_handshake['session_id'] = match.group(1)
            
            # Certificate chain
            if 'BEGIN CERTIFICATE' in line:
                in_certificate = True
                cert_lines = [line]
            elif 'END CERTIFICATE' in line:
                cert_lines.append(line)
                in_certificate = False
                current_handshake.setdefault('certificates', []).append(
                    ''.join(cert_lines)
                )
            elif in_certificate:
                cert_lines.append(line)
        
        if current_handshake:
            self.handshakes.append(current_handshake)
    
    def detect_vulnerabilities(self):
        """Detect known TLS vulnerabilities"""
        vulnerabilities = []
        
        for hs in self.handshakes:
            # SSLv3/TLSv1.0 usage (POODLE, BEAST)
            version = hs.get('version') or hs.get('protocol')
            if version in ['SSLv3', 'TLSv1.0']:
                vulnerabilities.append({
                    'type': 'Weak Protocol Version',
                    'severity': 'HIGH',
                    'description': f'{version} is vulnerable to POODLE/BEAST attacks',
                    'handshake': hs
                })
            
            # Weak ciphers
            ciphers = hs.get('ciphers', []) + [hs.get('cipher', '')]
            weak_patterns = ['RC4', 'DES', 'MD5', 'NULL', 'EXPORT', 'anon']
            
            for cipher in ciphers:
                if isinstance(cipher, str):
                    for weak in weak_patterns:
                        if weak in cipher.upper():
                            vulnerabilities.append({
                                'type': 'Weak Cipher',
                                'severity': 'HIGH',
                                'description': f'Weak cipher detected: {cipher}',
                                'handshake': hs
                            })
                            break
            
            # Missing security extensions
            extensions = hs.get('extensions', [])
            recommended = ['extended_master_secret', 'renegotiation_info']
            
            for ext in recommended:
                if ext not in [e.lower() for e in extensions]:
                    vulnerabilities.append({
                        'type': 'Missing Extension',
                        'severity': 'MEDIUM',
                        'description': f'Missing recommended extension: {ext}',
                        'handshake': hs
                    })
        
        return vulnerabilities
    
    def print_summary(self):
        """Print handshake analysis summary"""
        print(f"\n=== TLS Handshake Analysis ===")
        print(f"Total handshakes: {len(self.handshakes)}")
        
        if self.statistics:
            print(f"\nProtocol versions:")
            for version, count in sorted(self.statistics.items()):
                print(f"  {version}: {count}")
        
        if self.handshakes:
            print(f"\n=== Sample Handshake ===")
            hs = self.handshakes[0]
            for key, value in hs.items():
                if key == 'ciphers' and value:
                    print(f"  {key}: {value[:5]}...")
                elif key != 'certificates':
                    print(f"  {key}: {value}")
        
        vulnerabilities = self.detect_vulnerabilities()
        if vulnerabilities:
            print(f"\n=== Vulnerabilities Detected: {len(vulnerabilities)} ===")
            for vuln in vulnerabilities[:5]:
                print(f"\n[{vuln['severity']}] {vuln['type']}")
                print(f"  {vuln['description']}")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python3 tls_parser.py <logfile>")
        sys.exit(1)
    
    parser = TLSHandshakeParser(sys.argv[1])
    parser.parse_handshake_log()
    parser.parse_openssl_debug()
    parser.print_summary()
```

### Extracting Session Keys and Secrets

**[Unverified]** The following techniques may reveal session keys in debug logs, but should not be relied upon for production analysis:

```bash
# Search for pre-master secrets (OpenSSL debug)
grep -E "Pre-Master Secret|CLIENT_RANDOM" ssl_debug.log

# Extract SSLKEYLOGFILE format
# Format: CLIENT_RANDOM <client_random> <master_secret>
grep "CLIENT_RANDOM" sslkey.log | head -5

# Look for leaked private keys in logs (CTF scenario)
grep -E "BEGIN.*PRIVATE KEY" *.log

# Search for hardcoded secrets
grep -iE "secret|password|key.*=" ssl.log | grep -v "KeyExchange"
```

**Wireshark/tshark with SSLKEYLOGFILE:**

```bash
# Set environment variable for key logging
export SSLKEYLOGFILE=/tmp/sslkeys.log

# Use with tshark to decrypt traffic
tshark -r encrypted.pcap -o ssl.keylog_file:/tmp/sslkeys.log \
    -Y "http" -T fields -e http.request.uri

# Extract decrypted HTTP data
tshark -r encrypted.pcap -o ssl.keylog_file:/tmp/sslkeys.log \
    -Y "http" -w decrypted.pcap
```

### Handshake Timing Analysis

**Detect unusual timing patterns:**

```python
#!/usr/bin/env python3
"""
Analyze TLS handshake timing for anomalies
"""
import re
from datetime import datetime
from collections import defaultdict

def parse_handshake_timing(logfile):
    """Extract and analyze handshake timing"""
    handshakes = []
    
    with open(logfile, 'r', errors='ignore') as f:
        for line in f:
            # Extract timestamp and handshake event
            # Format: [timestamp] event
            match = re.search(
                r'\[?(\d{4}-\d{2}-\d{2}[T\s]\d{2}:\d{2}:\d{2}(?:\.\d+)?)\]?.*?(ClientHello|ServerHello|Certificate|Finished)',
                line,
                re.IGNORECASE
            )
            
            if match:
                timestamp_str, event = match.groups()
                try:
                    # Try different timestamp formats
                    for fmt in ['%Y-%m-%d %H:%M:%S.%f', '%Y-%m-%dT%H:%M:%S.%f', 
                               '%Y-%m-%d %H:%M:%S']:
                        try:
                            timestamp = datetime.strptime(timestamp_str[:26], fmt)
                            break
                        except:
                            continue
                    
                    handshakes.append({
                        'timestamp': timestamp,
                        'event': event
                    })
                except:
                    pass
    
    # Calculate timing statistics
    if len(handshakes) < 2:
        return None
    
    # Group handshakes by session (assuming sequential events)
    sessions = []
    current_session = []
    
    for hs in handshakes:
        current_session.append(hs)
        if hs['event'].lower() == 'finished':
            sessions.append(current_session)
            current_session = []
    
    # Analyze session durations
    durations = []
    for session in sessions:
        if len(session) >= 2:
            duration = (session[-1]['timestamp'] - session[0]['timestamp']).total_seconds()
            durations.append(duration)
    
    if durations:
        avg_duration = sum(durations) / len(durations)
        
        print(f"\n=== Handshake Timing Analysis ===")
        print(f"Total sessions: {len(sessions)}")
        print(f"Average duration: {avg_duration:.3f} seconds")
        print(f"Min duration: {min(durations):.3f} seconds")
        print(f"Max duration: {max(durations):.3f} seconds")
        
        # Detect anomalies (duration > 2 * average)
        anomalies = [d for d in durations if d > avg_duration * 2]
        if anomalies:
            print(f"\n Anomalous sessions: {len(anomalies)}")
            print(f"  Durations: {[f'{d:.3f}s' for d in anomalies[:5]]}")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python3 timing_analysis.py <logfile>")
        sys.exit(1)
    
    parse_handshake_timing(sys.argv[1])
```

## Certificate Information

SSL/TLS certificates contain identity information, validity periods, public keys, and signatures. Certificate data in logs can reveal misconfigurations, expired certificates, or embedded secrets.

### Extracting Certificate Data from Logs

**Find certificates in logs:**

```bash
# Extract PEM-encoded certificates
awk '/BEGIN CERTIFICATE/,/END CERTIFICATE/' logfile.log > cert.pem

# Extract multiple certificates
csplit -f cert- -b "%02d.pem" logfile.log '/BEGIN CERTIFICATE/' '{*}'

# Extract from base64-encoded logs
grep -oE '[A-Za-z0-9+/]{64,}' logfile.log | \
    while read line; do
        echo "$line" | base64 -d 2>/dev/null | \
            openssl x509 -inform DER -noout -text 2>/dev/null && echo "Valid cert found"
    done
```

**Parse certificate details:**

```bash
# Basic certificate information
openssl x509 -in cert.pem -noout -subject -issuer -dates

# Detailed certificate dump
openssl x509 -in cert.pem -noout -text

# Extract specific fields
openssl x509 -in cert.pem -noout -subject | sed 's/subject=//'
openssl x509 -in cert.pem -noout -issuer | sed 's/issuer=//'
openssl x509 -in cert.pem -noout -serial
openssl x509 -in cert.pem -noout -fingerprint -sha256

# Subject Alternative Names (SAN)
openssl x509 -in cert.pem -noout -text | grep -A1 "Subject Alternative Name"

# Extract public key
openssl x509 -in cert.pem -noout -pubkey > pubkey.pem

# Get certificate validity dates
openssl x509 -in cert.pem -noout -startdate -enddate
```

**Certificate chain validation:**

```bash
# Verify certificate against CA
openssl verify -CAfile ca.pem cert.pem

# Check certificate chain
openssl s_client -connect example.com:443 -showcerts < /dev/null 2>&1 | \
    awk '/BEGIN/,/END/ {print}'

# Extract full chain from logs
awk '/Certificate chain/,/Server certificate/ {print}' openssl_debug.log
```

### Certificate Analysis Script

```python
#!/usr/bin/env python3
"""
Extract and analyze SSL/TLS certificates from logs
"""
import re
import subprocess
import tempfile
from datetime import datetime

class CertificateAnalyzer:
    def __init__(self, logfile):
        self.logfile = logfile
        self.certificates = []
        
    def extract_certificates(self):
        """Extract PEM certificates from logs"""
        with open(self.logfile, 'r', errors='ignore') as f:
            content = f.read()
        
        # Find all PEM certificates
        cert_pattern = r'(-----BEGIN CERTIFICATE-----.*?-----END CERTIFICATE-----)'
        matches = re.findall(cert_pattern, content, re.DOTALL)
        
        for cert_pem in matches:
            cert_info = self.parse_certificate(cert_pem)
            if cert_info:
                self.certificates.append(cert_info)
        
        return len(self.certificates)
    
    def parse_certificate(self, cert_pem):
        """Parse certificate using OpenSSL"""
        try:
            # Write certificate to temp file
            with tempfile.NamedTemporaryFile(mode='w', suffix='.pem', delete=False) as f:
                f.write(cert_pem)
                temp_file = f.name
            
            # Extract certificate details
            cmd = ['openssl', 'x509', '-in', temp_file, '-noout', '-text']
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode != 0:
                return None
            
            cert_text = result.stdout
            
            # Parse key information
            cert_info = {
                'pem': cert_pem,
                'subject': self._extract_field(cert_text, r'Subject:\s*(.+)'),
                'issuer': self._extract_field(cert_text, r'Issuer:\s*(.+)'),
                'serial': self._extract_field(cert_text, r'Serial Number:\s*\n?\s*(.+)'),
                'not_before': self._extract_field(cert_text, r'Not Before:\s*(.+)'),
                'not_after': self._extract_field(cert_text, r'Not After\s*:\s*(.+)'),
                'signature_algo': self._extract_field(cert_text, r'Signature Algorithm:\s*(.+)'),
                'public_key_algo': self._extract_field(cert_text, r'Public Key Algorithm:\s*(.+)'),
                'key_size': self._extract_key_size(cert_text),
                'san': self._extract_san(cert_text),
                'extensions': self._extract_extensions(cert_text)
            }
            
            # Calculate fingerprints
            for hash_algo in ['sha256', 'sha1', 'md5']:
                cmd = ['openssl', 'x509', '-in', temp_file, '-noout', 
                       '-fingerprint', f'-{hash_algo}']
                result = subprocess.run(cmd, capture_output=True, text=True)
                if result.returncode == 0:
                    fp = result.stdout.strip().split('=')[1] if '=' in result.stdout else ''
                    cert_info[f'{hash_algo}_fingerprint'] = fp
            
            # Clean up temp file
            import os
            os.unlink(temp_file)
            
            return cert_info
            
        except Exception as e:
            print(f"Error parsing certificate: {e}")
            return None
    
    def _extract_field(self, text, pattern):
        """Extract single field using regex"""
        match = re.search(pattern, text, re.MULTILINE)
        return match.group(1).strip() if match else None
    
    def _extract_key_size(self, text):
        """Extract public key size"""
        match = re.search(r'Public-Key:\s*\((\d+)\s*bit\)', text)
        return int(match.group(1)) if match else None
    
    def _extract_san(self, text):
        """Extract Subject Alternative Names"""
        match = re.search(r'X509v3 Subject Alternative Name:\s*\n\s*(.+)', text)
        if match:
            san_str = match.group(1)
            # Parse DNS names
            dns_names = re.findall(r'DNS:([^,\s]+)', san_str)
            return dns_names
        return []
    
    def _extract_extensions(self, text):
        """Extract X.509v3 extensions"""
        extensions = []
        ext_pattern = r'(X509v3 [^:]+):'
        matches = re.findall(ext_pattern, text)
        return matches
    
    def check_vulnerabilities(self):
        """Check for common certificate vulnerabilities"""
        vulnerabilities = []
        
        for cert in self.certificates:
            # Check expiration
            if cert.get('not_after'):
                try:
                    # Parse date
                    expiry = datetime.strptime(cert['not_after'], '%b %d %H:%M:%S %Y %Z')
                    if expiry < datetime.now():
                        vulnerabilities.append({
                            'type': 'Expired Certificate',
                            'severity': 'CRITICAL',
                            'subject': cert.get('subject'),
                            'expired_on': cert['not_after']
                        })
                except:
                    pass
            
            # Check weak key size
            key_size = cert.get('key_size')
            if key_size and key_size < 2048:
                vulnerabilities.append({
                    'type': 'Weak Key Size',
                    'severity': 'HIGH',
                    'subject': cert.get('subject'),
                    'key_size': key_size,
                    'recommendation': 'Use at least 2048-bit RSA keys'
                })
            
            # Check weak signature algorithm
            sig_algo = cert.get('signature_algo', '')
            if 'md5' in sig_algo.lower() or 'sha1' in sig_algo.lower():
                vulnerabilities.append({
                    'type': 'Weak Signature Algorithm',
                    'severity': 'HIGH',
                    'subject': cert.get('subject'),
                    'algorithm': sig_algo,
                    'recommendation': 'Use SHA-256 or stronger'
                })
            
            # Check for self-signed certificates
            if cert.get('subject') == cert.get('issuer'):
                vulnerabilities.append({
                    'type': 'Self-Signed Certificate',
                    'severity': 'MEDIUM',
                    'subject': cert.get('subject')
                })
        
        return vulnerabilities
    
    def extract_ctf_clues(self):
        """Look for CTF-specific clues in certificates"""
        clues = []
        
        for cert in self.certificates:
            # Check CN for flags
            subject = cert.get('subject', '')
            if 'flag' in subject.lower():
                clues.append({
                    'location': 'Common Name',
                    'data': subject
                })
            
            # Check SAN for flags
            san_list = cert.get('san', [])
            for san in san_list:
                if 'flag' in san.lower():
                    clues.append({
                        'location': 'Subject Alternative Name',
                        'data': san
                    })
            
            # Check serial number for encoded data
            serial = cert.get('serial', '')
            if serial:
                # Try hex decode
                try:
                    hex_clean = serial.replace(':', '')
                    decoded = bytes.fromhex(hex_clean).decode('utf-8', errors='ignore')
                    if decoded and 'flag' in decoded.lower():
                        clues.append({
                            'location': 'Serial Number (hex decoded)',
                            'data': decoded
                        })
                except:
                    pass
            
            # Check extensions for unusual data
            extensions = cert.get('extensions', [])
            unusual = [ext for ext in extensions if 'netscape' in ext.lower() or 'custom' in ext.lower()]
            if unusual:
                clues.append({
                    'location': 'Unusual Extensions',
                    'data': unusual
                })
        
        return clues
    
    def print_summary(self):
        """Print certificate analysis summary"""
        print(f"\n=== Certificate Analysis ===")
        print(f"Certificates found: {len(self.certificates)}")
        
        for i, cert in enumerate(self.certificates, 1):
            print(f"\n[Certificate {i}]")
            print(f"Subject: {cert.get('subject')}")
            print(f"Issuer: {cert.get('issuer')}")
            print(f"Valid: {cert.get('not_before')} to {cert.get('not_after')}")
            print(f"Key: {cert.get('public_key_algo')} ({cert.get('key_size')} bit)")
            print(f"Signature: {cert.get('signature_algo')}")
            print(f"SHA256: {cert.get('sha256_fingerprint')}")
            
            san = cert.get('san', [])
            if san:
                print(f"SAN: {', '.join(san[:5])}")
        
        # Check vulnerabilities
        vulns = self.check_vulnerabilities()
        if vulns:
            print(f"\n=== Vulnerabilities: {len(vulns)} ===")
            for vuln in vulns:
                print(f"\n[{vuln['severity']}] {vuln['type']}")
                for key, value in vuln.items():
                    if key not in ['type', 'severity']:
                        print(f"  {key}: {value}")
        
        # Check CTF clues
        clues = self.extract_ctf_clues()
        if clues:
            print(f"\n=== CTF Clues Found: {len(clues)} ===")
            for clue in clues:
                print(f"\nLocation: {clue['location']}")
                print(f"Data: {clue['data']}")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python3 cert_analyzer.py <logfile>")
        sys.exit(1)
    
    analyzer = CertificateAnalyzer(sys.argv[1])
    analyzer.extract_certificates()
    analyzer.print_summary()
```

### Certificate Transparency Logs

**[Inference]** Certificate Transparency (CT) logs may be referenced in SSL/TLS logs through SCT (Signed Certificate Timestamp) extensions:

```bash
# Extract SCT from certificate
openssl x509 -in cert.pem -noout -text | grep -A10 "CT Precertificate SCTs"

# Query CT logs (requires online access)
# This technique is most useful for identifying historical certificates
curl -s "https://crt.sh/?q=example.com&output=json" | jq '.[] | {id, name_value, not_before, not_after}'
```

## Cipher Suite Identification

Cipher suites define the algorithms used for key exchange, authentication, encryption, and message authentication in TLS connections.

### Extracting Cipher Suite Information

**Identify negotiated cipher suites:**

```bash
# From OpenSSL s_client output
openssl s_client -connect example.com:443 < /dev/null 2>&1 | grep "Cipher"

# List all supported ciphers
openssl ciphers -v 'ALL:COMPLEMENTOFALL'

# Test specific ciphers
openssl s_client -connect example.com:443 -cipher 'ECDHE-RSA-AES256-GCM-SHA384' < /dev/null

# Extract from logs
grep -oE "TLS_[A-Z0-9_]+" ssl.log | sort | uniq -c | sort -rn

# Hex cipher suite codes
grep -oE "0x[0-9A-Fa-f]{4}" ssl.log | sort | uniq -c
```

**Enumerate supported cipher suites:**

```bash
#!/bin/bash
# enumerate_ciphers.sh - Test which ciphers a server accepts

HOST="$1"
PORT="${2:-443}"

if [ -z "$HOST" ]; then
    echo "Usage: $0 <host> [port]"
    exit 1
fi

echo "[*] Testing cipher suites for $HOST:$PORT..."

# Get list of all ciphers
ciphers=$(openssl ciphers 'ALL:eNULL' | tr ':' ' ')

for cipher in $ciphers; do
    result=$(echo -n | timeout 2 openssl s_client -connect "$HOST:$PORT" -cipher "$cipher" 2>&1)
    
    if echo "$result" | grep -q "Cipher is"; then
        negotiated=$(echo "$result" | grep "Cipher is" | cut -d' ' -f3-)
        echo "[+] Accepted: $cipher ($negotiated)"
    fi
done

echo "[*] Enumeration complete"
```

### Cipher Suite Analysis

**Decode cipher suite components:**

```python
#!/usr/bin/env python3
"""
Analyze and decode TLS cipher suites
"""
import re


class CipherSuiteAnalyzer:
    # Cipher suite registry (partial - IANA maintains full list)
    CIPHER_SUITES = {
        '0x1301': 'TLS_AES_128_GCM_SHA256',
        '0x1302': 'TLS_AES_256_GCM_SHA384',
        '0x1303': 'TLS_CHACHA20_POLY1305_SHA256',
        '0xC02F': 'TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256',
        '0xC030': 'TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384',
        '0xC02B': 'TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256',
        '0xC02C': 'TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384',
        '0x009C': 'TLS_RSA_WITH_AES_128_GCM_SHA256',
        '0x009D': 'TLS_RSA_WITH_AES_256_GCM_SHA384',
        '0x002F': 'TLS_RSA_WITH_AES_128_CBC_SHA',
        '0x0035': 'TLS_RSA_WITH_AES_256_CBC_SHA',
        '0x000A': 'TLS_RSA_WITH_3DES_EDE_CBC_SHA',
        '0x0005': 'TLS_RSA_WITH_RC4_128_SHA',
        '0x0004': 'TLS_RSA_WITH_RC4_128_MD5',
    }
    
    # Cipher strength classification
    WEAK_CIPHERS = ['RC4', 'DES', 'MD5', 'NULL', 'EXPORT', 'anon']
    DEPRECATED_CIPHERS = ['3DES', 'CBC', 'SHA1']
    
    def __init__(self):
        self.ciphers_found = []
    
    def parse_cipher_suite(self, cipher_name):
        """Parse cipher suite name into components"""
        # TLS_<KeyExchange>_<Authentication>_WITH_<Encryption>_<MAC>
        # or TLS_<Encryption>_<MAC> for TLS 1.3
        
        components = {
            'full_name': cipher_name,
            'key_exchange': None,
            'authentication': None,
            'encryption': None,
            'mac': None,
            'protocol': 'TLS 1.2'  # Default assumption
        }
        
        # TLS 1.3 ciphers (simplified format)
        if re.match(r'TLS_(AES|CHACHA20)', cipher_name):
            components['protocol'] = 'TLS 1.3'
            match = re.search(r'TLS_([A-Z0-9_]+)_([A-Z0-9]+)$', cipher_name)
            if match:
                components['encryption'] = match.group(1)
                components['mac'] = match.group(2)
            return components
        
        # TLS 1.2 and earlier
        parts = cipher_name.replace('TLS_', '').replace('_WITH_', '_').split('_')
        
        if len(parts) >= 3:
            # Extract key exchange
            if parts[0] in ['ECDHE', 'DHE', 'RSA', 'DH', 'ECDH', 'PSK']:
                components['key_exchange'] = parts[0]
                parts = parts[1:]
            
            # Extract authentication
            if parts[0] in ['RSA', 'ECDSA', 'DSS', 'PSK', 'anon']:
                components['authentication'] = parts[0]
                parts = parts[1:]
            
            # Extract encryption (usually next 2-3 parts)
            enc_parts = []
            while parts and parts[0] not in ['SHA', 'SHA256', 'SHA384', 'MD5']:
                enc_parts.append(parts.pop(0))
            components['encryption'] = '_'.join(enc_parts)
            
            # Remaining is MAC
            if parts:
                components['mac'] = '_'.join(parts)
        
        return components
    
    def assess_cipher_strength(self, cipher_name):
        """Assess cipher suite security strength"""
        cipher_upper = cipher_name.upper()
        
        # Check for weak algorithms
        for weak in self.WEAK_CIPHERS:
            if weak.upper() in cipher_upper:
                return {
                    'strength': 'WEAK',
                    'reason': f'Contains weak algorithm: {weak}',
                    'recommendation': 'Do not use'
                }
        
        # Check for deprecated algorithms
        for deprecated in self.DEPRECATED_CIPHERS:
            if deprecated.upper() in cipher_upper:
                return {
                    'strength': 'DEPRECATED',
                    'reason': f'Contains deprecated algorithm: {deprecated}',
                    'recommendation': 'Upgrade to modern cipher'
                }
        
        # Check for modern ciphers
        if any(x in cipher_upper for x in ['GCM', 'CHACHA20', 'POLY1305']):
            if any(x in cipher_upper for x in ['ECDHE', 'DHE']):
                return {
                    'strength': 'STRONG',
                    'reason': 'Forward secrecy + AEAD encryption',
                    'recommendation': 'Recommended'
                }
            return {
                'strength': 'GOOD',
                'reason': 'AEAD encryption',
                'recommendation': 'Acceptable'
            }
        
        # Check key exchange
        if 'RSA' in cipher_upper and 'ECDHE' not in cipher_upper and 'DHE' not in cipher_upper:
            return {
                'strength': 'MEDIUM',
                'reason': 'No forward secrecy (static RSA)',
                'recommendation': 'Use ECDHE or DHE for forward secrecy'
            }
        
        return {
            'strength': 'MEDIUM',
            'reason': 'Standard cipher',
            'recommendation': 'Consider upgrading'
        }
    
    def analyze_log_ciphers(self, logfile):
        """Extract and analyze ciphers from log file"""
        with open(logfile, 'r', errors='ignore') as f:
            content = f.read()
        
        # Find cipher suite names
        cipher_names = re.findall(r'TLS_[A-Z0-9_]+', content)
        
        # Find hex codes
        hex_codes = re.findall(r'\b0x[0-9A-Fa-f]{4}\b', content)
        
        # Combine and deduplicate
        all_ciphers = set()
        
        for name in cipher_names:
            all_ciphers.add(name)
        
        for code in hex_codes:
            if code in self.CIPHER_SUITES:
                all_ciphers.add(self.CIPHER_SUITES[code])
        
        # Analyze each cipher
        results = []
        for cipher in all_ciphers:
            components = self.parse_cipher_suite(cipher)
            strength = self.assess_cipher_strength(cipher)
            
            results.append({
                'cipher': cipher,
                'components': components,
                'strength': strength
            })
        
        return sorted(results, key=lambda x: ['WEAK', 'DEPRECATED', 'MEDIUM', 'GOOD', 'STRONG'].index(x['strength']['strength']))
    
    def print_analysis(self, results):
        """Print cipher analysis results"""
        if not results:
            print("No cipher suites found")
            return
        
        print(f"\n=== Cipher Suite Analysis ===")
        print(f"Total unique ciphers: {len(results)}\n")
        
        # Group by strength
        by_strength = {}
        for r in results:
            strength = r['strength']['strength']
            by_strength.setdefault(strength, []).append(r)
        
        for strength in ['WEAK', 'DEPRECATED', 'MEDIUM', 'GOOD', 'STRONG']:
            if strength in by_strength:
                print(f"\n=== {strength} Ciphers: {len(by_strength[strength])} ===")
                
                for r in by_strength[strength][:5]:  # Show first 5 of each category
                    print(f"\n{r['cipher']}")
                    comp = r['components']
                    if comp['key_exchange']:
                        print(f"  Key Exchange: {comp['key_exchange']}")
                    if comp['authentication']:
                        print(f"  Authentication: {comp['authentication']}")
                    print(f"  Encryption: {comp['encryption']}")
                    print(f"  MAC: {comp['mac']}")
                    print(f"  Protocol: {comp['protocol']}")
                    print(f"  Reason: {r['strength']['reason']}")
                    print(f"  Recommendation: {r['strength']['recommendation']}")
                
                if len(by_strength[strength]) > 5:
                    print(f"\n  ... and {len(by_strength[strength]) - 5} more")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python3 cipher_analyzer.py <logfile>")
        sys.exit(1)
    
    analyzer = CipherSuiteAnalyzer()
    results = analyzer.analyze_log_ciphers(sys.argv[1])
    analyzer.print_analysis(results)
````

### Known Cipher Suite Vulnerabilities

**Test for specific vulnerabilities:**
```bash
#!/bin/bash
# vuln_cipher_test.sh - Test for known cipher vulnerabilities

HOST="$1"
PORT="${2:-443}"

echo "=== Testing $HOST:$PORT for cipher vulnerabilities ==="

# Test for POODLE (SSLv3)
echo -e "\n[*] Testing for POODLE (SSLv3)..."
timeout 2 openssl s_client -connect "$HOST:$PORT" -ssl3 < /dev/null 2>&1 | \
    grep -q "Cipher is" && echo "[!] VULNERABLE: SSLv3 enabled" || echo "[+] Not vulnerable"

# Test for BEAST (TLS 1.0 with CBC)
echo -e "\n[*] Testing for BEAST (TLS 1.0 CBC)..."
timeout 2 openssl s_client -connect "$HOST:$PORT" -tls1 -cipher 'CBC' < /dev/null 2>&1 | \
    grep -q "Cipher is.*CBC" && echo "[!] VULNERABLE: TLS 1.0 with CBC enabled" || echo "[+] Not vulnerable"

# Test for FREAK (Export ciphers)
echo -e "\n[*] Testing for FREAK (Export ciphers)..."
timeout 2 openssl s_client -connect "$HOST:$PORT" -cipher 'EXPORT' < /dev/null 2>&1 | \
    grep -q "Cipher is" && echo "[!] VULNERABLE: Export ciphers enabled" || echo "[+] Not vulnerable"

# Test for CRIME (TLS compression)
echo -e "\n[*] Testing for CRIME (TLS compression)..."
result=$(timeout 2 openssl s_client -connect "$HOST:$PORT" < /dev/null 2>&1)
echo "$result" | grep -q "Compression: NONE" && echo "[+] Not vulnerable" || echo "[!] VULNERABLE: TLS compression enabled"

# Test for Logjam (Export DHE)
echo -e "\n[*] Testing for Logjam (Weak DHE)..."
timeout 2 openssl s_client -connect "$HOST:$PORT" -cipher 'DHE' < /dev/null 2>&1 | \
    grep "Server Temp Key" | grep -q "DH, 512" && echo "[!] VULNERABLE: 512-bit DHE" || echo "[+] Not vulnerable"

# Test for RC4 (Bar Mitzvah)
echo -e "\n[*] Testing for RC4 ciphers..."
timeout 2 openssl s_client -connect "$HOST:$PORT" -cipher 'RC4' < /dev/null 2>&1 | \
    grep -q "Cipher is.*RC4" && echo "[!] VULNERABLE: RC4 enabled" || echo "[+] Not vulnerable"

# Test for NULL ciphers
echo -e "\n[*] Testing for NULL ciphers..."
timeout 2 openssl s_client -connect "$HOST:$PORT" -cipher 'NULL' < /dev/null 2>&1 | \
    grep -q "Cipher is" && echo "[!] VULNERABLE: NULL ciphers enabled" || echo "[+] Not vulnerable"

# Test for SWEET32 (64-bit block ciphers)
echo -e "\n[*] Testing for SWEET32 (3DES)..."
timeout 2 openssl s_client -connect "$HOST:$PORT" -cipher '3DES' < /dev/null 2>&1 | \
    grep -q "Cipher is.*3DES" && echo "[!] VULNERABLE: 3DES enabled" || echo "[+] Not vulnerable"

echo -e "\n[*] Vulnerability scan complete"
````

### Cipher Suite CTF Techniques

**Extract hidden data from cipher preferences:**

```python
#!/usr/bin/env python3
"""
Extract potential hidden data from cipher suite ordering
"""
import re

def analyze_cipher_ordering(logfile):
    """
    Analyze cipher suite order for hidden messages
    CTF creators may encode data in the order of offered ciphers
    """
    with open(logfile, 'r', errors='ignore') as f:
        content = f.read()
    
    # Find cipher lists (usually in ClientHello)
    cipher_lists = re.findall(
        r'(?:Cipher Suites|Ciphers):\s*\n((?:[\s]*(?:TLS_[A-Z0-9_]+|0x[0-9A-Fa-f]{4})[\s,]*\n?)+)',
        content,
        re.MULTILINE | re.IGNORECASE
    )
    
    for cipher_list in cipher_lists:
        ciphers = re.findall(r'(?:TLS_[A-Z0-9_]+|0x[0-9A-Fa-f]{4})', cipher_list)
        
        if len(ciphers) < 5:
            continue
        
        # Method 1: First letter of each cipher
        first_letters = ''.join(c[4] if c.startswith('TLS_') else c[2] for c in ciphers)
        print(f"First letters: {first_letters}")
        
        # Method 2: Count of ciphers as ASCII
        if 32 <= len(ciphers) < 127:
            print(f"Cipher count as ASCII: {chr(len(ciphers))}")
        
        # Method 3: Hex codes as data
        hex_codes = [c for c in ciphers if c.startswith('0x')]
        if hex_codes:
            hex_string = ''.join(c[2:] for c in hex_codes)
            try:
                decoded = bytes.fromhex(hex_string).decode('utf-8', errors='ignore')
                if decoded and len(decoded) > 5:
                    print(f"Hex codes decoded: {decoded[:100]}")
            except:
                pass

def extract_from_cipher_parameters(logfile):
    """Extract data from cipher parameters like key lengths"""
    with open(logfile, 'r', errors='ignore') as f:
        content = f.read()
    
    # Extract key exchange parameters
    dh_params = re.findall(r'(?:DH|ECDH)[^:]*:\s*(\d+)\s*bit', content)
    
    if dh_params:
        # Convert bit lengths to ASCII
        message = ''.join(chr(int(p) % 256) for p in dh_params if 32 <= int(p) % 256 < 127)
        if message:
            print(f"DH parameter encoding: {message}")
    
    # Extract from cipher suite hex codes
    hex_codes = re.findall(r'0x([0-9A-Fa-f]{2})([0-9A-Fa-f]{2})', content)
    
    if hex_codes:
        # Try both bytes
        message1 = ''.join(chr(int(h[0], 16)) for h in hex_codes if 32 <= int(h[0], 16) < 127)
        message2 = ''.join(chr(int(h[1], 16)) for h in hex_codes if 32 <= int(h[1], 16) < 127)
        
        if 'flag' in message1.lower():
            print(f"First byte encoding: {message1}")
        if 'flag' in message2.lower():
            print(f"Second byte encoding: {message2}")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python3 cipher_ctf.py <logfile>")
        sys.exit(1)
    
    print("=== Analyzing Cipher Suite Ordering ===")
    analyze_cipher_ordering(sys.argv[1])
    
    print("\n=== Analyzing Cipher Parameters ===")
    extract_from_cipher_parameters(sys.argv[1])
```

### Comprehensive Cryptographic Analysis

```python
#!/usr/bin/env python3
"""
Comprehensive cryptographic artifact analyzer for CTF logs
"""
import re
import subprocess
import tempfile
import os

class CryptoArtifactAnalyzer:
    def __init__(self, logfile):
        self.logfile = logfile
        self.findings = {
            'handshakes': [],
            'certificates': [],
            'ciphers': [],
            'vulnerabilities': [],
            'ctf_clues': []
        }
    
    def analyze_all(self):
        """Run complete cryptographic analysis"""
        print(f"[*] Analyzing cryptographic artifacts in {self.logfile}...")
        
        self.extract_handshakes()
        self.extract_certificates()
        self.extract_ciphers()
        self.check_vulnerabilities()
        self.find_ctf_clues()
        
        return self.findings
    
    def extract_handshakes(self):
        """Extract TLS handshake information"""
        with open(self.logfile, 'r', errors='ignore') as f:
            content = f.read()
        
        # Find handshake events
        handshake_events = re.findall(
            r'(ClientHello|ServerHello|Certificate|ServerKeyExchange|ClientKeyExchange|Finished)',
            content,
            re.IGNORECASE
        )
        
        self.findings['handshakes'] = handshake_events
        print(f"[+] Found {len(handshake_events)} handshake events")
    
    def extract_certificates(self):
        """Extract and parse certificates"""
        with open(self.logfile, 'r', errors='ignore') as f:
            content = f.read()
        
        # Extract PEM certificates
        cert_pattern = r'(-----BEGIN CERTIFICATE-----.*?-----END CERTIFICATE-----)'
        certs = re.findall(cert_pattern, content, re.DOTALL)
        
        for cert_pem in certs:
            try:
                with tempfile.NamedTemporaryFile(mode='w', suffix='.pem', delete=False) as f:
                    f.write(cert_pem)
                    temp_file = f.name
                
                # Parse certificate
                cmd = ['openssl', 'x509', '-in', temp_file, '-noout', '-subject', '-dates']
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=5)
                
                if result.returncode == 0:
                    self.findings['certificates'].append({
                        'pem': cert_pem[:100] + '...',
                        'info': result.stdout.strip()
                    })
                
                os.unlink(temp_file)
            except:
                pass
        
        print(f"[+] Found {len(self.findings['certificates'])} certificates")
    
    def extract_ciphers(self):
        """Extract cipher suite information"""
        with open(self.logfile, 'r', errors='ignore') as f:
            content = f.read()
        
        # Find cipher names
        ciphers = set(re.findall(r'TLS_[A-Z0-9_]+', content))
        
        self.findings['ciphers'] = list(ciphers)
        print(f"[+] Found {len(ciphers)} unique cipher suites")
    
    def check_vulnerabilities(self):
        """Check for cryptographic vulnerabilities"""
        vulns = []
        
        # Check cipher suites for weaknesses
        for cipher in self.findings['ciphers']:
            cipher_upper = cipher.upper()
            
            if any(weak in cipher_upper for weak in ['RC4', 'DES', 'MD5', 'NULL', 'EXPORT']):
                vulns.append({
                    'type': 'Weak Cipher',
                    'detail': cipher,
                    'severity': 'HIGH'
                })
            elif any(dep in cipher_upper for dep in ['3DES', 'CBC', 'SHA1']):
                vulns.append({
                    'type': 'Deprecated Cipher',
                    'detail': cipher,
                    'severity': 'MEDIUM'
                })
        
        # Check for protocol version issues
        with open(self.logfile, 'r', errors='ignore') as f:
            content = f.read()
        
        if re.search(r'SSLv[23]', content):
            vulns.append({
                'type': 'Weak Protocol',
                'detail': 'SSLv2/SSLv3 detected',
                'severity': 'CRITICAL'
            })
        
        if re.search(r'TLSv1\.0', content):
            vulns.append({
                'type': 'Deprecated Protocol',
                'detail': 'TLS 1.0 detected',
                'severity': 'MEDIUM'
            })
        
        self.findings['vulnerabilities'] = vulns
        print(f"[+] Found {len(vulns)} potential vulnerabilities")
    
    def find_ctf_clues(self):
        """Look for CTF-specific clues in crypto artifacts"""
        clues = []
        
        # Check certificates for flags
        for cert in self.findings['certificates']:
            if 'flag' in cert['info'].lower():
                clues.append({
                    'location': 'Certificate',
                    'data': cert['info']
                })
        
        # Check cipher ordering
        if len(self.findings['ciphers']) > 5:
            first_letters = ''.join(c[4] for c in sorted(self.findings['ciphers']) if len(c) > 4)
            if 'flag' in first_letters.lower():
                clues.append({
                    'location': 'Cipher ordering (first letters)',
                    'data': first_letters
                })
        
        # Check for unusual session IDs
        with open(self.logfile, 'r', errors='ignore') as f:
            content = f.read()
        
        session_ids = re.findall(r'Session-ID:\s*([A-F0-9]{64})', content)
        for sid in session_ids:
            try:
                decoded = bytes.fromhex(sid).decode('utf-8', errors='ignore')
                if 'flag' in decoded.lower():
                    clues.append({
                        'location': 'Session ID',
                        'data': decoded
                    })
            except:
                pass
        
        self.findings['ctf_clues'] = clues
        print(f"[+] Found {len(clues)} potential CTF clues")
    
    def print_report(self):
        """Print comprehensive analysis report"""
        print(f"\n{'='*60}")
        print("CRYPTOGRAPHIC ARTIFACT ANALYSIS REPORT")
        print(f"{'='*60}")
        
        print(f"\n=== Summary ===")
        print(f"Handshake events: {len(self.findings['handshakes'])}")
        print(f"Certificates: {len(self.findings['certificates'])}")
        print(f"Cipher suites: {len(self.findings['ciphers'])}")
        print(f"Vulnerabilities: {len(self.findings['vulnerabilities'])}")
        print(f"CTF clues: {len(self.findings['ctf_clues'])}")
        
        if self.findings['vulnerabilities']:
            print(f"\n=== Vulnerabilities ===")
            for vuln in self.findings['vulnerabilities']:
                print(f"\n[{vuln['severity']}] {vuln['type']}")
                print(f"  Detail: {vuln['detail']}")
        
        if self.findings['ctf_clues']:
            print(f"\n=== CTF Clues ===")
            for clue in self.findings['ctf_clues']:
                print(f"\nLocation: {clue['location']}")
                print(f"Data: {clue['data']}")
        
        if self.findings['ciphers']:
            print(f"\n=== Cipher Suites (first 10) ===")
            for cipher in list(self.findings['ciphers'])[:10]:
                print(f"  {cipher}")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python3 crypto_analyzer.py <logfile>")
        sys.exit(1)
    
    analyzer = CryptoArtifactAnalyzer(sys.argv[1])
    analyzer.analyze_all()
    analyzer.print_report()
```

### Quick Reference Commands

```bash
# Extract all TLS/SSL related data
grep -iE "ssl|tls|certificate|cipher|handshake" logfile.log > crypto_artifacts.txt

# Decode all base64 certificate data
grep -A50 "BEGIN CERTIFICATE" logfile.log | \
    openssl x509 -inform PEM -noout -text

# Check for weak protocol versions
grep -E "SSLv[23]|TLSv1\.0" logfile.log

# Extract and sort all cipher suites
grep -oE "TLS_[A-Z0-9_]+" logfile.log | sort -u

# Find certificate fingerprints
grep -oE "[A-F0-9]{2}(:[A-F0-9]{2}){19,31}" logfile.log

# Extract session IDs
grep -oE "Session-ID: [A-F0-9]{64}" logfile.log
```

**[Inference]**: The presence of cryptographic artifacts in logs suggests SSL/TLS debugging was enabled or the application specifically logs security events. In CTF scenarios, verbose crypto logging often indicates that certificate or cipher analysis is part of the challenge solution.

---

**Important related topics:**

- **Perfect Forward Secrecy (PFS) analysis** - Identifying ephemeral key exchange mechanisms
- **OCSP stapling in logs** - Certificate revocation checking artifacts
- **Encrypted SNI (ESNI) detection** - Privacy-enhancing TLS extensions
- **Certificate pinning failures** - Logs indicating pinning violations

---

## Encryption Protocol Logs

Encryption protocol logs capture SSL/TLS handshakes, cipher negotiations, and encrypted session details that may reveal vulnerabilities or contain hidden information.

### SSL/TLS handshake analysis

**Extract TLS negotiation details:**

```bash
# Extract SSL/TLS cipher suites from Apache/Nginx SSL logs
grep -i "ssl.*cipher" /var/log/apache2/ssl_request.log | \
    awk -F'Cipher: ' '{print $2}' | \
    sort | uniq -c | sort -rn

# Find deprecated/weak ciphers
grep -iE "(RC4|DES|MD5|SSLv2|SSLv3)" /var/log/apache2/ssl_request.log
```

**Python SSL/TLS log parser:**

```python
import re
from collections import Counter, defaultdict

def analyze_ssl_logs(log_file):
    """
    Extract and analyze SSL/TLS protocol information from logs
    """
    ssl_versions = Counter()
    cipher_suites = Counter()
    key_exchanges = Counter()
    
    ssl_patterns = {
        'version': r'(TLSv1\.[0-3]|SSLv[23])',
        'cipher': r'(?:Cipher|cipher)[:\s]+([A-Z0-9_-]+)',
        'key_exchange': r'(ECDHE|DHE|RSA)[-_]',
        'encryption': r'[-_](AES|3DES|RC4|CHACHA20)[-_]?(\d+)?',
        'mac': r'[-_](SHA\d+|MD5|GCM|CCM|POLY1305)'
    }
    
    sessions = []
    
    with open(log_file, 'r', errors='ignore') as f:
        for line in f:
            session_data = {}
            
            # Extract SSL version
            version_match = re.search(ssl_patterns['version'], line, re.IGNORECASE)
            if version_match:
                version = version_match.group(1)
                ssl_versions[version] += 1
                session_data['version'] = version
            
            # Extract cipher suite
            cipher_match = re.search(ssl_patterns['cipher'], line)
            if cipher_match:
                cipher = cipher_match.group(1)
                cipher_suites[cipher] += 1
                session_data['cipher'] = cipher
                
                # Parse cipher components
                kex_match = re.search(ssl_patterns['key_exchange'], cipher)
                if kex_match:
                    key_exchanges[kex_match.group(1)] += 1
            
            if session_data:
                sessions.append(session_data)
    
    print("=== SSL/TLS PROTOCOL ANALYSIS ===\n")
    
    print("Protocol versions:")
    for version, count in ssl_versions.most_common():
        print(f"  {version:10s}: {count:5d}")
    
    print("\nCipher suites (top 10):")
    for cipher, count in cipher_suites.most_common(10):
        print(f"  {cipher:40s}: {count:5d}")
    
    print("\nKey exchange algorithms:")
    for kex, count in key_exchanges.most_common():
        print(f"  {kex:10s}: {count:5d}")
    
    # Identify weak configurations
    print("\n=== SECURITY ASSESSMENT ===")
    weak_ciphers = ['RC4', 'DES', '3DES', 'MD5']
    weak_versions = ['SSLv2', 'SSLv3', 'TLSv1.0']
    
    weak_found = False
    for version, count in ssl_versions.items():
        if version in weak_versions:
            print(f"[!] Weak protocol: {version} ({count} sessions)")
            weak_found = True
    
    for cipher, count in cipher_suites.items():
        if any(weak in cipher for weak in weak_ciphers):
            print(f"[!] Weak cipher: {cipher} ({count} sessions)")
            weak_found = True
    
    if not weak_found:
        print("[+] No obvious weak configurations detected")

analyze_ssl_logs('/var/log/apache2/ssl_request.log')
```

### Certificate information extraction

**Extract and analyze certificate details:**

```python
import re
from datetime import datetime

def extract_certificate_info(log_file):
    """
    Parse certificate information from SSL/TLS logs
    """
    cert_patterns = {
        'subject': r'subject[=:\s]+(.+?)(?:\s|$|,)',
        'issuer': r'issuer[=:\s]+(.+?)(?:\s|$|,)',
        'serial': r'serial[=:\s]+([A-F0-9:]+)',
        'fingerprint': r'fingerprint[=:\s]+([a-fA-F0-9:]{47,95})',
        'not_before': r'notBefore[=:\s]+([\w\s:]+)',
        'not_after': r'notAfter[=:\s]+([\w\s:]+)',
        'common_name': r'CN\s*=\s*([^,/]+)'
    }
    
    certificates = []
    
    with open(log_file, 'r', errors='ignore') as f:
        content = f.read()
        
        # Split by certificate boundaries if present
        cert_blocks = re.split(r'-----BEGIN CERTIFICATE-----|Certificate:|New cert', content)
        
        for block in cert_blocks:
            cert_info = {}
            
            for field, pattern in cert_patterns.items():
                match = re.search(pattern, block, re.IGNORECASE)
                if match:
                    cert_info[field] = match.group(1).strip()
            
            if cert_info:
                certificates.append(cert_info)
    
    print("=== CERTIFICATE INFORMATION ===\n")
    
    if not certificates:
        print("No certificate information found in logs")
        return
    
    print(f"Total certificates found: {len(certificates)}\n")
    
    for idx, cert in enumerate(certificates, 1):
        print(f"Certificate {idx}:")
        if 'common_name' in cert:
            print(f"  Common Name: {cert['common_name']}")
        if 'subject' in cert:
            print(f"  Subject: {cert['subject'][:80]}")
        if 'issuer' in cert:
            print(f"  Issuer: {cert['issuer'][:80]}")
        if 'serial' in cert:
            print(f"  Serial: {cert['serial']}")
        if 'fingerprint' in cert:
            print(f"  Fingerprint: {cert['fingerprint']}")
        print()

extract_certificate_info('/var/log/ssl_handshake.log')
```

### Perfect Forward Secrecy (PFS) detection

**Analyze ephemeral key exchanges:**

```python
import re
from collections import Counter

def analyze_pfs_usage(log_file):
    """
    Detect Perfect Forward Secrecy usage in TLS connections
    """
    pfs_algorithms = ['ECDHE', 'DHE']
    non_pfs_algorithms = ['RSA', 'DH']
    
    key_exchange_counter = Counter()
    pfs_sessions = 0
    non_pfs_sessions = 0
    
    with open(log_file, 'r', errors='ignore') as f:
        for line in f:
            # Look for key exchange in cipher suite
            cipher_match = re.search(r'(?:Cipher|cipher)[:\s]+([A-Z0-9_-]+)', line)
            if cipher_match:
                cipher = cipher_match.group(1)
                
                # Check for PFS algorithms
                is_pfs = False
                for pfs_alg in pfs_algorithms:
                    if pfs_alg in cipher:
                        key_exchange_counter[pfs_alg] += 1
                        pfs_sessions += 1
                        is_pfs = True
                        break
                
                if not is_pfs:
                    for non_pfs_alg in non_pfs_algorithms:
                        if cipher.startswith(non_pfs_alg):
                            key_exchange_counter[non_pfs_alg] += 1
                            non_pfs_sessions += 1
                            break
    
    print("=== PERFECT FORWARD SECRECY ANALYSIS ===\n")
    
    total = pfs_sessions + non_pfs_sessions
    if total == 0:
        print("No key exchange information found")
        return
    
    pfs_percentage = (pfs_sessions / total) * 100
    
    print(f"Total sessions analyzed: {total}")
    print(f"PFS-enabled sessions: {pfs_sessions} ({pfs_percentage:.1f}%)")
    print(f"Non-PFS sessions: {non_pfs_sessions} ({100-pfs_percentage:.1f}%)")
    
    print("\nKey exchange breakdown:")
    for kex, count in key_exchange_counter.most_common():
        percentage = (count / total) * 100
        pfs_status = "[PFS]" if kex in pfs_algorithms else "[Non-PFS]"
        print(f"  {kex:10s} {pfs_status:10s}: {count:5d} ({percentage:.1f}%)")

analyze_pfs_usage('/var/log/apache2/ssl_request.log')
```

### Encrypted payload identification

**Detect and extract encrypted data from logs:**

```bash
# Find Base64-encoded encrypted data (often symmetric encryption output)
python3 << 'EOF'
import re
import base64
import string

def identify_encrypted_payloads(log_file):
    """
    Identify potentially encrypted data in logs
    [Inference: Based on entropy and pattern analysis]
    """
    
    def calculate_entropy(data):
        """Calculate Shannon entropy of data"""
        if not data:
            return 0
        
        entropy = 0
        for x in range(256):
            p_x = data.count(bytes([x])) / len(data)
            if p_x > 0:
                entropy += - p_x * (p_x.bit_length() - 1)
        return entropy
    
    def is_high_entropy(text, threshold=4.5):
        """Check if text has high entropy (likely encrypted)"""
        try:
            entropy = calculate_entropy(text.encode())
            return entropy > threshold
        except:
            return False
    
    encrypted_candidates = []
    
    with open(log_file, 'r', errors='ignore') as f:
        for line_num, line in enumerate(f, 1):
            # Look for Base64-like patterns
            b64_matches = re.findall(r'[A-Za-z0-9+/]{40,}={0,2}', line)
            
            for match in b64_matches:
                try:
                    decoded = base64.b64decode(match)
                    
                    # Calculate entropy of decoded data
                    entropy = calculate_entropy(decoded)
                    
                    # High entropy suggests encryption
                    if entropy > 4.5:
                        encrypted_candidates.append({
                            'line': line_num,
                            'data': match[:60] + '...' if len(match) > 60 else match,
                            'entropy': entropy,
                            'decoded_len': len(decoded)
                        })
                except:
                    pass
    
    print("=== ENCRYPTED PAYLOAD DETECTION ===")
    print("[Inference: Based on entropy analysis]\n")
    
    if not encrypted_candidates:
        print("No high-entropy encrypted payloads detected")
        return
    
    print(f"Found {len(encrypted_candidates)} potential encrypted payloads:\n")
    
    for candidate in encrypted_candidates[:10]:
        print(f"Line {candidate['line']}:")
        print(f"  Data: {candidate['data']}")
        print(f"  Entropy: {candidate['entropy']:.2f}")
        print(f"  Decoded size: {candidate['decoded_len']} bytes")
        print()

identify_encrypted_payloads('access.log')
EOF
```

### Protocol downgrade detection

**Identify SSL/TLS downgrade attacks:**

```python
import re
from datetime import datetime
from collections import defaultdict

def detect_protocol_downgrades(log_file):
    """
    Detect potential SSL/TLS downgrade attacks
    """
    
    version_hierarchy = {
        'TLSv1.3': 4,
        'TLSv1.2': 3,
        'TLSv1.1': 2,
        'TLSv1.0': 1,
        'TLSv1': 1,
        'SSLv3': 0,
        'SSLv2': -1
    }
    
    connections_by_ip = defaultdict(list)
    
    with open(log_file, 'r', errors='ignore') as f:
        for line in f:
            ip_match = re.search(r'(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})', line)
            version_match = re.search(r'(TLSv1\.[0-3]|TLSv1|SSLv[23])', line)
            ts_match = re.search(r'\[(\d{2}/\w{3}/\d{4}:\d{2}:\d{2}:\d{2})', line)
            
            if ip_match and version_match:
                ip = ip_match.group(1)
                version = version_match.group(1)
                timestamp = ts_match.group(1) if ts_match else 'unknown'
                
                connections_by_ip[ip].append({
                    'timestamp': timestamp,
                    'version': version,
                    'version_score': version_hierarchy.get(version, 0),
                    'line': line.strip()
                })
    
    print("=== PROTOCOL DOWNGRADE DETECTION ===\n")
    
    downgrades_found = False
    
    for ip, connections in connections_by_ip.items():
        if len(connections) < 2:
            continue
        
        # Check for version downgrades
        for i in range(len(connections) - 1):
            current = connections[i]
            next_conn = connections[i + 1]
            
            if current['version_score'] > next_conn['version_score']:
                if not downgrades_found:
                    downgrades_found = True
                
                print(f"[!] Downgrade detected from IP: {ip}")
                print(f"    {current['timestamp']}: {current['version']}")
                print(f"       {current['line'][:100]}")
                print(f"    {next_conn['timestamp']}: {next_conn['version']}")
                print(f"       {next_conn['line'][:100]}")
                print()
    
    if not downgrades_found:
        print("[+] No protocol downgrades detected")

detect_protocol_downgrades('/var/log/apache2/ssl_request.log')
```

## Key Exchange Logs

Key exchange logs contain cryptographic key material, Diffie-Hellman parameters, and session key information that may be exploitable or contain hidden flags.

### Diffie-Hellman parameter extraction

**Extract DH parameters from logs:**

```python
import re

def extract_dh_parameters(log_file):
    """
    Extract Diffie-Hellman parameters from logs
    """
    dh_patterns = {
        'prime': r'(?:prime|p)[=:\s]+([0-9a-fA-F]{64,})',
        'generator': r'(?:generator|g)[=:\s]+([0-9a-fA-F]+)',
        'public_key': r'(?:public[_\s]?key|pub)[=:\s]+([0-9a-fA-F]{64,})',
        'private_key': r'(?:private[_\s]?key|priv)[=:\s]+([0-9a-fA-F]{32,})',
        'shared_secret': r'(?:shared[_\s]?secret|secret)[=:\s]+([0-9a-fA-F]{32,})'
    }
    
    dh_sessions = []
    
    with open(log_file, 'r', errors='ignore') as f:
        content = f.read()
        
        # Try to find DH parameter blocks
        blocks = re.split(r'(?:DH|Diffie-Hellman).*?(?:parameters|exchange)', content, flags=re.IGNORECASE)
        
        for block in blocks:
            session_params = {}
            
            for param_name, pattern in dh_patterns.items():
                matches = re.findall(pattern, block, re.IGNORECASE)
                if matches:
                    session_params[param_name] = matches[0]
            
            if session_params:
                dh_sessions.append(session_params)
    
    print("=== DIFFIE-HELLMAN PARAMETER EXTRACTION ===\n")
    
    if not dh_sessions:
        print("No DH parameters found in logs")
        return
    
    print(f"Found {len(dh_sessions)} DH key exchange sessions\n")
    
    for idx, session in enumerate(dh_sessions, 1):
        print(f"Session {idx}:")
        
        if 'prime' in session:
            prime_len = len(session['prime']) * 4  # Hex to bits
            print(f"  Prime (p): {session['prime'][:64]}... ({prime_len} bits)")
        
        if 'generator' in session:
            print(f"  Generator (g): {session['generator']}")
        
        if 'public_key' in session:
            print(f"  Public Key: {session['public_key'][:64]}...")
        
        if 'private_key' in session:
            print(f"  [!] Private Key Found: {session['private_key'][:32]}...")
        
        if 'shared_secret' in session:
            print(f"  [!] Shared Secret Found: {session['shared_secret'][:32]}...")
        
        print()

extract_dh_parameters('/var/log/key_exchange.log')
```

### RSA key material detection

**Find RSA keys and parameters:**

```bash
# Extract RSA key components from logs
python3 << 'EOF'
import re

def extract_rsa_keys(log_file):
    """
    Extract RSA key material from logs
    """
    
    # Look for PEM-encoded keys
    pem_patterns = {
        'private_key': r'-----BEGIN (?:RSA )?PRIVATE KEY-----(.*?)-----END',
        'public_key': r'-----BEGIN PUBLIC KEY-----(.*?)-----END',
        'certificate': r'-----BEGIN CERTIFICATE-----(.*?)-----END'
    }
    
    # Look for raw RSA parameters
    param_patterns = {
        'modulus': r'(?:modulus|n)[=:\s]+([0-9a-fA-F]{256,})',
        'public_exponent': r'(?:publicExponent|e)[=:\s]+([0-9a-fA-F]+)',
        'private_exponent': r'(?:privateExponent|d)[=:\s]+([0-9a-fA-F]{256,})',
        'prime1': r'(?:prime1|p)[=:\s]+([0-9a-fA-F]{128,})',
        'prime2': r'(?:prime2|q)[=:\s]+([0-9a-fA-F]{128,})'
    }
    
    findings = {
        'pem_keys': [],
        'raw_params': []
    }
    
    with open(log_file, 'r', errors='ignore') as f:
        content = f.read()
        
        # Extract PEM keys
        for key_type, pattern in pem_patterns.items():
            matches = re.findall(pattern, content, re.DOTALL)
            for match in matches:
                findings['pem_keys'].append({
                    'type': key_type,
                    'data': match[:100].strip()
                })
        
        # Extract raw parameters
        params = {}
        for param_name, pattern in param_patterns.items():
            matches = re.findall(pattern, content, re.IGNORECASE)
            if matches:
                params[param_name] = matches[0]
        
        if params:
            findings['raw_params'].append(params)
    
    print("=== RSA KEY MATERIAL EXTRACTION ===\n")
    
    if findings['pem_keys']:
        print(f"PEM-encoded keys found: {len(findings['pem_keys'])}\n")
        for key in findings['pem_keys']:
            print(f"  Type: {key['type']}")
            print(f"  Data: {key['data'][:60]}...")
            print()
    
    if findings['raw_params']:
        print(f"Raw RSA parameters found: {len(findings['raw_params'])}\n")
        for params in findings['raw_params']:
            print("  Parameters:")
            for param_name, value in params.items():
                bit_len = len(value) * 4
                print(f"    {param_name}: {value[:40]}... ({bit_len} bits)")
            print()
    
    if not findings['pem_keys'] and not findings['raw_params']:
        print("No RSA key material found")

extract_rsa_keys('/var/log/crypto.log')
EOF
```

### Elliptic curve parameter extraction

**Extract ECDH/ECDSA parameters:**

```python
import re

def extract_ec_parameters(log_file):
    """
    Extract Elliptic Curve cryptography parameters
    """
    ec_patterns = {
        'curve_name': r'(?:curve|curve_name)[=:\s]+([\w\d-]+)',
        'point_x': r'(?:x|point_x)[=:\s]+([0-9a-fA-F]{32,})',
        'point_y': r'(?:y|point_y)[=:\s]+([0-9a-fA-F]{32,})',
        'private_key': r'(?:private|d)[=:\s]+([0-9a-fA-F]{32,})',
        'public_key': r'(?:public|Q)[=:\s]+([0-9a-fA-F]{64,})'
    }
    
    standard_curves = [
        'secp256r1', 'prime256v1', 'secp384r1', 'secp521r1',
        'secp256k1', 'curve25519', 'Ed25519', 'X25519'
    ]
    
    ec_sessions = []
    curve_usage = {}
    
    with open(log_file, 'r', errors='ignore') as f:
        for line in f:
            session_params = {}
            
            for param_name, pattern in ec_patterns.items():
                match = re.search(pattern, line, re.IGNORECASE)
                if match:
                    value = match.group(1)
                    session_params[param_name] = value
                    
                    if param_name == 'curve_name':
                        curve_usage[value] = curve_usage.get(value, 0) + 1
            
            if session_params:
                ec_sessions.append(session_params)
    
    print("=== ELLIPTIC CURVE PARAMETER EXTRACTION ===\n")
    
    if not ec_sessions:
        print("No EC parameters found")
        return
    
    print(f"Total EC sessions: {len(ec_sessions)}\n")
    
    if curve_usage:
        print("Curve usage:")
        for curve, count in sorted(curve_usage.items(), key=lambda x: x[1], reverse=True):
            print(f"  {curve:20s}: {count:5d}")
        print()
    
    print("Sample sessions:\n")
    for idx, session in enumerate(ec_sessions[:5], 1):
        print(f"Session {idx}:")
        for param, value in session.items():
            if len(value) > 64:
                print(f"  {param}: {value[:64]}...")
            else:
                print(f"  {param}: {value}")
        print()

extract_ec_parameters('/var/log/ssl_handshake.log')
```

### Session key reconstruction

**Attempt to reconstruct session keys:**

```python
import re
import hashlib

def reconstruct_session_keys(log_file):
    """
    Attempt to reconstruct or identify session keys
    [Inference: Based on available cryptographic material]
    """
    
    key_patterns = {
        'master_secret': r'(?:master[_\s]?secret|MS)[=:\s]+([0-9a-fA-F]{96})',
        'pre_master': r'(?:pre[_\s]?master[_\s]?secret|PMS)[=:\s]+([0-9a-fA-F]{96})',
        'client_random': r'(?:client[_\s]?random|CR)[=:\s]+([0-9a-fA-F]{64})',
        'server_random': r'(?:server[_\s]?random|SR)[=:\s]+([0-9a-fA-F]{64})',
        'session_id': r'(?:session[_\s]?id|SID)[=:\s]+([0-9a-fA-F]{32,64})',
        'session_ticket': r'(?:session[_\s]?ticket)[=:\s]+([0-9a-fA-F]{32,})'
    }
    
    sessions = []
    
    with open(log_file, 'r', errors='ignore') as f:
        content = f.read()
        
        # Look for TLS session key log format (NSS Key Log Format)
        nss_matches = re.findall(
            r'(CLIENT_RANDOM|RSA) ([0-9a-fA-F]{64}) ([0-9a-fA-F]{96})',
            content
        )
        
        if nss_matches:
            print("=== SESSION KEY RECONSTRUCTION ===")
            print("[!] NSS Key Log Format detected\n")
            
            for key_type, client_random, master_secret in nss_matches[:5]:
                print(f"Session ({key_type}):")
                print(f"  Client Random: {client_random}")
                print(f"  Master Secret: {master_secret}")
                print(f"  [!] This can be used with Wireshark to decrypt TLS traffic")
                print()
            return
        
        # Look for individual key components
        for line in content.split('\n'):
            session_data = {}
            
            for key_name, pattern in key_patterns.items():
                match = re.search(pattern, line, re.IGNORECASE)
                if match:
                    session_data[key_name] = match.group(1)
            
            if session_data:
                sessions.append(session_data)
    
    print("=== SESSION KEY RECONSTRUCTION ===")
    print("[Inference: Based on extracted cryptographic material]\n")
    
    if not sessions:
        print("No session key material found")
        return
    
    print(f"Found {len(sessions)} sessions with key material\n")
    
    for idx, session in enumerate(sessions[:5], 1):
        print(f"Session {idx}:")
        
        for key_name, value in session.items():
            print(f"  {key_name}: {value[:64]}{'...' if len(value) > 64 else ''}")
        
        # Check if we have enough to reconstruct keys
        if 'pre_master' in session and 'client_random' in session and 'server_random' in session:
            print("  [!] Sufficient material for key derivation present")
        
        print()

reconstruct_session_keys('/var/log/ssl_debug.log')
```

### Key reuse detection

**Detect cryptographic key reuse:**

```python
import re
from collections import Counter

def detect_key_reuse(log_files):
    """
    Detect if same cryptographic keys are reused across sessions
    """
    
    all_keys = {
        'public_keys': [],
        'session_ids': [],
        'certificates': []
    }
    
    patterns = {
        'public_key': r'(?:public[_\s]?key|pubkey)[=:\s]+([0-9a-fA-F]{64,})',
        'session_id': r'(?:session[_\s]?id)[=:\s]+([0-9a-fA-F]{32,64})',
        'cert_fingerprint': r'(?:fingerprint|thumbprint)[=:\s]+([0-9a-fA-F:]{47,95})'
    }
    
    for log_file in log_files:
        try:
            with open(log_file, 'r', errors='ignore') as f:
                content = f.read()
                
                for key_type, pattern in patterns.items():
                    matches = re.findall(pattern, content, re.IGNORECASE)
                    
                    if key_type == 'public_key':
                        all_keys['public_keys'].extend(matches)
                    elif key_type == 'session_id':
                        all_keys['session_ids'].extend(matches)
                    elif key_type == 'cert_fingerprint':
                        all_keys['certificates'].extend(matches)
        except FileNotFoundError:
            continue
    
    print("=== KEY REUSE DETECTION ===\n")
    
    for key_type, key_list in all_keys.items():
        if not key_list:
            continue
        
        key_counts = Counter(key_list)
        reused = {k: v for k, v in key_counts.items() if v > 1}
        
        print(f"{key_type}:")
        print(f"  Total: {len(key_list)}")
        print(f"  Unique: {len(key_counts)}")
        
        if reused:
            print(f"  Reused: {len(reused)}")
            print(f"  Top reused keys:")
            for key, count in sorted(reused.items(), key=lambda x: x[1], reverse=True)[:3]:
                print(f"    {key[:40]}... (used {count} times)")
        else:
            print(f"  [+] No key reuse detected")
        
        print()

detect_key_reuse(['/var/log/ssl_request.log', '/var/log/apache2/access.log'])
```

## Hash Algorithm Usage

Hash algorithms in logs appear in password authentication, file integrity checks, digital signatures, and message authentication codes (MACs).

### Password hash extraction

**Extract and identify password hashes:**

```bash
# Extract common hash formats from logs
python3 << 'EOF'
import re
from collections import Counter


def extract_password_hashes(log_file):
    """
    Identify and extract password hashes from logs
    """
    
    hash_patterns = {
        'MD5': r'\b[a-fA-F0-9]{32}\b',
        'SHA1': r'\b[a-fA-F0-9]{40}\b',
        'SHA256': r'\b[a-fA-F0-9]{64}\b',
        'SHA512': r'\b[a-fA-F0-9]{128}\b',
        'bcrypt': r'\$2[aby]\$\d{2}\$[./A-Za-z0-9]{53}',
        'NTLM': r'\b[a-fA-F0-9]{32}\b',  # Same format as MD5
        'MySQL': r'\*[A-F0-9]{40}',
        'Unix_DES': r'[a-zA-Z0-9./]{13}',
        'Unix_MD5': r'\$1\$[a-zA-Z0-9./]{8}\$[a-zA-Z0-9./]{22}',
        'Unix_SHA256': r'\$5\$[a-zA-Z0-9./]+\$[a-zA-Z0-9./]{43}',
        'Unix_SHA512': r'\$6\$[a-zA-Z0-9./]+\$[a-zA-Z0-9./]{86}',
        'Django': r'pbkdf2_sha256\$\d+\$[a-zA-Z0-9+/=]+',
        'Argon2': r'\$argon2[id]\$v=\d+\$m=\d+,t=\d+,p=\d+\$[A-Za-z0-9+/]+\$[A-Za-z0-9+/]+'
    }
    
    found_hashes = {}
    hash_contexts = []
    
    with open(log_file, 'r', errors='ignore') as f:
        for line_num, line in enumerate(f, 1):
            for hash_type, pattern in hash_patterns.items():
                matches = re.findall(pattern, line)
                
                if matches:
                    if hash_type not in found_hashes:
                        found_hashes[hash_type] = []
                    
                    for match in matches:
                        found_hashes[hash_type].append(match)
                        hash_contexts.append({
                            'line': line_num,
                            'type': hash_type,
                            'hash': match,
                            'context': line.strip()[:120]
                        })
    
    print("=== PASSWORD HASH EXTRACTION ===\n")
    
    if not found_hashes:
        print("No password hashes found")
        return
    
    # Summary by type
    print("Hash types found:")
    for hash_type, hashes in sorted(found_hashes.items()):
        unique_hashes = len(set(hashes))
        print(f"  {hash_type:15s}: {len(hashes):5d} total, {unique_hashes:5d} unique")
    
    print("\n=== EXTRACTED HASHES ===\n")
    
    # Group by type and show samples
    for hash_type in sorted(found_hashes.keys()):
        print(f"{hash_type} hashes:")
        unique_hashes = list(set(found_hashes[hash_type]))
        
        for hash_value in unique_hashes[:5]:  # Show first 5 unique
            print(f"  {hash_value}")
            
            # Show context for this hash
            contexts = [h for h in hash_contexts if h['hash'] == hash_value]
            if contexts:
                print(f"    Context: {contexts[0]['context']}")
        
        if len(unique_hashes) > 5:
            print(f"  ... and {len(unique_hashes) - 5} more unique hashes")
        print()


extract_password_hashes('/var/log/auth.log')
EOF
````

**Hash identification and cracking preparation:**
```python
import re
import hashlib

def identify_and_prepare_hashes(log_file, output_file='hashes.txt'):
    """
    Identify hash types and prepare for cracking tools (hashcat, john)
    """
    
    # Hashcat mode numbers for reference
    hashcat_modes = {
        'MD5': 0,
        'SHA1': 100,
        'SHA256': 1400,
        'SHA512': 1700,
        'bcrypt': 3200,
        'NTLM': 1000,
        'Unix_MD5': 500,
        'Unix_SHA256': 7400,
        'Unix_SHA512': 1800,
        'MySQL': 300,
        'Django': 10000,
        'Argon2': 19600
    }
    
    hash_patterns = {
        'bcrypt': r'\$2[aby]\$\d{2}\$[./A-Za-z0-9]{53}',
        'Unix_SHA512': r'\$6\$[a-zA-Z0-9./]+\$[a-zA-Z0-9./]{86}',
        'Unix_SHA256': r'\$5\$[a-zA-Z0-9./]+\$[a-zA-Z0-9./]{43}',
        'Unix_MD5': r'\$1\$[a-zA-Z0-9./]{8}\$[a-zA-Z0-9./]{22}',
        'Django': r'pbkdf2_sha256\$\d+\$[a-zA-Z0-9+/=]+',
        'Argon2': r'\$argon2[id]\$v=\d+\$m=\d+,t=\d+,p=\d+\$[A-Za-z0-9+/]+\$[A-Za-z0-9+/]+',
        'MySQL': r'\*[A-F0-9]{40}',
        'SHA512': r'\b[a-fA-F0-9]{128}\b',
        'SHA256': r'\b[a-fA-F0-9]{64}\b',
        'SHA1': r'\b[a-fA-F0-9]{40}\b',
        'MD5': r'\b[a-fA-F0-9]{32}\b'
    }
    
    extracted_hashes = {}
    
    with open(log_file, 'r', errors='ignore') as f:
        content = f.read()
        
        # Extract hashes in order of specificity (most specific first)
        for hash_type, pattern in hash_patterns.items():
            matches = re.findall(pattern, content)
            if matches:
                extracted_hashes[hash_type] = list(set(matches))
    
    print("=== HASH IDENTIFICATION & CRACKING PREP ===\n")
    
    if not extracted_hashes:
        print("No hashes found")
        return
    
    # Write to file for cracking
    with open(output_file, 'w') as f:
        for hash_type, hashes in extracted_hashes.items():
            mode = hashcat_modes.get(hash_type, 'unknown')
            
            print(f"{hash_type}:")
            print(f"  Count: {len(hashes)}")
            print(f"  Hashcat mode: {mode}")
            print(f"  Sample: {hashes[0]}")
            
            # Write to output file with labels
            f.write(f"# {hash_type} - Hashcat mode: {mode}\n")
            for hash_value in hashes:
                f.write(f"{hash_value}\n")
            f.write("\n")
            
            print(f"  Hashcat command:")
            print(f"    hashcat -m {mode} -a 0 {output_file} wordlist.txt")
            print()
    
    print(f"\nHashes written to: {output_file}")

identify_and_prepare_hashes('/var/log/auth.log', 'extracted_hashes.txt')
````

### HMAC detection and extraction

**Identify HMAC usage in logs:**

```python
import re
import hmac
import hashlib

def detect_hmac_usage(log_file):
    """
    Detect and extract HMAC values from logs
    """
    
    hmac_patterns = {
        'hmac_sha256': r'(?:hmac[_-]?sha256|signature)[=:\s]+([a-fA-F0-9]{64})',
        'hmac_sha1': r'(?:hmac[_-]?sha1)[=:\s]+([a-fA-F0-9]{40})',
        'hmac_md5': r'(?:hmac[_-]?md5)[=:\s]+([a-fA-F0-9]{32})',
        'api_signature': r'(?:X-Signature|signature|sig)[=:\s]+([a-fA-F0-9]{40,64})',
        'webhook_signature': r'(?:X-Hub-Signature|X-Webhook-Signature)[=:\s]+sha\d+=([a-fA-F0-9]{40,64})'
    }
    
    hmac_findings = []
    
    with open(log_file, 'r', errors='ignore') as f:
        for line_num, line in enumerate(f, 1):
            for hmac_type, pattern in hmac_patterns.items():
                matches = re.findall(pattern, line, re.IGNORECASE)
                
                if matches:
                    for match in matches:
                        hmac_findings.append({
                            'line': line_num,
                            'type': hmac_type,
                            'value': match,
                            'context': line.strip()
                        })
    
    print("=== HMAC DETECTION ===\n")
    
    if not hmac_findings:
        print("No HMAC values detected")
        return
    
    print(f"Found {len(hmac_findings)} HMAC signatures\n")
    
    # Group by type
    from collections import defaultdict
    by_type = defaultdict(list)
    for finding in hmac_findings:
        by_type[finding['type']].append(finding)
    
    for hmac_type, findings in by_type.items():
        print(f"{hmac_type}:")
        print(f"  Count: {len(findings)}")
        print(f"  Samples:")
        
        for finding in findings[:3]:
            print(f"    Line {finding['line']}: {finding['value']}")
            print(f"      Context: {finding['context'][:100]}")
        
        if len(findings) > 3:
            print(f"    ... and {len(findings) - 3} more")
        print()

detect_hmac_usage('/var/log/apache2/access.log')
```

**HMAC verification attempt:**

```python
import re
import hmac
import hashlib

def attempt_hmac_verification(log_file, known_keys=None):
    """
    Attempt to verify HMAC signatures if keys are available
    [Inference: Verification depends on correct key]
    """
    
    if known_keys is None:
        known_keys = [
            b'secret',
            b'key',
            b'password',
            b'admin',
            b'test',
            b'default'
        ]
    
    print("=== HMAC VERIFICATION ATTEMPT ===")
    print("[Inference: Results depend on correct secret key]\n")
    
    # Extract potential HMAC signatures and their data
    with open(log_file, 'r', errors='ignore') as f:
        for line_num, line in enumerate(f, 1):
            # Look for signature with associated data
            sig_match = re.search(r'signature[=:\s]+([a-fA-F0-9]{40,64})', line, re.IGNORECASE)
            data_match = re.search(r'(?:data|payload|message)[=:\s]+([^\s&]+)', line, re.IGNORECASE)
            
            if sig_match and data_match:
                provided_sig = sig_match.group(1)
                data = data_match.group(1)
                
                print(f"Line {line_num}:")
                print(f"  Data: {data[:60]}...")
                print(f"  Signature: {provided_sig}")
                
                # Try known keys
                for key in known_keys:
                    # Try different hash algorithms
                    for hash_func in [hashlib.sha256, hashlib.sha1]:
                        computed_sig = hmac.new(key, data.encode(), hash_func).hexdigest()
                        
                        if computed_sig == provided_sig:
                            print(f"  [!] VERIFIED with key: {key.decode()} (hash: {hash_func.__name__})")
                            break
                
                print()

attempt_hmac_verification('/var/log/api.log')
```

### File integrity hash analysis

**Extract and verify file integrity hashes:**

```python
import re
import os
import hashlib
from collections import defaultdict

def analyze_file_hashes(log_file):
    """
    Extract file hashes and attempt to identify files
    """
    
    # Pattern for common file hash log formats
    hash_patterns = [
        r'([a-fA-F0-9]{32,128})\s+([/\w\.-]+)',  # hash filename
        r'([/\w\.-]+)[:\s]+([a-fA-F0-9]{32,128})',  # filename: hash
        r'MD5\(([^)]+)\)\s*=\s*([a-fA-F0-9]{32})',  # MD5(file) = hash
        r'SHA256\(([^)]+)\)\s*=\s*([a-fA-F0-9]{64})'  # SHA256(file) = hash
    ]
    
    file_hashes = []
    
    with open(log_file, 'r', errors='ignore') as f:
        for line in f:
            for pattern in hash_patterns:
                matches = re.findall(pattern, line)
                
                for match in matches:
                    if len(match[0]) in [32, 40, 64, 128]:
                        # Hash first, filename second
                        hash_value = match[0]
                        filename = match[1]
                    else:
                        # Filename first, hash second
                        filename = match[0]
                        hash_value = match[1]
                    
                    hash_len = len(hash_value)
                    if hash_len == 32:
                        hash_type = 'MD5'
                    elif hash_len == 40:
                        hash_type = 'SHA1'
                    elif hash_len == 64:
                        hash_type = 'SHA256'
                    elif hash_len == 128:
                        hash_type = 'SHA512'
                    else:
                        hash_type = 'Unknown'
                    
                    file_hashes.append({
                        'filename': filename,
                        'hash': hash_value,
                        'type': hash_type
                    })
    
    print("=== FILE INTEGRITY HASH ANALYSIS ===\n")
    
    if not file_hashes:
        print("No file hashes found")
        return
    
    print(f"Total file hashes found: {len(file_hashes)}\n")
    
    # Group by hash type
    by_type = defaultdict(list)
    for fh in file_hashes:
        by_type[fh['type']].append(fh)
    
    for hash_type, hashes in sorted(by_type.items()):
        print(f"{hash_type} hashes: {len(hashes)}")
        
        for fh in hashes[:5]:
            print(f"  {fh['filename']}: {fh['hash']}")
            
            # Attempt to verify if file exists
            if os.path.exists(fh['filename']):
                print(f"    [+] File exists - can verify")
        
        if len(hashes) > 5:
            print(f"  ... and {len(hashes) - 5} more")
        print()

analyze_file_hashes('/var/log/audit.log')
```

**File hash verification:**

```bash
# Verify file hashes from logs
python3 << 'EOF'
import re
import hashlib
import os

def verify_file_hashes(log_file):
    """
    Verify file integrity by computing hashes
    """
    
    hash_records = []
    
    with open(log_file, 'r', errors='ignore') as f:
        for line in f:
            # Extract hash and filename
            match = re.search(r'([a-fA-F0-9]{32,128})\s+([/\w\.-]+)', line)
            if match:
                hash_value = match.group(1)
                filename = match.group(2)
                
                # Determine hash type by length
                hash_len = len(hash_value)
                if hash_len == 32:
                    hash_func = hashlib.md5
                elif hash_len == 40:
                    hash_func = hashlib.sha1
                elif hash_len == 64:
                    hash_func = hashlib.sha256
                elif hash_len == 128:
                    hash_func = hashlib.sha512
                else:
                    continue
                
                hash_records.append({
                    'file': filename,
                    'expected_hash': hash_value.lower(),
                    'hash_func': hash_func
                })
    
    print("=== FILE HASH VERIFICATION ===\n")
    
    if not hash_records:
        print("No file hashes to verify")
        return
    
    verified = 0
    failed = 0
    missing = 0
    
    for record in hash_records:
        filename = record['file']
        expected = record['expected_hash']
        hash_func = record['hash_func']
        
        if not os.path.exists(filename):
            print(f"[!] MISSING: {filename}")
            missing += 1
            continue
        
        # Compute actual hash
        try:
            with open(filename, 'rb') as f:
                computed = hash_func(f.read()).hexdigest()
            
            if computed == expected:
                print(f"[+] VERIFIED: {filename}")
                verified += 1
            else:
                print(f"[!] MISMATCH: {filename}")
                print(f"    Expected: {expected}")
                print(f"    Computed: {computed}")
                failed += 1
        except Exception as e:
            print(f"[!] ERROR: {filename} - {e}")
            failed += 1
    
    print(f"\nSummary:")
    print(f"  Verified: {verified}")
    print(f"  Failed: {failed}")
    print(f"  Missing: {missing}")

verify_file_hashes('integrity.log')
EOF
```

### Hash collision detection

**Detect potential hash collisions:**

```python
import re
from collections import defaultdict

def detect_hash_collisions(log_files):
    """
    Detect if different data produces same hash (collision)
    """
    
    # Store hash -> data mappings
    hash_to_data = defaultdict(set)
    
    for log_file in log_files:
        try:
            with open(log_file, 'r', errors='ignore') as f:
                for line in f:
                    # Look for patterns like: data=X hash=Y
                    data_match = re.search(r'(?:data|input|message)[=:\s]+([^\s&]+)', line)
                    hash_match = re.search(r'(?:hash|digest|checksum)[=:\s]+([a-fA-F0-9]{32,64})', line)
                    
                    if data_match and hash_match:
                        data = data_match.group(1)
                        hash_value = hash_match.group(1).lower()
                        hash_to_data[hash_value].add(data)
        except FileNotFoundError:
            continue
    
    print("=== HASH COLLISION DETECTION ===\n")
    
    collisions = {h: d for h, d in hash_to_data.items() if len(d) > 1}
    
    if not collisions:
        print("No hash collisions detected")
        return
    
    print(f"[!] Found {len(collisions)} potential collisions\n")
    
    for hash_value, data_values in collisions.items():
        hash_len = len(hash_value)
        if hash_len == 32:
            hash_type = 'MD5'
        elif hash_len == 40:
            hash_type = 'SHA1'
        elif hash_len == 64:
            hash_type = 'SHA256'
        else:
            hash_type = 'Unknown'
        
        print(f"Hash ({hash_type}): {hash_value}")
        print(f"  Multiple data values produce this hash:")
        for data in data_values:
            print(f"    - {data}")
        print()

detect_hash_collisions(['/var/log/app.log', '/var/log/api.log'])
```

### Message authentication code (MAC) analysis

**Extract and analyze MAC values:**

```python
import re

def analyze_mac_usage(log_file):
    """
    Analyze Message Authentication Code usage in logs
    """
    
    mac_patterns = {
        'CMAC': r'(?:cmac|aes-cmac)[=:\s]+([a-fA-F0-9]{32})',
        'GMAC': r'(?:gmac|aes-gmac)[=:\s]+([a-fA-F0-9]{32})',
        'Poly1305': r'(?:poly1305)[=:\s]+([a-fA-F0-9]{32})',
        'CBC-MAC': r'(?:cbc-mac)[=:\s]+([a-fA-F0-9]{32})',
        'Generic_MAC': r'(?:mac|tag)[=:\s]+([a-fA-F0-9]{16,64})'
    }
    
    mac_findings = []
    
    with open(log_file, 'r', errors='ignore') as f:
        for line_num, line in enumerate(f, 1):
            for mac_type, pattern in mac_patterns.items():
                matches = re.findall(pattern, line, re.IGNORECASE)
                
                if matches:
                    for match in matches:
                        mac_findings.append({
                            'line': line_num,
                            'type': mac_type,
                            'value': match,
                            'context': line.strip()[:100]
                        })
    
    print("=== MESSAGE AUTHENTICATION CODE ANALYSIS ===\n")
    
    if not mac_findings:
        print("No MAC values detected")
        return
    
    print(f"Total MAC values found: {len(mac_findings)}\n")
    
    from collections import Counter
    mac_types = Counter(f['type'] for f in mac_findings)
    
    print("MAC types:")
    for mac_type, count in mac_types.most_common():
        print(f"  {mac_type:15s}: {count:5d}")
    
    print("\nSample MACs:\n")
    for mac_type in sorted(set(f['type'] for f in mac_findings)):
        samples = [f for f in mac_findings if f['type'] == mac_type][:2]
        
        print(f"{mac_type}:")
        for sample in samples:
            print(f"  Line {sample['line']}: {sample['value']}")
            print(f"    {sample['context']}")
        print()

analyze_mac_usage('/var/log/crypto.log')
```

### Cryptographic nonce/IV extraction

**Extract initialization vectors and nonces:**

```python
import re

def extract_ivs_and_nonces(log_file):
    """
    Extract initialization vectors (IV) and nonces from logs
    """
    
    iv_nonce_patterns = {
        'IV': r'(?:iv|initialization.vector)[=:\s]+([a-fA-F0-9]{16,32})',
        'Nonce': r'(?:nonce|number.once)[=:\s]+([a-fA-F0-9]{16,32})',
        'Salt': r'(?:salt)[=:\s]+([a-fA-F0-9]{16,64})',
        'Counter': r'(?:counter|ctr)[=:\s]+([a-fA-F0-9]{16,32})'
    }
    
    findings = []
    
    with open(log_file, 'r', errors='ignore') as f:
        for line_num, line in enumerate(f, 1):
            for param_type, pattern in iv_nonce_patterns.items():
                matches = re.findall(pattern, line, re.IGNORECASE)
                
                if matches:
                    for match in matches:
                        findings.append({
                            'line': line_num,
                            'type': param_type,
                            'value': match,
                            'context': line.strip()
                        })
    
    print("=== IV/NONCE EXTRACTION ===\n")
    
    if not findings:
        print("No IVs or nonces found")
        return
    
    print(f"Total findings: {len(findings)}\n")
    
    # Check for IV/nonce reuse (security issue)
    from collections import Counter
    values_by_type = {}
    
    for finding in findings:
        if finding['type'] not in values_by_type:
            values_by_type[finding['type']] = []
        values_by_type[finding['type']].append(finding['value'])
    
    for param_type, values in values_by_type.items():
        value_counts = Counter(values)
        reused = {v: c for v, c in value_counts.items() if c > 1}
        
        print(f"{param_type}:")
        print(f"  Total: {len(values)}")
        print(f"  Unique: {len(value_counts)}")
        
        if reused:
            print(f"  [!] REUSE DETECTED (security issue):")
            for value, count in sorted(reused.items(), key=lambda x: x[1], reverse=True)[:3]:
                print(f"    {value} (used {count} times)")
        else:
            print(f"  [+] No reuse detected")
        
        print()

extract_ivs_and_nonces('/var/log/encryption.log')
```

---

**Important related topics:**

- Certificate transparency log analysis
- PKI (Public Key Infrastructure) chain validation
- Quantum-resistant algorithm detection
- Side-channel attack indicators in timing logs
- Cryptographic protocol version negotiation vulnerabilities

---

# Malware Activity Indicators

Identifying malware activity in log files is critical for CTF challenges involving compromised systems, incident response scenarios, and forensic analysis. Effective detection requires understanding behavioral patterns, network signatures, and system-level artifacts.

## C2 communication patterns

Command and Control (C2) communication represents the channel through which malware receives instructions and exfiltrates data. Detecting C2 patterns in logs is often the primary indicator of active compromise.

### Network-level C2 indicators

**Beaconing behavior:**

C2 malware typically communicates on regular intervals (beacons) to check for commands. This creates distinctive patterns in network logs.

**Detecting regular intervals in connection logs:**

```python
import csv
from datetime import datetime
from collections import defaultdict
import statistics

def detect_beaconing(logfile, threshold_variance=10):
    """
    Detect beaconing by analyzing time intervals between connections.
    Low variance in intervals suggests automated periodic communication.
    
    Args:
        logfile: Path to connection log CSV
        threshold_variance: Maximum acceptable variance (seconds) for beaconing
    """
    # Group connections by destination IP
    connections = defaultdict(list)
    
    with open(logfile, 'r') as f:
        reader = csv.DictReader(f)
        for row in reader:
            dest_ip = row['destination_ip']
            timestamp = datetime.fromisoformat(row['timestamp'].replace('Z', '+00:00'))
            connections[dest_ip].append(timestamp)
    
    # Analyze intervals for each destination
    beaconing_hosts = []
    
    for dest_ip, timestamps in connections.items():
        if len(timestamps) < 10:  # Need sufficient samples
            continue
        
        timestamps.sort()
        intervals = []
        
        for i in range(1, len(timestamps)):
            interval = (timestamps[i] - timestamps[i-1]).total_seconds()
            intervals.append(interval)
        
        if len(intervals) < 5:
            continue
        
        mean_interval = statistics.mean(intervals)
        variance = statistics.variance(intervals) if len(intervals) > 1 else 0
        
        # Low variance indicates consistent beaconing
        if variance < threshold_variance and mean_interval > 0:
            beaconing_hosts.append({
                'dest_ip': dest_ip,
                'connection_count': len(timestamps),
                'mean_interval': mean_interval,
                'variance': variance,
                'first_seen': timestamps[0],
                'last_seen': timestamps[-1]
            })
    
    return sorted(beaconing_hosts, key=lambda x: x['variance'])

# Usage
beacons = detect_beaconing('network_connections.csv')
for beacon in beacons:
    print(f"Potential C2: {beacon['dest_ip']}")
    print(f"  Interval: {beacon['mean_interval']:.2f}s (variance: {beacon['variance']:.2f})")
    print(f"  Connections: {beacon['connection_count']}")
    print(f"  Duration: {beacon['first_seen']} to {beacon['last_seen']}")
```

**Visualizing beaconing patterns:**

```python
import matplotlib.pyplot as plt
from datetime import datetime

def visualize_beaconing(logfile, suspicious_ips):
    """Plot connection timestamps to visualize beaconing."""
    connections = {ip: [] for ip in suspicious_ips}
    
    with open(logfile, 'r') as f:
        reader = csv.DictReader(f)
        for row in reader:
            dest_ip = row['destination_ip']
            if dest_ip in suspicious_ips:
                timestamp = datetime.fromisoformat(row['timestamp'].replace('Z', '+00:00'))
                connections[dest_ip].append(timestamp)
    
    fig, ax = plt.subplots(figsize=(14, 6))
    
    for i, (ip, timestamps) in enumerate(connections.items()):
        timestamps.sort()
        y_values = [i] * len(timestamps)
        ax.scatter(timestamps, y_values, alpha=0.6, s=50, label=ip)
    
    ax.set_yticks(range(len(suspicious_ips)))
    ax.set_yticklabels(suspicious_ips)
    ax.set_xlabel('Time')
    ax.set_title('C2 Beaconing Pattern Visualization')
    ax.legend()
    plt.tight_layout()
    plt.savefig('beaconing_pattern.png', dpi=150)
```

**Common beaconing intervals observed in malware** [Inference based on documented malware families]:

- **Fast beacons**: 1-10 seconds (aggressive malware, DDoS bots)
- **Medium beacons**: 30-300 seconds (common C2, balanced stealth/responsiveness)
- **Slow beacons**: 5-60 minutes (stealthy APT malware)
- **Variable beacons**: Random jitter added to intervals (evasion technique)

**Detecting beaconing with jitter:**

```python
def detect_jittered_beaconing(intervals, jitter_tolerance=0.3):
    """
    Detect beaconing even when jitter is applied.
    
    Args:
        intervals: List of time intervals between connections (seconds)
        jitter_tolerance: Acceptable coefficient of variation (std/mean)
    """
    if len(intervals) < 10:
        return False
    
    mean_interval = statistics.mean(intervals)
    std_interval = statistics.stdev(intervals)
    
    # Coefficient of variation (CV)
    cv = std_interval / mean_interval if mean_interval > 0 else float('inf')
    
    # Beaconing with jitter typically has CV < 0.3
    # Pure random traffic has CV > 1.0
    return cv < jitter_tolerance

# Example
intervals = [58, 63, 61, 59, 62, 60, 64, 58, 61, 63]  # ~60s with jitter
is_beacon = detect_jittered_beaconing(intervals)
print(f"Jittered beaconing detected: {is_beacon}")
```

### HTTP/HTTPS C2 patterns

**User-Agent analysis:**

Malware often uses distinctive or outdated User-Agent strings.

```bash
# Extract unique user agents from web logs
cat access.log | awk -F'"' '{print $6}' | sort -u > user_agents.txt

# Find suspicious patterns
grep -iE "(python|curl|wget|powershell|winhttp)" user_agents.txt

# Find very old browser versions
grep -E "MSIE [0-8]\.|Firefox/[0-3][0-9]\." user_agents.txt

# Empty or minimal user agents
awk -F'"' 'length($6) < 10 {print $0}' access.log
```

**Common malicious User-Agent patterns:**

```
Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)  # Very outdated
python-requests/2.25.1                               # Scripting library
curl/7.64.1                                          # Command-line tool
Mozilla/5.0 (Windows NT 6.1; rv:52.0) Gecko/20100101 # Generic template
''                                                   # Empty UA
```

**Detecting C2 User-Agents with Python:**

```python
import re
from collections import Counter

suspicious_ua_patterns = [
    r'python-requests',
    r'curl/',
    r'wget/',
    r'powershell',
    r'winhttp',
    r'MSIE [0-8]\.',  # Old IE versions
    r'Go-http-client',
    r'^Mozilla/[45]\.0$',  # Generic without specifics
]

def analyze_user_agents(logfile):
    """Identify suspicious User-Agent strings."""
    ua_counts = Counter()
    suspicious_entries = []
    
    with open(logfile, 'r') as f:
        for line in f:
            # Extract UA from Apache combined log format
            match = re.search(r'"([^"]*)" "([^"]*)"$', line)
            if match:
                referer = match.group(1)
                user_agent = match.group(2)
                ua_counts[user_agent] += 1
                
                # Check against suspicious patterns
                for pattern in suspicious_ua_patterns:
                    if re.search(pattern, user_agent, re.IGNORECASE):
                        suspicious_entries.append({
                            'user_agent': user_agent,
                            'line': line.strip()
                        })
                        break
    
    return ua_counts, suspicious_entries

ua_counts, suspicious = analyze_user_agents('access.log')

print("Top 10 User Agents:")
for ua, count in ua_counts.most_common(10):
    print(f"{count:6d}: {ua}")

print(f"\nSuspicious User Agents: {len(suspicious)}")
for entry in suspicious[:10]:
    print(entry['user_agent'])
```

**URI patterns in C2 communication:**

C2 servers often use distinctive URI patterns for different command channels.

```python
import re
from collections import defaultdict

def analyze_c2_uri_patterns(logfile):
    """Detect C2-like URI patterns."""
    
    # Common C2 URI characteristics
    patterns = {
        'base64_param': re.compile(r'[?&][a-zA-Z]+=([A-Za-z0-9+/]{20,}={0,2})'),
        'hex_param': re.compile(r'[?&][a-zA-Z]+=([0-9a-fA-F]{32,})'),
        'random_path': re.compile(r'/[a-zA-Z0-9]{32,}'),
        'suspicious_endpoints': re.compile(r'/(api|gate|panel|admin|config|update|check)\.(php|asp|jsp)'),
        'single_char_param': re.compile(r'[?&][a-z]=[^&]+')
    }
    
    findings = defaultdict(list)
    
    with open(logfile, 'r') as f:
        reader = csv.DictReader(f)
        for row in reader:
            uri = row['request_uri']
            
            for pattern_name, pattern in patterns.items():
                if pattern.search(uri):
                    findings[pattern_name].append({
                        'timestamp': row['timestamp'],
                        'ip': row['remote_addr'],
                        'uri': uri
                    })
    
    return findings

# Usage
c2_patterns = analyze_c2_uri_patterns('web_access.csv')

for pattern_type, matches in c2_patterns.items():
    print(f"\n{pattern_type}: {len(matches)} occurrences")
    for match in matches[:3]:
        print(f"  {match['timestamp']}: {match['uri']}")
```

**POST request body analysis:**

C2 communication often involves POST requests with encoded payloads.

```bash
# Extract POST requests from Apache logs
awk '$6 == "\"POST"' access.log > post_requests.log

# Find POST to suspicious endpoints
grep -E "POST.*(\/api|\/gate|\/panel)" access.log

# Large POST bodies (potential exfiltration)
awk '$10 > 10000 && $6 == "\"POST"' access.log
```

**HTTP header anomalies:**

```python
def analyze_http_headers(logfile):
    """Identify anomalous HTTP headers that may indicate C2."""
    
    header_anomalies = []
    
    with open(logfile, 'r') as f:
        for line in f:
            # Check for missing or unusual Referer
            if 'POST' in line and '"-"' in line.split('"')[3]:  # Empty referer
                header_anomalies.append(('empty_referer', line.strip()))
            
            # Check for suspicious Accept headers
            if 'Accept: */*' in line or 'Accept: text/html' in line:
                header_anomalies.append(('generic_accept', line.strip()))
    
    return header_anomalies
```

### DNS C2 patterns

DNS is commonly used for C2 due to its typically unrestricted network access.

**DNS tunneling detection:**

```python
def detect_dns_tunneling(dns_logfile):
    """
    Detect DNS tunneling by analyzing query characteristics.
    
    Indicators:
    - Long subdomain names (>40 chars)
    - High entropy in subdomain
    - Multiple queries to same domain
    - TXT record queries
    """
    import math
    from collections import Counter
    
    def calculate_entropy(string):
        """Calculate Shannon entropy of string."""
        if not string:
            return 0
        
        counts = Counter(string)
        length = len(string)
        
        entropy = -sum((count/length) * math.log2(count/length) 
                       for count in counts.values())
        return entropy
    
    suspicious_domains = []
    
    with open(dns_logfile, 'r') as f:
        reader = csv.DictReader(f)
        for row in reader:
            query = row['query_name']
            query_type = row['query_type']
            
            # Extract subdomain (everything before last two labels)
            parts = query.split('.')
            if len(parts) > 2:
                subdomain = '.'.join(parts[:-2])
                
                # Long subdomain (potential data encoding)
                if len(subdomain) > 40:
                    entropy = calculate_entropy(subdomain)
                    
                    suspicious_domains.append({
                        'timestamp': row['timestamp'],
                        'query': query,
                        'type': query_type,
                        'subdomain_length': len(subdomain),
                        'entropy': entropy,
                        'reason': 'long_subdomain'
                    })
                
                # High entropy subdomain
                elif calculate_entropy(subdomain) > 4.0:
                    suspicious_domains.append({
                        'timestamp': row['timestamp'],
                        'query': query,
                        'type': query_type,
                        'entropy': calculate_entropy(subdomain),
                        'reason': 'high_entropy'
                    })
            
            # TXT queries often used for C2
            if query_type == 'TXT':
                suspicious_domains.append({
                    'timestamp': row['timestamp'],
                    'query': query,
                    'type': query_type,
                    'reason': 'txt_query'
                })
    
    return suspicious_domains

# Usage
dns_c2 = detect_dns_tunneling('dns_queries.csv')
for finding in dns_c2[:10]:
    print(f"{finding['timestamp']}: {finding['query']} - {finding['reason']}")
    if 'entropy' in finding:
        print(f"  Entropy: {finding['entropy']:.2f}")
```

**DNS query frequency analysis:**

```bash
# Count queries per domain
cat dns.log | awk '{print $6}' | sort | uniq -c | sort -rn | head -20

# Domains queried only once (reconnaissance)
cat dns.log | awk '{print $6}' | sort | uniq -c | awk '$1 == 1' | wc -l

# Rapid successive queries to same domain (beaconing)
awk '{print $1, $6}' dns.log | uniq -f1 -c | awk '$1 > 10'
```

**Base32/Base64 encoded subdomains:**

```python
import base64
import re

def check_encoded_dns(query):
    """Check if DNS query contains base32/base64 encoded data."""
    # Extract subdomain
    parts = query.split('.')
    if len(parts) < 3:
        return False
    
    subdomain = parts[0]
    
    # Base64 pattern (alphanumeric + /+)
    if re.match(r'^[A-Za-z0-9+/]{20,}={0,2}$', subdomain):
        try:
            decoded = base64.b64decode(subdomain)
            return True, 'base64', decoded
        except:
            pass
    
    # Base32 pattern (A-Z2-7)
    if re.match(r'^[A-Z2-7]{20,}={0,6}$', subdomain, re.IGNORECASE):
        try:
            decoded = base64.b32decode(subdomain.upper())
            return True, 'base32', decoded
        except:
            pass
    
    return False, None, None

# Example
query = "aGVsbG8gd29ybGQgZGF0YQ.example.com"
is_encoded, encoding_type, decoded = check_encoded_dns(query)
if is_encoded:
    print(f"Encoded with {encoding_type}: {decoded}")
```

### Protocol-specific C2 indicators

**SSL/TLS certificate anomalies:**

```bash
# Extract SSL certificate info from logs (if available)
# Self-signed certificates
grep "self signed" ssl.log

# Certificate CN mismatches
awk -F',' '$3 != $4 {print $0}' ssl_connections.csv

# Suspicious certificate subjects
grep -iE "(localhost|example\.com|test|default)" ssl.log
```

**ICMP tunneling detection:**

```python
def detect_icmp_tunneling(pcap_file):
    """
    Detect ICMP tunneling by analyzing payload characteristics.
    Requires scapy.
    """
    from scapy.all import rdpcap, ICMP
    from collections import defaultdict
    
    packets = rdpcap(pcap_file)
    
    icmp_stats = defaultdict(lambda: {'count': 0, 'payload_sizes': []})
    
    for pkt in packets:
        if ICMP in pkt:
            src = pkt['IP'].src
            payload_size = len(bytes(pkt[ICMP].payload))
            
            icmp_stats[src]['count'] += 1
            icmp_stats[src]['payload_sizes'].append(payload_size)
    
    # Identify suspicious patterns
    suspicious = []
    for src, stats in icmp_stats.items():
        # High volume of ICMP from single source
        if stats['count'] > 100:
            # Varying payload sizes (data transmission)
            unique_sizes = len(set(stats['payload_sizes']))
            if unique_sizes > 10:
                suspicious.append({
                    'source': src,
                    'packet_count': stats['count'],
                    'unique_payload_sizes': unique_sizes,
                    'reason': 'icmp_tunneling'
                })
    
    return suspicious
```

### C2 frameworks signatures

**Common C2 framework indicators** [Inference based on documented framework characteristics]:

**Cobalt Strike:**

- Default beacon over HTTP/HTTPS
- User-Agent: Often spoofs legitimate browsers
- URI patterns: `/dpixel`, `/pixel.gif`, `/__utm.gif`
- POST to `/submit.php`
- Malleable C2 profiles allow customization

**Metasploit:**

- Default URI: `/INITJM`, `/INITM`
- Specific User-Agents depending on payload
- Meterpreter reverse HTTPS uses certificate

**Empire/PowerShell Empire:**

- Default staging URI: `/index.asp`, `/index.jsp`
- PowerShell-based, may see `powershell.exe` in process logs
- Base64-encoded PowerShell commands

**Detecting Cobalt Strike beacons:**

```bash
# Look for common Cobalt Strike URIs
grep -E "(/__utm\.gif|/pixel\.gif|/dpixel|/submit\.php)" access.log

# Check for specific patterns in SSL certificates
grep -i "cobaltstrike" ssl_cert.log

# Analyze response sizes (CS returns specific sizes)
awk '$10 == 0 || $10 == 48 {print $0}' access.log
```

**Detecting Metasploit sessions:**

```bash
# Default Metasploit URIs
grep -E "(/INITJM|/INITM)" access.log

# Meterpreter reverse HTTPS patterns
grep -E "METERPRETER|msf" process_logs.txt
```

### Data exfiltration patterns

**Large outbound transfers:**

```python
def detect_exfiltration(netflow_log):
    """Identify potential data exfiltration by analyzing transfer sizes."""
    
    threshold_bytes = 10 * 1024 * 1024  # 10 MB
    
    exfil_candidates = []
    
    with open(netflow_log, 'r') as f:
        reader = csv.DictReader(f)
        for row in reader:
            bytes_out = int(row['bytes_sent'])
            bytes_in = int(row['bytes_received'])
            
            # High egress, low ingress (upload)
            if bytes_out > threshold_bytes and bytes_out > bytes_in * 10:
                exfil_candidates.append({
                    'timestamp': row['timestamp'],
                    'source_ip': row['source_ip'],
                    'dest_ip': row['destination_ip'],
                    'dest_port': row['destination_port'],
                    'bytes_sent': bytes_out,
                    'ratio': bytes_out / bytes_in if bytes_in > 0 else float('inf')
                })
    
    return sorted(exfil_candidates, key=lambda x: x['bytes_sent'], reverse=True)

# Usage
exfiltration = detect_exfiltration('netflow.csv')
for event in exfiltration[:5]:
    print(f"{event['timestamp']}: {event['source_ip']} -> {event['dest_ip']}:{event['dest_port']}")
    print(f"  Sent: {event['bytes_sent']:,} bytes (ratio: {event['ratio']:.1f}:1)")
```

**Unusual upload times:**

```python
from datetime import datetime

def detect_off_hours_transfers(logfile, business_hours=(9, 17)):
    """Detect large transfers outside business hours."""
    
    suspicious = []
    
    with open(logfile, 'r') as f:
        reader = csv.DictReader(f)
        for row in reader:
            timestamp = datetime.fromisoformat(row['timestamp'].replace('Z', '+00:00'))
            hour = timestamp.hour
            
            # Outside business hours
            if hour < business_hours[0] or hour >= business_hours[1]:
                bytes_sent = int(row['bytes_sent'])
                if bytes_sent > 1024 * 1024:  # > 1 MB
                    suspicious.append({
                        'timestamp': timestamp,
                        'hour': hour,
                        'dest_ip': row['destination_ip'],
                        'bytes': bytes_sent
                    })
    
    return suspicious
```

## Backdoor connections

Backdoors provide persistent unauthorized access to compromised systems. Detection focuses on unusual network connections, listening ports, and access patterns.

### Network backdoor indicators

**Unusual listening ports:**

```bash
# From netstat logs - identify uncommon listening ports
awk '$6 == "LISTEN" {print $4}' netstat.log | sed 's/.*://' | sort -n | uniq -c

# Common backdoor ports (examples, not exhaustive):
# 31337, 12345, 1337, 6666, 6667 (IRC-based)
# 4444, 5555 (Metasploit defaults)
# 8080, 8443, 443, 80 (hiding in plain sight)

# Find non-standard high ports
awk '$6 == "LISTEN" {port = $4; sub(/.*:/, "", port); if (port > 10000 && port < 65535) print port}' netstat.log | sort | uniq -c
```

**Detecting backdoor listeners:**

```python
def analyze_listening_ports(netstat_log):
    """Identify suspicious listening ports."""
    
    # Common legitimate ports
    legitimate_ports = {
        20, 21,    # FTP
        22,        # SSH
        23,        # Telnet
        25,        # SMTP
        53,        # DNS
        80, 443,   # HTTP/HTTPS
        110, 143,  # POP3, IMAP
        3306,      # MySQL
        5432,      # PostgreSQL
        6379,      # Redis
        27017,     # MongoDB
    }
    
    # Known backdoor ports
    backdoor_ports = {
        31337, 12345, 1337, 6666, 6667, 6668, 6669,
        4444, 5555, 7777, 8888, 9999,
        2222, 3333, 5000, 8000
    }
    
    suspicious_listeners = []
    
    with open(netstat_log, 'r') as f:
        for line in f:
            if 'LISTEN' in line:
                # Parse port from address (format: 0.0.0.0:PORT or :::PORT)
                match = re.search(r':(\d+)\s', line)
                if match:
                    port = int(match.group(1))
                    
                    # Check against known backdoor ports
                    if port in backdoor_ports:
                        suspicious_listeners.append({
                            'port': port,
                            'line': line.strip(),
                            'reason': 'known_backdoor_port'
                        })
                    # Non-standard port
                    elif port not in legitimate_ports and port > 1024:
                        suspicious_listeners.append({
                            'port': port,
                            'line': line.strip(),
                            'reason': 'non_standard_port'
                        })
    
    return suspicious_listeners

# Usage
backdoors = analyze_listening_ports('netstat.txt')
for backdoor in backdoors:
    print(f"Port {backdoor['port']}: {backdoor['reason']}")
    print(f"  {backdoor['line']}")
```

**Reverse shell detection:**

Reverse shells connect outbound from victim to attacker, bypassing firewall restrictions.

```python
def detect_reverse_shells(conn_log):
    """
    Detect potential reverse shells by analyzing:
    - Outbound connections to unusual ports
    - Shell process network activity
    - Interactive session patterns
    """
    
    shell_processes = ['bash', 'sh', 'cmd.exe', 'powershell.exe', 'python', 'perl', 'nc', 'ncat']
    suspicious_dest_ports = [4444, 4445, 5555, 7777, 8888, 9999]
    
    reverse_shells = []
    
    with open(conn_log, 'r') as f:
        reader = csv.DictReader(f)
        for row in reader:
            process = row.get('process_name', '').lower()
            dest_port = int(row['destination_port'])
            direction = row.get('direction', '')
            
            # Shell process making outbound connection
            if any(shell in process for shell in shell_processes) and direction == 'outbound':
                reverse_shells.append({
                    'timestamp': row['timestamp'],
                    'process': row['process_name'],
                    'dest_ip': row['destination_ip'],
                    'dest_port': dest_port,
                    'reason': 'shell_outbound_connection'
                })
            
            # Connection to common reverse shell ports
            elif dest_port in suspicious_dest_ports and direction == 'outbound':
                reverse_shells.append({
                    'timestamp': row['timestamp'],
                    'process': row.get('process_name', 'unknown'),
                    'dest_ip': row['destination_ip'],
                    'dest_port': dest_port,
                    'reason': 'suspicious_port'
                })
    
    return reverse_shells
```

### Web backdoors

**PHP web shells:**

Common indicators in web server logs:

```bash
# Look for suspicious PHP files
grep -E "\.php\.(jpg|png|gif|txt)" access.log

# Common web shell names
grep -iE "(c99|r57|b374k|shell|cmd|webshell|backdoor|bypass|exploit|phpshe11)" access.log

# Suspicious PHP function usage in URLs
grep -iE "(eval|base64_decode|system|exec|passthru|shell_exec|assert|preg_replace.*\/e)" access.log

# File upload followed by access to uploaded file
awk '/POST.*upload/ {upload=$0; getline; if (/GET.*\.php/) print upload"\n"$0}' access.log
```

**Web shell detection script:**

```python
import re
from collections import defaultdict

def detect_webshells(access_log):
    """Detect web shell activity in access logs."""
    
    webshell_indicators = {
        'suspicious_names': re.compile(r'(c99|r57|b374k|wso|shell|cmd|backdoor|phpspy|mysql|404|403)', re.I),
        'suspicious_params': re.compile(r'(\?cmd=|\?exec=|\?command=|&e=|&c=)', re.I),
        'eval_usage': re.compile(r'(eval\(|base64_decode\(|system\(|exec\()', re.I),
        'encoded_payload': re.compile(r'(%27|%20|%28|%29){10,}'),  # Heavy URL encoding
    }
    
    detections = defaultdict(list)
    
    with open(access_log, 'r') as f:
        for line in f:
            # Extract URI
            match = re.search(r'"(?:GET|POST)\s+([^\s]+)\s+HTTP', line)
            if not match:
                continue
            
            uri = match.group(1)
            
            for indicator_type, pattern in webshell_indicators.items():
                if pattern.search(uri):
                    # Extract timestamp and IP
                    ip_match = re.match(r'^(\S+)', line)
                    time_match = re.search(r'\[([^\]]+)\]', line)
                    
                    detections[indicator_type].append({
                        'ip': ip_match.group(1) if ip_match else 'unknown',
                        'timestamp': time_match.group(1) if time_match else 'unknown',
                        'uri': uri,
                        'full_line': line.strip()
                    })
    
    return detections

# Usage
webshells = detect_webshells('access.log')
for indicator, findings in webshells.items():
    print(f"\n{indicator}: {len(findings)} detections")
    for finding in findings[:3]:
        print(f"  {finding['timestamp']}: {finding['ip']} -> {finding['uri'][:100]}")
```

**POST request analysis for web shells:**

```python
def analyze_webshell_posts(access_log):
    """Analyze POST requests that may indicate web shell usage."""
    
    suspicious_posts = []
    
    with open(access_log, 'r') as f:
        for line in f:
            if 'POST' not in line:
                continue
            
            # Extract relevant fields
            match = re.match(r'^(\S+).*?\[([^\]]+)\].*?"POST\s+([^\s]+).*?"\s+(\d+)\s+(\d+)', line)
            if not match:
                continue
            
            ip, timestamp, uri, status, size = match.groups()
            status = int(status)
            size = int(size)
            
            # Suspicious patterns
            reasons = []
            
            # POST to PHP files (not typical forms)
            if uri.endswith('.php') and 'admin' not in uri and 'login' not in uri:
                reasons.append('post_to_php')
            
            # Small response after POST (command execution returns minimal output)
            if status == 200 and size < 100:
                reasons.append('minimal_response')
            
            # POST to image files (disguised web shell)
            if re.search(r'\.(jpg|jpeg|png|gif)\.php', uri):
                reasons.append('image_php_combo')
            
            if reasons:
                suspicious_posts.append({
                    'timestamp': timestamp,
                    'ip': ip,
                    'uri': uri,
                    'status': status,
                    'size': size,
                    'reasons': reasons
                })
    
    return suspicious_posts

# Usage
webshell_posts = analyze_webshell_posts('access.log')
for post in webshell_posts[:10]:
    print(f"{post['timestamp']}: {post['ip']} -> {post['uri']}")
    print(f"  Status: {post['status']}, Size: {post['size']}, Reasons: {', '.join(post['reasons'])}")
```

**Common web shell command patterns:**

```bash
# Commands commonly executed through web shells
grep -iE "(whoami|id|uname|cat\s+/etc/passwd|netstat|ifconfig|wget|curl|nc\s+-)" access.log

# Directory traversal attempts
grep -E "(\.\./){3,}" access.log

# Suspicious file access
grep -E "(passwd|shadow|hosts|sudoers|\.ssh|\.bash_history)" access.log
```

### SSH backdoor indicators

**Unauthorized SSH keys:**

```bash
# From auth.log - accepted public key authentications
grep "Accepted publickey" auth.log

# New key additions (if logged)
grep "Adding.*authorized_keys" syslog

# SSH from unexpected locations
awk '/Accepted publickey/ {print $1, $2, $3, $9, $11}' auth.log | sort -u
```

**SSH tunneling detection:**

```python
def detect_ssh_tunnels(auth_log):
    """Detect SSH tunneling which may indicate backdoor usage."""
    
    tunnel_indicators = []
    
    with open(auth_log, 'r') as f:
        for line in f:
            # Port forwarding
            if 'forwarding' in line.lower():
                tunnel_indicators.append({
                    'type': 'port_forwarding',
                    'line': line.strip()
                })
            
            # Dynamic forwarding (SOCKS proxy)
            if 'dynamic forwarding' in line.lower():
                tunnel_indicators.append({
                    'type': 'dynamic_forwarding',
                    'line': line.strip()
                })
            
            # Remote port forwarding
            if 'remote port forwarding' in line.lower():
                tunnel_indicators.append({
                    'type': 'remote_forwarding',
                    'line': line.strip()
                })
            
            # X11 forwarding (can be abused)
            if 'X11 forwarding' in line.lower():
                tunnel_indicators.append({
                    'type': 'x11_forwarding',
                    'line': line.strip()
                })
    
    return tunnel_indicators
```

**Detecting modified SSH configuration:**

```bash
# Check for suspicious SSH config changes (from system logs)
grep -i "sshd_config" syslog

# Look for permitrootlogin changes
grep -i "permitrootlogin" syslog

# Password authentication enabled (when it should be key-only)
grep -i "passwordauthentication" syslog

# Suspicious allowed users
grep -i "allowusers\|allowgroups" syslog
```

### Process-based backdoor detection

**Unusual parent-child process relationships:**

```python
def detect_suspicious_process_trees(process_log):
    """
    Identify suspicious parent-child process relationships.
    Common backdoor pattern: web server spawning shell
    """
    
    suspicious_relationships = {
        'apache2': ['bash', 'sh', 'nc', 'ncat', 'python', 'perl'],
        'nginx': ['bash', 'sh', 'nc', 'ncat', 'python', 'perl'],
        'httpd': ['bash', 'sh', 'nc', 'ncat', 'python', 'perl'],
        'php-fpm': ['bash', 'sh', 'nc', 'ncat'],
        'www-data': ['bash', 'sh', 'nc', 'ncat', 'python', 'perl'],
        'tomcat': ['bash', 'sh', 'nc', 'cmd.exe'],
        'iis': ['cmd.exe', 'powershell.exe', 'wscript.exe']
    }
    
    findings = []
    
    with open(process_log, 'r') as f:
        reader = csv.DictReader(f)
        for row in reader:
            parent_process = row.get('parent_process', '').lower()
            child_process = row.get('process_name', '').lower()
            
            for parent, suspicious_children in suspicious_relationships.items():
                if parent in parent_process:
                    if any(child in child_process for child in suspicious_children):
                        findings.append({
                            'timestamp': row['timestamp'],
                            'parent': row.get('parent_process'),
                            'child': row.get('process_name'),
                            'pid': row.get('pid'),
                            'command_line': row.get('command_line', ''),
                            'reason': 'web_server_spawning_shell'
                        })
    
    return findings

# Usage
suspicious_procs = detect_suspicious_process_trees('process_list.csv')
for proc in suspicious_procs:
    print(f"{proc['timestamp']}: {proc['parent']} -> {proc['child']}")
    print(f"  Command: {proc['command_line']}")
```

**Hidden processes:**

```bash
# Processes with suspicious names (spaces, special chars)
ps aux | grep -E "^.{10}\s+.*\s{2,}"

# Processes running from /tmp or /var/tmp (often malicious)
ps aux | grep -E "/tmp|/var/tmp"

# Processes with deleted binaries (binary deleted after execution)
ls -l /proc/*/exe 2>/dev/null | grep deleted
```

**Detecting process masquerading:**

```python
def detect_masquerading(process_log):
    """
    Detect processes masquerading as legitimate system processes.
    Checks for mismatched names/paths.
    """
    
    # Legitimate process paths
    legitimate_paths = {
        'sshd': ['/usr/sbin/sshd'],
        'apache2': ['/usr/sbin/apache2'],
        'nginx': ['/usr/sbin/nginx'],
        'systemd': ['/lib/systemd/systemd', '/usr/lib/systemd/systemd'],
        'cron': ['/usr/sbin/cron'],
        'rsyslogd': ['/usr/sbin/rsyslogd']
    }
    
    masquerading = []
    
    with open(process_log, 'r') as f:
        reader = csv.DictReader(f)
        for row in reader:
            process_name = row.get('process_name', '').lower()
            process_path = row.get('process_path', '').lower()
            
            # Check known system processes
            for legit_name, legit_paths in legitimate_paths.items():
                if legit_name in process_name:
                    # Process name matches but path doesn't
                    if not any(lp in process_path for lp in legit_paths):
                        masquerading.append({
                            'timestamp': row['timestamp'],
                            'process_name': row['process_name'],
                            'process_path': process_path,
                            'expected_paths': legit_paths,
                            'pid': row.get('pid'),
                            'reason': 'path_mismatch'
                        })
    
    return masquerading
```

### Network connection backdoor patterns

**Established connections to suspicious IPs:**

```python
def analyze_outbound_connections(netstat_log, threat_intel_ips):
    """
    Analyze outbound connections for backdoor activity.
    
    Args:
        netstat_log: Path to netstat output
        threat_intel_ips: Set of known malicious IPs
    """
    
    suspicious_connections = []
    
    with open(netstat_log, 'r') as f:
        for line in f:
            if 'ESTABLISHED' not in line:
                continue
            
            # Parse connection (format varies by OS)
            # Example: tcp    0   0 192.168.1.100:54321  203.0.113.50:4444  ESTABLISHED
            match = re.search(r'(\d+\.\d+\.\d+\.\d+):(\d+)\s+(\d+\.\d+\.\d+\.\d+):(\d+)\s+ESTABLISHED', line)
            if match:
                local_ip, local_port, remote_ip, remote_port = match.groups()
                
                # Check against threat intel
                if remote_ip in threat_intel_ips:
                    suspicious_connections.append({
                        'local_ip': local_ip,
                        'local_port': local_port,
                        'remote_ip': remote_ip,
                        'remote_port': remote_port,
                        'reason': 'known_malicious_ip',
                        'line': line.strip()
                    })
                
                # Check for suspicious ports
                remote_port_int = int(remote_port)
                if remote_port_int in [4444, 5555, 6666, 7777, 8888, 31337]:
                    suspicious_connections.append({
                        'local_ip': local_ip,
                        'local_port': local_port,
                        'remote_ip': remote_ip,
                        'remote_port': remote_port,
                        'reason': 'suspicious_port',
                        'line': line.strip()
                    })
    
    return suspicious_connections

# Usage with threat intel
threat_ips = {'203.0.113.50', '198.51.100.25', '192.0.2.100'}
backdoor_conns = analyze_outbound_connections('netstat.txt', threat_ips)
```

**Long-duration connections:**

```python
def detect_long_connections(flow_log, duration_threshold=3600):
    """
    Detect connections that persist for unusually long periods.
    May indicate backdoor sessions.
    
    Args:
        flow_log: Netflow or connection log
        duration_threshold: Minimum duration in seconds (default 1 hour)
    """
    
    long_connections = []
    
    with open(flow_log, 'r') as f:
        reader = csv.DictReader(f)
        for row in reader:
            duration = int(row.get('duration_seconds', 0))
            
            if duration >= duration_threshold:
                long_connections.append({
                    'start_time': row['start_time'],
                    'source_ip': row['source_ip'],
                    'dest_ip': row['destination_ip'],
                    'dest_port': row['destination_port'],
                    'duration_hours': duration / 3600,
                    'bytes_sent': int(row.get('bytes_sent', 0)),
                    'bytes_received': int(row.get('bytes_received', 0))
                })
    
    return sorted(long_connections, key=lambda x: x['duration_hours'], reverse=True)
```

## Persistence mechanisms

Persistence mechanisms allow malware to survive reboots and maintain access. Detection requires analyzing system configuration, scheduled tasks, startup items, and service modifications.

### Registry-based persistence (Windows)

**Common registry persistence locations:**

```python
PERSISTENCE_REGISTRY_KEYS = [
    # Run keys
    r'HKCU\Software\Microsoft\Windows\CurrentVersion\Run',
    r'HKLM\Software\Microsoft\Windows\CurrentVersion\Run',
    r'HKCU\Software\Microsoft\Windows\CurrentVersion\RunOnce',
    r'HKLM\Software\Microsoft\Windows\CurrentVersion\RunOnce',
    r'HKLM\Software\Microsoft\Windows\CurrentVersion\RunServices',
    r'HKLM\Software\Microsoft\Windows\CurrentVersion\RunServicesOnce',
    
    # Explorer extensions
    r'HKLM\Software\Microsoft\Windows\CurrentVersion\Explorer\ShellExecuteHooks',
    r'HKLM\Software\Microsoft\Windows\CurrentVersion\Explorer\Browser Helper Objects',
    
    # Winlogon
    r'HKLM\Software\Microsoft\Windows NT\CurrentVersion\Winlogon\Userinit',
    r'HKLM\Software\Microsoft\Windows NT\CurrentVersion\Winlogon\Shell',
    r'HKLM\Software\Microsoft\Windows NT\CurrentVersion\Winlogon\Notify',
    
    # Image File Execution Options (debugger hijacking)
    r'HKLM\Software\Microsoft\Windows NT\CurrentVersion\Image File Execution Options',
    
    # Active Setup
    r'HKLM\Software\Microsoft\Active Setup\Installed Components',
    r'HKCU\Software\Microsoft\Active Setup\Installed Components',
    
    # AppInit DLLs
    r'HKLM\Software\Microsoft\Windows NT\CurrentVersion\Windows\AppInit_DLLs',
    
    # Services
    r'HKLM\System\CurrentControlSet\Services',
]

def analyze_registry_persistence(reg_export_file):
    """
    Analyze Windows registry export for persistence mechanisms.
    Expects .reg file format or CSV with key/value pairs.
    """
    
    suspicious_entries = []
    
    with open(reg_export_file, 'r', encoding='utf-16') as f:
        current_key = None
        
        for line in f:
            line = line.strip()
            
            # Track current registry key
            if line.startswith('['):
                current_key = line.strip('[]')
            
            # Check if we're in a persistence location
            if current_key:
                for persist_key in PERSISTENCE_REGISTRY_KEYS:
                    if persist_key.replace('\\', '\\\\').lower() in current_key.lower():
                        # Parse value
                        if '=' in line:
                            value_name, value_data = line.split('=', 1)
                            
                            # Check for suspicious indicators
                            reasons = []
                            
                            # Suspicious paths
                            if re.search(r'(\\temp\\|\\appdata\\|\\public\\|%temp%)', value_data, re.I):
                                reasons.append('suspicious_path')
                            
                            # Encoded commands
                            if re.search(r'(powershell.*-enc|cmd.*/c|mshta|rundll32)', value_data, re.I):
                                reasons.append('suspicious_command')
                            
                            # Uncommon extensions
                            if re.search(r'\.(vbs|js|hta|bat|cmd|ps1)\"?$', value_data, re.I):
                                reasons.append('script_file')
                            
                            if reasons:
                                suspicious_entries.append({
                                    'registry_key': current_key,
                                    'value_name': value_name.strip('"'),
                                    'value_data': value_data.strip('"'),
                                    'reasons': reasons
                                })
    
    return suspicious_entries

# Usage
reg_persistence = analyze_registry_persistence('registry_export.reg')
for entry in reg_persistence:
    print(f"Key: {entry['registry_key']}")
    print(f"  Value: {entry['value_name']} = {entry['value_data']}")
    print(f"  Reasons: {', '.join(entry['reasons'])}")
```

**Detecting registry modifications in event logs:**

```bash
# Windows Event IDs for registry changes
# Event ID 4657: Registry value was modified
# Event ID 4663: An attempt was made to access an object (registry)

# Extract registry modification events
grep "EventID.*465[37]" Security.log

# Filter for persistence keys
grep -E "(CurrentVersion\\\\Run|Winlogon|Image File Execution)" Security.log
```

**PowerShell-based registry persistence detection:**

```python
def detect_powershell_registry_persistence(evtx_log):
    """
    Detect PowerShell commands that modify registry for persistence.
    Analyzes PowerShell operational logs.
    """
    
    persistence_commands = [
        r'New-ItemProperty.*CurrentVersion\\Run',
        r'Set-ItemProperty.*CurrentVersion\\Run',
        r'reg\s+add.*CurrentVersion\\Run',
        r'New-PSDrive.*HKLM:',
    ]
    
    detections = []
    
    with open(evtx_log, 'r') as f:
        for line in f:
            for pattern in persistence_commands:
                if re.search(pattern, line, re.IGNORECASE):
                    detections.append({
                        'log_line': line.strip(),
                        'pattern_matched': pattern
                    })
                    break
    
    return detections
```

### Scheduled task persistence

**Linux cron jobs:**

```bash
# Examine all user crontabs
for user in $(cut -f1 -d: /etc/passwd); do 
    echo "=== Crontab for $user ==="
    crontab -u $user -l 2>/dev/null
done

# System-wide cron
ls -la /etc/cron.* /etc/crontab

# Suspicious cron job indicators
grep -r "curl\|wget\|nc\|bash -i\|python -c" /var/spool/cron/ /etc/cron*
```

**Analyzing cron logs:**

```python
def analyze_cron_persistence(cron_log, syslog):
    """
    Detect suspicious cron job executions.
    """
    
    suspicious_patterns = [
        r'curl.*\|\s*bash',
        r'wget.*\|\s*sh',
        r'nc\s+-[el]',
        r'python\s+-c.*socket',
        r'perl\s+-e.*socket',
        r'/tmp/\.\w+',  # Hidden files in /tmp
        r'chmod\s+\+x.*&&',
    ]
    
    detections = []
    
    with open(syslog, 'r') as f:
        for line in f:
            if 'CRON' not in line and 'cron' not in line:
                continue
            
            for pattern in suspicious_patterns:
                if re.search(pattern, line, re.IGNORECASE):
                    detections.append({
                        'log_entry': line.strip(),
                        'pattern': pattern
                    })
                    break
    
    return detections
```

**Windows scheduled tasks:**

```python
def analyze_scheduled_tasks(task_xml_dir):
    """
    Analyze Windows scheduled task XML files for persistence.
    Tasks located in: C:\Windows\System32\Tasks
    """
    import xml.etree.ElementTree as ET
    from pathlib import Path
    
    suspicious_tasks = []
    
    # Suspicious indicators in tasks
    suspicious_indicators = {
        'command': [r'powershell', r'cmd\.exe', r'mshta', r'rundll32', r'regsvr32', r'wscript', r'cscript'],
        'arguments': [r'-enc', r'-e\s+[A-Za-z0-9+/=]{20,}', r'/c\s+', r'downloadstring', r'invoke-expression'],
        'path': [r'\\temp\\', r'\\appdata\\', r'\\public\\', r'%temp%'],
    }
    
    for task_file in Path(task_xml_dir).rglob('*.xml'):
        try:
            tree = ET.parse(task_file)
            root = tree.getroot()
            
            # Extract execution details
            namespace = {'t': 'http://schemas.microsoft.com/windows/2004/02/mit/task'}
            actions = root.findall('.//t:Exec', namespace)
            
            for action in actions:
                command_elem = action.find('t:Command', namespace)
                args_elem = action.find('t:Arguments', namespace)
                
                if command_elem is not None:
                    command = command_elem.text or ''
                    arguments = args_elem.text if args_elem is not None else ''
                    
                    reasons = []
                    
                    # Check command
                    for pattern in suspicious_indicators['command']:
                        if re.search(pattern, command, re.IGNORECASE):
                            reasons.append(f'suspicious_command:{pattern}')
                    
                    # Check arguments
                    for pattern in suspicious_indicators['arguments']:
                        if re.search(pattern, arguments, re.IGNORECASE):
                            reasons.append(f'suspicious_args:{pattern}')
                    
                    # Check paths
                    full_command = command + ' ' + arguments
                    for pattern in suspicious_indicators['path']:
                        if re.search(pattern, full_command, re.IGNORECASE):
                            reasons.append(f'suspicious_path:{pattern}')
                    
                    if reasons:
                        # Get trigger info
                        triggers = root.findall('.//t:Triggers/*', namespace)
                        trigger_types = [t.tag.split('}')[1] for t in triggers]
                        
                        suspicious_tasks.append({
                            'task_name': task_file.name,
                            'task_path': str(task_file),
                            'command': command,
                            'arguments': arguments,
                            'triggers': trigger_types,
                            'reasons': reasons
                        })
        
        except Exception as e:
            print(f"Error parsing {task_file}: {e}")
            continue
    
    return suspicious_tasks

# Usage
tasks = analyze_scheduled_tasks('/path/to/tasks/directory')
for task in tasks:
    print(f"\nTask: {task['task_name']}")
    print(f"  Command: {task['command']} {task['arguments']}")
    print(f"  Triggers: {', '.join(task['triggers'])}")
    print(f"  Suspicious indicators: {', '.join(task['reasons'])}")
```

**Detecting task creation in Windows event logs:**

```bash
# Event ID 4698: Scheduled task created
# Event ID 4699: Scheduled task deleted
# Event ID 4700: Scheduled task enabled
# Event ID 4701: Scheduled task disabled
# Event ID 4702: Scheduled task updated

grep "EventID.*4698" Security.log | grep -iE "(powershell|cmd|mshta|rundll32)"
```

### Service-based persistence

**Linux systemd services:**

```bash
# List all systemd services
systemctl list-units --type=service --all

# Check for suspicious service files
ls -la /etc/systemd/system/
ls -la /lib/systemd/system/
ls -la ~/.config/systemd/user/

# Examine service files for suspicious content
grep -r "ExecStart" /etc/systemd/system/ | grep -iE "(curl|wget|nc|bash -i|/tmp)"
```

**Analyzing systemd service files:**

```python
def analyze_systemd_services(service_dir):
    """
    Analyze systemd service files for malicious persistence.
    """
    from pathlib import Path
    
    suspicious_services = []
    
    suspicious_patterns = {
        'exec_start': [r'curl.*\|.*bash', r'wget.*\|.*sh', r'nc\s+-[el]', r'/tmp/\.'],
        'working_directory': [r'/tmp', r'/var/tmp', r'/dev/shm'],
        'user': [r'root'],  # Services running as root with suspicious commands
    }
    
    for service_file in Path(service_dir).glob('*.service'):
        suspicious_indicators = []
        service_content = {}
        
        with open(service_file, 'r') as f:
            for line in f:
                line = line.strip()
                
                if '=' in line and not line.startswith('#'):
                    key, value = line.split('=', 1)
                    key = key.strip().lower()
                    service_content[key] = value.strip()
                    
                    # Check for suspicious patterns
                    if key == 'execstart':
                        for pattern in suspicious_patterns['exec_start']:
                            if re.search(pattern, value, re.IGNORECASE):
                                suspicious_indicators.append(f'suspicious_exec:{pattern}')
                    
                    elif key == 'workingdirectory':
                        for pattern in suspicious_patterns['working_directory']:
                            if re.search(pattern, value, re.IGNORECASE):
                                suspicious_indicators.append('suspicious_workdir')
        
        if suspicious_indicators:
            suspicious_services.append({
                'service_name': service_file.name,
                'service_path': str(service_file),
                'exec_start': service_content.get('execstart', 'N/A'),
                'user': service_content.get('user', 'N/A'),
                'indicators': suspicious_indicators
            })
    
    return suspicious_services
```

**Windows services:**

```python
def analyze_windows_services(services_csv):
    """
    Analyze Windows services for persistence mechanisms.
    Export services with: sc query type= service state= all > services.txt
    or Get-Service | Export-Csv services.csv
    """
    
    suspicious_services = []
    
    # Suspicious service indicators
    suspicious_binaries = [
        r'powershell\.exe',
        r'cmd\.exe',
        r'mshta\.exe',
        r'rundll32\.exe',
        r'regsvr32\.exe',
        r'\\temp\\',
        r'\\appdata\\',
    ]
    
    with open(services_csv, 'r') as f:
        reader = csv.DictReader(f)
        for row in reader:
            service_name = row.get('Name', row.get('ServiceName', ''))
            binary_path = row.get('BinaryPathName', row.get('PathName', ''))
            start_type = row.get('StartType', row.get('Start', ''))
            
            # Check binary path
            for pattern in suspicious_binaries:
                if re.search(pattern, binary_path, re.IGNORECASE):
                    suspicious_services.append({
                        'service_name': service_name,
                        'binary_path': binary_path,
                        'start_type': start_type,
                        'reason': f'suspicious_binary:{pattern}'
                    })
                    break
            
            # Services that start automatically
            if start_type.lower() in ['automatic', 'auto']:
                if any(re.search(p, binary_path, re.I) for p in suspicious_binaries):
                    suspicious_services.append({
                        'service_name': service_name,
                        'binary_path': binary_path,
                        'start_type': start_type,
                        'reason': 'auto_start_suspicious_binary'
                    })
    
    return suspicious_services
```

### Startup folder persistence

**Linux:**

```bash
# User-specific autostart
ls -la ~/.config/autostart/

# System-wide
ls -la /etc/xdg/autostart/

# Profile scripts
ls -la ~/.bash_profile ~/.bashrc ~/.profile /etc/profile /etc/bash.bashrc
```

**Detecting malicious profile modifications:**

```python
def analyze_profile_scripts(profile_dir='/etc'):
    """
    Analyze shell profile scripts for malicious modifications.
    """
    
    profile_files = [
        '/etc/profile',
        '/etc/bash.bashrc',
        '/etc/zsh/zshrc',
        '~/.bashrc',
        '/.profile',
        '~/.bash_profile',
        '~/.zshrc',
    ]
    
    suspicious_content = []
    
    suspicious_patterns = [
        r'curl.*\|.*bash',
        r'wget.*\|.*sh',
        r'nc\s+-[el]',
        r'python.*socket',
        r'perl.*socket',
        r'/tmp/\.\w+',
        r'export\s+LD_PRELOAD',
    ]
    
    for profile_file in profile_files:
        try:
            with open(profile_file, 'r') as f:
                for line_num, line in enumerate(f, 1):
                    for pattern in suspicious_patterns:
                        if re.search(pattern, line, re.IGNORECASE):
                            suspicious_content.append({
                                'file': profile_file,
                                'line_number': line_num,
                                'content': line.strip(),
                                'pattern': pattern
                            })
        except FileNotFoundError:
            continue
        except PermissionError:
            continue
    
    return suspicious_content
```

**Windows startup folders:**

```bash
# User startup
dir "%APPDATA%\Microsoft\Windows\Start Menu\Programs\Startup"

# All users startup
dir "%PROGRAMDATA%\Microsoft\Windows\Start Menu\Programs\Startup"
```

### Boot/Logon script persistence

**Group Policy scripts (Windows):**

```python
def analyze_gpo_scripts(gpo_log):
    """
    Analyze Group Policy logon/startup scripts.
    Check: User Configuration -> Windows Settings -> Scripts (Logon/Logoff)
           Computer Configuration -> Windows Settings -> Scripts (Startup/Shutdown)
    """
    
    suspicious_scripts = []
    
    with open(gpo_log, 'r') as f:
        for line in f:
            # Look for script execution events
            if 'scripts' in line.lower() and ('logon' in line.lower() or 'startup' in line.lower()):
                # Extract script path
                match = re.search(r'([A-Z]:\\[^\s]+\.(?:bat|cmd|vbs|ps1|js))', line, re.IGNORECASE)
                if match:
                    script_path = match.group(1)
                    
                    # Check for suspicious locations
                    if re.search(r'\\temp\\|\\appdata\\|\\public\\', script_path, re.IGNORECASE):
                        suspicious_scripts.append({
                            'script_path': script_path,
                            'log_entry': line.strip(),
                            'reason': 'suspicious_location'
                        })
    
    return suspicious_scripts
```

### WMI/CIM persistence

**WMI event subscriptions (Windows):**

```python
def analyze_wmi_persistence(wmi_export):
    """
    Analyze WMI Event Subscriptions for persistence.
    Export with: Get-WMIObject -Namespace root\Subscription -Class __EventFilter
                 Get-WMIObject -Namespace root\Subscription -Class __EventConsumer
                 Get-WMIObject -Namespace root\Subscription -Class __FilterToConsumerBinding
    """
    
    suspicious_wmi = []
    
    # Common WMI persistence patterns
    suspicious_consumers = [
        r'powershell',
        r'cmd\.exe',
        r'mshta',
        r'rundll32',
        r'-enc\s+[A-Za-z0-9+/=]{20,}',
    ]
    
    with open(wmi_export, 'r') as f:
        content = f.read()
        
        # Check CommandLineEventConsumer
        if 'CommandLineEventConsumer' in content:
            for pattern in suspicious_consumers:
                if re.search(pattern, content, re.IGNORECASE):
                    suspicious_wmi.append({
                        'type': 'CommandLineEventConsumer',
                        'pattern': pattern,
                        'content_snippet': content[max(0, content.find(pattern)-100):content.find(pattern)+200]
                    })
        
        # Check ActiveScriptEventConsumer
        if 'ActiveScriptEventConsumer' in content:
            suspicious_wmi.append({
                'type': 'ActiveScriptEventConsumer',
                'reason': 'script_execution',
                'content_snippet': content[:500]
            })
    
    return suspicious_wmi
```

### Browser extension persistence

**Detecting Browser Extension Persistence**

```python
def detect_browser_extension_persistence(extension_dirs):
    """
    Analyze browser extensions for malicious persistence.
    
    Args:
        extension_dirs: List of browser extension directories
            Chrome: ~/.config/google-chrome/Default/Extensions/ (Linux)
                   %LOCALAPPDATA%\Google\Chrome\User Data\Default\Extensions\ (Windows)
            Firefox: ~/.mozilla/firefox/*.default/extensions/ (Linux)
    """
    import json
    from pathlib import Path
    
    suspicious_extensions = []
    
    # Suspicious permission combinations
    dangerous_permissions = [
        ['webRequest', 'webRequestBlocking'],  # Can intercept traffic
        ['cookies', 'webRequest'],              # Can steal session tokens
        ['tabs', 'webRequest'],                 # Can monitor browsing
        ['debugger'],                           # Can inject code
        ['management'],                         # Can control other extensions
    ]
    
    for ext_dir in extension_dirs:
        for manifest_file in Path(ext_dir).rglob('manifest.json'):
            try:
                with open(manifest_file, 'r', encoding='utf-8') as f:
                    manifest = json.load(f)
                
                ext_name = manifest.get('name', 'Unknown')
                ext_version = manifest.get('version', 'Unknown')
                permissions = manifest.get('permissions', [])
                content_scripts = manifest.get('content_scripts', [])
                background_scripts = manifest.get('background', {}).get('scripts', [])
                
                reasons = []
                
                # Check dangerous permission combinations
                for dangerous_combo in dangerous_permissions:
                    if all(perm in permissions for perm in dangerous_combo):
                        reasons.append(f"dangerous_permissions:{','.join(dangerous_combo)}")
                
                # Check for excessive permissions
                if 'webRequest' in permissions and 'webRequestBlocking' in permissions:
                    if '<all_urls>' in permissions or '*://*/*' in permissions:
                        reasons.append('intercept_all_traffic')
                
                # Content scripts injecting into all pages
                for cs in content_scripts:
                    matches = cs.get('matches', [])
                    if '<all_urls>' in matches or '*://*/*' in matches:
                        reasons.append('inject_all_pages')
                
                # External connectivity
                if manifest.get('externally_connectable'):
                    reasons.append('external_connectivity')
                
                # Native messaging (can execute native code)
                if 'nativeMessaging' in permissions:
                    reasons.append('native_messaging')
                
                if reasons:
                    suspicious_extensions.append({
                        'name': ext_name,
                        'version': ext_version,
                        'path': str(manifest_file.parent),
                        'permissions': permissions,
                        'reasons': reasons
                    })
            
            except (json.JSONDecodeError, FileNotFoundError, PermissionError) as e:
                continue
    
    return suspicious_extensions

# Usage
extension_paths = [
    '/home/user/.config/google-chrome/Default/Extensions/',
    '/home/user/.mozilla/firefox/*.default/extensions/'
]
malicious_extensions = detect_browser_extension_persistence(extension_paths)
for ext in malicious_extensions:
    print(f"\nExtension: {ext['name']} v{ext['version']}")
    print(f"  Path: {ext['path']}")
    print(f"  Permissions: {', '.join(ext['permissions'][:5])}...")
    print(f"  Suspicious indicators: {', '.join(ext['reasons'])}")
```

### DLL hijacking and injection

**Detecting DLL injection:**

```python
def detect_dll_injection(process_log):
    """
    Detect DLL injection by analyzing loaded DLLs.
    Look for DLLs loaded from suspicious locations.
    """
    
    suspicious_dll_locations = [
        r'\\temp\\',
        r'\\appdata\\local\\temp\\',
        r'\\users\\public\\',
        r'\\programdata\\',
        r'\\windows\\temp\\',
    ]
    
    dll_injections = []
    
    with open(process_log, 'r') as f:
        reader = csv.DictReader(f)
        for row in reader:
            loaded_dlls = row.get('loaded_modules', '').split(';')
            process_name = row.get('process_name', '')
            pid = row.get('pid', '')
            
            for dll in loaded_dlls:
                dll = dll.strip().lower()
                
                # Check suspicious locations
                for suspicious_loc in suspicious_dll_locations:
                    if re.search(suspicious_loc, dll, re.IGNORECASE):
                        dll_injections.append({
                            'timestamp': row.get('timestamp'),
                            'process': process_name,
                            'pid': pid,
                            'dll_path': dll,
                            'reason': 'suspicious_dll_location'
                        })
                        break
                
                # Unnamed or hidden DLLs
                if re.search(r'\\\.\w+\.dll$', dll):
                    dll_injections.append({
                        'timestamp': row.get('timestamp'),
                        'process': process_name,
                        'pid': pid,
                        'dll_path': dll,
                        'reason': 'hidden_dll'
                    })
    
    return dll_injections
```

**AppInit_DLLs persistence:**

```bash
# Check AppInit_DLLs registry key (Windows)
reg query "HKLM\Software\Microsoft\Windows NT\CurrentVersion\Windows" /v AppInit_DLLs
reg query "HKLM\Software\Wow6432Node\Microsoft\Windows NT\CurrentVersion\Windows" /v AppInit_DLLs
```

### LD_PRELOAD hijacking (Linux)

**Detecting LD_PRELOAD abuse:**

```bash
# Check environment variables in process listings
ps auxe | grep LD_PRELOAD

# Check system-wide preload configuration
cat /etc/ld.so.preload

# Check for suspicious .so files in common locations
find /tmp /var/tmp /dev/shm -name "*.so" 2>/dev/null

# Check user-specific configurations
grep -r "LD_PRELOAD" ~/.bashrc ~/.profile ~/.bash_profile /etc/profile.d/
```

**Analyzing LD_PRELOAD in logs:**

```python
def detect_ld_preload_persistence(syslog, process_log):
    """
    Detect LD_PRELOAD-based persistence mechanism.
    """
    
    preload_detections = []
    
    # Check system logs
    with open(syslog, 'r') as f:
        for line in f:
            if 'LD_PRELOAD' in line:
                preload_detections.append({
                    'source': 'syslog',
                    'log_entry': line.strip()
                })
    
    # Check process environment
    with open(process_log, 'r') as f:
        reader = csv.DictReader(f)
        for row in reader:
            env_vars = row.get('environment', '')
            if 'LD_PRELOAD' in env_vars:
                # Extract the preloaded library
                match = re.search(r'LD_PRELOAD=([^\s;]+)', env_vars)
                if match:
                    preload_lib = match.group(1)
                    preload_detections.append({
                        'source': 'process',
                        'process': row.get('process_name'),
                        'pid': row.get('pid'),
                        'preload_lib': preload_lib
                    })
    
    # Check /etc/ld.so.preload
    try:
        with open('/etc/ld.so.preload', 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    preload_detections.append({
                        'source': 'ld.so.preload',
                        'library': line
                    })
    except FileNotFoundError:
        pass
    
    return preload_detections
```

### Rootkit indicators

**User-mode rootkit detection:**

```python
def detect_usermode_rootkit_artifacts(logs_dir):
    """
    Detect user-mode rootkit indicators in logs.
    """
    
    rootkit_indicators = []
    
    # Common rootkit file names and locations
    known_rootkit_files = [
        r'/dev/\.\w+',
        r'/tmp/\.\w+',
        r'/usr/bin/\.\w+',
        r'/lib/.*\.so\.\d+\.\d+\.\d+\.old',  # Replaced system libraries
    ]
    
    # Check file access logs
    file_log = logs_dir + '/file_access.log'
    try:
        with open(file_log, 'r') as f:
            for line in f:
                for pattern in known_rootkit_files:
                    if re.search(pattern, line):
                        rootkit_indicators.append({
                            'type': 'suspicious_file',
                            'log_entry': line.strip(),
                            'pattern': pattern
                        })
    except FileNotFoundError:
        pass
    
    return rootkit_indicators
```

**Kernel-mode rootkit indicators:**

```bash
# Check for kernel module anomalies (Linux)
lsmod | grep -vE "^Module|^(ip_tables|iptable|nf_|x_tables|overlay|bridge)"

# Check for hidden modules (compare lsmod vs /sys/module)
diff <(lsmod | awk '{print $1}' | sort) <(ls /sys/module | sort)

# Check module signing
modinfo -F signer <module_name>

# Suspicious module locations
find /lib/modules/$(uname -r) -name "*.ko" -path "*/updates/*" -o -path "*/extras/*"
```

### File modification timestamps

**Detecting timestamp manipulation:**

```python
def detect_timestamp_anomalies(file_metadata_log):
    """
    Detect suspicious file timestamp modifications.
    Malware often manipulates timestamps to hide tracks.
    """
    from datetime import datetime, timedelta
    
    anomalies = []
    
    with open(file_metadata_log, 'r') as f:
        reader = csv.DictReader(f)
        for row in reader:
            try:
                created = datetime.fromisoformat(row['created'])
                modified = datetime.fromisoformat(row['modified'])
                accessed = datetime.fromisoformat(row['accessed'])
                
                file_path = row['file_path']
                
                # Modified time before created time (impossible)
                if modified < created:
                    anomalies.append({
                        'file': file_path,
                        'reason': 'modified_before_created',
                        'created': created,
                        'modified': modified
                    })
                
                # Very old timestamps on recently created files
                now = datetime.now()
                if created < now - timedelta(days=365*10):  # More than 10 years old
                    if modified > now - timedelta(days=7):  # Modified in last week
                        anomalies.append({
                            'file': file_path,
                            'reason': 'ancient_creation_recent_modification',
                            'created': created,
                            'modified': modified
                        })
                
                # Accessed time way in the past compared to modified
                if accessed < modified - timedelta(days=365):
                    anomalies.append({
                        'file': file_path,
                        'reason': 'access_before_modification',
                        'accessed': accessed,
                        'modified': modified
                    })
                
                # All timestamps identical (often sign of timestomping)
                if created == modified == accessed:
                    anomalies.append({
                        'file': file_path,
                        'reason': 'all_timestamps_identical',
                        'timestamp': created
                    })
            
            except (ValueError, KeyError):
                continue
    
    return anomalies
```

### Account persistence

**New account creation:**

```bash
# Linux - check for new users in passwd
diff <(cat /etc/passwd.old) <(cat /etc/passwd)

# Check auth logs for user additions
grep -i "useradd\|adduser\|new user" /var/log/auth.log

# Windows - Event ID 4720 (user account created)
grep "EventID.*4720" Security.log
```

**Analyzing account creation events:**

```python
def detect_suspicious_accounts(auth_log):
    """
    Detect suspicious account creation and modifications.
    """
    
    suspicious_accounts = []
    
    # Patterns indicating account manipulation
    patterns = {
        'user_added': re.compile(r'(useradd|adduser|new user|user.*created)', re.I),
        'uid_0': re.compile(r'UID.*0'),  # Root UID
        'password_change': re.compile(r'password changed', re.I),
        'sudo_granted': re.compile(r'(sudoers|sudo.*granted)', re.I),
    }
    
    with open(auth_log, 'r') as f:
        for line in f:
            for pattern_name, pattern in patterns.items():
                if pattern.search(line):
                    # Extract username if possible
                    user_match = re.search(r'user[:\s]+(\w+)', line, re.I)
                    username = user_match.group(1) if user_match else 'unknown'
                    
                    suspicious_accounts.append({
                        'pattern': pattern_name,
                        'username': username,
                        'log_entry': line.strip()
                    })
    
    return suspicious_accounts
```

**SSH key additions:**

```bash
# Check authorized_keys modifications
for user_home in /home/*; do
    if [ -f "$user_home/.ssh/authorized_keys" ]; then
        echo "=== $user_home/.ssh/authorized_keys ==="
        ls -la "$user_home/.ssh/authorized_keys"
        cat "$user_home/.ssh/authorized_keys"
    fi
done

# Check for recently modified authorized_keys
find /home -name "authorized_keys" -mtime -7

# Check auth logs for public key additions
grep -i "authorized_keys" /var/log/auth.log
```

### Backdoor account characteristics

**Detecting backdoor accounts:**

```python
def analyze_user_accounts(passwd_file, shadow_file=None):
    """
    Analyze user accounts for backdoor characteristics.
    
    Suspicious indicators:
    - UID 0 (root) with non-root username
    - Accounts with shell access in unusual directories
    - Recently created accounts with suspicious names
    - Accounts with empty or weak passwords
    """
    
    suspicious_accounts = []
    
    with open(passwd_file, 'r') as f:
        for line in f:
            if line.startswith('#') or not line.strip():
                continue
            
            parts = line.strip().split(':')
            if len(parts) < 7:
                continue
            
            username = parts[0]
            uid = int(parts[2])
            gid = int(parts[3])
            home_dir = parts[5]
            shell = parts[6]
            
            reasons = []
            
            # UID 0 but not root
            if uid == 0 and username != 'root':
                reasons.append('uid_0_non_root')
            
            # Shell access from suspicious home directory
            if shell not in ['/usr/sbin/nologin', '/bin/false', '/sbin/nologin']:
                if home_dir in ['/tmp', '/var/tmp', '/dev/shm']:
                    reasons.append('shell_access_suspicious_home')
            
            # Suspicious username patterns
            suspicious_names = [r'^\$', r'^\s', r'^\.\w+', r'test', r'temp', r'admin$', r'^adm$']
            for pattern in suspicious_names:
                if re.search(pattern, username, re.I):
                    reasons.append(f'suspicious_name:{pattern}')
                    break
            
            # Hidden username (starts with .)
            if username.startswith('.'):
                reasons.append('hidden_username')
            
            if reasons:
                suspicious_accounts.append({
                    'username': username,
                    'uid': uid,
                    'gid': gid,
                    'home_dir': home_dir,
                    'shell': shell,
                    'reasons': reasons
                })
    
    # Check shadow file if available (Linux)
    if shadow_file:
        try:
            with open(shadow_file, 'r') as f:
                for line in f:
                    parts = line.strip().split(':')
                    if len(parts) < 2:
                        continue
                    
                    username = parts[0]
                    password_hash = parts[1]
                    
                    # Empty password or easily crackable
                    if password_hash in ['', '*', '!', '!!']:
                        continue  # Locked accounts, normal
                    
                    # Very short hash (weak password)
                    if len(password_hash) < 13:
                        for account in suspicious_accounts:
                            if account['username'] == username:
                                account['reasons'].append('weak_password_hash')
        except (FileNotFoundError, PermissionError):
            pass
    
    return suspicious_accounts

# Usage
backdoor_accounts = analyze_user_accounts('/etc/passwd', '/etc/shadow')
for account in backdoor_accounts:
    print(f"\nSuspicious account: {account['username']}")
    print(f"  UID: {account['uid']}, Home: {account['home_dir']}, Shell: {account['shell']}")
    print(f"  Indicators: {', '.join(account['reasons'])}")
```

### Golden Ticket / Silver Ticket attacks (Windows)

**Detecting Kerberos ticket anomalies:**

```python
def detect_kerberos_ticket_anomalies(event_log):
    """
    Detect Golden/Silver ticket attacks via event log analysis.
    
    Event IDs:
    - 4768: Kerberos TGT requested
    - 4769: Kerberos service ticket requested
    - 4770: Kerberos service ticket renewed
    - 4771: Kerberos pre-authentication failed
    """
    
    anomalies = []
    
    with open(event_log, 'r') as f:
        for line in f:
            # Event 4769 - Service ticket requested
            if 'EventID.*4769' in line:
                # Check for suspicious characteristics
                
                # Ticket encryption type (RC4 often indicates Golden Ticket)
                if '0x17' in line:  # RC4 encryption
                    anomalies.append({
                        'event_id': '4769',
                        'reason': 'rc4_encryption_downgrade',
                        'log_entry': line.strip()
                    })
                
                # Service ticket requested for unusual service
                if re.search(r'krbtgt|KRBTGT', line):
                    anomalies.append({
                        'event_id': '4769',
                        'reason': 'krbtgt_service_ticket',
                        'log_entry': line.strip()
                    })
            
            # Event 4768 - TGT requested with unusual characteristics
            if 'EventID.*4768' in line:
                # Long ticket lifetime (Golden Ticket often has 10 years)
                if re.search(r'Ticket.*Life.*[5-9]\d{4,}', line):
                    anomalies.append({
                        'event_id': '4768',
                        'reason': 'excessive_ticket_lifetime',
                        'log_entry': line.strip()
                    })
    
    return anomalies
```

### Comprehensive persistence detection script

**All-in-one persistence scanner:**

```python
#!/usr/bin/env python3
"""
Comprehensive persistence mechanism detection for CTF log analysis.
"""

import os
import sys
import json
from collections import defaultdict

class PersistenceDetector:
    def __init__(self, logs_dir):
        self.logs_dir = logs_dir
        self.detections = defaultdict(list)
    
    def run_all_checks(self):
        """Execute all persistence detection checks."""
        print("[*] Starting comprehensive persistence detection...")
        
        # Registry persistence (Windows)
        if os.path.exists(f"{self.logs_dir}/registry_export.reg"):
            print("[*] Checking registry persistence...")
            self.detections['registry'] = analyze_registry_persistence(
                f"{self.logs_dir}/registry_export.reg"
            )
        
        # Scheduled tasks
        if os.path.exists(f"{self.logs_dir}/cron.log"):
            print("[*] Checking cron jobs...")
            self.detections['cron'] = analyze_cron_persistence(
                f"{self.logs_dir}/cron.log",
                f"{self.logs_dir}/syslog"
            )
        
        # Services
        if os.path.exists(f"{self.logs_dir}/services"):
            print("[*] Checking systemd services...")
            self.detections['services'] = analyze_systemd_services(
                f"{self.logs_dir}/services"
            )
        
        # Network backdoors
        if os.path.exists(f"{self.logs_dir}/netstat.txt"):
            print("[*] Checking listening ports...")
            self.detections['network'] = analyze_listening_ports(
                f"{self.logs_dir}/netstat.txt"
            )
        
        # User accounts
        if os.path.exists(f"{self.logs_dir}/passwd"):
            print("[*] Checking user accounts...")
            shadow_file = f"{self.logs_dir}/shadow" if os.path.exists(f"{self.logs_dir}/shadow") else None
            self.detections['accounts'] = analyze_user_accounts(
                f"{self.logs_dir}/passwd",
                shadow_file
            )
        
        # Process analysis
        if os.path.exists(f"{self.logs_dir}/processes.csv"):
            print("[*] Checking process trees...")
            self.detections['processes'] = detect_suspicious_process_trees(
                f"{self.logs_dir}/processes.csv"
            )
        
        return self.detections
    
    def generate_report(self, output_file='persistence_report.json'):
        """Generate JSON report of all detections."""
        report = {
            'total_detections': sum(len(v) for v in self.detections.values()),
            'categories': {k: len(v) for k, v in self.detections.items()},
            'detections': dict(self.detections)
        }
        
        with open(output_file, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        print(f"\n[+] Report saved to {output_file}")
        print(f"[+] Total detections: {report['total_detections']}")
        for category, count in report['categories'].items():
            print(f"    - {category}: {count}")

# Usage
if __name__ == '__main__':
    if len(sys.argv) < 2:
        print(f"Usage: {sys.argv[0]} <logs_directory>")
        sys.exit(1)
    
    detector = PersistenceDetector(sys.argv[1])
    detector.run_all_checks()
    detector.generate_report()
```

---

**Important related topics for comprehensive malware detection:**

- **Memory forensics**: Analyzing process memory dumps for injected code, hidden processes, and malicious artifacts
- **Network traffic analysis**: Deep packet inspection for C2 protocol details, encrypted channel analysis, and payload extraction
- **Timeline analysis**: Correlating multiple log sources to construct complete attack narrative from initial compromise through persistence establishment
- **Indicator of Compromise (IOC) matching**: Automated detection using YARA rules, Sigma rules, and threat intelligence feeds

---

## File System Modifications

File system changes provide crucial indicators of malware presence, including dropped files, modified executables, and suspicious access patterns.

### Linux File System Monitoring

**Using auditd for file system tracking:**

```bash
# Install auditd
sudo apt install auditd audispd-plugins

# Check audit service status
sudo systemctl status auditd

# View audit logs
sudo ausearch -f /etc/passwd
sudo ausearch -k file_modifications
```

**Configuring audit rules for malware detection:**

```bash
# Monitor sensitive directories
sudo auditctl -w /bin -p wa -k binaries_modified
sudo auditctl -w /sbin -p wa -k system_binaries
sudo auditctl -w /usr/bin -p wa -k user_binaries
sudo auditctl -w /usr/sbin -p wa -k user_system_binaries

# Monitor sensitive files
sudo auditctl -w /etc/passwd -p wa -k passwd_changes
sudo auditctl -w /etc/shadow -p wa -k shadow_changes
sudo auditctl -w /etc/sudoers -p wa -k sudoers_changes

# Monitor web directories (common malware targets)
sudo auditctl -w /var/www/html -p wa -k webroot_changes

# Monitor cron directories
sudo auditctl -w /etc/cron.d -p wa -k cron_changes
sudo auditctl -w /etc/cron.daily -p wa -k cron_changes
sudo auditctl -w /etc/cron.hourly -p wa -k cron_changes
sudo auditctl -w /var/spool/cron/crontabs -p wa -k cron_changes

# Monitor user home directories
sudo auditctl -w /home -p wa -k home_directory_changes

# Monitor startup scripts
sudo auditctl -w /etc/rc.local -p wa -k startup_scripts
sudo auditctl -w /etc/init.d -p wa -k init_scripts

# List active rules
sudo auditctl -l

# Make rules persistent
sudo cp /etc/audit/audit.rules /etc/audit/rules.d/malware_detection.rules
```

**Analyzing audit logs for suspicious activity:**

```bash
# Search for file modifications in the last hour
sudo ausearch -ts recent -k file_modifications

# Find all write/append operations
sudo ausearch -m FILE_WRITE -i

# Search by process name
sudo ausearch -c wget
sudo ausearch -c curl

# Search for specific file access
sudo ausearch -f /bin/bash -i

# Format output for analysis
sudo ausearch -k binaries_modified --format csv > audit_binaries.csv

# Generate audit report
sudo aureport --file --summary

# Report on executable file changes
sudo aureport -x --summary

# Timeline of events
sudo ausearch -ts 10/29/2025 08:00:00 -te 10/29/2025 10:00:00
```

**Parsing audit logs with ausearch and aureport:**

```bash
# Find unauthorized file modifications
sudo ausearch -m PATH -sv no | grep -E "(WRITE|CREATE)"

# Identify processes creating executable files
sudo ausearch -i | grep -A5 "mode=0755"

# Extract file operations by user
sudo ausearch -ua www-data -k webroot_changes

# Find files created with suspicious extensions
sudo ausearch -i | grep -E "\.(sh|py|pl|exe|dll)$"

# Generate statistical summary
sudo aureport --start 10/29/2025 --end 10/30/2025 --summary
```

**Python script for advanced audit log analysis:**

```python
import subprocess
import re
from collections import defaultdict
from datetime import datetime

def parse_audit_logs(start_time=None, end_time=None):
    """Parse audit logs for suspicious file operations"""
    cmd = ['sudo', 'ausearch', '-m', 'PATH', '-i']
    
    if start_time:
        cmd.extend(['-ts', start_time])
    if end_time:
        cmd.extend(['-te', end_time])
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True)
        return result.stdout
    except Exception as e:
        print(f"Error running ausearch: {e}")
        return ""

def analyze_file_modifications(audit_data):
    """Identify suspicious file modification patterns"""
    suspicious_patterns = {
        'webshells': re.compile(r'\.php\d*$|\.jsp$|\.aspx?$'),
        'scripts': re.compile(r'\.(sh|py|pl|rb)$'),
        'binaries': re.compile(r'/bin/|/sbin/|\.exe$|\.elf$'),
        'hidden_files': re.compile(r'/\.[^/]+$'),
        'tmp_executables': re.compile(r'/tmp/.*\.(sh|elf|bin)')
    }
    
    findings = defaultdict(list)
    
    for line in audit_data.split('\n'):
        if 'name=' in line:
            # Extract filename
            match = re.search(r'name="([^"]+)"', line)
            if match:
                filename = match.group(1)
                
                # Check against suspicious patterns
                for category, pattern in suspicious_patterns.items():
                    if pattern.search(filename):
                        # Extract additional context
                        timestamp_match = re.search(r'time->(\S+)', line)
                        user_match = re.search(r'auid=(\w+)', line)
                        
                        findings[category].append({
                            'file': filename,
                            'timestamp': timestamp_match.group(1) if timestamp_match else 'unknown',
                            'user': user_match.group(1) if user_match else 'unknown',
                            'line': line
                        })
    
    return findings

# Usage
audit_data = parse_audit_logs('10/29/2025 00:00:00', '10/29/2025 23:59:59')
findings = analyze_file_modifications(audit_data)

for category, items in findings.items():
    print(f"\n{category.upper()}: {len(items)} findings")
    for item in items[:5]:  # Show first 5
        print(f"  {item['timestamp']} - {item['user']}: {item['file']}")
```

**Using inotify-tools for real-time monitoring:**

```bash
# Install inotify-tools
sudo apt install inotify-tools

# Monitor directory for any changes
inotifywait -m -r /var/www/html

# Monitor specific events
inotifywait -m -r -e create,modify,delete /var/www/html

# Log to file with timestamps
inotifywait -m -r -e create,modify,delete --timefmt '%Y-%m-%d %H:%M:%S' --format '%T %w%f %e' /var/www/html > /var/log/inotify.log

# Monitor multiple directories
inotifywait -m -r /bin /sbin /usr/bin -e modify,create

# Filter specific file types
inotifywait -m -r /home --include '.*\.(php|sh|py)$' -e create,modify
```

**Script for continuous monitoring with alerts:**

```bash
#!/bin/bash
# malware_file_monitor.sh

WATCH_DIRS=("/var/www/html" "/tmp" "/home")
LOG_FILE="/var/log/malware_monitor.log"
ALERT_FILE="/var/log/malware_alerts.log"

# Suspicious patterns
SUSPICIOUS_EXTENSIONS="php[0-9]|jsp|aspx?|sh|py|pl|exe"
SUSPICIOUS_NAMES="c99|r57|b374k|shell|backdoor|webshell"

inotifywait -m -r "${WATCH_DIRS[@]}" \
    -e create,modify,moved_to \
    --timefmt '%Y-%m-%d %H:%M:%S' \
    --format '%T %w%f %e' | \
while read timestamp file event; do
    echo "$timestamp $event $file" >> "$LOG_FILE"
    
    # Check for suspicious patterns
    if echo "$file" | grep -qiE "($SUSPICIOUS_EXTENSIONS)$"; then
        echo "[ALERT] $timestamp Suspicious file extension: $file ($event)" | tee -a "$ALERT_FILE"
    fi
    
    if echo "$file" | grep -qiE "$SUSPICIOUS_NAMES"; then
        echo "[ALERT] $timestamp Suspicious filename: $file ($event)" | tee -a "$ALERT_FILE"
    fi
    
    # Check if file is executable
    if [[ -f "$file" && -x "$file" ]]; then
        echo "[ALERT] $timestamp Executable file created: $file ($event)" | tee -a "$ALERT_FILE"
    fi
done
```

### Windows File System Monitoring

**Analyzing Windows file system events:**

```powershell
# Query Security event log for file access
Get-WinEvent -FilterHashtable @{
    LogName='Security'
    ID=4663  # Object access attempt
} | Select-Object TimeCreated, Message | Format-List

# Filter by specific file path
Get-WinEvent -FilterHashtable @{
    LogName='Security'
    ID=4663
} | Where-Object {$_.Message -like "*System32*"} | 
  Select-Object TimeCreated, @{Name='User';Expression={$_.Properties[1].Value}}, 
                @{Name='Object';Expression={$_.Properties[6].Value}}

# File deletion events
Get-WinEvent -FilterHashtable @{
    LogName='Security'
    ID=4660  # Object deleted
} | Select-Object TimeCreated, Message

# File creation/modification (requires audit policy enabled)
Get-WinEvent -FilterHashtable @{
    LogName='Security'
    ID=4656,4658,4663
} | Select-Object TimeCreated, ID, Message | Format-Table -AutoSize
```

**Enabling file system auditing:**

```powershell
# Enable object access auditing
auditpol /set /category:"Object Access" /success:enable /failure:enable

# Audit specific folder
$path = "C:\Windows\System32"
$acl = Get-Acl $path
$auditRule = New-Object System.Security.AccessControl.FileSystemAuditRule(
    "Everyone",
    "CreateFiles,AppendData,WriteData,WriteExtendedAttributes,WriteAttributes",
    "ContainerInherit,ObjectInherit",
    "None",
    "Success,Failure"
)
$acl.AddAuditRule($auditRule)
Set-Acl $path $acl

# Verify auditing is enabled
Get-Acl $path -Audit | Select-Object -ExpandProperty Audit
```

**Searching for suspicious file modifications:**

```powershell
# Find recently modified executables
Get-ChildItem -Path C:\Windows\System32 -Filter *.exe -Recurse -ErrorAction SilentlyContinue |
    Where-Object {$_.LastWriteTime -gt (Get-Date).AddDays(-1)} |
    Select-Object FullName, LastWriteTime, Length |
    Sort-Object LastWriteTime -Descending

# Find files with suspicious extensions in web directories
Get-ChildItem -Path C:\inetpub\wwwroot -Recurse -Include *.php,*.jsp,*.aspx |
    Where-Object {$_.LastWriteTime -gt (Get-Date).AddDays(-7)} |
    Select-Object FullName, LastWriteTime, Length

# Find hidden files
Get-ChildItem -Path C:\ -Recurse -Force -ErrorAction SilentlyContinue |
    Where-Object {$_.Attributes -match "Hidden"} |
    Select-Object FullName, LastWriteTime, Attributes

# Find files in suspicious locations
$suspiciousPaths = @(
    "C:\Users\*\AppData\Roaming\*",
    "C:\Users\*\AppData\Local\Temp\*",
    "C:\ProgramData\*"
)

foreach ($path in $suspiciousPaths) {
    Get-ChildItem -Path $path -Filter *.exe -Recurse -ErrorAction SilentlyContinue |
        Where-Object {$_.CreationTime -gt (Get-Date).AddDays(-1)} |
        Select-Object FullName, CreationTime, LastWriteTime
}
```

**USN Journal analysis for comprehensive file tracking:**

```powershell
# Extract USN Journal entries
fsutil usn readjournal C: csv > usn_journal.csv

# Parse USN Journal for specific timeframe
$startTime = Get-Date "2025-10-29 08:00:00"
$endTime = Get-Date "2025-10-29 10:00:00"

Import-Csv usn_journal.csv | 
    Where-Object {
        $timestamp = [datetime]::Parse($_.'Time stamp')
        $timestamp -ge $startTime -and $timestamp -le $endTime
    } |
    Where-Object {$_.Reason -match "FILE_CREATE|DATA_EXTEND"} |
    Select-Object 'Time stamp', 'File name', 'Reason', 'File attributes'
```

**MFT (Master File Table) analysis with Python:**

```python
# Using analyzeMFT
# Install: pip3 install analyzeMFT

import subprocess
import csv
from datetime import datetime

def extract_mft(output_csv):
    """Extract MFT to CSV format"""
    # Requires admin privileges
    cmd = ['analyzeMFT.py', '-f', 'C:\\$MFT', '-o', output_csv]
    subprocess.run(cmd)

def analyze_mft_for_malware(mft_csv):
    """Analyze MFT for malware indicators"""
    suspicious_extensions = ['.exe', '.dll', '.sys', '.bat', '.cmd', '.ps1', '.vbs']
    suspicious_paths = ['\\Temp\\', '\\AppData\\', '\\ProgramData\\']
    
    findings = []
    
    with open(mft_csv, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f, delimiter='|')
        
        for row in reader:
            filename = row.get('Filename', '')
            filepath = row.get('Path', '')
            created = row.get('Created', '')
            modified = row.get('Modified', '')
            
            # Check for suspicious characteristics
            is_suspicious = False
            reason = []
            
            # Suspicious extension
            if any(filename.lower().endswith(ext) for ext in suspicious_extensions):
                is_suspicious = True
                reason.append('suspicious_extension')
            
            # Suspicious path
            if any(path.lower() in filepath.lower() for path in suspicious_paths):
                is_suspicious = True
                reason.append('suspicious_path')
            
            # Recent creation (last 24 hours)
            try:
                created_time = datetime.strptime(created, '%Y-%m-%d %H:%M:%S')
                if (datetime.now() - created_time).days < 1:
                    is_suspicious = True
                    reason.append('recently_created')
            except:
                pass
            
            if is_suspicious:
                findings.append({
                    'file': filepath + filename,
                    'created': created,
                    'modified': modified,
                    'reasons': ','.join(reason)
                })
    
    return findings

# Usage (requires admin privileges)
# extract_mft('mft_output.csv')
# findings = analyze_mft_for_malware('mft_output.csv')
```

### Indicators of Compromise in File Systems

**Common malware file system signatures:**

```bash
# Webshell signatures
WEBSHELL_PATTERNS=(
    "eval.*base64_decode"
    "system.*\$_"
    "exec.*\$_"
    "passthru"
    "shell_exec"
    "c99|r57|b374k|WSO|FilesMan"
    "<?php.*eval"
)

# Search web directories for webshells
find /var/www -type f -name "*.php" -exec grep -l -E "${WEBSHELL_PATTERNS[*]}" {} \;

# Find PHP files with suspicious functions
grep -r -E "eval\(|base64_decode\(|gzinflate\(|str_rot13\(" /var/www --include="*.php"

# Find recently modified PHP files
find /var/www -name "*.php" -mtime -7 -ls

# Find PHP files with unusual permissions
find /var/www -name "*.php" -perm /111 -ls
```

**Analyzing file access patterns:**

```bash
# Files accessed by suspicious processes
sudo lsof | grep -E "(nc|ncat|socat|python|perl|ruby)" | awk '{print $9}' | sort -u

# Recently accessed files
find / -type f -atime -1 2>/dev/null | head -n 100

# Files with recent metadata changes
find /etc /bin /sbin /usr/bin /usr/sbin -type f -ctime -1 2>/dev/null

# Hidden files in suspicious locations
find /tmp /var/tmp /dev/shm -name ".*" -type f

# Setuid/setgid files (potential privilege escalation)
find / -type f \( -perm -4000 -o -perm -2000 \) -ls 2>/dev/null

# World-writable files
find / -type f -perm -002 -ls 2>/dev/null | grep -v "/proc\|/sys"
```

**File hash analysis for known malware:**

```bash
# Generate hash inventory
find /bin /sbin /usr/bin /usr/sbin -type f -exec sha256sum {} \; > system_hashes.txt

# Compare against known good baseline
diff baseline_hashes.txt system_hashes.txt

# Check against known malware hashes (VirusTotal, etc.)
# Example with single file
sha256sum /suspicious/file.exe

# Bulk hash checking
find /var/www -type f -exec sha256sum {} \; | \
while read hash file; do
    # Check hash against threat intelligence
    echo "$hash $file"
done
```

## Registry Changes (Windows)

Windows Registry modifications are primary persistence and configuration mechanisms for malware.

### Monitoring Registry Changes

**Using registry auditing:**

```powershell
# Enable registry auditing for HKLM\Software
$acl = Get-Acl "HKLM:\Software"
$rule = New-Object System.Security.AccessControl.RegistryAuditRule(
    "Everyone",
    "SetValue,CreateSubKey,Delete",
    "ContainerInherit",
    "None",
    "Success,Failure"
)
$acl.AddAuditRule($rule)
Set-Acl "HKLM:\Software" $acl

# Enable for Run keys (common persistence location)
$runKey = "HKLM:\SOFTWARE\Microsoft\Windows\CurrentVersion\Run"
$acl = Get-Acl $runKey
$rule = New-Object System.Security.AccessControl.RegistryAuditRule(
    "Everyone",
    "SetValue,CreateSubKey,Delete",
    "None",
    "None",
    "Success,Failure"
)
$acl.AddAuditRule($rule)
Set-Acl $runKey $acl
```

**Querying registry modification events:**

```powershell
# Registry value modifications (Event ID 4657)
Get-WinEvent -FilterHashtable @{
    LogName='Security'
    ID=4657
} | Select-Object TimeCreated, 
    @{Name='User';Expression={$_.Properties[3].Value}},
    @{Name='ObjectName';Expression={$_.Properties[6].Value}},
    @{Name='OperationType';Expression={$_.Properties[8].Value}} |
  Format-Table -AutoSize

# Filter for Run key modifications
Get-WinEvent -FilterHashtable @{
    LogName='Security'
    ID=4657
} | Where-Object {
    $_.Message -like "*CurrentVersion\Run*"
} | Select-Object TimeCreated, Message | Format-List

# Registry handle operations (Event ID 4656, 4658, 4663)
Get-WinEvent -FilterHashtable @{
    LogName='Security'
    ID=4656,4658,4663
} | Where-Object {
    $_.Properties[5].Value -match "\\REGISTRY\\"
} | Select-Object TimeCreated, ID, @{Name='Object';Expression={$_.Properties[5].Value}}
```

### Critical Registry Locations for Malware

**Autorun/Persistence locations:**

```powershell
# Common autorun registry keys
$autorunKeys = @(
    "HKLM:\SOFTWARE\Microsoft\Windows\CurrentVersion\Run",
    "HKLM:\SOFTWARE\Microsoft\Windows\CurrentVersion\RunOnce",
    "HKCU:\SOFTWARE\Microsoft\Windows\CurrentVersion\Run",
    "HKCU:\SOFTWARE\Microsoft\Windows\CurrentVersion\RunOnce",
    "HKLM:\SOFTWARE\WOW6432Node\Microsoft\Windows\CurrentVersion\Run",
    "HKLM:\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders",
    "HKLM:\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\User Shell Folders",
    "HKLM:\SOFTWARE\Microsoft\Windows NT\CurrentVersion\Winlogon",
    "HKLM:\SYSTEM\CurrentControlSet\Services",
    "HKLM:\SOFTWARE\Microsoft\Windows\CurrentVersion\RunServicesOnce",
    "HKCU:\SOFTWARE\Microsoft\Windows\CurrentVersion\RunServicesOnce",
    "HKLM:\SOFTWARE\Microsoft\Windows\CurrentVersion\RunServices",
    "HKCU:\SOFTWARE\Microsoft\Windows\CurrentVersion\RunServices",
    "HKLM:\SOFTWARE\Microsoft\Windows\CurrentVersion\Policies\Explorer\Run",
    "HKCU:\SOFTWARE\Microsoft\Windows\CurrentVersion\Policies\Explorer\Run"
)

# Check each key
foreach ($key in $autorunKeys) {
    if (Test-Path $key) {
        Write-Host "`n[*] Checking: $key"
        Get-ItemProperty $key -ErrorAction SilentlyContinue | 
            Format-List * -Force
    }
}
```

**Service registry keys:**

```powershell
# List all services
Get-ChildItem "HKLM:\SYSTEM\CurrentControlSet\Services" | 
    ForEach-Object {
        $serviceName = $_.PSChildName
        $imagePath = (Get-ItemProperty $_.PSPath -Name ImagePath -ErrorAction SilentlyContinue).ImagePath
        $displayName = (Get-ItemProperty $_.PSPath -Name DisplayName -ErrorAction SilentlyContinue).DisplayName
        $start = (Get-ItemProperty $_.PSPath -Name Start -ErrorAction SilentlyContinue).Start
        
        [PSCustomObject]@{
            ServiceName = $serviceName
            DisplayName = $displayName
            ImagePath = $imagePath
            StartType = $start
        }
    } | Where-Object {$_.ImagePath -ne $null} |
    Sort-Object StartType

# Find suspicious service paths
Get-ChildItem "HKLM:\SYSTEM\CurrentControlSet\Services" |
    Get-ItemProperty |
    Where-Object {
        $_.ImagePath -match "temp|appdata|users|programdata" -and
        $_.ImagePath -notmatch "system32|program files"
    } |
    Select-Object PSChildName, ImagePath, DisplayName
```

**Browser hijacking registry keys:**

```powershell
# Internet Explorer settings
Get-ItemProperty "HKCU:\Software\Microsoft\Internet Explorer\Main" |
    Select-Object "Start Page", "Search Page", "Default_Page_URL"

# Browser Helper Objects (BHOs)
Get-ChildItem "HKLM:\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\Browser Helper Objects" |
    ForEach-Object {
        $clsid = $_.PSChildName
        Get-ItemProperty "HKLM:\SOFTWARE\Classes\CLSID\$clsid" -ErrorAction SilentlyContinue |
            Select-Object @{Name='CLSID';Expression={$clsid}}, '(default)', InprocServer32
    }

# Winsock LSP (Layered Service Provider) - potential traffic interception
Get-ItemProperty "HKLM:\SYSTEM\CurrentControlSet\Services\WinSock2\Parameters" -Name Protocol_Catalog9 -ErrorAction SilentlyContinue
```

### Analyzing Registry Hives Offline

**Extracting registry hives for analysis:**

```powershell
# Export specific registry keys
reg export "HKLM\SOFTWARE\Microsoft\Windows\CurrentVersion\Run" run_key.reg

# Export entire hive
reg save HKLM\SOFTWARE software.hiv
reg save HKLM\SYSTEM system.hiv
reg save HKU\.DEFAULT default.hiv

# Copy SAM, SYSTEM, SOFTWARE from offline system
# From Volume Shadow Copy or mounted disk
Copy-Item "\\?\GLOBALROOT\Device\HarddiskVolumeShadowCopy1\Windows\System32\config\SAM" .
Copy-Item "\\?\GLOBALROOT\Device\HarddiskVolumeShadowCopy1\Windows\System32\config\SYSTEM" .
Copy-Item "\\?\GLOBALROOT\Device\HarddiskVolumeShadowCopy1\Windows\System32\config\SOFTWARE" .
```

**Using RegRipper for analysis:**

```bash
# Install RegRipper
git clone https://github.com/keydet89/RegRipper3.0.git
cd RegRipper3.0

# Run specific plugin
perl rip.pl -r /path/to/SOFTWARE -p soft_run

# Run all plugins
perl rip.pl -r /path/to/SOFTWARE -a

# Parse SYSTEM hive for services
perl rip.pl -r /path/to/SYSTEM -p services

# Parse SAM for user accounts
perl rip.pl -r /path/to/SAM -p samparse

# Generate comprehensive report
perl rip.pl -r /path/to/NTUSER.DAT -a > ntuser_analysis.txt
```

**Python registry parsing with python-registry:**

```python
from Registry import Registry
import sys

def analyze_run_keys(software_hive_path):
    """Extract and analyze Run keys from SOFTWARE hive"""
    reg = Registry.Registry(software_hive_path)
    
    run_paths = [
        "Microsoft\\Windows\\CurrentVersion\\Run",
        "Microsoft\\Windows\\CurrentVersion\\RunOnce",
        "WOW6432Node\\Microsoft\\Windows\\CurrentVersion\\Run"
    ]
    
    findings = []
    
    for path in run_paths:
        try:
            key = reg.open(path)
            print(f"\n[*] Analyzing: {path}")
            
            for value in key.values():
                entry = {
                    'key_path': path,
                    'value_name': value.name(),
                    'value_data': value.value(),
                    'value_type': value.value_type_str()
                }
                
                # Check for suspicious indicators
                suspicious = False
                reasons = []
                
                data = str(value.value()).lower()
                
                if any(x in data for x in ['temp', 'appdata', 'programdata']):
                    suspicious = True
                    reasons.append('suspicious_path')
                
                if any(x in data for x in ['.exe', '.dll', '.bat', '.cmd', '.ps1']):
                    if 'system32' not in data and 'program files' not in data:
                        suspicious = True
                        reasons.append('non_standard_location')
                
                if suspicious:
                    entry['suspicious'] = True
                    entry['reasons'] = reasons
                    findings.append(entry)
                
                print(f"  {value.name()}: {value.value()}")
                if suspicious:
                    print(f"    [!] SUSPICIOUS: {', '.join(reasons)}")
        
        except Registry.RegistryKeyNotFoundException:
            print(f"  Key not found: {path}")
    
    return findings

def analyze_services(system_hive_path):
    """Extract service configurations from SYSTEM hive"""
    reg = Registry.Registry(system_hive_path)
    
    # Determine current control set
    try:
        select_key = reg.open("Select")
        current = select_key.value("Current").value()
        control_set = f"ControlSet{current:03d}"
    except:
        control_set = "ControlSet001"
    
    services_path = f"{control_set}\\Services"
    
    try:
        services_key = reg.open(services_path)
        
        for service in services_key.subkeys():
            try:
                image_path = service.value("ImagePath").value()
                display_name = service.value("DisplayName").value() if service.value("DisplayName") else ""
                start_type = service.value("Start").value()
                
                # Check for suspicious characteristics
                if isinstance(image_path, str):
                    image_lower = image_path.lower()
                    
                    if any(x in image_lower for x in ['temp', 'appdata', 'programdata']):
                        if 'system32' not in image_lower:
                            print(f"\n[!] Suspicious service: {service.name()}")
                            print(f"    Display Name: {display_name}")
                            print(f"    Image Path: {image_path}")
                            print(f"    Start Type: {start_type}")
            
            except Registry.RegistryValueNotFoundException:
                pass
    
    except Registry.RegistryKeyNotFoundException:
        print(f"Services key not found: {services_path}")

# Usage
if __name__ == "__main__":
    print("[*] Analyzing SOFTWARE hive...")
    findings = analyze_run_keys("SOFTWARE")
    
    print("\n[*] Analyzing SYSTEM hive...")
    analyze_services("SYSTEM")
```

### Detecting Registry Modifications via Event Logs

**Parsing registry-related events from EVTX:**

```python
import evtx
from datetime import datetime

def parse_registry_events(evtx_path):
    """Parse Security.evtx for registry modifications"""
    registry_events = []
    
    with evtx.Evtx(evtx_path) as log:
        for record in log.records():
            try:
                event_id = record.event_id()
                
                # Event ID 4657 - Registry value modification
                if event_id == 4657:
                    xml_data = record.xml()
                    
                    # Parse XML to extract details
                    # [Inference] Specific parsing depends on XML structure
                    
                    registry_events.append({
                        'event_id': event_id,
                        'timestamp': record.timestamp(),
                        'record_num': record.record_num(),
                        'data': xml_data
                    })
            
            except Exception as e:
                continue
    
    return registry_events

# Usage
events = parse_registry_events('Security.evtx')
print(f"Found {len(events)} registry modification events")
```

## Scheduled Task Logs

Scheduled tasks are common malware persistence mechanisms on both Windows and Linux.

### Windows Scheduled Tasks

**Analyzing Task Scheduler logs:**

```powershell
# Task Scheduler operational log
Get-WinEvent -LogName "Microsoft-Windows-TaskScheduler/Operational" |
    Select-Object TimeCreated, ID, Message |
    Format-Table -AutoSize

# Task registered/created (Event ID 106)
Get-WinEvent -FilterHashtable @{
    LogName='Microsoft-Windows-TaskScheduler/Operational'
    ID=106
} | ForEach-Object {
    [xml]$xml = $_.ToXml()
    [PSCustomObject]@{
        TimeCreated = $_.TimeCreated
        TaskName = $xml.Event.EventData.Data.'#text'[0] User = $xml.Event.EventData.Data.'#text'[1] } } | Format-Table -AutoSize

# Task execution started (Event ID 129)
Get-WinEvent -FilterHashtable @{ LogName='Microsoft-Windows-TaskScheduler/Operational' ID=129 } | Select-Object TimeCreated, Message -First 20

# Task execution completed (Event ID 102)
Get-WinEvent -FilterHashtable @{ LogName='Microsoft-Windows-TaskScheduler/Operational' ID=102 } | Select-Object TimeCreated, Message -First 20

# Task registration updated (Event ID 140)
Get-WinEvent -FilterHashtable @{ LogName='Microsoft-Windows-TaskScheduler/Operational' ID=140 } | Select-Object TimeCreated, Message

# Failed task execution (Event ID 111)
Get-WinEvent -FilterHashtable @{ LogName='Microsoft-Windows-TaskScheduler/Operational' ID=111 } | Select-Object TimeCreated, Message
````

**Event ID reference for Task Scheduler:**

```powershell
# Key Event IDs
# 106 - Task registered
# 140 - Task updated
# 141 - Task deleted
# 200 - Task executed
# 201 - Task completed successfully
# 129 - Task started (action)
# 102 - Task completed
# 103 - Task failed to start
# 111 - Task Scheduler failed to start task
# 322 - Launch request ignored (instance already running)
````

**Enumerating all scheduled tasks:**

```powershell
# List all scheduled tasks
Get-ScheduledTask | Select-Object TaskName, TaskPath, State, Author |
    Format-Table -AutoSize

# Get detailed task information
Get-ScheduledTask | Get-ScheduledTaskInfo |
    Select-Object TaskName, LastRunTime, LastTaskResult, NextRunTime |
    Format-Table -AutoSize

# Find tasks in suspicious locations
Get-ScheduledTask | Where-Object {
    $_.TaskPath -notlike "\Microsoft\*"
} | Select-Object TaskName, TaskPath, State

# Find tasks with suspicious actions
Get-ScheduledTask | ForEach-Object {
    $task = $_
    $actions = $task.Actions
    
    foreach ($action in $actions) {
        if ($action.Execute) {
            $exe = $action.Execute.ToLower()
            $args = $action.Arguments
            
            # Check for suspicious patterns
            if ($exe -match "powershell|cmd|wscript|cscript|mshta|rundll32|regsvr32") {
                [PSCustomObject]@{
                    TaskName = $task.TaskName
                    TaskPath = $task.TaskPath
                    Execute = $action.Execute
                    Arguments = $action.Arguments
                    State = $task.State
                }
            }
        }
    }
} | Format-Table -AutoSize

# Export all tasks to XML
$tasks = Get-ScheduledTask
foreach ($task in $tasks) {
    $taskName = $task.TaskName -replace '[\\/:*?"<>|]', '_'
    Export-ScheduledTask -TaskName $task.TaskName -TaskPath $task.TaskPath |
        Out-File "tasks\${taskName}.xml"
}
```

**Analyzing task XML definitions:**

```powershell
function Analyze-ScheduledTask {
    param([string]$TaskName, [string]$TaskPath = "\")
    
    $task = Get-ScheduledTask -TaskName $TaskName -TaskPath $TaskPath
    $info = Get-ScheduledTaskInfo -TaskName $TaskName -TaskPath $TaskPath
    
    Write-Host "`n[*] Task: $TaskPath$TaskName"
    Write-Host "State: $($task.State)"
    Write-Host "Author: $($task.Author)"
    Write-Host "Last Run: $($info.LastRunTime)"
    Write-Host "Next Run: $($info.NextRunTime)"
    Write-Host "Last Result: $($info.LastTaskResult)"
    
    Write-Host "`n[*] Actions:"
    foreach ($action in $task.Actions) {
        Write-Host "  Execute: $($action.Execute)"
        Write-Host "  Arguments: $($action.Arguments)"
        Write-Host "  WorkingDirectory: $($action.WorkingDirectory)"
        
        # Suspicious indicators
        $suspicious = @()
        
        $exe = $action.Execute.ToLower()
        if ($exe -match "temp|appdata|programdata") {
            $suspicious += "suspicious_path"
        }
        
        if ($exe -match "powershell|cmd|wscript") {
            if ($action.Arguments -match "hidden|bypass|encodedcommand|downloadstring") {
                $suspicious += "obfuscated_command"
            }
        }
        
        if ($suspicious.Count -gt 0) {
            Write-Host "  [!] SUSPICIOUS: $($suspicious -join ', ')" -ForegroundColor Red
        }
    }
    
    Write-Host "`n[*] Triggers:"
    foreach ($trigger in $task.Triggers) {
        Write-Host "  Type: $($trigger.CimClass.CimClassName)"
        if ($trigger.StartBoundary) {
            Write-Host "  Start: $($trigger.StartBoundary)"
        }
        if ($trigger.Repetition) {
            Write-Host "  Interval: $($trigger.Repetition.Interval)"
        }
    }
    
    Write-Host "`n[*] Principal:"
    Write-Host "  User: $($task.Principal.UserId)"
    Write-Host "  LogonType: $($task.Principal.LogonType)"
    Write-Host "  RunLevel: $($task.Principal.RunLevel)"
}

# Analyze specific task
Analyze-ScheduledTask -TaskName "SuspiciousTask" -TaskPath "\"

# Analyze all non-Microsoft tasks
Get-ScheduledTask | Where-Object {
    $_.TaskPath -notlike "\Microsoft\*"
} | ForEach-Object {
    Analyze-ScheduledTask -TaskName $_.TaskName -TaskPath $_.TaskPath
}
```

**Detecting hidden or malicious tasks:**

```powershell
# Find tasks with hidden attribute
Get-ScheduledTask | Where-Object {
    $_.Settings.Hidden -eq $true
} | Select-Object TaskName, TaskPath, State, Author

# Tasks running as SYSTEM
Get-ScheduledTask | Where-Object {
    $_.Principal.UserId -eq "SYSTEM" -and
    $_.TaskPath -notlike "\Microsoft\*"
} | Select-Object TaskName, TaskPath, State

# Tasks with no triggers (manually executed only)
Get-ScheduledTask | Where-Object {
    $_.Triggers.Count -eq 0
} | Select-Object TaskName, TaskPath, State

# Recently created tasks
Get-ScheduledTask | Where-Object {
    $_.Date -gt (Get-Date).AddDays(-7)
} | Select-Object TaskName, TaskPath, Date, Author

# Tasks executing from user directories
Get-ScheduledTask | ForEach-Object {
    $task = $_
    foreach ($action in $task.Actions) {
        if ($action.Execute -match "C:\\Users\\") {
            [PSCustomObject]@{
                TaskName = $task.TaskName
                Execute = $action.Execute
                Arguments = $action.Arguments
            }
        }
    }
}
```

**Parsing Task Scheduler event logs with Python:**

```python
import evtx
from xml.etree import ElementTree as ET
from datetime import datetime

def parse_task_scheduler_events(evtx_path):
    """Parse Task Scheduler Operational log"""
    events = {
        'created': [],
        'executed': [],
        'deleted': [],
        'modified': []
    }
    
    with evtx.Evtx(evtx_path) as log:
        for record in log.records():
            try:
                event_id = record.event_id()
                timestamp = record.timestamp()
                
                # Parse XML
                xml_str = record.xml()
                root = ET.fromstring(xml_str)
                
                # Extract event data
                ns = {'ns': 'http://schemas.microsoft.com/win/2004/08/events/event'}
                event_data = root.find('.//ns:EventData', ns)
                
                if event_data is not None:
                    data_dict = {}
                    for data in event_data.findall('ns:Data', ns):
                        name = data.get('Name')
                        value = data.text
                        if name:
                            data_dict[name] = value
                    
                    # Categorize by event ID
                    if event_id == 106:  # Task registered
                        events['created'].append({
                            'timestamp': timestamp,
                            'task_name': data_dict.get('TaskName', ''),
                            'user': data_dict.get('UserContext', '')
                        })
                    
                    elif event_id in [129, 200]:  # Task executed
                        events['executed'].append({
                            'timestamp': timestamp,
                            'task_name': data_dict.get('TaskName', ''),
                            'action': data_dict.get('ActionName', '')
                        })
                    
                    elif event_id == 141:  # Task deleted
                        events['deleted'].append({
                            'timestamp': timestamp,
                            'task_name': data_dict.get('TaskName', ''),
                            'user': data_dict.get('UserContext', '')
                        })
                    
                    elif event_id == 140:  # Task updated
                        events['modified'].append({
                            'timestamp': timestamp,
                            'task_name': data_dict.get('TaskName', ''),
                            'user': data_dict.get('UserContext', '')
                        })
            
            except Exception as e:
                continue
    
    return events

# Usage
events = parse_task_scheduler_events('Microsoft-Windows-TaskScheduler%4Operational.evtx')

print(f"Tasks Created: {len(events['created'])}")
print(f"Tasks Executed: {len(events['executed'])}")
print(f"Tasks Deleted: {len(events['deleted'])}")
print(f"Tasks Modified: {len(events['modified'])}")

# Show recently created tasks
print("\n[*] Recently Created Tasks:")
for task in events['created'][-10:]:
    print(f"  {task['timestamp']}: {task['task_name']} (by {task['user']})")
```

### Linux Cron Job Analysis

**Analyzing cron logs:**

```bash
# View cron execution logs
sudo grep CRON /var/log/syslog

# Filter cron job executions
sudo grep CRON /var/log/syslog | grep CMD

# Find cron jobs by specific user
sudo grep CRON /var/log/syslog | grep "(www-data)"

# Recent cron activity
sudo grep CRON /var/log/syslog | tail -n 50

# Cron errors
sudo grep CRON /var/log/syslog | grep -i error

# Extract unique commands executed via cron
sudo grep "CRON.*CMD" /var/log/syslog | awk -F'CMD ' '{print $2}' | sort -u
```

**Enumerating all cron jobs:**

```bash
# System-wide crontabs
sudo cat /etc/crontab
sudo ls -la /etc/cron.d/
sudo ls -la /etc/cron.hourly/
sudo ls -la /etc/cron.daily/
sudo ls -la /etc/cron.weekly/
sudo ls -la /etc/cron.monthly/

# User crontabs
sudo ls -la /var/spool/cron/crontabs/

# List all user cron jobs
for user in $(cut -f1 -d: /etc/passwd); do
    echo "[*] Crontab for $user:"
    sudo crontab -u $user -l 2>/dev/null
done

# Check for suspicious cron files
find /etc/cron* -type f -ls
find /var/spool/cron -type f -ls
```

**Searching for malicious cron patterns:**

```bash
# Find cron jobs with network activity
sudo grep -r "curl\|wget\|nc\|ncat" /etc/cron* /var/spool/cron/

# Find cron jobs downloading and executing
sudo grep -r "curl.*sh\|wget.*sh\|bash.*http" /etc/cron* /var/spool/cron/

# Find cron jobs with encoded commands
sudo grep -r "base64\|xxd\|openssl enc" /etc/cron* /var/spool/cron/

# Find reverse shell patterns
sudo grep -r "sh -i\|bash -i\|/dev/tcp\|mkfifo" /etc/cron* /var/spool/cron/

# Hidden cron files
sudo find /etc/cron* /var/spool/cron -name ".*"

# Recently modified cron files
sudo find /etc/cron* /var/spool/cron -type f -mtime -7 -ls
```

**Comprehensive cron analysis script:**

```bash
#!/bin/bash
# cron_malware_detector.sh

echo "[*] Analyzing cron configuration for malware indicators"
echo "======================================================"

# Function to check cron entry
check_cron_entry() {
    local entry="$1"
    local location="$2"
    local suspicious=0
    
    # Suspicious patterns
    if echo "$entry" | grep -qiE "(curl|wget|nc|ncat|socat)"; then
        echo "  [!] Network activity: $entry"
        suspicious=1
    fi
    
    if echo "$entry" | grep -qiE "(bash -i|sh -i|/dev/tcp|mkfifo)"; then
        echo "  [!] Reverse shell pattern: $entry"
        suspicious=1
    fi
    
    if echo "$entry" | grep -qiE "(base64|xxd|openssl enc)"; then
        echo "  [!] Encoded/obfuscated command: $entry"
        suspicious=1
    fi
    
    if echo "$entry" | grep -qiE "/tmp|/var/tmp|/dev/shm"; then
        echo "  [!] Execution from temp directory: $entry"
        suspicious=1
    fi
    
    if echo "$entry" | grep -qE "^\." || echo "$location" | grep -qE "/\."; then
        echo "  [!] Hidden file: $entry"
        suspicious=1
    fi
    
    return $suspicious
}

# Check system crontab
echo -e "\n[*] Checking /etc/crontab"
if [ -f /etc/crontab ]; then
    while IFS= read -r line; do
        if [[ ! "$line" =~ ^#.* ]] && [ -n "$line" ]; then
            check_cron_entry "$line" "/etc/crontab"
        fi
    done < /etc/crontab
fi

# Check cron directories
for dir in /etc/cron.d /etc/cron.hourly /etc/cron.daily /etc/cron.weekly /etc/cron.monthly; do
    if [ -d "$dir" ]; then
        echo -e "\n[*] Checking $dir"
        find "$dir" -type f | while read -r file; do
            echo "  File: $file"
            while IFS= read -r line; do
                if [[ ! "$line" =~ ^#.* ]] && [ -n "$line" ]; then
                    check_cron_entry "$line" "$file"
                fi
            done < "$file"
        done
    fi
done

# Check user crontabs
echo -e "\n[*] Checking user crontabs"
if [ -d /var/spool/cron/crontabs ]; then
    find /var/spool/cron/crontabs -type f | while read -r file; do
        user=$(basename "$file")
        echo "  User: $user"
        while IFS= read -r line; do
            if [[ ! "$line" =~ ^#.* ]] && [ -n "$line" ]; then
                check_cron_entry "$line" "$file"
            fi
        done < "$file"
    done
fi

# Check for hidden cron files
echo -e "\n[*] Checking for hidden cron files"
find /etc/cron* /var/spool/cron -name ".*" 2>/dev/null | while read -r file; do
    echo "  [!] Hidden file found: $file"
done

# Check recent modifications
echo -e "\n[*] Recently modified cron files (last 7 days)"
find /etc/cron* /var/spool/cron -type f -mtime -7 2>/dev/null | while read -r file; do
    mod_time=$(stat -c %y "$file" 2>/dev/null || stat -f %Sm "$file")
    echo "  $mod_time - $file"
done

echo -e "\n[*] Analysis complete"
```

### Systemd Timer Analysis (Modern Linux)

Systemd timers are increasingly used as cron alternatives and persistence mechanisms.

**Listing systemd timers:**

```bash
# List all timers
systemctl list-timers --all

# Show detailed timer information
systemctl list-timers --all --no-pager

# Check specific timer
systemctl status timer_name.timer

# Show timer unit file
systemctl cat timer_name.timer

# Find all timer files
find /etc/systemd/system /usr/lib/systemd/system -name "*.timer"
```

**Analyzing timer configurations:**

```bash
# Extract timer details
for timer in $(systemctl list-unit-files --type=timer --no-pager | awk '{print $1}'); do
    echo "[*] Timer: $timer"
    systemctl cat "$timer" 2>/dev/null
    echo ""
done

# Find timers not from packages
find /etc/systemd/system -name "*.timer" -type f | while read -r timer; do
    echo "[*] Custom timer: $timer"
    cat "$timer"
    
    # Get associated service
    service_name=$(basename "$timer" .timer).service
    if [ -f "/etc/systemd/system/$service_name" ]; then
        echo "[*] Associated service: $service_name"
        cat "/etc/systemd/system/$service_name"
    fi
    echo ""
done

# Recently modified timers
find /etc/systemd/system /usr/lib/systemd/system -name "*.timer" -mtime -7 -ls
```

**Checking timer execution logs:**

```bash
# View timer activity in journal
journalctl -u timer_name.timer

# All timer-related events
journalctl -t systemd --no-pager | grep -i timer

# Timer activation events
journalctl --no-pager | grep "Started.*timer"

# Failed timer activations
journalctl -p err --no-pager | grep timer

# Recent timer activity
journalctl --since "1 hour ago" --no-pager | grep -i timer
```

**Detecting malicious timers:**

```bash
#!/bin/bash
# systemd_timer_checker.sh

echo "[*] Analyzing systemd timers for malware indicators"

systemctl list-unit-files --type=timer --no-pager | awk '{print $1}' | while read -r timer; do
    if [ -z "$timer" ] || [ "$timer" = "UNIT" ]; then
        continue
    fi
    
    # Get timer configuration
    timer_content=$(systemctl cat "$timer" 2>/dev/null)
    
    # Get associated service
    service_name=$(echo "$timer" | sed 's/\.timer$/\.service/')
    service_content=$(systemctl cat "$service_name" 2>/dev/null)
    
    suspicious=0
    reasons=()
    
    # Check for suspicious patterns in service
    if echo "$service_content" | grep -qiE "(curl|wget|nc|ncat|bash -i)"; then
        suspicious=1
        reasons+=("network_activity")
    fi
    
    if echo "$service_content" | grep -qiE "/tmp|/var/tmp|/dev/shm"; then
        suspicious=1
        reasons+=("temp_directory_execution")
    fi
    
    if echo "$service_content" | grep -qiE "Type=oneshot.*RemainAfterExit=no"; then
        # Common for cleanup/hiding
        suspicious=1
        reasons+=("ephemeral_service")
    fi
    
    # Check if timer/service is in user space
    if echo "$timer_content" | grep -q "/home/"; then
        suspicious=1
        reasons+=("user_space_timer")
    fi
    
    if [ $suspicious -eq 1 ]; then
        echo ""
        echo "[!] SUSPICIOUS TIMER: $timer"
        echo "    Reasons: ${reasons[*]}"
        echo "    Service: $service_name"
        echo "--- Timer Config ---"
        echo "$timer_content"
        echo "--- Service Config ---"
        echo "$service_content"
    fi
done
```

### At Jobs (Legacy Scheduling)

**Checking at jobs:**

```bash
# List all at jobs
atq

# Show specific job details
at -c job_number

# List jobs for specific user
sudo atq -u username

# Check at spool directory
sudo ls -la /var/spool/cron/atjobs/

# View at log
sudo grep "at" /var/log/syslog
```

**Finding suspicious at jobs:**

```bash
# List all at jobs with details
for job in $(atq | awk '{print $1}'); do
    echo "[*] Job ID: $job"
    at -c "$job" | tail -n 20
    echo ""
done

# Check for recent at jobs
sudo find /var/spool/cron/atjobs -type f -mtime -7 -exec cat {} \;

# Search for network activity in at jobs
sudo grep -r "curl\|wget\|nc" /var/spool/cron/atjobs/
```

### Cross-Platform Scheduled Task Indicators

**Common malware scheduling patterns:**

```bash
# Patterns to look for across all scheduling mechanisms:

# 1. Frequent execution (every minute/5 minutes)
# 2. Execution during off-hours (2-4 AM)
# 3. Network download/upload commands
# 4. Reverse shell establishment
# 5. Base64 or otherwise encoded commands
# 6. Execution from temporary directories
# 7. Hidden or obfuscated file names
# 8. Privilege escalation attempts
# 9. Credential harvesting scripts
# 10. Lateral movement tools (psexec, wmic, ssh)
```

**Unified analysis approach:**

```python
import os
import re
import subprocess
from datetime import datetime

class ScheduledTaskAnalyzer:
    def __init__(self):
        self.suspicious_patterns = [
            r'(curl|wget|nc|ncat|socat)',
            r'(bash -i|sh -i|/dev/tcp|mkfifo)',
            r'(base64|xxd|openssl enc)',
            r'(/tmp|/var/tmp|/dev/shm)',
            r'(powershell.*-enc|powershell.*-e\s)',
            r'(downloadstring|invoke-expression|iex)',
            r'(mimikatz|procdump|psexec)',
        ]
    
    def is_suspicious(self, command):
        """Check if command contains suspicious patterns"""
        for pattern in self.suspicious_patterns:
            if re.search(pattern, command, re.IGNORECASE):
                return True, pattern
        return False, None
    
    def analyze_linux_cron(self):
        """Analyze Linux cron jobs"""
        findings = []
        
        # System crontab
        if os.path.exists('/etc/crontab'):
            with open('/etc/crontab', 'r') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        suspicious, pattern = self.is_suspicious(line)
                        if suspicious:
                            findings.append({
                                'type': 'cron',
                                'location': '/etc/crontab',
                                'command': line,
                                'pattern': pattern
                            })
        
        # User crontabs
        cron_dir = '/var/spool/cron/crontabs'
        if os.path.exists(cron_dir):
            for user_cron in os.listdir(cron_dir):
                path = os.path.join(cron_dir, user_cron)
                try:
                    with open(path, 'r') as f:
                        for line in f:
                            line = line.strip()
                            if line and not line.startswith('#'):
                                suspicious, pattern = self.is_suspicious(line)
                                if suspicious:
                                    findings.append({
                                        'type': 'cron',
                                        'location': f'user:{user_cron}',
                                        'command': line,
                                        'pattern': pattern
                                    })
                except:
                    pass
        
        return findings
    
    def analyze_systemd_timers(self):
        """Analyze systemd timers"""
        findings = []
        
        try:
            # List all timers
            result = subprocess.run(
                ['systemctl', 'list-unit-files', '--type=timer', '--no-pager'],
                capture_output=True,
                text=True
            )
            
            for line in result.stdout.split('\n'):
                if '.timer' in line:
                    timer_name = line.split()[0]
                    
                    # Get service content
                    service_name = timer_name.replace('.timer', '.service')
                    service_result = subprocess.run(
                        ['systemctl', 'cat', service_name],
                        capture_output=True,
                        text=True
                    )
                    
                    suspicious, pattern = self.is_suspicious(service_result.stdout)
                    if suspicious:
                        findings.append({
                            'type': 'systemd_timer',
                            'timer': timer_name,
                            'service': service_name,
                            'pattern': pattern
                        })
        except:
            pass
        
        return findings
    
    def generate_report(self):
        """Generate comprehensive report"""
        print("[*] Scheduled Task Malware Analysis Report")
        print("=" * 60)
        print(f"Timestamp: {datetime.now()}\n")
        
        all_findings = []
        
        # Analyze Linux scheduling
        if os.name == 'posix':
            print("[*] Analyzing cron jobs...")
            cron_findings = self.analyze_linux_cron()
            all_findings.extend(cron_findings)
            print(f"    Found {len(cron_findings)} suspicious cron entries")
            
            print("[*] Analyzing systemd timers...")
            timer_findings = self.analyze_systemd_timers()
            all_findings.extend(timer_findings)
            print(f"    Found {len(timer_findings)} suspicious timers")
        
        # Display findings
        if all_findings:
            print(f"\n[!] Total Suspicious Findings: {len(all_findings)}\n")
            for i, finding in enumerate(all_findings, 1):
                print(f"Finding #{i}:")
                for key, value in finding.items():
                    print(f"  {key}: {value}")
                print()
        else:
            print("\n[+] No suspicious scheduled tasks detected")

# Usage
if __name__ == "__main__":
    analyzer = ScheduledTaskAnalyzer()
    analyzer.generate_report()
```

---

**Related topics:** Windows persistence mechanisms (DLL hijacking, COM hijacking, WMI event consumers), Linux persistence techniques (LD_PRELOAD, PAM backdoors, kernel modules), behavioral analysis of malware execution patterns, YARA rules for malware detection in logs, memory forensics correlation with log artifacts.

---

# Web Application Attack Logs

## OWASP Top 10 Indicators

### A01:2021  Broken Access Control

#### Detection Patterns

**Horizontal Privilege Escalation**

```python
#!/usr/bin/env python3
import re
from collections import defaultdict

class AccessControlDetector:
    """Detect broken access control attempts"""
    
    def __init__(self):
        self.user_access_patterns = defaultdict(set)
        self.suspicious_patterns = []
        
        # Patterns indicating access control testing
        self.access_patterns = {
            'user_id_manipulation': re.compile(r'[?&](?:user_?id|uid|account)=(\d+)', re.I),
            'profile_access': re.compile(r'/(?:profile|account|user)/(\d+|[a-z0-9]+)', re.I),
            'admin_path': re.compile(r'/(?:admin|administrator|manage|dashboard|console)', re.I),
            'direct_object_ref': re.compile(r'[?&](?:file|doc|id|object)=([^&\s]+)', re.I),
            'role_manipulation': re.compile(r'[?&](?:role|privilege|access|level|admin)=', re.I),
        }
    
    def parse_log_entry(self, line):
        """Extract user session and accessed resource"""
        # Apache/Nginx format
        pattern = r'([\d.]+) .* \[([^\]]+)\] "(\w+) ([^"]+) [^"]+" (\d+) (\d+)(?: "[^"]*" "([^"]*)")?'
        match = re.search(pattern, line)
        
        if match:
            return {
                'ip': match.group(1),
                'timestamp': match.group(2),
                'method': match.group(3),
                'url': match.group(4),
                'status': int(match.group(5)),
                'size': int(match.group(6)),
                'user_agent': match.group(7) if match.lastindex >= 7 else ''
            }
        return None
    
    def detect_horizontal_escalation(self, entry):
        """Detect attempts to access other users' resources"""
        if not entry:
            return None
        
        ip = entry['ip']
        url = entry['url']
        
        # Extract user/object IDs from URL
        user_ids = []
        
        for pattern_name, pattern in self.access_patterns.items():
            matches = pattern.findall(url)
            if matches:
                for match in matches:
                    user_ids.append((pattern_name, match))
        
        if user_ids:
            # Track which IDs this IP has accessed
            for pattern_name, user_id in user_ids:
                self.user_access_patterns[ip].add((pattern_name, user_id))
            
            # Flag if accessing many different user IDs
            unique_ids = len(self.user_access_patterns[ip])
            if unique_ids > 5:  # Threshold for suspicious behavior
                return {
                    'type': 'horizontal_privilege_escalation',
                    'ip': ip,
                    'url': url,
                    'unique_resources_accessed': unique_ids,
                    'status': entry['status']
                }
        
        return None
    
    def detect_vertical_escalation(self, entry):
        """Detect attempts to access admin/privileged functions"""
        if not entry:
            return None
        
        url = entry['url']
        
        # Check for admin path access
        if self.access_patterns['admin_path'].search(url):
            # Suspicious if not authenticated (401/403) or successful (200)
            if entry['status'] in [200, 302]:
                return {
                    'type': 'vertical_privilege_escalation',
                    'severity': 'critical',
                    'ip': entry['ip'],
                    'url': url,
                    'status': entry['status'],
                    'reason': 'Successful admin path access'
                }
            elif entry['status'] in [401, 403]:
                return {
                    'type': 'vertical_privilege_escalation',
                    'severity': 'warning',
                    'ip': entry['ip'],
                    'url': url,
                    'status': entry['status'],
                    'reason': 'Attempted admin path access'
                }
        
        # Check for role manipulation
        if self.access_patterns['role_manipulation'].search(url):
            return {
                'type': 'role_manipulation_attempt',
                'severity': 'high',
                'ip': entry['ip'],
                'url': url,
                'status': entry['status']
            }
        
        return None
    
    def detect_idor(self, entry):
        """Detect Insecure Direct Object Reference attempts"""
        if not entry:
            return None
        
        url = entry['url']
        
        # Look for sequential ID access patterns
        match = self.access_patterns['direct_object_ref'].search(url)
        if match:
            object_ref = match.group(1)
            
            # Check if accessing numeric IDs sequentially
            if object_ref.isdigit():
                ip = entry['ip']
                # Store pattern for this IP
                if not hasattr(self, 'idor_tracking'):
                    self.idor_tracking = defaultdict(list)
                
                self.idor_tracking[ip].append(int(object_ref))
                
                # Check for sequential access (enumeration)
                if len(self.idor_tracking[ip]) >= 3:
                    recent_ids = sorted(self.idor_tracking[ip][-10:])
                    # Check if mostly sequential
                    sequential_count = sum(
                        1 for i in range(len(recent_ids)-1)
                        if recent_ids[i+1] - recent_ids[i] <= 2
                    )
                    
                    if sequential_count >= len(recent_ids) - 2:
                        return {
                            'type': 'idor_enumeration',
                            'severity': 'high',
                            'ip': ip,
                            'url': url,
                            'pattern': 'Sequential ID access',
                            'ids_accessed': len(self.idor_tracking[ip])
                        }
        
        return None
    
    def analyze_log(self, logfile):
        """Analyze log file for access control violations"""
        findings = []
        
        with open(logfile, 'r', encoding='utf-8', errors='ignore') as f:
            for line_num, line in enumerate(f, 1):
                entry = self.parse_log_entry(line)
                if not entry:
                    continue
                
                # Run all detection methods
                horizontal = self.detect_horizontal_escalation(entry)
                if horizontal:
                    horizontal['line'] = line_num
                    findings.append(horizontal)
                
                vertical = self.detect_vertical_escalation(entry)
                if vertical:
                    vertical['line'] = line_num
                    findings.append(vertical)
                
                idor = self.detect_idor(entry)
                if idor:
                    idor['line'] = line_num
                    findings.append(idor)
        
        return findings
    
    def report(self, findings):
        """Generate report of access control violations"""
        if not findings:
            print("[+] No access control violations detected")
            return
        
        print(f"[!] ACCESS CONTROL VIOLATIONS DETECTED: {len(findings)}\n")
        
        # Group by type
        by_type = defaultdict(list)
        for finding in findings:
            by_type[finding['type']].append(finding)
        
        for attack_type, instances in by_type.items():
            print(f"\n[{attack_type.upper().replace('_', ' ')}] - {len(instances)} instances")
            
            # Show unique IPs
            unique_ips = set(f['ip'] for f in instances)
            print(f"  Unique IPs: {len(unique_ips)}")
            
            # Show severity distribution
            severities = defaultdict(int)
            for instance in instances:
                severity = instance.get('severity', 'medium')
                severities[severity] += 1
            
            if severities:
                print(f"  Severity: ", end='')
                print(', '.join(f"{sev}: {count}" for sev, count in severities.items()))
            
            # Show sample instances
            print(f"  Sample incidents:")
            for instance in instances[:3]:
                print(f"    Line {instance['line']}: {instance['ip']} -> {instance['url'][:70]}")
                if 'reason' in instance:
                    print(f"      Reason: {instance['reason']}")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print(f"Usage: {sys.argv[0]} <access.log>")
        sys.exit(1)
    
    detector = AccessControlDetector()
    findings = detector.analyze_log(sys.argv[1])
    detector.report(findings)
```

### A02:2021  Cryptographic Failures

#### Sensitive Data Exposure Detection

```python
#!/usr/bin/env python3
import re
from collections import defaultdict

class SensitiveDataDetector:
    """Detect sensitive data exposure in logs"""
    
    def __init__(self):
        # Patterns for sensitive data
        self.patterns = {
            'credit_card': {
                'pattern': re.compile(r'\b(?:\d{4}[-\s]?){3}\d{4}\b'),
                'severity': 'critical',
                'description': 'Credit card number'
            },
            'ssn': {
                'pattern': re.compile(r'\b\d{3}-\d{2}-\d{4}\b'),
                'severity': 'critical',
                'description': 'Social Security Number'
            },
            'email': {
                'pattern': re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'),
                'severity': 'medium',
                'description': 'Email address'
            },
            'api_key': {
                'pattern': re.compile(r'(?i)(?:api[_-]?key|apikey|api[_-]?token)[:=]\s*[\'"]?([a-zA-Z0-9_-]{20,})[\'"]?'),
                'severity': 'critical',
                'description': 'API key'
            },
            'aws_key': {
                'pattern': re.compile(r'\b(AKIA[0-9A-Z]{16})\b'),
                'severity': 'critical',
                'description': 'AWS Access Key'
            },
            'password': {
                'pattern': re.compile(r'(?i)(?:password|passwd|pwd)[:=]\s*[\'"]?([^\s&\'"]{8,})[\'"]?'),
                'severity': 'critical',
                'description': 'Password in URL/parameter'
            },
            'jwt': {
                'pattern': re.compile(r'\beyJ[A-Za-z0-9_-]+\.eyJ[A-Za-z0-9_-]+\.[A-Za-z0-9_-]+\b'),
                'severity': 'high',
                'description': 'JWT token'
            },
            'private_key': {
                'pattern': re.compile(r'-----BEGIN (?:RSA |EC |OPENSSH )?PRIVATE KEY-----'),
                'severity': 'critical',
                'description': 'Private key'
            },
            'hash_md5': {
                'pattern': re.compile(r'\b[a-fA-F0-9]{32}\b'),
                'severity': 'low',
                'description': 'MD5 hash'
            },
            'bearer_token': {
                'pattern': re.compile(r'(?i)bearer\s+([a-zA-Z0-9_-]{20,})'),
                'severity': 'high',
                'description': 'Bearer token'
            }
        }
        
        # Track findings
        self.findings = []
    
    def scan_line(self, line_num, line):
        """Scan line for sensitive data"""
        detections = []
        
        for data_type, config in self.patterns.items():
            matches = config['pattern'].findall(line)
            
            if matches:
                for match in matches:
                    # Extract actual value if captured group exists
                    value = match if isinstance(match, str) else match[0] if match else ''
                    
                    # Mask sensitive values in output
                    if len(value) > 10:
                        masked = value[:4] + '*' * (len(value)-8) + value[-4:]
                    else:
                        masked = '*' * len(value)
                    
                    detections.append({
                        'line': line_num,
                        'type': data_type,
                        'severity': config['severity'],
                        'description': config['description'],
                        'value': masked,
                        'context': line[:100]
                    })
        
        return detections
    
    def analyze_log(self, logfile):
        """Analyze log for sensitive data exposure"""
        print(f"[*] Scanning {logfile} for sensitive data...")
        
        with open(logfile, 'r', encoding='utf-8', errors='ignore') as f:
            for line_num, line in enumerate(f, 1):
                detections = self.scan_line(line_num, line)
                self.findings.extend(detections)
                
                if line_num % 10000 == 0:
                    print(f"  Scanned {line_num:,} lines...", end='\r')
        
        print(f"\n[+] Scan complete: {len(self.findings)} sensitive data exposures found")
        return self.findings
    
    def report(self):
        """Generate detailed report"""
        if not self.findings:
            print("[+] No sensitive data exposures detected")
            return
        
        print(f"\n{'='*70}")
        print("SENSITIVE DATA EXPOSURE REPORT")
        print(f"{'='*70}\n")
        
        # Group by severity
        by_severity = defaultdict(list)
        for finding in self.findings:
            by_severity[finding['severity']].append(finding)
        
        severity_order = ['critical', 'high', 'medium', 'low']
        
        for severity in severity_order:
            if severity not in by_severity:
                continue
            
            findings = by_severity[severity]
            print(f"\n[{severity.upper()}] - {len(findings)} exposures")
            
            # Group by type
            by_type = defaultdict(list)
            for finding in findings:
                by_type[finding['type']].append(finding)
            
            for data_type, instances in sorted(by_type.items()):
                print(f"\n  {instances[0]['description']} ({data_type}): {len(instances)} instances")
                
                # Show unique values
                unique_values = set(f['value'] for f in instances)
                if len(unique_values) <= 5:
                    print(f"    Exposed values: {', '.join(unique_values)}")
                else:
                    print(f"    Unique values exposed: {len(unique_values)}")
                
                # Show sample lines
                print(f"    Sample occurrences:")
                for instance in instances[:2]:
                    print(f"      Line {instance['line']}: {instance['context']}")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print(f"Usage: {sys.argv[0]} <logfile>")
        sys.exit(1)
    
    detector = SensitiveDataDetector()
    detector.analyze_log(sys.argv[1])
    detector.report()
```

### A03:2021  Injection

#### SQL Injection Detection

```python
#!/usr/bin/env python3
import re
from urllib.parse import unquote, parse_qs
from collections import Counter, defaultdict

class SQLInjectionDetector:
    """Detect SQL injection attempts in web logs"""
    
    def __init__(self):
        # SQL injection patterns with severity
        self.patterns = {
            'union_based': {
                'pattern': re.compile(r'(?i)union\s+(all\s+)?select', re.I),
                'severity': 'critical',
                'technique': 'Union-based SQLi'
            },
            'error_based': {
                'pattern': re.compile(r"(?i)(?:\'|\"|%27|%22)\s*(?:or|and)\s*(?:\'|\"|%27|%22)", re.I),
                'severity': 'high',
                'technique': 'Error-based SQLi'
            },
            'boolean_based': {
                'pattern': re.compile(r"(?i)(?:\'|\")?\s*(?:or|and)\s+(?:1|true)\s*=\s*(?:1|true)", re.I),
                'severity': 'high',
                'technique': 'Boolean-based blind SQLi'
            },
            'time_based': {
                'pattern': re.compile(r'(?i)(?:sleep|benchmark|waitfor\s+delay|pg_sleep)\s*\(', re.I),
                'severity': 'high',
                'technique': 'Time-based blind SQLi'
            },
            'stacked_queries': {
                'pattern': re.compile(r'(?i);\s*(?:drop|delete|insert|update|create|alter)\s+', re.I),
                'severity': 'critical',
                'technique': 'Stacked queries'
            },
            'comment_injection': {
                'pattern': re.compile(r'(?i)(?:--|#|/\*|\*/|;%00)', re.I),
                'severity': 'medium',
                'technique': 'Comment-based SQLi'
            },
            'information_schema': {
                'pattern': re.compile(r'(?i)information_schema|sysobjects|syscolumns', re.I),
                'severity': 'critical',
                'technique': 'Schema enumeration'
            },
            'hex_encoded': {
                'pattern': re.compile(r'(?i)0x[0-9a-f]{10,}', re.I),
                'severity': 'medium',
                'technique': 'Hex-encoded SQLi'
            },
            'version_fingerprint': {
                'pattern': re.compile(r'(?i)@@version|version\(\)|@@global|user\(\)', re.I),
                'severity': 'high',
                'technique': 'Database fingerprinting'
            },
            'load_file': {
                'pattern': re.compile(r'(?i)load_file|into\s+outfile|into\s+dumpfile', re.I),
                'severity': 'critical',
                'technique': 'File operation SQLi'
            },
            'exec_command': {
                'pattern': re.compile(r'(?i)exec(?:ute)?\s*\(|xp_cmdshell', re.I),
                'severity': 'critical',
                'technique': 'Command execution via SQL'
            }
        }
        
        self.findings = []
        self.ip_stats = defaultdict(lambda: defaultdict(int))
    
    def decode_url(self, url):
        """Decode URL-encoded strings multiple times"""
        decoded = url
        for _ in range(3):  # Decode up to 3 times for double/triple encoding
            try:
                new_decoded = unquote(decoded)
                if new_decoded == decoded:
                    break
                decoded = new_decoded
            except:
                break
        return decoded
    
    def extract_parameters(self, url):
        """Extract URL parameters and values"""
        params = {}
        
        # Split URL and query string
        if '?' in url:
            path, query = url.split('?', 1)
            params['path'] = path
            
            try:
                parsed = parse_qs(query)
                for key, values in parsed.items():
                    params[key] = ' '.join(values)
            except:
                # Manual parsing if parse_qs fails
                for param in query.split('&'):
                    if '=' in param:
                        key, value = param.split('=', 1)
                        params[key] = value
        else:
            params['path'] = url
        
        return params
    
    def detect_sqli(self, line_num, entry):
        """Detect SQL injection attempts"""
        if not entry:
            return []
        
        url = entry['url']
        decoded_url = self.decode_url(url)
        
        detections = []
        
        # Check full URL and each parameter
        test_strings = [decoded_url]
        params = self.extract_parameters(decoded_url)
        test_strings.extend(params.values())
        
        for test_str in test_strings:
            for pattern_name, config in self.patterns.items():
                if config['pattern'].search(test_str):
                    detection = {
                        'line': line_num,
                        'ip': entry['ip'],
                        'url': url[:100],
                        'decoded_url': decoded_url[:100],
                        'pattern': pattern_name,
                        'severity': config['severity'],
                        'technique': config['technique'],
                        'status': entry['status'],
                        'method': entry['method']
                    }
                    
                    detections.append(detection)
                    
                    # Track by IP
                    self.ip_stats[entry['ip']]['total'] += 1
                    self.ip_stats[entry['ip']][pattern_name] += 1
        
        return detections
    
    def parse_log_entry(self, line):
        """Parse Apache/Nginx log entry"""
        pattern = r'([\d.]+) .* \[([^\]]+)\] "(\w+) ([^"]+) [^"]+" (\d+) (\d+)'
        match = re.search(pattern, line)
        
        if match:
            return {
                'ip': match.group(1),
                'timestamp': match.group(2),
                'method': match.group(3),
                'url': match.group(4),
                'status': int(match.group(5)),
                'size': int(match.group(6))
            }
        return None
    
    def analyze_log(self, logfile):
        """Analyze log file for SQL injection attempts"""
        print(f"[*] Analyzing {logfile} for SQL injection...")
        
        with open(logfile, 'r', encoding='utf-8', errors='ignore') as f:
            for line_num, line in enumerate(f, 1):
                entry = self.parse_log_entry(line)
                if entry:
                    detections = self.detect_sqli(line_num, entry)
                    self.findings.extend(detections)
                
                if line_num % 10000 == 0:
                    print(f"  Processed {line_num:,} lines...", end='\r')
        
        print(f"\n[+] Analysis complete: {len(self.findings)} SQL injection attempts detected")
        return self.findings
    
    def report(self):
        """Generate comprehensive report"""
        if not self.findings:
            print("[+] No SQL injection attempts detected")
            return
        
        print(f"\n{'='*70}")
        print("SQL INJECTION DETECTION REPORT")
        print(f"{'='*70}\n")
        
        print(f"Total Attempts: {len(self.findings)}")
        print(f"Unique IPs: {len(self.ip_stats)}\n")
        
        # Severity breakdown
        by_severity = defaultdict(list)
        for finding in self.findings:
            by_severity[finding['severity']].append(finding)
        
        print("Severity Distribution:")
        for severity in ['critical', 'high', 'medium', 'low']:
            if severity in by_severity:
                count = len(by_severity[severity])
                print(f"  {severity.upper():<10}: {count:>6} ({count/len(self.findings)*100:.1f}%)")
        
        # Technique breakdown
        print("\nAttack Techniques:")
        technique_counter = Counter(f['technique'] for f in self.findings)
        for technique, count in technique_counter.most_common():
            print(f"  {technique:<30}: {count:>5}")
        
        # Top attackers
        print("\nTop 10 Attacking IPs:")
        sorted_ips = sorted(self.ip_stats.items(), key=lambda x: x[1]['total'], reverse=True)
        for i, (ip, stats) in enumerate(sorted_ips[:10], 1):
            print(f"  {i:2d}. {ip:<15} : {stats['total']:>4} attempts")
            
            # Show technique distribution for this IP
            techniques = [(k, v) for k, v in stats.items() if k != 'total']
            techniques.sort(key=lambda x: x[1], reverse=True)
            if techniques:
                print(f"      Techniques: {', '.join(f'{k}({v})' for k, v in techniques[:3])}")
        
        # Status code analysis
        print("\nResponse Status Codes:")
        status_counter = Counter(f['status'] for f in self.findings)
        for status, count in sorted(status_counter.items()):
            print(f"  {status}: {count:>5} ({count/len(self.findings)*100:.1f}%)")
        
        # Successful attacks (200 responses)
        successful = [f for f in self.findings if f['status'] == 200]
        if successful:
            print(f"\n[!] WARNING: {len(successful)} attempts received 200 OK response!")
            print("  Potentially successful SQLi attacks:")
            for finding in successful[:5]:
                print(f"    Line {finding['line']}: {finding['ip']} -> {finding['technique']}")
                print(f"      URL: {finding['decoded_url']}")
        
        # Sample attacks by technique
        print("\nSample Attacks by Technique:")
        shown_techniques = set()
        for finding in self.findings:
            technique = finding['technique']
            if technique not in shown_techniques:
                shown_techniques.add(technique)
                print(f"\n  [{technique}]")
                print(f"    IP: {finding['ip']}")
                print(f"    URL: {finding['decoded_url']}")
                print(f"    Status: {finding['status']}")
                
                if len(shown_techniques) >= 5:
                    break

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print(f"Usage: {sys.argv[0]} <access.log>")
        sys.exit(1)
    
    detector = SQLInjectionDetector()
    detector.analyze_log(sys.argv[1])
    detector.report()
```

### A07:2021  Cross-Site Scripting (XSS)

```python
#!/usr/bin/env python3
import re
from urllib.parse import unquote
from collections import defaultdict, Counter


class XSSDetector:
    """Detect Cross-Site Scripting attacks in web logs"""
    
    def __init__(self):
        # XSS patterns categorized by type
        self.patterns = {
            'script_tag': {
                'pattern': re.compile(r'<script[^>]*>.*?</script>|<script[^>]*>', re.I | re.S),
                'severity': 'high',
                'type': 'Reflected XSS - Script Tag'
            },
            'event_handler': {
                'pattern': re.compile(r'(?i)on(?:load|error|click|mouse|focus|blur|change|submit)\s*=', re.I),
                'severity': 'high',
                'type': 'Reflected XSS - Event Handler'
            },
            'javascript_protocol': {
                'pattern': re.compile(r'(?i)javascript\s*:', re.I),
                'severity': 'high',
                'type': 'Reflected XSS - JavaScript Protocol'
            },
            'iframe_injection': {
                'pattern': re.compile(r'<iframe[^>]*>', re.I),
                'severity': 'high',
                'type': 'Reflected XSS - IFrame'
            },
            'img_tag': {
                'pattern': re.compile(r'<img[^>]+(?:onerror|onload)[^>]*>', re.I),
                'severity': 'high',
                'type': 'Reflected XSS - Image Tag'
            },
            'svg_xss': {
                'pattern': re.compile(r'<svg[^>]*>.*?<script|<svg[^>]+onload', re.I | re.S),
                'severity': 'medium',
                'type': 'Reflected XSS - SVG'
            },
            'data_uri': {
                'pattern': re.compile(r'data:text/html[,;]', re.I),
                'severity': 'medium',
                'type': 'Reflected XSS - Data URI'
            },
            'document_manipulation': {
                'pattern': re.compile(r'(?i)document\.(write|cookie|location|domain)', re.I),
                'severity': 'high',
                'type': 'DOM-based XSS'
            },
            'eval_execution': {
                'pattern': re.compile(r'(?i)(?:eval|setTimeout|setInterval|Function)\s*\(', re.I),
                'severity': 'high',
                'type': 'JavaScript Eval/Execution'
            },
            'alert_function': {
                'pattern': re.compile(r'(?i)(?:alert|confirm|prompt)\s*\(', re.I),
                'severity': 'medium',
                'type': 'XSS Probe - Alert Function'
            },
            'html_entities': {
                'pattern': re.compile(r'&#(?:x[0-9a-f]+|[0-9]+);', re.I),
                'severity': 'low',
                'type': 'HTML Entity Encoded XSS'
            },
            'style_expression': {
                'pattern': re.compile(r'(?i)style\s*=.*?expression\s*\(', re.I),
                'severity': 'medium',
                'type': 'CSS Expression XSS'
            }
        }
        
        self.findings = []
        self.ip_analysis = defaultdict(lambda: {'attempts': 0, 'techniques': Counter()})
    
    def decode_multilevel(self, text, max_depth=3):
        """Decode URL-encoded strings multiple times"""
        decoded = text
        for _ in range(max_depth):
            try:
                new_decoded = unquote(decoded)
                if new_decoded == decoded:
                    break
                decoded = new_decoded
            except:
                break
        
        # Also decode HTML entities
        import html
        decoded = html.unescape(decoded)
        
        return decoded
    
    def detect_xss(self, line_num, entry):
        """Detect XSS attempts in log entry"""
        if not entry:
            return []
        
        url = entry['url']
        decoded_url = self.decode_multilevel(url)
        
        detections = []
        
        for pattern_name, config in self.patterns.items():
            matches = config['pattern'].findall(decoded_url)
            
            if matches:
                # Extract the actual matched payload
                payload = matches[0] if matches else ''
                if isinstance(payload, tuple):
                    payload = ' '.join(payload)
                
                detection = {
                    'line': line_num,
                    'ip': entry['ip'],
                    'timestamp': entry['timestamp'],
                    'method': entry['method'],
                    'url': url[:100],
                    'decoded_url': decoded_url[:150],
                    'payload': payload[:100],
                    'pattern': pattern_name,
                    'severity': config['severity'],
                    'type': config['type'],
                    'status': entry['status'],
                    'user_agent': entry.get('user_agent', '')[:50]
                }
                
                detections.append(detection)
                
                # Track statistics
                self.ip_analysis[entry['ip']]['attempts'] += 1
                self.ip_analysis[entry['ip']]['techniques'][config['type']] += 1
        
        return detections
    
    def parse_log_entry(self, line):
        """Parse Apache/Nginx log entry with User-Agent"""
        pattern = r'([\d.]+) .* \[([^\]]+)\] "(\w+) ([^"]+) [^"]+" (\d+) (\d+)(?: "[^"]*" "([^"]*)")?'
        match = re.search(pattern, line)
        
        if match:
            return {
                'ip': match.group(1),
                'timestamp': match.group(2),
                'method': match.group(3),
                'url': match.group(4),
                'status': int(match.group(5)),
                'size': int(match.group(6)),
                'user_agent': match.group(7) if match.lastindex >= 7 else ''
            }
        return None
    
    def analyze_log(self, logfile):
        """Analyze log file for XSS attacks"""
        print(f"[*] Analyzing {logfile} for XSS attacks...")
        
        with open(logfile, 'r', encoding='utf-8', errors='ignore') as f:
            for line_num, line in enumerate(f, 1):
                entry = self.parse_log_entry(line)
                if entry:
                    detections = self.detect_xss(line_num, entry)
                    self.findings.extend(detections)
                
                if line_num % 10000 == 0:
                    print(f"  Processed {line_num:,} lines...", end='\r')
        
        print(f"\n[+] Analysis complete: {len(self.findings)} XSS attempts detected")
        return self.findings
    
    def report(self):
        """Generate comprehensive XSS report"""
        if not self.findings:
            print("[+] No XSS attempts detected")
            return
        
        print(f"\n{'='*70}")
        print("CROSS-SITE SCRIPTING (XSS) DETECTION REPORT")
        print(f"{'='*70}\n")
        
        print(f"Total XSS Attempts: {len(self.findings)}")
        print(f"Unique Attacking IPs: {len(self.ip_analysis)}\n")
        
        # Severity distribution
        by_severity = defaultdict(list)
        for finding in self.findings:
            by_severity[finding['severity']].append(finding)
        
        print("Severity Distribution:")
        for severity in ['high', 'medium', 'low']:
            if severity in by_severity:
                count = len(by_severity[severity])
                pct = count / len(self.findings) * 100
                print(f"  {severity.upper():<8}: {count:>5} ({pct:.1f}%)")
        
        # XSS technique breakdown
        print("\nXSS Techniques Detected:")
        technique_counter = Counter(f['type'] for f in self.findings)
        for technique, count in technique_counter.most_common():
            pct = count / len(self.findings) * 100
            print(f"  {technique:<40}: {count:>4} ({pct:.1f}%)")
        
        # Top attackers
        print("\nTop 10 Attacking IPs:")
        sorted_ips = sorted(
            self.ip_analysis.items(),
            key=lambda x: x[1]['attempts'],
            reverse=True
        )
        
        for i, (ip, stats) in enumerate(sorted_ips[:10], 1):
            print(f"\n  {i:2d}. {ip:<15} - {stats['attempts']} attempts")
            
            # Show top 3 techniques for this IP
            top_techniques = stats['techniques'].most_common(3)
            if top_techniques:
                print(f"      Top techniques:")
                for tech, count in top_techniques:
                    print(f"        - {tech}: {count}")
        
        # Response status analysis
        print("\nServer Response Analysis:")
        status_counter = Counter(f['status'] for f in self.findings)
        for status, count in sorted(status_counter.items()):
            pct = count / len(self.findings) * 100
            status_desc = {
                200: "OK (Potentially Successful)",
                301: "Moved Permanently",
                302: "Found (Redirect)",
                400: "Bad Request",
                403: "Forbidden",
                404: "Not Found",
                500: "Internal Server Error"
            }.get(status, "")
            
            print(f"  {status} {status_desc:<30}: {count:>5} ({pct:.1f}%)")
        
        # Successful attacks (200 responses)
        successful = [f for f in self.findings if f['status'] == 200]
        if successful:
            print(f"\n[!] WARNING: {len(successful)} XSS attempts received 200 OK!")
            print("    These may indicate successful exploitation")
            print("\n  Sample successful attempts:")
            
            for finding in successful[:3]:
                print(f"\n    Line {finding['line']}: {finding['ip']}")
                print(f"    Type: {finding['type']}")
                print(f"    Payload: {finding['payload']}")
                print(f"    URL: {finding['decoded_url'][:80]}")
        
        # User-Agent analysis
        print("\nUser-Agent Analysis:")
        scanner_patterns = {
            'nikto': r'nikto',
            'sqlmap': r'sqlmap',
            'burp': r'burp',
            'zap': r'zap',
            'acunetix': r'acunetix',
            'nessus': r'nessus',
            'nmap': r'nmap',
            'masscan': r'masscan'
        }
        
        tool_detections = defaultdict(int)
        for finding in self.findings:
            ua = finding['user_agent'].lower()
            for tool, pattern in scanner_patterns.items():
                if re.search(pattern, ua):
                    tool_detections[tool] += 1
        
        if tool_detections:
            print("  Automated scanning tools detected:")
            for tool, count in sorted(tool_detections.items(), key=lambda x: x[1], reverse=True):
                print(f"    {tool.upper():<15}: {count:>4} requests")
        else:
            print("  No obvious automated tools detected")
        
        # Sample payloads by type
        print("\nSample XSS Payloads by Type:")
        shown_types = set()
        for finding in self.findings:
            xss_type = finding['type']
            if xss_type not in shown_types:
                shown_types.add(xss_type)
                print(f"\n  [{xss_type}]")
                print(f"    Payload: {finding['payload']}")
                print(f"    Full URL: {finding['decoded_url'][:100]}")
                
                if len(shown_types) >= 5:
                    break


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print(f"Usage: {sys.argv[0]} <access.log>")
        sys.exit(1)
    
    detector = XSSDetector()
    detector.analyze_log(sys.argv[1])
    detector.report()
````

### A08:2021  Software and Data Integrity Failures

```bash
#!/bin/bash
# Detect deserialization and integrity attacks

LOG_FILE="$1"

if [[ -z "$LOG_FILE" ]]; then
    echo "Usage: $0 <logfile>"
    exit 1
fi

echo "[*] Scanning for Software/Data Integrity attacks..."
echo ""

# Java deserialization patterns
echo "[Java Deserialization]"
grep -iE "(rO0|aced0005|java\.lang\.|ObjectInputStream|readObject)" "$LOG_FILE" | \
    awk '{print "  Line", NR":", substr($0, 1, 100)}' | head -10

echo ""

# Python pickle deserialization
echo "[Python Pickle]"
grep -iE "(__reduce__|pickle|cPickle|b'\\x80)" "$LOG_FILE" | \
    awk '{print "  Line", NR":", substr($0, 1, 100)}' | head -10

echo ""

# PHP serialization attacks
echo "[PHP Object Injection]"
grep -iE "(O:[0-9]+:|a:[0-9]+:|s:[0-9]+:)" "$LOG_FILE" | \
    awk '{print "  Line", NR":", substr($0, 1, 100)}' | head -10

echo ""

# Base64-encoded suspicious patterns
echo "[Base64-Encoded Payloads]"
grep -oE "[A-Za-z0-9+/]{40,}={0,2}" "$LOG_FILE" | while read encoded; do
    decoded=$(echo "$encoded" | base64 -d 2>/dev/null)
    if echo "$decoded" | grep -qE "(exec|eval|system|shell|cmd|deserialize)"; then
        echo "  Suspicious base64: $encoded"
        echo "    Decoded: ${decoded:0:80}"
    fi
done | head -20

echo ""
echo "[+] Scan complete"
````

---

## API Abuse Patterns

### API Enumeration Detection

```python
#!/usr/bin/env python3
import re
from collections import defaultdict, Counter
from datetime import datetime
from dateutil import parser as date_parser

class APIAbuseDetector:
    """Detect API abuse and enumeration patterns"""
    
    def __init__(self):
        self.api_patterns = {
            'restful': re.compile(r'/api/v?\d*/([^/\s?]+)(?:/([^/\s?]+))?'),
            'graphql': re.compile(r'/graphql'),
            'json_rpc': re.compile(r'/(?:json-?)?rpc'),
        }
        
        # Track API access patterns
        self.endpoint_stats = defaultdict(lambda: {
            'total_requests': 0,
            'unique_ips': set(),
            'status_codes': Counter(),
            'methods': Counter(),
            'response_sizes': []
        })
        
        self.ip_behavior = defaultdict(lambda: {
            'endpoints_accessed': set(),
            'total_requests': 0,
            'failed_requests': 0,
            'methods_used': Counter(),
            'time_pattern': [],
            'user_agents': set()
        })
        
        self.findings = []
    
    def parse_log_entry(self, line):
        """Parse API log entry"""
        # Extended Apache format with response time
        pattern = r'([\d.]+) .* \[([^\]]+)\] "(\w+) ([^"]+) [^"]+" (\d+) (\d+)(?: "[^"]*" "([^"]*)")?(?:\s+(\d+))?'
        match = re.search(pattern, line)
        
        if match:
            return {
                'ip': match.group(1),
                'timestamp': match.group(2),
                'method': match.group(3),
                'url': match.group(4),
                'status': int(match.group(5)),
                'size': int(match.group(6)),
                'user_agent': match.group(7) if match.lastindex >= 7 else '',
                'response_time': int(match.group(8)) if match.lastindex >= 8 else 0
            }
        return None
    
    def extract_api_endpoint(self, url):
        """Extract API endpoint from URL"""
        # Remove query parameters
        if '?' in url:
            url = url.split('?')[0]
        
        # Try to match API patterns
        for pattern_type, pattern in self.api_patterns.items():
            match = pattern.search(url)
            if match:
                if pattern_type == 'restful':
                    resource = match.group(1)
                    # Normalize IDs to generic placeholder
                    if match.group(2) and match.group(2).isdigit():
                        return f"/api/{resource}/:id"
                    elif match.group(2):
                        return f"/api/{resource}/{match.group(2)}"
                    return f"/api/{resource}"
                else:
                    return url.split('?')[0]
        
        return None
    
    def detect_enumeration(self, ip_data):
        """Detect API enumeration attempts"""
        findings = []
        
        # Check for sequential ID enumeration
        endpoints = list(ip_data['endpoints_accessed'])
        
        # Look for patterns like /api/users/1, /api/users/2, /api/users/3
        id_patterns = defaultdict(list)
        for endpoint in endpoints:
            # Extract numeric IDs from endpoints
            matches = re.findall(r'/(\d+)(?:/|$|\?)', endpoint)
            if matches:
                base = re.sub(r'/\d+(?:/|$|\?)', '/:id', endpoint)
                id_patterns[base].extend([int(m) for m in matches])
        
        for base_endpoint, ids in id_patterns.items():
            if len(ids) >= 10:  # Threshold for enumeration
                ids_sorted = sorted(set(ids))
                
                # Check if mostly sequential
                sequential_count = sum(
                    1 for i in range(len(ids_sorted)-1)
                    if ids_sorted[i+1] - ids_sorted[i] <= 2
                )
                
                if sequential_count >= len(ids_sorted) * 0.7:  # 70% sequential
                    findings.append({
                        'type': 'api_enumeration',
                        'severity': 'high',
                        'endpoint': base_endpoint,
                        'ids_accessed': len(ids),
                        'sequential_ratio': sequential_count / len(ids_sorted),
                        'id_range': f"{min(ids_sorted)} to {max(ids_sorted)}"
                    })
        
        return findings
    
    def detect_excessive_access(self, ip_data, threshold=100):
        """Detect excessive API access from single IP"""
        if ip_data['total_requests'] > threshold:
            # Calculate failure rate
            failure_rate = ip_data['failed_requests'] / ip_data['total_requests']
            
            return {
                'type': 'excessive_api_access',
                'severity': 'high' if failure_rate > 0.5 else 'medium',
                'total_requests': ip_data['total_requests'],
                'failure_rate': failure_rate,
                'unique_endpoints': len(ip_data['endpoints_accessed']),
                'methods': dict(ip_data['methods_used'])
            }
        
        return None
    
    def detect_credential_stuffing(self, ip_data):
        """Detect credential stuffing on API endpoints"""
        # Look for multiple failed auth attempts
        if ip_data['failed_requests'] > 20:
            # Check for auth-related endpoints
            auth_endpoints = [
                ep for ep in ip_data['endpoints_accessed']
                if any(term in ep.lower() for term in ['login', 'auth', 'token', 'signin'])
            ]
            
            if auth_endpoints:
                return {
                    'type': 'credential_stuffing',
                    'severity': 'critical',
                    'failed_attempts': ip_data['failed_requests'],
                    'auth_endpoints': auth_endpoints,
                    'total_requests': ip_data['total_requests']
                }
        
        return None
    
    def detect_rate_limit_bypass(self, ip_data):
        """Detect attempts to bypass rate limiting"""
        # Check for rotating user agents
        if len(ip_data['user_agents']) > 5:
            return {
                'type': 'rate_limit_bypass',
                'severity': 'medium',
                'reason': 'Multiple User-Agents from single IP',
                'user_agent_count': len(ip_data['user_agents']),
                'total_requests': ip_data['total_requests']
            }
        
        return None
    
    def detect_graphql_introspection(self, entry):
        """Detect GraphQL introspection queries"""
        if '/graphql' in entry['url'].lower():
            # Check POST body would require access to request body
            # For logs, check query parameters
            if '__schema' in entry['url'] or '__type' in entry['url']:
                return {
                    'type': 'graphql_introspection',
                    'severity': 'medium',
                    'ip': entry['ip'],
                    'url': entry['url']
                }
        
        return None
    
    def analyze_log(self, logfile):
        """Analyze API logs for abuse patterns"""
        print(f"[*] Analyzing {logfile} for API abuse...")
        
        with open(logfile, 'r', encoding='utf-8', errors='ignore') as f:
            for line_num, line in enumerate(f, 1):
                entry = self.parse_log_entry(line)
                if not entry:
                    continue
                
                # Extract API endpoint
                endpoint = self.extract_api_endpoint(entry['url'])
                if not endpoint:
                    continue
                
                # Update endpoint statistics
                self.endpoint_stats[endpoint]['total_requests'] += 1
                self.endpoint_stats[endpoint]['unique_ips'].add(entry['ip'])
                self.endpoint_stats[endpoint]['status_codes'][entry['status']] += 1
                self.endpoint_stats[endpoint]['methods'][entry['method']] += 1
                self.endpoint_stats[endpoint]['response_sizes'].append(entry['size'])
                
                # Update IP behavior tracking
                ip = entry['ip']
                self.ip_behavior[ip]['endpoints_accessed'].add(entry['url'])
                self.ip_behavior[ip]['total_requests'] += 1
                self.ip_behavior[ip]['methods_used'][entry['method']] += 1
                self.ip_behavior[ip]['user_agents'].add(entry['user_agent'])
                
                if entry['status'] >= 400:
                    self.ip_behavior[ip]['failed_requests'] += 1
                
                try:
                    timestamp = date_parser.parse(entry['timestamp'])
                    self.ip_behavior[ip]['time_pattern'].append(timestamp)
                except:
                    pass
                
                # Check for GraphQL introspection
                introspection = self.detect_graphql_introspection(entry)
                if introspection:
                    introspection['line'] = line_num
                    self.findings.append(introspection)
                
                if line_num % 10000 == 0:
                    print(f"  Processed {line_num:,} lines...", end='\r')
        
        print(f"\n[*] Analyzing IP behavior patterns...")
        
        # Analyze per-IP behavior
        for ip, data in self.ip_behavior.items():
            # Enumeration detection
            enum_findings = self.detect_enumeration(data)
            for finding in enum_findings:
                finding['ip'] = ip
                self.findings.append(finding)
            
            # Excessive access
            excessive = self.detect_excessive_access(data)
            if excessive:
                excessive['ip'] = ip
                self.findings.append(excessive)
            
            # Credential stuffing
            cred_stuff = self.detect_credential_stuffing(data)
            if cred_stuff:
                cred_stuff['ip'] = ip
                self.findings.append(cred_stuff)
            
            # Rate limit bypass
            rate_bypass = self.detect_rate_limit_bypass(data)
            if rate_bypass:
                rate_bypass['ip'] = ip
                self.findings.append(rate_bypass)
        
        print(f"[+] Analysis complete: {len(self.findings)} abuse patterns detected")
        return self.findings
    
    def report(self):
        """Generate comprehensive API abuse report"""
        print(f"\n{'='*70}")
        print("API ABUSE DETECTION REPORT")
        print(f"{'='*70}\n")
        
        if not self.findings:
            print("[+] No API abuse patterns detected")
            return
        
        print(f"Total Abuse Patterns: {len(self.findings)}")
        print(f"Unique Attacking IPs: {len(set(f['ip'] for f in self.findings if 'ip' in f))}\n")
        
        # Group by abuse type
        by_type = defaultdict(list)
        for finding in self.findings:
            by_type[finding['type']].append(finding)
        
        # Report each type
        for abuse_type, instances in sorted(by_type.items()):
            print(f"\n[{abuse_type.upper().replace('_', ' ')}] - {len(instances)} instances")
            
            # Severity distribution
            severities = Counter(f['severity'] for f in instances)
            print(f"  Severity: {dict(severities)}")
            
            # Unique IPs
            unique_ips = set(f['ip'] for f in instances if 'ip' in f)
            print(f"  Unique IPs: {len(unique_ips)}")
            
            # Show samples
            print(f"  Sample incidents:")
            for instance in instances[:3]:
                ip = instance.get('ip', 'N/A')
                print(f"\n    IP: {ip}")
                
                for key, value in instance.items():
                    if key not in ['type', 'severity', 'ip', 'line']:
                        if isinstance(value, (list, set)):
                            if len(value) <= 3:
                                print(f"      {key}: {value}")
                            else:
                                print(f"      {key}: {list(value)[:3]}... ({len(value)} total)")
                        elif isinstance(value, float):
                            print(f"      {key}: {value:.2f}")
                        else:
                            print(f"      {key}: {value}")
        
        # Endpoint statistics
        print(f"\n{'='*70}")
        print("API ENDPOINT STATISTICS")
        print(f"{'='*70}\n")
        
        print(f"Total Endpoints Accessed: {len(self.endpoint_stats)}\n")
        
        # Most accessed endpoints
        print("Top 10 Most Accessed Endpoints:")
        sorted_endpoints = sorted(
            self.endpoint_stats.items(),
            key=lambda x: x[1]['total_requests'],
            reverse=True
        )
        
        for i, (endpoint, stats) in enumerate(sorted_endpoints[:10], 1):
            print(f"\n  {i:2d}. {endpoint}")
            print(f"      Requests: {stats['total_requests']:,}")
            print(f"      Unique IPs: {len(stats['unique_ips'])}")
            print(f"      Methods: {dict(stats['methods'])}")
            print(f"      Status codes: {dict(stats['status_codes'])}")
            
            # Average response size
            if stats['response_sizes']:
                avg_size = sum(stats['response_sizes']) / len(stats['response_sizes'])
                print(f"      Avg response size: {avg_size:,.0f} bytes")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print(f"Usage: {sys.argv[0]} <api_access.log>")
        sys.exit(1)
    
    detector = APIAbuseDetector()
    detector.analyze_log(sys.argv[1])
    detector.report()
```

### API Rate Limiting Analysis

```python
#!/usr/bin/env python3
import re
from datetime import datetime, timedelta
from dateutil import parser as date_parser
from collections import defaultdict, deque


class RateLimitAnalyzer:
    """Analyze and detect rate limiting bypass attempts"""
    
    def __init__(self, window_seconds=60, threshold=100):
        self.window = timedelta(seconds=window_seconds)
        self.threshold = threshold
        
        # Sliding window for each IP
        self.ip_windows = defaultdict(lambda: deque())
        
        # Track rate limit violations
        self.violations = []
        
        # Track distributed attacks
        self.coordinated_attacks = defaultdict(lambda: {
            'ips': set(),
            'timestamps': [],
            'endpoints': set()
        })
    
    def parse_log_entry(self, line):
        """Parse log entry with timestamp"""
        pattern = r'([\d.]+) .* \[([^\]]+)\] "(\w+) ([^"]+) [^"]+" (\d+) (\d+)'
        match = re.search(pattern, line)
        
        if match:
            try:
                timestamp = date_parser.parse(match.group(2))
                return {
                    'ip': match.group(1),
                    'timestamp': timestamp,
                    'method': match.group(3),
                    'url': match.group(4),
                    'status': int(match.group(5)),
                    'size': int(match.group(6))
                }
            except:
                pass
        
        return None
    
    def check_rate_limit(self, entry):
        """Check if IP exceeds rate limit"""
        ip = entry['ip']
        timestamp = entry['timestamp']
        
        # Add current request to window
        self.ip_windows[ip].append((timestamp, entry))
        
        # Remove requests outside time window
        window_start = timestamp - self.window
        while self.ip_windows[ip] and self.ip_windows[ip][0][0] < window_start:
            self.ip_windows[ip].popleft()
        
        # Check if threshold exceeded
        request_count = len(self.ip_windows[ip])
        
        if request_count > self.threshold:
            return {
                'ip': ip,
                'timestamp': timestamp,
                'request_count': request_count,
                'window_seconds': self.window.total_seconds(),
                'threshold': self.threshold,
                'exceeded_by': request_count - self.threshold
            }
        
        return None
    
    def detect_distributed_attack(self, entry):
        """Detect coordinated attacks from multiple IPs"""
        # Group by endpoint and time bucket (e.g., per minute)
        endpoint = entry['url'].split('?')[0]
        time_bucket = entry['timestamp'].replace(second=0, microsecond=0)
        
        key = f"{endpoint}_{time_bucket}"
        
        self.coordinated_attacks[key]['ips'].add(entry['ip'])
        self.coordinated_attacks[key]['timestamps'].append(entry['timestamp'])
        self.coordinated_attacks[key]['endpoints'].add(endpoint)
        
        # Check if many IPs hit same endpoint in short time
        if len(self.coordinated_attacks[key]['ips']) > 10:  # Threshold
            return {
                'type': 'distributed_attack',
                'endpoint': endpoint,
                'time_bucket': time_bucket,
                'unique_ips': len(self.coordinated_attacks[key]['ips']),
                'total_requests': len(self.coordinated_attacks[key]['timestamps'])
            }
        
        return None
    
    def detect_header_rotation(self, ip_data):
        """Detect header rotation to bypass rate limits"""
        # Track unique user agents, X-Forwarded-For, etc.
        # [Inference] This requires access to full headers in logs
        pass
    
    def analyze_log(self, logfile):
        """Analyze log for rate limit violations"""
        print(f"[*] Analyzing {logfile} for rate limit violations...")
        print(f"    Window: {self.window.total_seconds()}s, Threshold: {self.threshold} req/window\n")
        
        distributed_findings = {}
        
        with open(logfile, 'r', encoding='utf-8', errors='ignore') as f:
            for line_num, line in enumerate(f, 1):
                entry = self.parse_log_entry(line)
                if not entry:
                    continue
                
                # Check rate limit
                violation = self.check_rate_limit(entry)
                if violation:
                    violation['line'] = line_num
                    self.violations.append(violation)
                
                # Check for distributed attacks
                distributed = self.detect_distributed_attack(entry)
                if distributed:
                    key = f"{distributed['endpoint']}_{distributed['time_bucket']}"
                    distributed_findings[key] = distributed
                
                if line_num % 10000 == 0:
                    print(f"  Processed {line_num:,} lines...", end='\r')
        
        print(f"\n[+] Analysis complete")
        print(f"    Rate limit violations: {len(self.violations)}")
        print(f"    Distributed attacks: {len(distributed_findings)}")
        
        return self.violations, list(distributed_findings.values())
    
    def report(self, distributed_findings):
        """Generate rate limiting report"""
        print(f"\n{'='*70}")
        print("RATE LIMITING ANALYSIS REPORT")
        print(f"{'='*70}\n")
        
        if not self.violations and not distributed_findings:
            print("[+] No rate limit violations or distributed attacks detected")
            return
        
        # Rate limit violations
        if self.violations:
            print(f"[RATE LIMIT VIOLATIONS] - {len(self.violations)} total\n")
            
            # Group by IP
            by_ip = defaultdict(list)
            for violation in self.violations:
                by_ip[violation['ip']].append(violation)
            
            print(f"Unique IPs violating rate limits: {len(by_ip)}\n")
            print("Top 10 Offending IPs:")
            
            sorted_ips = sorted(by_ip.items(), key=lambda x: len(x[1]), reverse=True)
            for i, (ip, violations) in enumerate(sorted_ips[:10], 1):
                max_exceeded = max(v['exceeded_by'] for v in violations)
                max_count = max(v['request_count'] for v in violations)
                
                print(f"\n  {i:2d}. {ip}")
                print(f"      Violations: {len(violations)}")
                print(f"      Peak request rate: {max_count} req/{self.window.total_seconds()}s")
                print(f"      Max exceeded by: {max_exceeded} requests")
                
                # Show violation timeline
                first_violation = min(v['timestamp'] for v in violations)
                last_violation = max(v['timestamp'] for v in violations)
                duration = (last_violation - first_violation).total_seconds()
                
                print(f"      Duration: {duration:.0f} seconds")
                print(f"      First violation: {first_violation}")
                print(f"      Last violation: {last_violation}")
        
        # Distributed attacks
        if distributed_findings:
            print(f"\n\n[DISTRIBUTED ATTACKS] - {len(distributed_findings)} detected\n")
            
            sorted_attacks = sorted(
                distributed_findings,
                key=lambda x: x['unique_ips'],
                reverse=True
            )
            
            for i, attack in enumerate(sorted_attacks[:10], 1):
                print(f"\n  {i:2d}. Endpoint: {attack['endpoint']}")
                print(f"      Time: {attack['time_bucket']}")
                print(f"      Unique IPs: {attack['unique_ips']}")
                print(f"      Total requests: {attack['total_requests']}")
                print(f"      Requests per IP: {attack['total_requests'] / attack['unique_ips']:.1f}")


if __name__ == "__main__":
    import sys
    import argparse
    
    parser = argparse.ArgumentParser(description="API Rate Limiting Analysis")
    parser.add_argument('logfile', help="Log file to analyze")
    parser.add_argument('--window', type=int, default=60,
                        help="Time window in seconds (default: 60)")
    parser.add_argument('--threshold', type=int, default=100,
                        help="Request threshold per window (default: 100)")
    
    args = parser.parse_args()
    
    analyzer = RateLimitAnalyzer(
        window_seconds=args.window,
        threshold=args.threshold
    )
    
    violations, distributed = analyzer.analyze_log(args.logfile)
    analyzer.report(distributed)
````

---

## Rate Limiting Bypass Techniques

### Detection of Bypass Methods

```python
#!/usr/bin/env python3
import re
from collections import defaultdict, Counter
from urllib.parse import parse_qs, urlparse

class RateLimitBypassDetector:
    """Detect various rate limiting bypass techniques"""
    
    def __init__(self):
        self.ip_profiles = defaultdict(lambda: {
            'user_agents': set(),
            'x_forwarded_for': set(),
            'referrers': set(),
            'session_tokens': set(),
            'query_variations': Counter(),
            'header_patterns': []
        })
        
        self.bypass_findings = []
    
    def parse_log_entry(self, line):
        """Parse log entry with extended headers"""
        # Extended format with X-Forwarded-For
        pattern = r'([\d.]+) .* \[([^\]]+)\] "(\w+) ([^"]+) [^"]+" (\d+) (\d+) "([^"]*)" "([^"]*)"(?:\s+"([^"]*)")?'
        match = re.search(pattern, line)
        
        if match:
            return {
                'ip': match.group(1),
                'timestamp': match.group(2),
                'method': match.group(3),
                'url': match.group(4),
                'status': int(match.group(5)),
                'size': int(match.group(6)),
                'referer': match.group(7),
                'user_agent': match.group(8),
                'x_forwarded_for': match.group(9) if match.lastindex >= 9 else ''
            }
        return None
    
    def detect_user_agent_rotation(self, ip, profile):
        """Detect rotating User-Agent headers"""
        if len(profile['user_agents']) > 5:
            # Check if they're significantly different
            agents = list(profile['user_agents'])
            
            # Simple heuristic: check for different browser families
            browser_families = set()
            for ua in agents:
                if 'firefox' in ua.lower():
                    browser_families.add('firefox')
                elif 'chrome' in ua.lower():
                    browser_families.add('chrome')
                elif 'safari' in ua.lower():
                    browser_families.add('safari')
                elif 'curl' in ua.lower():
                    browser_families.add('curl')
                elif 'python' in ua.lower():
                    browser_families.add('python')
            
            if len(browser_families) >= 3:
                return {
                    'type': 'user_agent_rotation',
                    'severity': 'high',
                    'ip': ip,
                    'unique_agents': len(profile['user_agents']),
                    'browser_families': list(browser_families),
                    'sample_agents': list(profile['user_agents'])[:3]
                }
        
        return None
    
    def detect_xff_spoofing(self, ip, profile):
        """Detect X-Forwarded-For spoofing"""
        if len(profile['x_forwarded_for']) > 3:
            # Multiple different X-Forwarded-For values from same IP
            return {
                'type': 'xff_spoofing',
                'severity': 'high',
                'ip': ip,
                'unique_xff': len(profile['x_forwarded_for']),
                'xff_values': list(profile['x_forwarded_for'])[:5]
            }
        
        return None
    
    def detect_query_randomization(self, ip, profile):
        """Detect random query parameter injection"""
        # Check if many unique query patterns for same endpoint
        if len(profile['query_variations']) > 20:
            # Sample some variations
            variations = list(profile['query_variations'].keys())[:5]
            
            # Check for random-looking parameters
            random_params = []
            for query in variations:
                params = parse_qs(query)
                for key in params.keys():
                    # Detect likely random parameters
                    if key.lower() in ['_', 'cache', 'rand', 'nocache', 'ts', 'cb']:
                        random_params.append(key)
                    # Or if key looks random (single letter, etc.)
                    elif len(key) <= 2:
                        random_params.append(key)
            
            if random_params:
                return {
                    'type': 'query_randomization',
                    'severity': 'medium',
                    'ip': ip,
                    'unique_queries': len(profile['query_variations']),
                    'random_params': list(set(random_params)),
                    'sample_queries': variations
                }
        
        return None
    
    def detect_session_rotation(self, ip, profile):
        """Detect session token rotation"""
        if len(profile['session_tokens']) > 10:
            return {
                'type': 'session_rotation',
                'severity': 'high',
                'ip': ip,
                'unique_sessions': len(profile['session_tokens']),
                'indication': 'Multiple session tokens from single IP'
            }
        
        return None
    
    def detect_referer_manipulation(self, ip, profile):
        """Detect referer header manipulation"""
        if len(profile['referrers']) > 10:
            # Check for diverse referrers
            domains = set()
            for ref in profile['referrers']:
                if ref and ref != '-':
                    try:
                        parsed = urlparse(ref)
                        if parsed.netloc:
                            domains.add(parsed.netloc)
                    except:
                        pass
            
            if len(domains) >= 5:
                return {
                    'type': 'referer_manipulation',
                    'severity': 'medium',
                    'ip': ip,
                    'unique_referrers': len(profile['referrers']),
                    'unique_domains': len(domains),
                    'sample_domains': list(domains)[:5]
                }
        
        return None
    
    def extract_session_token(self, url):
        """Extract session tokens from URL or cookies"""
        # Look for common session parameter names
        session_patterns = [
            r'(?i)[?&](?:session|sess|sid|token|jsessionid|phpsessid)=([^&\s]+)',
            r'(?i)Cookie:.*?(?:session|sess|sid|token)=([^;\s]+)'
        ]
        
        for pattern in session_patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)
        
        return None
    
    def analyze_entry(self, entry):
        """Analyze log entry for bypass techniques"""
        if not entry:
            return
        
        ip = entry['ip']
        profile = self.ip_profiles[ip]
        
        # Track User-Agent
        if entry['user_agent']:
            profile['user_agents'].add(entry['user_agent'])
        
        # Track X-Forwarded-For
        if entry['x_forwarded_for']:
            profile['x_forwarded_for'].add(entry['x_forwarded_for'])
        
        # Track Referer
        if entry['referer']:
            profile['referrers'].add(entry['referer'])
        
        # Track query variations
        parsed = urlparse(entry['url'])
        if parsed.query:
            profile['query_variations'][parsed.query] += 1
        
        # Extract session tokens
        session = self.extract_session_token(entry['url'])
        if session:
            profile['session_tokens'].add(session)
    
    def analyze_log(self, logfile):
        """Analyze log for rate limit bypass techniques"""
        print(f"[*] Analyzing {logfile} for rate limit bypass techniques...")
        
        with open(logfile, 'r', encoding='utf-8', errors='ignore') as f:
            for line_num, line in enumerate(f, 1):
                entry = self.parse_log_entry(line)
                if entry:
                    self.analyze_entry(entry)
                
                if line_num % 10000 == 0:
                    print(f"  Processed {line_num:,} lines...", end='\r')
        
        print(f"\n[*] Analyzing IP profiles for bypass patterns...")
        
        # Analyze each IP's profile
        for ip, profile in self.ip_profiles.items():
            # Run all detection methods
            detections = [
                self.detect_user_agent_rotation(ip, profile),
                self.detect_xff_spoofing(ip, profile),
                self.detect_query_randomization(ip, profile),
                self.detect_session_rotation(ip, profile),
                self.detect_referer_manipulation(ip, profile)
            ]
            
            # Add non-None findings
            self.bypass_findings.extend([d for d in detections if d])
        
        print(f"[+] Analysis complete: {len(self.bypass_findings)} bypass attempts detected")
        return self.bypass_findings
    
    def report(self):
        """Generate bypass detection report"""
        if not self.bypass_findings:
            print("[+] No rate limit bypass attempts detected")
            return
        
        print(f"\n{'='*70}")
        print("RATE LIMIT BYPASS DETECTION REPORT")
        print(f"{'='*70}\n")
        
        print(f"Total Bypass Attempts: {len(self.bypass_findings)}")
        print(f"Unique IPs: {len(set(f['ip'] for f in self.bypass_findings))}\n")
        
        # Group by bypass type
        by_type = defaultdict(list)
        for finding in self.bypass_findings:
            by_type[finding['type']].append(finding)
        
        # Report each type
        for bypass_type, instances in sorted(by_type.items()):
            print(f"\n[{bypass_type.upper().replace('_', ' ')}]")
            print(f"  Instances: {len(instances)}")
            
            # Severity distribution
            severities = Counter(f['severity'] for f in instances)
            print(f"  Severity: {dict(severities)}")
            
            # Unique IPs
            unique_ips = set(f['ip'] for f in instances)
            print(f"  Unique IPs: {len(unique_ips)}")
            
            # Show detailed samples
            print(f"\n  Sample cases:")
            for i, instance in enumerate(instances[:3], 1):
                print(f"\n    {i}. IP: {instance['ip']}")
                for key, value in instance.items():
                    if key not in ['type', 'severity', 'ip']:
                        if isinstance(value, (list, set)):
                            if len(value) <= 3:
                                print(f"       {key}: {list(value)}")
                            else:
                                print(f"       {key}: {list(value)[:3]}... ({len(value)} total)")
                        else:
                            print(f"       {key}: {value}")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print(f"Usage: {sys.argv[0]} <access.log>")
        sys.exit(1)
    
    detector = RateLimitBypassDetector()
    detector.analyze_log(sys.argv[1])
    detector.report()
````

### Comprehensive OWASP Detection Script

```python
#!/usr/bin/env python3
"""
Comprehensive OWASP Top 10 detector for CTF log analysis
Combines multiple detection techniques
"""

import sys
from pathlib import Path

# Import all detector classes (assuming they're in same directory)
# In practice, these would be separate modules

class OWASPDetectionSuite:
    """Master class coordinating all OWASP detections"""
    
    def __init__(self, logfile):
        self.logfile = logfile
        self.detectors = {}
        self.all_findings = []
    
    def initialize_detectors(self):
        """Initialize all detector modules"""
        print("[*] Initializing OWASP detection suite...")
        
        self.detectors = {
            'access_control': AccessControlDetector(),
            'sensitive_data': SensitiveDataDetector(),
            'sqli': SQLInjectionDetector(),
            'xss': XSSDetector(),
            'api_abuse': APIAbuseDetector(),
            'rate_bypass': RateLimitBypassDetector()
        }
        
        print(f"[+] Initialized {len(self.detectors)} detectors\n")
    
    def run_all_detections(self):
        """Run all detectors on the log file"""
        results = {}
        
        for name, detector in self.detectors.items():
            print(f"[*] Running {name} detector...")
            
            try:
                if hasattr(detector, 'analyze_log'):
                    findings = detector.analyze_log(self.logfile)
                    results[name] = {
                        'findings': findings if isinstance(findings, list) else [findings],
                        'detector': detector
                    }
                    print(f"[+] {name}: {len(results[name]['findings'])} findings\n")
            except Exception as e:
                print(f"[!] Error in {name} detector: {e}\n")
                results[name] = {'findings': [], 'detector': detector, 'error': str(e)}
        
        return results
    
    def generate_master_report(self, results):
        """Generate comprehensive security report"""
        print(f"\n{'='*70}")
        print("COMPREHENSIVE OWASP TOP 10 SECURITY ANALYSIS")
        print(f"{'='*70}\n")
        
        print(f"Log File: {self.logfile}")
        print(f"File Size: {Path(self.logfile).stat().st_size / (1024*1024):.2f} MB\n")
        
        # Executive summary
        total_findings = sum(len(r['findings']) for r in results.values())
        print(f"EXECUTIVE SUMMARY")
        print(f"-" * 70)
        print(f"Total Security Findings: {total_findings}\n")
        
        # Findings by category
        print("Findings by OWASP Category:")
        category_map = {
            'access_control': 'A01:2021 - Broken Access Control',
            'sensitive_data': 'A02:2021 - Cryptographic Failures',
            'sqli': 'A03:2021 - Injection (SQL)',
            'xss': 'A03:2021 - Injection (XSS)',
            'api_abuse': 'API Security',
            'rate_bypass': 'Rate Limiting'
        }
        
        for name, result in results.items():
            count = len(result['findings'])
            category = category_map.get(name, name)
            status = "" if count == 0 else ""
            print(f"  {status} {category:<45}: {count:>5} findings")
        
        print(f"\n{'='*70}\n")
        
        # Detailed reports from each detector
        for name, result in results.items():
            if result['findings'] and hasattr(result['detector'], 'report'):
                print(f"\n--- {category_map.get(name, name).upper()} ---\n")
                result['detector'].report()
                print(f"\n{'='*70}\n")
    
    def export_json(self, results, output_file):
        """Export results to JSON"""
        import json
        from datetime import datetime
        
        export_data = {
            'timestamp': datetime.now().isoformat(),
            'logfile': str(self.logfile),
            'total_findings': sum(len(r['findings']) for r in results.values()),
            'results': {}
        }
        
        for name, result in results.items():
            export_data['results'][name] = {
                'count': len(result['findings']),
                'findings': result['findings']
            }
        
        with open(output_file, 'w') as f:
            json.dump(export_data, f, indent=2, default=str)
        
        print(f"[+] Results exported to {output_file}")

def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Comprehensive OWASP Top 10 Log Analysis for CTF"
    )
    parser.add_argument('logfile', help="Web server access log file")
    parser.add_argument('--json', help="Export results to JSON file")
    parser.add_argument('--quick', action='store_true',
                       help="Quick scan (skip some slower detectors)")
    
    args = parser.parse_args()
    
    if not Path(args.logfile).exists():
        print(f"[!] Error: Log file not found: {args.logfile}")
        sys.exit(1)
    
    # Run detection suite
    suite = OWASPDetectionSuite(args.logfile)
    suite.initialize_detectors()
    
    results = suite.run_all_detections()
    suite.generate_master_report(results)
    
    if args.json:
        suite.export_json(results, args.json)

if __name__ == "__main__":
    # Note: This script assumes all detector classes are available
    # In production, import them from separate modules
    main()
```

---

## Important Related Topics

For comprehensive web application attack log analysis in CTF scenarios:

1. **WAF Evasion Detection** - Identifying techniques to bypass Web Application Firewalls (encoding, fragmentation, obfuscation)
2. **WebSocket & HTTP/2 Analysis** - Modern protocol attack patterns
3. **Authentication Attack Patterns** - Brute force, credential stuffing, password spraying detection
4. **Business Logic Abuse** - Account takeover, privilege escalation, workflow manipulation
5. **Bot Detection** - Distinguishing automated tools from legitimate users
6. **Correlation with IDS/IPS Logs** - Cross-referencing application logs with network security devices
7. **Cloud-Specific Attack Patterns** - S3 bucket enumeration, metadata service abuse, serverless attacks

---

# Data Exfiltration Detection

Data exfiltration detection involves identifying unauthorized transfer of sensitive information from compromised systems. In CTF scenarios, recognizing exfiltration patterns is critical for understanding attacker objectives, tracking stolen data, and completing forensic analysis challenges. This section covers detection techniques for volume-based transfers, protocol anomalies, and temporal indicators.

## Large Data Transfers

Large data transfers often indicate bulk data exfiltration, database dumps, or file archive uploads to attacker-controlled infrastructure. Detection focuses on identifying abnormal volume patterns, transfer destinations, and associated user activity.

### Network Traffic Analysis for Volume Detection

**Using netstat and ss for active connection monitoring**:

```bash
# Monitor current connections with data transfer statistics
ss -tnp | awk '{print $5, $6}' | grep ESTAB

# Display connections with detailed statistics
ss -tni

# Monitor specific ports for large transfers
watch -n 1 'ss -tnp | grep ":443\|:80\|:22"'

# Track bytes sent/received per connection
ss -ti | grep -E 'bytes_acked|bytes_received'

# Find connections transferring large amounts of data
ss -ti | awk '/bytes_sent/ {getline; if($2 > 10485760) print}'  # >10MB
```

**Real-time bandwidth monitoring per process**:

```bash
# Install nethogs for per-process bandwidth monitoring
sudo apt install nethogs -y

# Monitor bandwidth usage by process
sudo nethogs -d 5  # Update every 5 seconds

# Monitor specific interface
sudo nethogs eth0

# Log output to file for analysis
sudo nethogs -t > nethogs-log.txt &
# Let run during monitoring period, then analyze
```

**Using iftop for interface-level monitoring**:

```bash
# Install iftop
sudo apt install iftop -y

# Monitor network traffic
sudo iftop -i eth0

# Show specific host traffic
sudo iftop -i eth0 -f "host 192.168.1.100"

# Display ports
sudo iftop -P

# Text mode output for logging
sudo iftop -t -s 10 > iftop-log.txt  # Run for 10 seconds
```

### Log Analysis for Large Transfers

**Apache/Nginx web server log analysis**:

```bash
# Find large file downloads (response size > 10MB)
awk '$10 > 10485760 {print $0}' /var/log/apache2/access.log

# Top 10 largest transfers
awk '{print $10, $1, $7}' /var/log/apache2/access.log | sort -rn | head -10

# Sum total bytes transferred per IP
awk '{ip[$1]+=$10} END {for (i in ip) print i, ip[i]}' /var/log/apache2/access.log | sort -rn -k2

# Identify potential database dumps (SQL files)
grep -E '\.sql|\.dump|\.bak' /var/log/apache2/access.log

# Find archives being downloaded
grep -E '\.zip|\.tar|\.gz|\.7z|\.rar' /var/log/apache2/access.log

# Calculate bandwidth per hour
awk '{print strftime("%Y-%m-%d %H:00", $4), $10}' /var/log/apache2/access.log | \
    awk '{hour[$1]+=$2} END {for (h in hour) print h, hour[h]/1024/1024 " MB"}'
```

**Python script for detailed transfer analysis**:

```python
#!/usr/bin/env python3
# large-transfer-detector.py - Detect anomalous data transfers in web logs

import re
import sys
from collections import defaultdict
from datetime import datetime

def parse_apache_log(logfile, threshold_mb=10):
    """Analyze Apache logs for large transfers"""
    
    # Common Apache log format regex
    log_pattern = r'(\S+) \S+ \S+ \[(.*?)\] "(\S+) (\S+) \S+" (\d+) (\d+|-)'
    
    transfers = []
    ip_totals = defaultdict(int)
    hourly_transfers = defaultdict(int)
    file_types = defaultdict(int)
    
    threshold_bytes = threshold_mb * 1024 * 1024
    
    with open(logfile, 'r', errors='ignore') as f:
        for line in f:
            match = re.search(log_pattern, line)
            if not match:
                continue
            
            ip = match.group(1)
            timestamp = match.group(2)
            method = match.group(3)
            url = match.group(4)
            status = match.group(5)
            size = match.group(6)
            
            # Skip if size is '-'
            if size == '-':
                continue
            
            size_bytes = int(size)
            
            # Track all transfers by IP
            ip_totals[ip] += size_bytes
            
            # Parse timestamp for hourly analysis
            try:
                dt = datetime.strptime(timestamp.split()[0], '%d/%b/%Y:%H:%M:%S')
                hour_key = dt.strftime('%Y-%m-%d %H:00')
                hourly_transfers[hour_key] += size_bytes
            except:
                pass
            
            # Track file types
            if '.' in url:
                ext = url.rsplit('.', 1)[-1].split('?')[0].split('#')[0].lower()
                file_types[ext] += size_bytes
            
            # Flag large individual transfers
            if size_bytes > threshold_bytes:
                transfers.append({
                    'ip': ip,
                    'timestamp': timestamp,
                    'method': method,
                    'url': url,
                    'status': status,
                    'size_mb': size_bytes / 1024 / 1024
                })
    
    return transfers, ip_totals, hourly_transfers, file_types

def generate_report(transfers, ip_totals, hourly_transfers, file_types):
    """Generate comprehensive transfer analysis report"""
    
    print("="*80)
    print("DATA EXFILTRATION ANALYSIS - LARGE TRANSFERS")
    print("="*80)
    print(f"Analysis Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Large Transfers Detected: {len(transfers)}")
    print()
    
    # Individual large transfers
    print("-"*80)
    print("INDIVIDUAL LARGE TRANSFERS (>10MB)")
    print("-"*80)
    for transfer in sorted(transfers, key=lambda x: x['size_mb'], reverse=True)[:20]:
        print(f"{transfer['timestamp']:<30} | {transfer['ip']:<15} | {transfer['size_mb']:>8.2f} MB | {transfer['url'][:50]}")
    print()
    
    # Top IPs by total volume
    print("-"*80)
    print("TOP 15 IPs BY TOTAL TRANSFER VOLUME")
    print("-"*80)
    for ip, total in sorted(ip_totals.items(), key=lambda x: x[1], reverse=True)[:15]:
        print(f"{ip:<15} : {total/1024/1024:>10.2f} MB ({total/1024/1024/1024:.2f} GB)")
    print()
    
    # Hourly transfer patterns
    print("-"*80)
    print("HOURLY TRANSFER VOLUME")
    print("-"*80)
    for hour in sorted(hourly_transfers.keys()):
        volume_mb = hourly_transfers[hour] / 1024 / 1024
        bar = '' * int(volume_mb / 100)  # Scale: 1 bar = 100MB
        print(f"{hour} | {volume_mb:>8.2f} MB | {bar}")
    print()
    
    # File type analysis
    print("-"*80)
    print("TOP FILE TYPES BY VOLUME")
    print("-"*80)
    for ext, total in sorted(file_types.items(), key=lambda x: x[1], reverse=True)[:15]:
        if total > 1024*1024:  # Only show >1MB
            print(f".{ext:<10} : {total/1024/1024:>10.2f} MB")
    print()
    
    # Anomaly detection
    print("-"*80)
    print("ANOMALY INDICATORS")
    print("-"*80)
    
    # Check for single IP with excessive volume
    for ip, total in ip_totals.items():
        if total > 1024*1024*1024:  # >1GB
            print(f"[!] HIGH VOLUME: {ip} transferred {total/1024/1024/1024:.2f} GB")
    
    # Check for suspicious file types
    suspicious_exts = ['sql', 'dump', 'bak', 'db', 'mdb', 'sqlite']
    for ext in suspicious_exts:
        if ext in file_types and file_types[ext] > 10*1024*1024:  # >10MB
            print(f"[!] SUSPICIOUS: {file_types[ext]/1024/1024:.2f} MB of .{ext} files transferred")
    
    print()
    print("="*80)

def main():
    if len(sys.argv) < 2:
        print("Usage: large-transfer-detector.py <apache-access.log> [threshold_mb]")
        sys.exit(1)
    
    logfile = sys.argv[1]
    threshold = int(sys.argv[2]) if len(sys.argv) > 2 else 10
    
    print(f"Analyzing: {logfile}")
    print(f"Threshold: {threshold} MB")
    print()
    
    transfers, ip_totals, hourly_transfers, file_types = parse_apache_log(logfile, threshold)
    generate_report(transfers, ip_totals, hourly_transfers, file_types)

if __name__ == '__main__':
    main()
```

```bash
# Usage
chmod +x large-transfer-detector.py
./large-transfer-detector.py /var/log/apache2/access.log 10 > transfer-analysis.txt
```

**FTP/SFTP log analysis**:

```bash
# Analyze vsftpd logs for large uploads
grep "OK UPLOAD" /var/log/vsftpd.log | awk '{print $NF, $(NF-1)}' | \
    awk '{if($2 > 10485760) print $0}'

# ProFTPD transfer log analysis
awk '$9 == "b" && $8 > 10485760 {print $0}' /var/log/proftpd/xferlog

# SFTP transfers from auth.log
grep "sftp-server" /var/log/auth.log | grep -E "open|close"

# Extract SFTP session details
grep "sftp-server" /var/log/auth.log | \
    awk '{print $1, $2, $3, $NF}' | \
    grep -E "open.*write|close.*write"
```

### Database Access Monitoring

**MySQL/MariaDB query log analysis**:

```bash
# Enable general query log (if not already enabled)
# Add to /etc/mysql/my.cnf:
# general_log = 1
# general_log_file = /var/log/mysql/query.log

# Analyze for large SELECT queries (potential data exfiltration)
grep "SELECT" /var/log/mysql/query.log | wc -l

# Find queries accessing sensitive tables
grep -E "users|passwords|credentials|customers|accounts" /var/log/mysql/query.log

# Detect SELECT INTO OUTFILE (direct file export)
grep -i "SELECT.*INTO OUTFILE" /var/log/mysql/query.log

# Identify database dumps via mysqldump
grep "mysqldump" /var/log/mysql/query.log

# Track large result sets (if slow query log enabled)
grep "Rows_examined" /var/log/mysql/slow.log | awk '{if($2 > 10000) print $0}'
```

**PostgreSQL log analysis**:

```bash
# Analyze PostgreSQL logs
# Location typically: /var/log/postgresql/postgresql-*.log

# Find COPY TO commands (data export)
grep "COPY.*TO" /var/log/postgresql/*.log

# Large SELECT statements
grep "SELECT" /var/log/postgresql/*.log | grep -v "FROM pg_"

# Track table access patterns
grep -E "SELECT \* FROM" /var/log/postgresql/*.log | \
    awk '{for(i=1;i<=NF;i++) if($i=="FROM") print $(i+1)}' | \
    sort | uniq -c | sort -rn
```

### CloudTrail Data Access Detection

**AWS S3 data exfiltration indicators**:

```bash
# Analyze CloudTrail for S3 GetObject operations (downloads)
cat cloudtrail.json | jq -r '.Records[] | 
    select(.eventName == "GetObject") | 
    "\(.eventTime) | \(.userIdentity.principalId) | \(.requestParameters.bucketName)/\(.requestParameters.key) | \(.sourceIPAddress)"'

# Detect bulk downloads from single IP
cat cloudtrail.json | jq -r '.Records[] | 
    select(.eventName == "GetObject") | 
    .sourceIPAddress' | sort | uniq -c | sort -rn

# Find CopyObject operations (potential staging for exfiltration)
cat cloudtrail.json | jq '.Records[] | select(.eventName == "CopyObject")'

# Detect snapshot creation (EBS volume exfiltration)
cat cloudtrail.json | jq '.Records[] | 
    select(.eventName == "CreateSnapshot") | 
    {time: .eventTime, user: .userIdentity.principalId, volume: .requestParameters.volumeId}'

# Check for snapshot sharing (exfiltration to external account)
cat cloudtrail.json | jq '.Records[] | 
    select(.eventName == "ModifySnapshotAttribute") | 
    select(.requestParameters.createVolumePermission.add != null)'

# Database snapshot exfiltration
cat cloudtrail.json | jq '.Records[] | 
    select(.eventName | test("CreateDBSnapshot|CopyDBSnapshot"))'
```

**Python script for CloudTrail data transfer analysis**:

```python
#!/usr/bin/env python3
# cloudtrail-exfiltration-detector.py - Detect data exfiltration in AWS

import json
import sys
from collections import defaultdict
from datetime import datetime

def analyze_cloudtrail_exfiltration(jsonfile):
    """Analyze CloudTrail logs for data exfiltration patterns"""
    
    with open(jsonfile, 'r') as f:
        data = json.load(f)
    
    records = data.get('Records', [])
    
    # Track exfiltration indicators
    s3_downloads = []
    snapshot_operations = []
    db_exports = []
    ip_activity = defaultdict(lambda: {'s3_gets': 0, 'snapshots': 0, 'events': []})
    user_activity = defaultdict(lambda: {'s3_gets': 0, 'snapshots': 0})
    
    exfiltration_events = [
        'GetObject',
        'CreateSnapshot',
        'ModifySnapshotAttribute',
        'CreateDBSnapshot',
        'CopyDBSnapshot',
        'ExportSnapshot',
        'CopySnapshot'
    ]
    
    for record in records:
        event_name = record.get('eventName', '')
        event_time = record.get('eventTime', '')
        source_ip = record.get('sourceIPAddress', 'Unknown')
        user_identity = record.get('userIdentity', {})
        principal = user_identity.get('principalId', 'Unknown')
        
        # Track S3 downloads
        if event_name == 'GetObject':
            params = record.get('requestParameters', {})
            bucket = params.get('bucketName', 'Unknown')
            key = params.get('key', 'Unknown')
            
            s3_downloads.append({
                'time': event_time,
                'ip': source_ip,
                'principal': principal,
                'bucket': bucket,
                'key': key
            })
            
            ip_activity[source_ip]['s3_gets'] += 1
            user_activity[principal]['s3_gets'] += 1
            ip_activity[source_ip]['events'].append(event_time)
        
        # Track snapshot operations
        elif 'Snapshot' in event_name:
            snapshot_operations.append({
                'time': event_time,
                'event': event_name,
                'ip': source_ip,
                'principal': principal,
                'params': record.get('requestParameters', {})
            })
            
            ip_activity[source_ip]['snapshots'] += 1
            user_activity[principal]['snapshots'] += 1
        
        # Track database exports
        elif 'DB' in event_name and ('Snapshot' in event_name or 'Export' in event_name):
            db_exports.append({
                'time': event_time,
                'event': event_name,
                'ip': source_ip,
                'principal': principal
            })
    
    return s3_downloads, snapshot_operations, db_exports, ip_activity, user_activity

def generate_exfiltration_report(s3_downloads, snapshot_ops, db_exports, ip_activity, user_activity):
    """Generate data exfiltration detection report"""
    
    print("="*80)
    print("AWS DATA EXFILTRATION DETECTION REPORT")
    print("="*80)
    print(f"Analysis Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # S3 Download Analysis
    print("-"*80)
    print(f"S3 GETOBJECT OPERATIONS: {len(s3_downloads)}")
    print("-"*80)
    if s3_downloads:
        print("Recent S3 Downloads:")
        for download in s3_downloads[:20]:
            print(f"  {download['time']} | {download['ip']:<15} | {download['bucket']}/{download['key'][:40]}")
    print()
    
    # Snapshot Operations
    print("-"*80)
    print(f"SNAPSHOT OPERATIONS: {len(snapshot_ops)}")
    print("-"*80)
    if snapshot_ops:
        for op in snapshot_ops[:15]:
            print(f"  {op['time']} | {op['event']:<30} | {op['principal'][:50]}")
    print()
    
    # Database Exports
    print("-"*80)
    print(f"DATABASE EXPORT OPERATIONS: {len(db_exports)}")
    print("-"*80)
    if db_exports:
        for exp in db_exports:
            print(f"  {exp['time']} | {exp['event']:<30} | {exp['principal'][:50]}")
    print()
    
    # IP Analysis
    print("-"*80)
    print("TOP SOURCE IPs BY ACTIVITY")
    print("-"*80)
    for ip, activity in sorted(ip_activity.items(), 
                               key=lambda x: x[1]['s3_gets'] + x[1]['snapshots'], 
                               reverse=True)[:10]:
        print(f"  {ip:<15} | S3 Downloads: {activity['s3_gets']:>4} | Snapshots: {activity['snapshots']:>3}")
    print()
    
    # User Analysis
    print("-"*80)
    print("TOP PRINCIPALS BY ACTIVITY")
    print("-"*80)
    for principal, activity in sorted(user_activity.items(), 
                                     key=lambda x: x[1]['s3_gets'] + x[1]['snapshots'], 
                                     reverse=True)[:10]:
        print(f"  {principal[:60]} | S3: {activity['s3_gets']:>4} | Snapshots: {activity['snapshots']:>3}")
    print()
    
    # Anomaly Detection
    print("-"*80)
    print("ANOMALY INDICATORS")
    print("-"*80)
    
    # Excessive S3 downloads from single IP
    for ip, activity in ip_activity.items():
        if activity['s3_gets'] > 100:
            print(f"  [!] HIGH VOLUME: {ip} performed {activity['s3_gets']} S3 downloads")
    
    # Rapid successive downloads
    for ip, activity in ip_activity.items():
        if len(activity['events']) > 10:
            events = sorted(activity['events'])
            # Check if 10+ downloads in 60 seconds
            for i in range(len(events) - 9):
                try:
                    t1 = datetime.fromisoformat(events[i].replace('Z', '+00:00'))
                    t10 = datetime.fromisoformat(events[i+9].replace('Z', '+00:00'))
                    if (t10 - t1).total_seconds() < 60:
                        print(f"  [!] RAPID ACTIVITY: {ip} performed 10+ downloads in <60 seconds")
                        break
                except:
                    pass
    
    # Snapshot sharing detected
    for op in snapshot_ops:
        if 'ModifySnapshotAttribute' in op['event']:
            if op['params'].get('createVolumePermission', {}).get('add'):
                print(f"  [!] SNAPSHOT SHARING: {op['principal']} shared snapshot at {op['time']}")
    
    print()
    print("="*80)

def main():
    if len(sys.argv) < 2:
        print("Usage: cloudtrail-exfiltration-detector.py <cloudtrail.json>")
        sys.exit(1)
    
    jsonfile = sys.argv[1]
    
    s3_downloads, snapshot_ops, db_exports, ip_activity, user_activity = \
        analyze_cloudtrail_exfiltration(jsonfile)
    
    generate_exfiltration_report(s3_downloads, snapshot_ops, db_exports, ip_activity, user_activity)

if __name__ == '__main__':
    main()
```

```bash
# Usage
./cloudtrail-exfiltration-detector.py cloudtrail-logs.json
```

### Network Flow Analysis

**Using tcpdump for traffic capture**:

```bash
# Capture traffic and calculate transfer volumes
sudo tcpdump -i eth0 -n -q -tttt | tee network-capture.txt

# Capture specific destination (potential C2 server)
sudo tcpdump -i eth0 dst 203.0.113.50 -w exfil-traffic.pcap

# Capture and display packet sizes
sudo tcpdump -i eth0 -n -q -tttt -v | awk '{print $1, $2, $NF}'

# Monitor outbound traffic only
sudo tcpdump -i eth0 'src net 192.168.1.0/24' -w outbound.pcap
```

**Analyzing packet captures with tshark**:

```bash
# Install tshark
sudo apt install tshark -y

# Extract transfer statistics by IP
tshark -r capture.pcap -q -z conv,ip

# Find large HTTP transfers
tshark -r capture.pcap -Y "http.content_length > 10485760" -T fields \
    -e frame.time -e ip.src -e ip.dst -e http.content_length

# Identify FTP data transfers
tshark -r capture.pcap -Y "ftp-data" -T fields \
    -e frame.time -e ip.src -e ip.dst -e ftp-data.len

# DNS tunneling detection (large DNS responses)
tshark -r capture.pcap -Y "dns.resp.len > 512" -T fields \
    -e frame.time -e ip.src -e dns.qry.name -e dns.resp.len

# Extract all HTTP file downloads
tshark -r capture.pcap -Y "http.request.method == GET" -T fields \
    -e frame.time -e ip.src -e http.host -e http.request.uri
```

## Unusual Protocols

Attackers often use non-standard or unexpected protocols to evade detection. Identifying protocol anomalies requires understanding normal baseline traffic and recognizing deviations.

### Protocol Identification

**Port-based protocol analysis**:

```bash
# List all active connections with protocols
sudo netstat -tulpn

# Identify non-standard ports in use
ss -tulpn | awk '{print $5}' | cut -d: -f2 | sort -n | uniq -c

# Find processes using unexpected ports
sudo lsof -i -P -n | grep LISTEN

# Detect services on non-standard ports
sudo lsof -i :8080,8443,3389,5900  # Common alternate ports
```

**Deep packet inspection with tshark**:

```bash
# Protocol hierarchy statistics
tshark -r capture.pcap -q -z io,phs

# Identify all protocols used
tshark -r capture.pcap -T fields -e frame.protocols | sort | uniq -c | sort -rn

# Find encrypted protocols (TLS/SSL)
tshark -r capture.pcap -Y "tls" -T fields -e ip.src -e ip.dst -e tls.handshake.type

# Detect SSH tunneling indicators
tshark -r capture.pcap -Y "ssh" -T fields -e frame.time -e ip.src -e ip.dst -e tcp.len | \
    awk '{if($4 > 1000) print $0}'  # Large SSH packets
```

### DNS Tunneling Detection

DNS tunneling is a common exfiltration technique that encodes data in DNS queries.

**Analyzing DNS query patterns**:

```bash
# Extract DNS queries from logs
grep "query" /var/log/syslog | grep "dnsmasq\|named"

# Find unusually long DNS queries (potential tunneling)
tshark -r capture.pcap -Y "dns.qry.name" -T fields -e dns.qry.name | \
    awk '{if(length($0) > 50) print length($0), $0}' | sort -rn

# Detect high query frequency to single domain
tshark -r capture.pcap -Y "dns" -T fields -e dns.qry.name | \
    sort | uniq -c | sort -rn | head -20

# Identify unusual TXT record queries
tshark -r capture.pcap -Y "dns.qry.type == 16" -T fields \
    -e frame.time -e ip.src -e dns.qry.name

# Find subdomain enumeration patterns
tshark -r capture.pcap -Y "dns" -T fields -e dns.qry.name | \
    sed 's/^[^.]*\.//' | sort | uniq -c | sort -rn
```

**Python DNS tunneling detector**:

```python
#!/usr/bin/env python3
# dns-tunnel-detector.py - Detect DNS tunneling patterns

import sys
import re
from collections import defaultdict, Counter

def analyze_dns_tunneling(pcap_file):
    """Analyze DNS queries for tunneling indicators"""
    
    import pyshark
    
    try:
        cap = pyshark.FileCapture(pcap_file, display_filter='dns')
    except:
        print("Error: pyshark not installed. Install with: pip3 install pyshark")
        sys.exit(1)
    
    queries = []
    domain_stats = defaultdict(lambda: {'count': 0, 'avg_length': 0, 'queries': []})
    
    for packet in cap:
        try:
            if hasattr(packet.dns, 'qry_name'):
                query = packet.dns.qry_name
                timestamp = packet.sniff_time
                src_ip = packet.ip.src
                
                queries.append({
                    'time': timestamp,
                    'src': src_ip,
                    'query': query,
                    'length': len(query)
                })
                
                # Extract base domain
                parts = query.split('.')
                if len(parts) >= 2:
                    base_domain = '.'.join(parts[-2:])
                    domain_stats[base_domain]['count'] += 1
                    domain_stats[base_domain]['queries'].append(query)
        except AttributeError:
            continue
    
    cap.close()
    
    # Analysis
    print("="*80)
    print("DNS TUNNELING DETECTION REPORT")
    print("="*80)
    print(f"Total DNS Queries: {len(queries)}")
    print()
    
    # Long query names (>50 chars)
    print("-"*80)
    print("UNUSUALLY LONG DNS QUERIES (>50 characters)")
    print("-"*80)
    long_queries = [q for q in queries if q['length'] > 50]
    for query in sorted(long_queries, key=lambda x: x['length'], reverse=True)[:15]:
        print(f"  Length: {query['length']:>3} | {query['src']:<15} | {query['query'][:70]}")
    print()
    
    # High frequency domains
    print("-"*80)
    print("TOP DOMAINS BY QUERY FREQUENCY")
    print("-"*80)
    for domain, stats in sorted(domain_stats.items(), key=lambda x: x[1]['count'], reverse=True)[:15]:
        avg_len = sum(len(q) for q in stats['queries']) / len(stats['queries'])
        print(f"  {domain:<30} | Queries: {stats['count']:>5} | Avg Length: {avg_len:.1f}")
    print()
    
    # Entropy analysis (high entropy = potential encoded data)
    print("-"*80)
    print("ANOMALY INDICATORS")
    print("-"*80)
    
    for domain, stats in domain_stats.items():
        if stats['count'] > 50:
            print(f"  [!] HIGH FREQUENCY: {domain} queried {stats['count']} times")
        
        # Check for randomized subdomains
        subdomains = [q.split('.')[0] for q in stats['queries'] if '.' in q]
        if len(set(subdomains)) > 20 and len(subdomains) > 20:
            uniqueness = len(set(subdomains)) / len(subdomains)
            if uniqueness > 0.8:
                print(f"  [!] RANDOM SUBDOMAINS: {domain} has {len(set(subdomains))} unique subdomains ({uniqueness*100:.1f}% unique)")
    
    print()
    print("="*80)

def analyze_dns_from_logs(logfile):
    """Analyze DNS from text logs when pcap not available"""
    
    dns_pattern = r'query\[.*?\]\s+(\S+)\s+from\s+(\S+)'
    
    queries = []
    
    with open(logfile, 'r', errors='ignore') as f:
        for line in f:
            match = re.search(dns_pattern, line)
            if match:
                domain = match.group(1)
                src_ip = match.group(2)
                queries.append({'domain': domain, 'src': src_ip, 'length': len(domain)})
    
    # Similar analysis as above
    print(f"Total DNS queries found: {len(queries)}")
    
    long_queries = [q for q in queries if q['length'] > 50]
    if long_queries:
        print("\nLong DNS queries detected (potential tunneling):")
        for q in long_queries[:10]:
            print(f"  {q['src']:<15} | Length: {q['length']:>3} | {q['domain']}")

def main():
    if len(sys.argv) < 2:
        print("Usage: dns-tunnel-detector.py <capture.pcap|syslog>")
        sys.exit(1)
    
    filepath = sys.argv[1]
    
    if filepath.endswith('.pcap') or filepath.endswith('.pcapng'):
        analyze_dns_tunneling(filepath)
```

---

# Data Exfiltration Detection

Data exfiltration is a critical phase in attack chains where adversaries steal sensitive information. Detecting exfiltration in logs requires identifying suspicious file operations, unusual data transfers, encryption activities, and staging behaviors that precede data theft.

## Compression Indicators

Attackers often compress data before exfiltration to reduce transfer time, evade detection, and consolidate multiple files. Detecting compression activities in logs reveals potential data staging.

### Common Compression Tools and Patterns

**Compression utilities typically used:**

- `tar` (with gzip/bzip2/xz)
- `zip`/`unzip`
- `gzip`/`gunzip`
- `bzip2`/`bunzip2`
- `7z`/`7za`
- `rar`/`unrar`
- `compress`
- Windows: `compact`, PowerShell `Compress-Archive`

### Detecting Compression in Command Logs

**Search for compression commands in bash history:**

```bash
# Check bash history for compression
grep -E '(tar|zip|gzip|bzip2|7z|rar|compress)' ~/.bash_history

# Check all users' bash history
find /home -name ".bash_history" -exec grep -H -E '(tar|zip|gzip|bzip2|7z)' {} \;

# Check root history
sudo grep -E '(tar|zip|gzip|bzip2|7z)' /root/.bash_history
```

**Detect compression in audit logs (auditd):**

```bash
# Search for execve calls to compression tools
ausearch -i -x tar | grep -v "Permission denied"
ausearch -i -x zip
ausearch -i -x gzip
ausearch -i -x 7z

# Search for compression with specific patterns
ausearch -i -sc execve | grep -E "(tar|zip|7z)" | grep -E "(czf|cjf|cJf)"

# Compression of sensitive directories
ausearch -i -x tar | grep -E "(/home|/var/www|/etc|/opt)"
```

**Detect in process logs (ps output/logs):**

```bash
# Check current processes
ps aux | grep -E '(tar|zip|gzip|7z|rar)' | grep -v grep

# Check syslog for process execution
grep -E "tar.*czf|zip.*-r|7z.*a" /var/log/syslog

# Check auth.log for sudo compression commands
grep -E "sudo.*COMMAND.*tar|sudo.*COMMAND.*zip" /var/log/auth.log
```

### Compression Pattern Detection Script

```bash
#!/bin/bash
# detect_compression.sh

LOGDIR="${1:-/var/log}"
OUTPUT="compression_indicators.txt"

echo "[*] Detecting compression indicators..."
echo "Compression Detection Report - $(date)" > "$OUTPUT"
echo "=======================================" >> "$OUTPUT"

# Function to search logs
search_logs() {
    local pattern="$1"
    local description="$2"
    
    echo -e "\n[+] $description" >> "$OUTPUT"
    echo "---" >> "$OUTPUT"
    
    find "$LOGDIR" -type f 2>/dev/null | while read logfile; do
        matches=$(grep -iE "$pattern" "$logfile" 2>/dev/null)
        if [ -n "$matches" ]; then
            echo "File: $logfile" >> "$OUTPUT"
            echo "$matches" | head -10 >> "$OUTPUT"
            echo "" >> "$OUTPUT"
        fi
    done
}

# Search for common compression commands
search_logs "tar\s+.*c[zjJ]f" "TAR compression commands"
search_logs "zip\s+-r" "ZIP recursive compression"
search_logs "7z\s+a" "7-Zip archive creation"
search_logs "gzip\s+.*\.tar" "GZIP compression"
search_logs "compress\s+" "Compress utility usage"

# Search for large file compression
search_logs "tar.*\s+/home|tar.*\s+/var/www|tar.*\s+/opt" "Compression of sensitive directories"

# Search for compression with unusual output locations
search_logs "tar.*\s+/tmp|tar.*\s+/dev/shm|tar.*\s+/var/tmp" "Compression to staging locations"

# Check for compression followed by network activity
echo -e "\n[+] Compression followed by network transfer" >> "$OUTPUT"
echo "---" >> "$OUTPUT"
grep -E "(tar|zip|gzip)" "$LOGDIR/auth.log" 2>/dev/null | \
    awk '{print $1, $2, $3, $0}' | sort > /tmp/compress_times.tmp
grep -E "(scp|sftp|curl|wget|nc|netcat)" "$LOGDIR/auth.log" 2>/dev/null | \
    awk '{print $1, $2, $3, $0}' | sort > /tmp/network_times.tmp

# Find temporal correlation (within 5 minutes)
join -j1 /tmp/compress_times.tmp /tmp/network_times.tmp 2>/dev/null >> "$OUTPUT"

rm -f /tmp/compress_times.tmp /tmp/network_times.tmp

# Summary statistics
echo -e "\n[+] Summary Statistics" >> "$OUTPUT"
echo "---" >> "$OUTPUT"
echo "TAR commands: $(grep -rE 'tar.*c[zjJ]f' "$LOGDIR" 2>/dev/null | wc -l)" >> "$OUTPUT"
echo "ZIP commands: $(grep -rE 'zip\s+-r' "$LOGDIR" 2>/dev/null | wc -l)" >> "$OUTPUT"
echo "7z commands: $(grep -rE '7z\s+a' "$LOGDIR" 2>/dev/null | wc -l)" >> "$OUTPUT"

echo "[*] Report saved to $OUTPUT"
cat "$OUTPUT"
```

**Usage:**

```bash
chmod +x detect_compression.sh
./detect_compression.sh /var/log
```

### Advanced Python Compression Detector

```python
#!/usr/bin/env python3
import re
import os
import sys
from datetime import datetime, timedelta
from collections import defaultdict

class CompressionDetector:
    def __init__(self):
        self.compression_patterns = {
            'tar': re.compile(r'tar\s+.*-?c[zjJx]?f?\s+(\S+)', re.IGNORECASE),
            'zip': re.compile(r'zip\s+-r\s+(\S+)', re.IGNORECASE),
            '7z': re.compile(r'7z\s+a\s+(\S+)', re.IGNORECASE),
            'gzip': re.compile(r'gzip\s+(-\d+)?\s+(\S+)', re.IGNORECASE),
            'bzip2': re.compile(r'bzip2\s+(\S+)', re.IGNORECASE),
            'rar': re.compile(r'rar\s+a\s+(\S+)', re.IGNORECASE),
        }
        
        # Suspicious patterns
        self.suspicious_paths = [
            r'/home/[^/]+/\.(ssh|gnupg)',  # SSH keys, GPG keys
            r'/var/www',                    # Web content
            r'/opt',                         # Applications
            r'/etc/(passwd|shadow|ssh)',    # System configs
            r'/root',                        # Root directory
            r'\.git',                        # Source code repos
            r'database|backup|dump',        # Database files
        ]
        
        self.staging_locations = [
            r'/tmp',
            r'/dev/shm',
            r'/var/tmp',
            r'/var/spool',
        ]
        
        self.events = []
        self.timestamp_pattern = re.compile(
            r'(\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2}|\w{3}\s+\d{1,2}\s+\d{2}:\d{2}:\d{2})'
        )
    
    def parse_timestamp(self, line):
        """Extract timestamp from log line"""
        match = self.timestamp_pattern.search(line)
        if match:
            ts_str = match.group(1)
            try:
                if '-' in ts_str:
                    return datetime.strptime(ts_str[:19], '%Y-%m-%d %H:%M:%S')
                else:
                    # Syslog format - add current year
                    return datetime.strptime(f"{datetime.now().year} {ts_str}", '%Y %b %d %H:%M:%S')
            except:
                pass
        return None
    
    def detect_compression(self, logfile):
        """Detect compression activities in logs"""
        print(f"[*] Analyzing {logfile}...")
        
        with open(logfile, 'r', errors='ignore') as f:
            for line_num, line in enumerate(f, 1):
                timestamp = self.parse_timestamp(line)
                
                for tool, pattern in self.compression_patterns.items():
                    match = pattern.search(line)
                    if match:
                        archive_name = match.group(1) if match.lastindex >= 1 else 'unknown'
                        
                        # Check for suspicious patterns
                        suspicious = any(re.search(sp, line, re.IGNORECASE) 
                                       for sp in self.suspicious_paths)
                        
                        # Check for staging locations
                        staging = any(re.search(sl, archive_name, re.IGNORECASE) 
                                     for sl in self.staging_locations)
                        
                        event = {
                            'timestamp': timestamp,
                            'line_num': line_num,
                            'tool': tool,
                            'archive': archive_name,
                            'suspicious': suspicious,
                            'staging': staging,
                            'content': line.strip()
                        }
                        
                        self.events.append(event)
    
    def analyze_patterns(self):
        """Analyze compression patterns for exfiltration indicators"""
        analysis = {
            'total_events': len(self.events),
            'by_tool': defaultdict(int),
            'suspicious_events': [],
            'staging_events': [],
            'temporal_clusters': [],
        }
        
        for event in self.events:
            analysis['by_tool'][event['tool']] += 1
            
            if event['suspicious']:
                analysis['suspicious_events'].append(event)
            
            if event['staging']:
                analysis['staging_events'].append(event)
        
        # Find temporal clusters (multiple compressions within 10 minutes)
        sorted_events = sorted([e for e in self.events if e['timestamp']], 
                              key=lambda x: x['timestamp'])
        
        cluster = []
        for i, event in enumerate(sorted_events):
            cluster.append(event)
            
            # Check if next event is within 10 minutes
            if i + 1 < len(sorted_events):
                next_event = sorted_events[i + 1]
                if (next_event['timestamp'] - event['timestamp']) > timedelta(minutes=10):
                    if len(cluster) >= 3:
                        analysis['temporal_clusters'].append(cluster)
                    cluster = []
        
        return analysis
    
    def generate_report(self, output_file='compression_report.txt'):
        """Generate comprehensive report"""
        analysis = self.analyze_patterns()
        
        with open(output_file, 'w') as f:
            f.write("=" * 80 + "\n")
            f.write("COMPRESSION ACTIVITY DETECTION REPORT\n")
            f.write("=" * 80 + "\n")
            f.write(f"Generated: {datetime.now().isoformat()}\n")
            f.write(f"Total Compression Events: {analysis['total_events']}\n\n")
            
            # By tool
            f.write("Compression Tools Used:\n")
            f.write("-" * 80 + "\n")
            for tool, count in sorted(analysis['by_tool'].items(), key=lambda x: x[1], reverse=True):
                f.write(f"  {tool:10s}: {count} events\n")
            
            # Suspicious events
            f.write(f"\n{'=' * 80}\n")
            f.write(f"SUSPICIOUS COMPRESSION ACTIVITIES: {len(analysis['suspicious_events'])}\n")
            f.write("=" * 80 + "\n")
            for event in analysis['suspicious_events'][:20]:
                f.write(f"\n[{event['timestamp']}] {event['tool'].upper()}\n")
                f.write(f"  Archive: {event['archive']}\n")
                f.write(f"  Line {event['line_num']}: {event['content'][:150]}\n")
            
            # Staging locations
            f.write(f"\n{'=' * 80}\n")
            f.write(f"COMPRESSION TO STAGING LOCATIONS: {len(analysis['staging_events'])}\n")
            f.write("=" * 80 + "\n")
            for event in analysis['staging_events'][:20]:
                f.write(f"\n[{event['timestamp']}] {event['tool'].upper()}\n")
                f.write(f"  Archive: {event['archive']}\n")
                f.write(f"  Line {event['line_num']}: {event['content'][:150]}\n")
            
            # Temporal clusters
            f.write(f"\n{'=' * 80}\n")
            f.write(f"TEMPORAL CLUSTERS (Multiple compressions within 10 min): {len(analysis['temporal_clusters'])}\n")
            f.write("=" * 80 + "\n")
            for i, cluster in enumerate(analysis['temporal_clusters'][:10], 1):
                f.write(f"\nCluster {i}: {len(cluster)} events\n")
                f.write(f"  Time Range: {cluster[0]['timestamp']} to {cluster[-1]['timestamp']}\n")
                for event in cluster[:5]:
                    f.write(f"    [{event['timestamp']}] {event['tool']}: {event['archive']}\n")
            
            # Recommendations
            f.write(f"\n{'=' * 80}\n")
            f.write("RECOMMENDATIONS\n")
            f.write("=" * 80 + "\n")
            f.write("1. Investigate suspicious compression events targeting sensitive directories\n")
            f.write("2. Review archives created in staging locations (/tmp, /dev/shm)\n")
            f.write("3. Check for subsequent network transfer activities\n")
            f.write("4. Verify legitimacy of temporal clusters\n")
            f.write("5. Examine archive contents if files still exist\n")
        
        print(f"[+] Report saved to {output_file}")

if __name__ == '__main__':
    if len(sys.argv) < 2:
        print(f"Usage: {sys.argv[0]} <logfile>")
        sys.exit(1)
    
    detector = CompressionDetector()
    detector.detect_compression(sys.argv[1])
    detector.generate_report()
```

**Usage:**

```bash
chmod +x compression_detector.py
./compression_detector.py /var/log/auth.log
```

### File System Indicators

**Detect created archives in file system:**

```bash
# Find recently created archives
find / -type f \( -name "*.tar.gz" -o -name "*.tgz" -o -name "*.zip" -o -name "*.7z" -o -name "*.rar" \) -mtime -7 2>/dev/null

# Find archives in suspicious locations
find /tmp /dev/shm /var/tmp -type f \( -name "*.tar*" -o -name "*.zip" -o -name "*.7z" \) 2>/dev/null

# Find large archives (potential data consolidation)
find / -type f \( -name "*.tar.gz" -o -name "*.zip" \) -size +100M -mtime -7 2>/dev/null

# Check archive metadata
stat /tmp/*.tar.gz 2>/dev/null

# List archive contents without extracting
tar -tzf suspicious.tar.gz | head -20
unzip -l suspicious.zip | head -20
7z l suspicious.7z | head -20
```

**Detect compression in web server logs:**

```bash
# Downloads of compressed files
awk '$9 == 200 && $7 ~ /\.(tar|gz|zip|7z|rar)$/ {print $1, $4, $7}' /var/log/apache2/access.log

# POST requests creating archives
awk '$6 == "\"POST" && $7 ~ /(backup|export|download|archive)/ {print $0}' /var/log/apache2/access.log
```

## Encryption Before Transfer

Encryption before exfiltration is a sophisticated technique to evade DLP (Data Loss Prevention) systems and hide the nature of transferred data.

### Encryption Tool Detection

**Common encryption tools:**

- `openssl`
- `gpg`/`gpg2`
- `age`
- `ccrypt`
- `mcrypt`
- Custom scripts using Python cryptography libraries
- Windows: `cipher`, `BitLocker`

### Detecting Encryption Activities

**Search for encryption commands:**

```bash
# OpenSSL encryption
grep -rE "openssl\s+(enc|aes|des)" /var/log/

# GPG encryption
grep -rE "gpg\s+(-c|--encrypt|-e|--symmetric)" /var/log/

# Check bash history
grep -E "(openssl|gpg|age)\s+(enc|--encrypt|-e)" ~/.bash_history
find /home -name ".bash_history" -exec grep -H -E "(openssl|gpg)" {} \;

# Audit logs
ausearch -i -x openssl | grep enc
ausearch -i -x gpg | grep encrypt
```

**Detect encryption in process logs:**

```bash
# Current processes
ps aux | grep -E '(openssl|gpg|ccrypt).*enc' | grep -v grep

# System logs
grep -E "openssl.*enc|gpg.*--encrypt" /var/log/syslog

# Command logs from sudo
grep -E "COMMAND.*(openssl|gpg)" /var/log/auth.log | grep -E "(enc|encrypt)"
```

### Encryption Detection Script

```bash
#!/bin/bash
# detect_encryption.sh

LOGDIR="${1:-/var/log}"
OUTPUT="encryption_indicators.txt"

echo "[*] Detecting encryption indicators..."
echo "Encryption Detection Report - $(date)" > "$OUTPUT"
echo "=======================================" >> "$OUTPUT"

# OpenSSL encryption
echo -e "\n[+] OpenSSL Encryption Commands" >> "$OUTPUT"
echo "---" >> "$OUTPUT"
grep -rE "openssl\s+(enc|aes-|des-)" "$LOGDIR" 2>/dev/null | head -20 >> "$OUTPUT"

# GPG encryption
echo -e "\n[+] GPG/PGP Encryption Commands" >> "$OUTPUT"
echo "---" >> "$OUTPUT"
grep -rE "gpg2?\s+(-c|--encrypt|-e|--symmetric)" "$LOGDIR" 2>/dev/null | head -20 >> "$OUTPUT"

# Age encryption
echo -e "\n[+] Age Encryption Tool" >> "$OUTPUT"
echo "---" >> "$OUTPUT"
grep -rE "age\s+-e" "$LOGDIR" 2>/dev/null | head -20 >> "$OUTPUT"

# Base64 encoding (often precedes encryption)
echo -e "\n[+] Base64 Encoding (potential obfuscation)" >> "$OUTPUT"
echo "---" >> "$OUTPUT"
grep -rE "base64\s+(--encode|-e)" "$LOGDIR" 2>/dev/null | head -20 >> "$OUTPUT"

# Python cryptography
echo -e "\n[+] Python Cryptography Usage" >> "$OUTPUT"
echo "---" >> "$OUTPUT"
grep -rE "python.*crypt|from\s+cryptography\s+import" "$LOGDIR" 2>/dev/null | head -20 >> "$OUTPUT"

# Encryption followed by transfer
echo -e "\n[+] Encryption + Network Transfer Correlation" >> "$OUTPUT"
echo "---" >> "$OUTPUT"

# Extract timestamps of encryption operations
grep -rE "(openssl|gpg).*enc" "$LOGDIR" 2>/dev/null | \
    grep -oE '\w{3}\s+\d{1,2}\s+\d{2}:\d{2}:\d{2}' | sort > /tmp/enc_times.tmp

# Extract timestamps of network operations
grep -rE "(scp|curl|wget|nc|ftp)" "$LOGDIR" 2>/dev/null | \
    grep -oE '\w{3}\s+\d{1,2}\s+\d{2}:\d{2}:\d{2}' | sort > /tmp/net_times.tmp

# Find correlations (within 2 minutes)
comm -12 /tmp/enc_times.tmp /tmp/net_times.tmp >> "$OUTPUT"

rm -f /tmp/enc_times.tmp /tmp/net_times.tmp

# Summary
echo -e "\n[+] Summary" >> "$OUTPUT"
echo "---" >> "$OUTPUT"
echo "OpenSSL encryptions: $(grep -rc 'openssl.*enc' "$LOGDIR" 2>/dev/null | awk -F: '{sum+=$2} END {print sum}')" >> "$OUTPUT"
echo "GPG encryptions: $(grep -rc 'gpg.*encrypt' "$LOGDIR" 2>/dev/null | awk -F: '{sum+=$2} END {print sum}')" >> "$OUTPUT"
echo "Base64 encodings: $(grep -rc 'base64.*encode' "$LOGDIR" 2>/dev/null | awk -F: '{sum+=$2} END {print sum}')" >> "$OUTPUT"

echo "[*] Report saved to $OUTPUT"
cat "$OUTPUT"
```

**Usage:**

```bash
chmod +x detect_encryption.sh
./detect_encryption.sh /var/log
```

### Advanced Python Encryption Detector

```python
#!/usr/bin/env python3
import re
import sys
from datetime import datetime, timedelta
from collections import defaultdict

class EncryptionDetector:
    def __init__(self):
        self.encryption_patterns = {
            'openssl_enc': re.compile(r'openssl\s+(enc|aes-\d+-\w+|des3?-\w+)\s+.*-in\s+(\S+)\s+-out\s+(\S+)', re.IGNORECASE),
            'openssl_simple': re.compile(r'openssl\s+enc\s+.*(\S+)', re.IGNORECASE),
            'gpg_encrypt': re.compile(r'gpg2?\s+(--encrypt|-e|--symmetric|-c)\s+.*?(\S+\.\w+)', re.IGNORECASE),
            'age': re.compile(r'age\s+-e\s+.*-o\s+(\S+)', re.IGNORECASE),
            'base64': re.compile(r'base64\s+(--encode|-e)\s+(\S+)', re.IGNORECASE),
            'python_crypt': re.compile(r'python3?\s+.*crypt|from\s+cryptography|from\s+Crypto', re.IGNORECASE),
        }
        
        self.transfer_patterns = {
            'scp': re.compile(r'scp\s+(\S+)\s+(\S+@\S+:\S+)', re.IGNORECASE),
            'curl': re.compile(r'curl\s+.*(-T|--upload-file)\s+(\S+)', re.IGNORECASE),
            'wget': re.compile(r'wget\s+.*--post-file[= ](\S+)', re.IGNORECASE),
            'nc': re.compile(r'nc\s+.*<\s*(\S+)', re.IGNORECASE),
            'ftp_put': re.compile(r'(ftp|sftp).*put\s+(\S+)', re.IGNORECASE),
        }
        
        self.encryption_events = []
        self.transfer_events = []
        
        self.timestamp_pattern = re.compile(
            r'(\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2}|\w{3}\s+\d{1,2}\s+\d{2}:\d{2}:\d{2})'
        )
    
    def parse_timestamp(self, line):
        """Extract timestamp from log line"""
        match = self.timestamp_pattern.search(line)
        if match:
            ts_str = match.group(1)
            try:
                if '-' in ts_str:
                    return datetime.strptime(ts_str[:19], '%Y-%m-%d %H:%M:%S')
                else:
                    return datetime.strptime(f"{datetime.now().year} {ts_str}", '%Y %b %d %H:%M:%S')
            except:
                pass
        return None
    
    def detect_activities(self, logfile):
        """Detect encryption and transfer activities"""
        print(f"[*] Analyzing {logfile}...")
        
        with open(logfile, 'r', errors='ignore') as f:
            for line_num, line in enumerate(f, 1):
                timestamp = self.parse_timestamp(line)
                
                # Detect encryption
                for tool, pattern in self.encryption_patterns.items():
                    match = pattern.search(line)
                    if match:
                        event = {
                            'timestamp': timestamp,
                            'line_num': line_num,
                            'tool': tool,
                            'content': line.strip(),
                            'files': match.groups() if match.groups() else ()
                        }
                        self.encryption_events.append(event)
                
                # Detect transfers
                for method, pattern in self.transfer_patterns.items():
                    match = pattern.search(line)
                    if match:
                        event = {
                            'timestamp': timestamp,
                            'line_num': line_num,
                            'method': method,
                            'content': line.strip(),
                            'files': match.groups() if match.groups() else ()
                        }
                        self.transfer_events.append(event)
    
    def find_enc_transfer_correlation(self, time_window_minutes=5):
        """Find encryption events followed by transfers"""
        correlations = []
        
        enc_with_time = [e for e in self.encryption_events if e['timestamp']]
        trans_with_time = [t for t in self.transfer_events if t['timestamp']]
        
        for enc_event in enc_with_time:
            for trans_event in trans_with_time:
                time_diff = trans_event['timestamp'] - enc_event['timestamp']
                
                if timedelta(0) <= time_diff <= timedelta(minutes=time_window_minutes):
                    # Check if same file or similar name
                    enc_files = set(enc_event.get('files', ()))
                    trans_files = set(trans_event.get('files', ()))
                    
                    file_match = bool(enc_files & trans_files)
                    
                    correlations.append({
                        'encryption': enc_event,
                        'transfer': trans_event,
                        'time_diff': time_diff,
                        'file_match': file_match
                    })
        
        return correlations
    
    def generate_report(self, output_file='encryption_report.txt'):
        """Generate comprehensive report"""
        correlations = self.find_enc_transfer_correlation()
        
        with open(output_file, 'w') as f:
            f.write("=" * 80 + "\n")
            f.write("ENCRYPTION & EXFILTRATION DETECTION REPORT\n")
            f.write("=" * 80 + "\n")
            f.write(f"Generated: {datetime.now().isoformat()}\n")
            f.write(f"Encryption Events: {len(self.encryption_events)}\n")
            f.write(f"Transfer Events: {len(self.transfer_events)}\n")
            f.write(f"Correlated Events: {len(correlations)}\n\n")
            
            # Encryption events
            f.write("=" * 80 + "\n")
            f.write("ENCRYPTION ACTIVITIES\n")
            f.write("=" * 80 + "\n")
            
            enc_by_tool = defaultdict(list)
            for event in self.encryption_events:
                enc_by_tool[event['tool']].append(event)
            
            for tool, events in sorted(enc_by_tool.items()):
                f.write(f"\n{tool.upper()}: {len(events)} events\n")
                f.write("-" * 80 + "\n")
                for event in events[:10]:
                    ts = event['timestamp'].isoformat() if event['timestamp'] else 'No timestamp'
                    f.write(f"[{ts}] Line {event['line_num']}\n")
                    f.write(f"  {event['content'][:150]}\n")
            
            # Transfer events
            f.write(f"\n{'=' * 80}\n")
            f.write("TRANSFER ACTIVITIES\n")
            f.write("=" * 80 + "\n")
            
            trans_by_method = defaultdict(list)
            for event in self.transfer_events:
                trans_by_method[event['method']].append(event)
            
            for method, events in sorted(trans_by_method.items()):
                f.write(f"\n{method.upper()}: {len(events)} events\n")
                f.write("-" * 80 + "\n")
                for event in events[:10]:
                    ts = event['timestamp'].isoformat() if event['timestamp'] else 'No timestamp'
                    f.write(f"[{ts}] Line {event['line_num']}\n")
                    f.write(f"  {event['content'][:150]}\n")
            
            # Correlations
            f.write(f"\n{'=' * 80}\n")
            f.write(f"ENCRYPTION  TRANSFER CORRELATIONS: {len(correlations)}\n")
            f.write("=" * 80 + "\n")
            
            for i, corr in enumerate(correlations[:20], 1):
                f.write(f"\n[Correlation {i}] Time Difference: {corr['time_diff']}\n")
                f.write(f"  File Match: {'YES' if corr['file_match'] else 'NO'}\n")
                
                enc = corr['encryption']
                f.write(f"\n  ENCRYPTION [{enc['timestamp']}]:\n")
                f.write(f"    {enc['content'][:120]}\n")
                
                trans = corr['transfer']
                f.write(f"\n  TRANSFER [{trans['timestamp']}]:\n")
                f.write(f"    {trans['content'][:120]}\n")
                f.write("  " + "-" * 76 + "\n")
            
            # Risk assessment
            f.write(f"\n{'=' * 80}\n")
            f.write("RISK ASSESSMENT\n")
            f.write("=" * 80 + "\n")
            
            high_risk_correlations = [c for c in correlations if c['file_match'] or c['time_diff'] < timedelta(minutes=2)]
            f.write(f"High Risk Correlations: {len(high_risk_correlations)}\n")
            f.write(f"  - File name matches: {len([c for c in correlations if c['file_match']])}\n")
            f.write(f"  - Within 2 minutes: {len([c for c in correlations if c['time_diff'] < timedelta(minutes=2)])}\n")
            
            # Recommendations
            f.write(f"\n{'=' * 80}\n")
            f.write("RECOMMENDATIONS\n") f.write("=" * 80 + "\n")
            f.write("1. Investigate all encryption  transfer correlations within 2 minutes\n") 
            f.write("2. Examine files that were encrypted then transferred\n") 
            f.write("3. Check destination IPs/domains for transfer events\n") 
            f.write("4. Review user accounts performing encryption operations\n") 
            f.write("5. Verify business justification for encryption activities\n") 
            f.write("6. Check if encrypted files still exist on system for analysis\n")

	    print(f"[+] Report saved to {output_file}")

if __name__ == '__main__': 
	if len(sys.argv) < 2: 
		print(f"Usage: {sys.argv[0]} <logfile>") 
		sys.exit(1)

	detector = EncryptionDetector()
	detector.detect_activities(sys.argv[1])
	detector.generate_report()
````

**Usage:**
```bash
chmod +x encryption_detector.py
./encryption_detector.py /var/log/auth.log
````

### Cryptographic Signature Detection

**Detect encrypted file signatures in file system:**

```bash
# Find files with encryption signatures
find / -type f -exec file {} \; 2>/dev/null | grep -iE "(encrypted|gpg|openssl|aes)"

# OpenSSL encrypted files (start with "Salted__")
find /tmp /dev/shm /var/tmp -type f -exec head -c 8 {} \; -exec echo " {}" \; 2>/dev/null | grep "Salted__"

# GPG encrypted files
find / -type f -name "*.gpg" -o -name "*.asc" -o -name "*.pgp" 2>/dev/null

# Check file magic bytes
xxd -l 16 suspicious_file | grep -E "(5361 6c74 6564 5f5f|8501|8502)"  # Salted__, GPG headers
```

## Staging Directories

Staging directories are temporary locations where attackers consolidate, compress, and prepare data before exfiltration. Detecting unusual activity in these locations is critical.

### Common Staging Locations

**Typical staging directories:**

- `/tmp` - World-writable, often not monitored
- `/dev/shm` - RAM-based filesystem, volatile
- `/var/tmp` - Persists across reboots
- `/var/spool` - Spool directories
- User home directories: `~/.cache`, `~/.local/tmp`
- Web server writable directories: `/var/www/html/uploads`, `/var/www/html/tmp`
- Application temp directories: `/opt/app/tmp`
- Hidden directories: `/var/lib/...`, system service directories

### Detecting Staging Activity

**Monitor staging directories for suspicious files:**

```bash
# Find recently created/modified files in staging locations
find /tmp /dev/shm /var/tmp -type f -mtime -1 -ls 2>/dev/null

# Find large files in staging areas (potential data consolidation)
find /tmp /dev/shm /var/tmp -type f -size +50M 2>/dev/null

# Find archives in staging directories
find /tmp /dev/shm /var/tmp -type f \( -name "*.tar*" -o -name "*.zip" -o -name "*.7z" -o -name "*.rar" \) 2>/dev/null

# Find encrypted files in staging
find /tmp /dev/shm /var/tmp -type f \( -name "*.enc" -o -name "*.gpg" -o -name "*.aes" \) 2>/dev/null

# Find files with suspicious naming patterns
find /tmp /dev/shm /var/tmp -type f -regextype posix-extended -regex '.*/[a-f0-9]{8,}.*' 2>/dev/null

# Hidden files in staging (often used by attackers)
find /tmp /dev/shm /var/tmp -type f -name ".*" 2>/dev/null
```

**Check inode usage (rapid file creation/deletion):**

```bash
# Monitor inode changes over time
df -i /tmp /dev/shm /var/tmp

# Check directory entry counts
ls -la /tmp | wc -l
ls -la /dev/shm | wc -l
```

### Staging Directory Monitoring Script

```bash
#!/bin/bash
# monitor_staging.sh

OUTPUT="staging_activity_$(date +%Y%m%d_%H%M%S).txt"

cat << EOF > "$OUTPUT"
================================================================================
STAGING DIRECTORY ACTIVITY REPORT
================================================================================
Generated: $(date)
Monitored Locations: /tmp, /dev/shm, /var/tmp

EOF

echo "[*] Scanning staging directories..."

# Function to analyze directory
analyze_staging_dir() {
    local dir="$1"
    
    echo "
================================================================================ 
DIRECTORY: $dir
================================================================================" >> "$OUTPUT"
    
    # Basic statistics
    echo "[+] Statistics:" >> "$OUTPUT"
    echo "  Total files: $(find "$dir" -type f 2>/dev/null | wc -l)" >> "$OUTPUT"
    echo "  Total size: $(du -sh "$dir" 2>/dev/null | awk '{print $1}')" >> "$OUTPUT"
    echo "  Inode usage: $(df -i "$dir" 2>/dev/null | tail -1 | awk '{print $5}')" >> "$OUTPUT"
    
    # Recently modified files (last 24 hours)
    echo -e "\n[+] Recently Modified Files (24h):" >> "$OUTPUT"
    find "$dir" -type f -mtime -1 -exec ls -lh {} \; 2>/dev/null | head -20 >> "$OUTPUT"
    
    # Large files
    echo -e "\n[+] Large Files (>50MB):" >> "$OUTPUT"
    find "$dir" -type f -size +50M -exec ls -lh {} \; 2>/dev/null >> "$OUTPUT"
    
    # Archives
    echo -e "\n[+] Archive Files:" >> "$OUTPUT"
    find "$dir" -type f \( -name "*.tar*" -o -name "*.zip" -o -name "*.7z" -o -name "*.rar" \) -exec ls -lh {} \; 2>/dev/null >> "$OUTPUT"
    
    # Encrypted files
    echo -e "\n[+] Potentially Encrypted Files:" >> "$OUTPUT"
    find "$dir" -type f \( -name "*.enc" -o -name "*.gpg" -o -name "*.aes" \) -exec ls -lh {} \; 2>/dev/null >> "$OUTPUT"
    find "$dir" -type f -exec file {} \; 2>/dev/null | grep -i "encrypted\|openssl\|gpg" >> "$OUTPUT"
    
    # Hidden files
    echo -e "\n[+] Hidden Files:" >> "$OUTPUT"
    find "$dir" -type f -name ".*" -exec ls -lh {} \; 2>/dev/null | head -20 >> "$OUTPUT"
    
    # Suspicious naming patterns
    echo -e "\n[+] Suspicious File Names:" >> "$OUTPUT"
    find "$dir" -type f -regextype posix-extended -regex '.*/[a-f0-9]{8,}' -exec ls -lh {} \; 2>/dev/null | head -20 >> "$OUTPUT"
    find "$dir" -type f -regextype posix-extended -regex '.*\.(tmp|temp|bak|backup|dump|data|out)' -exec ls -lh {} \; 2>/dev/null | head -20 >> "$OUTPUT"
    
    # World-writable files (potential drops)
    echo -e "\n[+] World-Writable Files:" >> "$OUTPUT"
    find "$dir" -type f -perm -002 -exec ls -lh {} \; 2>/dev/null | head -20 >> "$OUTPUT"
    
    # Files owned by unexpected users
    echo -e "\n[+] Files by Owner:" >> "$OUTPUT"
    find "$dir" -type f -exec stat -c "%U %n" {} \; 2>/dev/null | sort | uniq -c | sort -rn | head -10 >> "$OUTPUT"
}

# Analyze each staging directory
for dir in /tmp /dev/shm /var/tmp; do
    if [ -d "$dir" ]; then
        analyze_staging_dir "$dir"
    fi
done

# Check for file access patterns in logs
echo "
================================================================================ 
LOG ANALYSIS - STAGING DIRECTORY ACCESS
================================================================================" >> "$OUTPUT"

echo "[+] Recent file operations in staging directories:" >> "$OUTPUT"
grep -rE "/tmp/|/dev/shm/|/var/tmp/" /var/log/ 2>/dev/null | grep -E "(created|wrote|copied|moved)" | tail -50 >> "$OUTPUT"

# Summary and risk indicators
echo "
================================================================================ 
RISK INDICATORS
================================================================================" >> "$OUTPUT"

large_files=$(find /tmp /dev/shm /var/tmp -type f -size +50M 2>/dev/null | wc -l)
archives=$(find /tmp /dev/shm /var/tmp -type f \( -name "*.tar*" -o -name "*.zip" \) 2>/dev/null | wc -l)
encrypted=$(find /tmp /dev/shm /var/tmp -type f \( -name "*.enc" -o -name "*.gpg" \) 2>/dev/null | wc -l)
hidden=$(find /tmp /dev/shm /var/tmp -type f -name ".*" 2>/dev/null | wc -l)

echo "Large files (>50MB): $large_files" >> "$OUTPUT"
echo "Archive files: $archives" >> "$OUTPUT"
echo "Encrypted files: $encrypted" >> "$OUTPUT"
echo "Hidden files: $hidden" >> "$OUTPUT"

risk_score=0
[ $large_files -gt 5 ] && risk_score=$((risk_score + 3))
[ $archives -gt 10 ] && risk_score=$((risk_score + 4))
[ $encrypted -gt 0 ] && risk_score=$((risk_score + 5))
[ $hidden -gt 20 ] && risk_score=$((risk_score + 2))

echo -e "\nRisk Score: $risk_score/14" >> "$OUTPUT"
if [ $risk_score -ge 8 ]; then
    echo "Risk Level: HIGH - Immediate investigation recommended" >> "$OUTPUT"
elif [ $risk_score -ge 5 ]; then
    echo "Risk Level: MEDIUM - Review findings" >> "$OUTPUT"
else
    echo "Risk Level: LOW - Normal activity" >> "$OUTPUT"
fi

echo "[*] Report saved to $OUTPUT"
cat "$OUTPUT"
```

**Usage:**

```bash
chmod +x monitor_staging.sh
./monitor_staging.sh
```

### Advanced Python Staging Analyzer

```python
#!/usr/bin/env python3
import os
import sys
import time
import hashlib
from pathlib import Path
from datetime import datetime, timedelta
from collections import defaultdict

class StagingAnalyzer:
    def __init__(self, staging_dirs=None):
        self.staging_dirs = staging_dirs or ['/tmp', '/dev/shm', '/var/tmp']
        self.suspicious_extensions = [
            '.tar', '.tar.gz', '.tgz', '.tar.bz2', '.zip', '.7z', '.rar',
            '.enc', '.gpg', '.aes', '.pgp',
            '.sql', '.dump', '.backup', '.bak', '.db'
        ]
        self.suspicious_patterns = [
            r'^[a-f0-9]{8,}$',  # Hex names
            r'^\.[a-z0-9]+$',   # Hidden files
            r'^(data|dump|backup|export|archive)',  # Common staging names
        ]
        
        self.findings = {
            'large_files': [],
            'archives': [],
            'encrypted': [],
            'suspicious_names': [],
            'rapid_creation': [],
            'unusual_ownership': [],
        }
    
    def analyze_file(self, filepath):
        """Analyze individual file for suspicious indicators"""
        try:
            stat_info = os.stat(filepath)
            file_info = {
                'path': filepath,
                'size': stat_info.st_size,
                'mtime': datetime.fromtimestamp(stat_info.st_mtime),
                'atime': datetime.fromtimestamp(stat_info.st_atime),
                'ctime': datetime.fromtimestamp(stat_info.st_ctime),
                'owner_uid': stat_info.st_uid,
                'mode': oct(stat_info.st_mode),
            }
            
            # Get file type
            try:
                import magic
                file_info['mime_type'] = magic.from_file(filepath, mime=True)
                file_info['description'] = magic.from_file(filepath)
            except:
                file_info['mime_type'] = 'unknown'
                file_info['description'] = 'unknown'
            
            return file_info
        except Exception as e:
            return None
    
    def check_encryption_signature(self, filepath):
        """Check if file has encryption signatures"""
        try:
            with open(filepath, 'rb') as f:
                header = f.read(16)
                
                # OpenSSL enc format
                if header.startswith(b'Salted__'):
                    return 'OpenSSL'
                
                # GPG format
                if header[0:2] in [b'\x85\x01', b'\x85\x02', b'\x99\x00']:
                    return 'GPG'
                
                # High entropy check (simplified)
                if len(set(header)) > 12:  # More than 12 unique bytes in 16
                    return 'Possible (high entropy)'
                
        except:
            pass
        return None
    
    def scan_staging_directories(self):
        """Scan all staging directories"""
        print("[*] Scanning staging directories...")
        
        now = datetime.now()
        recent_threshold = now - timedelta(hours=24)
        
        for staging_dir in self.staging_dirs:
            if not os.path.exists(staging_dir):
                continue
                
            print(f"[*] Analyzing {staging_dir}...")
            
            try:
                for root, dirs, files in os.walk(staging_dir):
                    for filename in files:
                        filepath = os.path.join(root, filename)
                        file_info = self.analyze_file(filepath)
                        
                        if not file_info:
                            continue
                        
                        # Check for large files
                        if file_info['size'] > 50 * 1024 * 1024:  # > 50MB
                            self.findings['large_files'].append(file_info)
                        
                        # Check for archives
                        if any(filename.endswith(ext) for ext in self.suspicious_extensions[:7]):
                            self.findings['archives'].append(file_info)
                        
                        # Check for encryption
                        enc_sig = self.check_encryption_signature(filepath)
                        if enc_sig or any(filename.endswith(ext) for ext in self.suspicious_extensions[7:10]):
                            file_info['encryption_type'] = enc_sig
                            self.findings['encrypted'].append(file_info)
                        
                        # Check suspicious naming
                        import re
                        basename = os.path.basename(filename)
                        if any(re.match(pattern, basename) for pattern in self.suspicious_patterns):
                            self.findings['suspicious_names'].append(file_info)
                        
                        # Check rapid creation (within last hour)
                        if file_info['ctime'] > now - timedelta(hours=1):
                            self.findings['rapid_creation'].append(file_info)
                        
                        # Check unusual ownership (root-owned files in /tmp)
                        if staging_dir == '/tmp' and file_info['owner_uid'] == 0:
                            self.findings['unusual_ownership'].append(file_info)
                        
            except PermissionError:
                print(f"[!] Permission denied: {staging_dir}")
                continue
    
    def detect_temporal_patterns(self):
        """Detect temporal patterns in file creation"""
        creation_times = defaultdict(list)
        
        for finding_type, files in self.findings.items():
            for file_info in files:
                # Group by 5-minute intervals
                time_bucket = file_info['ctime'].replace(second=0, microsecond=0)
                time_bucket = time_bucket.replace(minute=(time_bucket.minute // 5) * 5)
                creation_times[time_bucket].append(file_info)
        
        # Find clusters (>5 files in 5-minute window)
        clusters = {time: files for time, files in creation_times.items() if len(files) > 5}
        
        return clusters
    
    def calculate_risk_score(self):
        """Calculate overall risk score"""
        score = 0
        
        score += min(len(self.findings['large_files']) * 2, 10)
        score += min(len(self.findings['archives']) * 3, 15)
        score += len(self.findings['encrypted']) * 5
        score += min(len(self.findings['suspicious_names']), 10)
        score += min(len(self.findings['rapid_creation']) // 5, 10)
        score += min(len(self.findings['unusual_ownership']), 10)
        
        return min(score, 100)
    
    def generate_report(self, output_file='staging_analysis.txt'):
        """Generate comprehensive report"""
        clusters = self.detect_temporal_patterns()
        risk_score = self.calculate_risk_score()
        
        with open(output_file, 'w') as f:
            f.write("=" * 80 + "\n")
            f.write("STAGING DIRECTORY ANALYSIS REPORT\n")
            f.write("=" * 80 + "\n")
            f.write(f"Generated: {datetime.now().isoformat()}\n")
            f.write(f"Analyzed Directories: {', '.join(self.staging_dirs)}\n")
            f.write(f"Overall Risk Score: {risk_score}/100\n\n")
            
            # Large files
            f.write("=" * 80 + "\n")
            f.write(f"LARGE FILES (>50MB): {len(self.findings['large_files'])}\n")
            f.write("=" * 80 + "\n")
            for file_info in sorted(self.findings['large_files'], key=lambda x: x['size'], reverse=True)[:20]:
                size_mb = file_info['size'] / (1024 * 1024)
                f.write(f"\nFile: {file_info['path']}\n")
                f.write(f"  Size: {size_mb:.2f} MB\n")
                f.write(f"  Created: {file_info['ctime']}\n")
                f.write(f"  Modified: {file_info['mtime']}\n")
                f.write(f"  Type: {file_info['description'][:60]}\n")
            
            # Archives
            f.write(f"\n{'=' * 80}\n")
            f.write(f"ARCHIVE FILES: {len(self.findings['archives'])}\n")
            f.write("=" * 80 + "\n")
            for file_info in self.findings['archives'][:30]:
                f.write(f"\nFile: {file_info['path']}\n")
                f.write(f"  Size: {file_info['size'] / 1024:.2f} KB\n")
                f.write(f"  Created: {file_info['ctime']}\n")
            
            # Encrypted files
            f.write(f"\n{'=' * 80}\n")
            f.write(f"ENCRYPTED/OBFUSCATED FILES: {len(self.findings['encrypted'])}\n")
            f.write("=" * 80 + "\n")
            for file_info in self.findings['encrypted']:
                f.write(f"\nFile: {file_info['path']}\n")
                f.write(f"  Encryption Type: {file_info.get('encryption_type', 'Unknown')}\n")
                f.write(f"  Size: {file_info['size'] / 1024:.2f} KB\n")
                f.write(f"  Created: {file_info['ctime']}\n")
            
            # Suspicious names
            f.write(f"\n{'=' * 80}\n")
            f.write(f"SUSPICIOUS FILE NAMES: {len(self.findings['suspicious_names'])}\n")
            f.write("=" * 80 + "\n")
            for file_info in self.findings['suspicious_names'][:30]:
                f.write(f"  {file_info['path']} (Created: {file_info['ctime']})\n")
            
            # Temporal clusters
            f.write(f"\n{'=' * 80}\n")
            f.write(f"TEMPORAL CLUSTERS (>5 files in 5-min window): {len(clusters)}\n")
            f.write("=" * 80 + "\n")
            for time_bucket, files in sorted(clusters.items(), key=lambda x: len(x[1]), reverse=True)[:10]:
                f.write(f"\n[{time_bucket}] - {len(files)} files created\n")
                for file_info in files[:5]:
                    f.write(f"  - {file_info['path']}\n")
            
            # Recommendations
            f.write(f"\n{'=' * 80}\n")
            f.write("RISK ASSESSMENT & RECOMMENDATIONS\n")
            f.write("=" * 80 + "\n")
            
            if risk_score >= 50:
                f.write("RISK LEVEL: CRITICAL\n\n")
                f.write("IMMEDIATE ACTIONS REQUIRED:\n")
                f.write("1. Isolate affected systems immediately\n")
                f.write("2. Capture memory dumps and disk images\n")
                f.write("3. Examine encrypted files and archives\n")
                f.write("4. Review network logs for data transfers\n")
                f.write("5. Initiate incident response procedures\n")
            elif risk_score >= 25:
                f.write("RISK LEVEL: HIGH\n\n")
                f.write("RECOMMENDED ACTIONS:\n")
                f.write("1. Investigate all flagged files\n")
                f.write("2. Review user activity logs\n")
                f.write("3. Check for network connections from affected systems\n")
                f.write("4. Verify legitimacy of staging activities\n")
            else:
                f.write("RISK LEVEL: LOW TO MEDIUM\n\n")
                f.write("RECOMMENDED ACTIONS:\n")
                f.write("1. Continue monitoring staging directories\n")
                f.write("2. Review findings for false positives\n")
                f.write("3. Implement automated alerting for threshold violations\n")
            
            f.write(f"\n{'=' * 80}\n")
            f.write("SPECIFIC FINDINGS TO INVESTIGATE:\n")
            f.write("=" * 80 + "\n")
            
            if self.findings['encrypted']:
                f.write(f"- {len(self.findings['encrypted'])} encrypted files detected (HIGH PRIORITY)\n")
            if len(self.findings['large_files']) > 10:
                f.write(f"- Unusual number of large files ({len(self.findings['large_files'])}) in staging\n")
            if len(clusters) > 3:
                f.write(f"- Multiple temporal clusters suggest automated/scripted activity\n")
            if self.findings['unusual_ownership']:
                f.write(f"- {len(self.findings['unusual_ownership'])} root-owned files in /tmp (unusual)\n")
        
        print(f"[+] Report saved to {output_file}")

if __name__ == '__main__':
    staging_dirs = sys.argv[1:] if len(sys.argv) > 1 else ['/tmp', '/dev/shm', '/var/tmp']
    
    analyzer = StagingAnalyzer(staging_dirs)
    analyzer.scan_staging_directories()
    analyzer.generate_report()
```

**Usage:**

```bash
chmod +x staging_analyzer.py
sudo ./staging_analyzer.py /tmp /dev/shm /var/tmp
```

### Continuous Monitoring Setup

**Using inotify for real-time monitoring:**

```bash
#!/bin/bash
# realtime_staging_monitor.sh

WATCH_DIRS=("/tmp" "/dev/shm" "/var/tmp")
LOG_FILE="/var/log/staging_monitor.log"

echo "[*] Starting real-time staging directory monitor..."
echo "Monitor started: $(date)" >> "$LOG_FILE"

# Install inotify-tools if not present
if ! command -v inotifywait &> /dev/null; then
    echo "[!] Installing inotify-tools..."
    sudo apt-get install -y inotify-tools
fi

# Monitor directories
inotifywait -m -r -e create,moved_to,modify "${WATCH_DIRS[@]}" --format '%T %w%f %e' --timefmt '%Y-%m-%d %H:%M:%S' | \
while read timestamp filepath event; do
    echo "[$timestamp] $event: $filepath" >> "$LOG_FILE"
    
    # Alert on specific patterns
    if [[ "$filepath" =~ \.(tar|gz|zip|7z|enc|gpg)$ ]]; then
        echo "[ALERT] Archive/Encrypted file detected: $filepath" >> "$LOG_FILE"
        # Add notification mechanism here (email, syslog, SIEM, etc.)
    fi
    
    # Alert on large files
    if [ -f "$filepath" ]; then
        size=$(stat -f%z "$filepath" 2>/dev/null || stat -c%s "$filepath" 2>/dev/null)
        if [ "$size" -gt $((50 * 1024 * 1024)) ]; then
            echo "[ALERT] Large file detected: $filepath ($(($size / 1024 / 1024))MB)" >> "$LOG_FILE"
        fi
    fi
done
```

---

**Important related topics:**

- Network traffic analysis for data exfiltration
- DNS tunneling detection
- Steganography detection in transferred files
- Cloud storage exfiltration (Dropbox, Google Drive API usage)
- Protocol-based exfiltration (HTTPS, DNS, ICMP tunneling)
- Memory forensics for in-memory staging

---

# Troubleshooting & Debugging

Log analysis frequently encounters data quality issues that require diagnostic and remediation techniques. Understanding how to identify, work around, and repair log integrity problems is essential for accurate forensic analysis.

## Incomplete Logs

Incomplete logs result from rotation during analysis, premature termination, buffer overflows, or intentional truncation. Detection and recovery strategies vary based on the incompleteness type.

**Detecting incomplete logs:**

```bash
# Check for abrupt log termination (no proper shutdown messages)
tail -20 /var/log/syslog | grep -i "shutdown\|stopping\|terminated"
if [ $? -ne 0 ]; then
    echo "[!] Warning: Log may be incomplete (no termination messages)"
fi

# Check for mid-entry truncation (incomplete lines)
awk 'length($0) < 10 {print NR": "$0}' /var/log/apache2/access.log | head -20

# Identify gaps in sequential log entries
awk '{
    if (NR > 1 && prev_time != "" && $1" "$2" "$3 != prev_time) {
        print "Potential gap at line "NR": "prev_time" -> "$1" "$2" "$3
    }
    prev_time = $1" "$2" "$3
}' /var/log/syslog | head -10

# Check for incomplete multiline entries (stack traces, etc.)
grep -n "^[[:space:]]" /var/log/app.log | \
    awk -F: '{
        if (prev+1 != $1) print "Orphaned continuation line at "$1
        prev=$1
    }'
```

**Detecting log rotation during capture:**

```bash
# Compare inode numbers (changed if rotated)
LOGFILE="/var/log/auth.log"
INODE_START=$(stat -c %i "$LOGFILE")

# Monitor for changes
while true; do
    INODE_CURRENT=$(stat -c %i "$LOGFILE")
    if [ "$INODE_START" != "$INODE_CURRENT" ]; then
        echo "[!] Log rotation detected at $(date)"
        echo "    Original inode: $INODE_START"
        echo "    New inode: $INODE_CURRENT"
        break
    fi
    sleep 5
done

# Check for rotated log files
ls -lah /var/log/auth.log* | awk '{print $6, $7, $8, $9}'

# Detect rotation by file size decrease
PREV_SIZE=0
while true; do
    CURR_SIZE=$(stat -c %s "$LOGFILE")
    if [ $CURR_SIZE -lt $PREV_SIZE ]; then
        echo "[!] File size decreased - rotation likely occurred"
        echo "    Previous: $PREV_SIZE bytes"
        echo "    Current: $CURR_SIZE bytes"
    fi
    PREV_SIZE=$CURR_SIZE
    sleep 10
done
```

**Recovering from log rotation:**

```bash
# Merge rotated logs chronologically
merge_rotated_logs() {
    local base_log="$1"
    local output="merged_${base_log##*/}"
    
    # Find all rotated versions
    # auth.log, auth.log.1, auth.log.2.gz, etc.
    
    {
        # Decompress and concatenate in reverse order (oldest first)
        for file in $(ls -r ${base_log}* | tac); do
            if [[ $file =~ \.gz$ ]]; then
                zcat "$file"
            elif [[ $file =~ \.bz2$ ]]; then
                bzcat "$file"
            else
                cat "$file"
            fi
        done
    } | sort -s -k1,3 > "$output"
    
    echo "[+] Merged logs saved to: $output"
}

# Usage
merge_rotated_logs /var/log/auth.log
```

**Handling truncated entries:**

```bash
# Identify and extract truncated lines
find_truncated_entries() {
    local logfile="$1"
    local expected_fields="$2"  # Number of expected fields
    
    echo "[*] Scanning for truncated entries..."
    
    awk -v fields="$expected_fields" '{
        if (NF < fields) {
            print NR": Expected "fields" fields, got "NF" - "$0
        }
    }' "$logfile"
}

# Example: Apache logs should have ~10 fields
find_truncated_entries /var/log/apache2/access.log 10

# Attempt reconstruction from partial data
reconstruct_partial_entry() {
    local partial_line="$1"
    
    # Extract available fields
    local ip=$(echo "$partial_line" | grep -oP '^\d+\.\d+\.\d+\.\d+')
    local timestamp=$(echo "$partial_line" | grep -oP '\[\d{2}/\w+/\d{4}:\d{2}:\d{2}:\d{2}[^\]]*\]')
    
    echo "Partial reconstruction:"
    echo "  IP: ${ip:-MISSING}"
    echo "  Timestamp: ${timestamp:-MISSING}"
    echo "  [Inference] Complete entry cannot be reliably reconstructed"
}
```

**Detecting log buffer overflow:**

```bash
# Check for kernel ring buffer overruns
dmesg | grep -i "buffer overflow\|overrun\|lost messages"

# Check syslog for dropped messages
grep -i "imuxsock.*rate-limit" /var/log/syslog | tail -20

# Example output:
# rsyslogd: imuxsock lost 142 messages from pid 1234 due to rate-limiting

# Increase rate limits to prevent future drops
cat >> /etc/rsyslog.conf << 'EOF'
# Increase rate limiting
$SystemLogRateLimitInterval 0
$SystemLogRateLimitBurst 0
EOF

systemctl restart rsyslog

# Check for message queue saturation
grep -i "queue.*full\|queue.*overflow" /var/log/syslog
```

**Reconstructing timeline with incomplete logs:**

```bash
# Build timeline from multiple partial sources
build_timeline() {
    local output="timeline_$(date +%Y%m%d_%H%M%S).txt"
    
    echo "[*] Building timeline from available sources..."
    
    {
        # System logs
        grep -h "$(date -d '1 hour ago' '+%b %d %H')" /var/log/syslog* 2>/dev/null | \
            awk '{print $1" "$2" "$3" [SYSLOG] "$0}'
        
        # Auth logs
        grep -h "$(date -d '1 hour ago' '+%b %d %H')" /var/log/auth.log* 2>/dev/null | \
            awk '{print $1" "$2" "$3" [AUTH] "$0}'
        
        # Web logs (convert Apache timestamp)
        grep "$(date -d '1 hour ago' '+%d/%b/%Y:%H')" /var/log/apache2/access.log 2>/dev/null | \
            awk '{match($4, /\[([0-9]{2})\/([A-Za-z]{3})\/([0-9]{4}):([0-9:]+)/, a); 
                  print a[1]" "a[2]" "a[4]" [WEB] "$0}'
        
    } | sort -k1M -k2n -k3 > "$output"
    
    echo "[+] Timeline created: $output"
    echo "[+] Entries: $(wc -l < $output)"
    
    # Identify gaps
    echo ""
    echo "[*] Analyzing gaps..."
    awk '{
        curr_time = $1" "$2" "$3
        if (NR > 1 && prev_time != curr_time) {
            print "Timeline gap: "prev_time" -> "curr_time
        }
        prev_time = curr_time
    }' "$output" | head -10
}

# Usage
build_timeline
```

**Handling incomplete JSON logs:**

```bash
# Detect and repair incomplete JSON entries
repair_json_logs() {
    local input="$1"
    local output="${input%.log}_repaired.log"
    
    echo "[*] Attempting to repair JSON logs..."
    
    python3 << EOF
import json
import sys

repaired = 0
failed = 0
output_lines = []

with open("$input", 'r') as f:
    for line_num, line in enumerate(f, 1):
        line = line.strip()
        if not line:
            continue
            
        try:
            # Try parsing as-is
            json.loads(line)
            output_lines.append(line)
        except json.JSONDecodeError as e:
            # Attempt repairs
            
            # Missing closing brace
            if line.count('{') > line.count('}'):
                repaired_line = line + '}'
                try:
                    json.loads(repaired_line)
                    output_lines.append(repaired_line)
                    repaired += 1
                    print(f"[+] Line {line_num}: Added missing closing brace", file=sys.stderr)
                    continue
                except:
                    pass
            
            # Missing closing bracket
            if line.count('[') > line.count(']'):
                repaired_line = line + ']'
                try:
                    json.loads(repaired_line)
                    output_lines.append(repaired_line)
                    repaired += 1
                    print(f"[+] Line {line_num}: Added missing closing bracket", file=sys.stderr)
                    continue
                except:
                    pass
            
            # Unclosed string
            if line.count('"') % 2 != 0:
                repaired_line = line + '"}'
                try:
                    json.loads(repaired_line)
                    output_lines.append(repaired_line)
                    repaired += 1
                    print(f"[+] Line {line_num}: Closed unclosed string", file=sys.stderr)
                    continue
                except:
                    pass
            
            # Cannot repair
            failed += 1
            print(f"[!] Line {line_num}: Cannot repair - {str(e)[:50]}", file=sys.stderr)

with open("$output", 'w') as f:
    for line in output_lines:
        f.write(line + '\n')

print(f"\n[*] Repaired: {repaired}, Failed: {failed}, Total: {len(output_lines)}", file=sys.stderr)
EOF
    
    echo "[+] Output saved to: $output"
}

# Usage
repair_json_logs app.log
```

**Extracting data from incomplete archives:**

```bash
# Recover data from partially corrupted compressed logs
recover_compressed_logs() {
    local compressed_file="$1"
    local output="${compressed_file%.gz}_recovered.log"
    
    echo "[*] Attempting recovery from $compressed_file..."
    
    # Try standard decompression
    if gzip -t "$compressed_file" 2>/dev/null; then
        echo "[+] File is intact"
        zcat "$compressed_file" > "$output"
        return 0
    fi
    
    echo "[!] File is corrupted, attempting partial recovery..."
    
    # Recover what's possible
    zcat "$compressed_file" 2>/dev/null > "$output" || true
    
    if [ -s "$output" ]; then
        echo "[+] Partial recovery successful: $(wc -l < $output) lines recovered"
    else
        echo "[!] Recovery failed - file may be completely corrupted"
        
        # Try gunzip with --force
        gunzip -c -f "$compressed_file" > "$output" 2>/dev/null || true
        
        if [ -s "$output" ]; then
            echo "[+] Forced recovery extracted some data: $(wc -l < $output) lines"
        fi
    fi
}

# Usage
recover_compressed_logs /var/log/syslog.2.gz
```

## Corrupted Log Files

Corrupted logs result from filesystem errors, hardware failures, improper shutdowns, or malicious tampering. Detection and remediation require understanding corruption patterns.

**Detecting file corruption:**

```bash
# Check filesystem integrity
check_filesystem_corruption() {
    local logfile="$1"
    
    echo "[*] Checking file system integrity for $logfile"
    
    # Basic file accessibility
    if [ ! -r "$logfile" ]; then
        echo "[!] File is not readable"
        return 1
    fi
    
    # Check for null bytes (corruption indicator)
    if grep -q $'\x00' "$logfile" 2>/dev/null; then
        echo "[!] Null bytes detected - file may be corrupted"
        
        # Count null bytes
        null_count=$(tr -cd '\000' < "$logfile" | wc -c)
        echo "    Found $null_count null bytes"
    fi
    
    # Check for non-printable characters (excessive amounts)
    non_printable=$(cat "$logfile" | tr -d '[:print:]\n\r\t' | wc -c)
    total_chars=$(wc -c < "$logfile")
    
    if [ $total_chars -gt 0 ]; then
        percent=$((non_printable * 100 / total_chars))
        if [ $percent -gt 5 ]; then
            echo "[!] High non-printable character count: $percent%"
            echo "    This may indicate binary corruption"
        fi
    fi
    
    # Check for truncated lines (all lines should end with newline)
    last_char=$(tail -c 1 "$logfile" | od -An -tx1)
    if [ "$last_char" != " 0a" ]; then
        echo "[!] File does not end with newline - may be truncated"
    fi
}

# Usage
check_filesystem_corruption /var/log/syslog
```

**Identifying corruption patterns:**

```bash
# Detect specific corruption types
detect_corruption_type() {
    local logfile="$1"
    
    echo "[*] Analyzing corruption patterns in $logfile"
    
    # Check for repeated patterns (might indicate buffer corruption)
    echo "[+] Checking for repeated byte patterns..."
    strings "$logfile" | awk 'length($0) > 50' | sort | uniq -c | sort -rn | head -5
    
    # Check for random binary data blocks
    echo ""
    echo "[+] Checking for binary data blocks..."
    hexdump -C "$logfile" | grep -E "([^0-9a-f]){10,}" | head -5
    
    # Look for sudden character set changes
    echo ""
    echo "[+] Analyzing character distribution..."
    python3 << EOF
import sys
from collections import Counter

with open("$logfile", 'rb') as f:
    data = f.read()
    
# Sample every 1000 bytes
samples = [data[i:i+1000] for i in range(0, len(data), 1000)]

for idx, sample in enumerate(samples[:10]):
    try:
        # Try to decode as UTF-8
        text = sample.decode('utf-8', errors='ignore')
        printable = sum(c.isprintable() or c.isspace() for c in text)
        ratio = printable / len(text) if len(text) > 0 else 0
        
        if ratio < 0.8:
            print(f"[!] Block {idx} (offset {idx*1000}): Low printable ratio {ratio:.2f}")
    except:
        print(f"[!] Block {idx}: Decode error")
EOF
}

# Usage
detect_corruption_type /var/log/corrupted.log
```

**Extracting valid data from corrupted files:**

```bash
# Extract readable portions from corrupted log
extract_valid_data() {
    local corrupted_file="$1"
    local output="${corrupted_file%.log}_extracted.log"
    
    echo "[*] Extracting valid data from $corrupted_file..."
    
    # Method 1: Extract printable strings
    strings -n 20 "$corrupted_file" > "${output}.strings"
    echo "[+] Extracted strings: ${output}.strings"
    
    # Method 2: Remove null bytes and non-printable chars
    tr -cd '[:print:]\n\r\t' < "$corrupted_file" > "${output}.cleaned"
    echo "[+] Cleaned output: ${output}.cleaned"
    
    # Method 3: Extract valid UTF-8 sequences
    python3 << EOF
with open("$corrupted_file", 'rb') as infile:
    with open("${output}.utf8", 'w', encoding='utf-8', errors='ignore') as outfile:
        data = infile.read()
        text = data.decode('utf-8', errors='ignore')
        outfile.write(text)
print("[+] UTF-8 extracted: ${output}.utf8")
EOF
    
    # Method 4: Line-by-line validation
    awk '
        /^[A-Za-z]{3} [ 0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}/ {
            print $0
        }
    ' "$corrupted_file" > "${output}.validated"
    echo "[+] Validated lines: ${output}.validated"
    
    # Compare results
    echo ""
    echo "[*] Extraction comparison:"
    echo "    Strings: $(wc -l < ${output}.strings) lines"
    echo "    Cleaned: $(wc -l < ${output}.cleaned) lines"
    echo "    UTF-8: $(wc -l < ${output}.utf8) lines"
    echo "    Validated: $(wc -l < ${output}.validated) lines"
}

# Usage
extract_valid_data /var/log/corrupted.log
```

**Repairing corrupted syslog format:**

```bash
# Attempt to reconstruct syslog entries
repair_syslog_format() {
    local input="$1"
    local output="${input%.log}_repaired.log"
    
    python3 << 'EOF'
import re
import sys

# Syslog regex pattern
# Format: Mon DD HH:MM:SS hostname process[pid]: message
syslog_pattern = re.compile(
    r'^([A-Z][a-z]{2}\s+\d{1,2}\s+\d{2}:\d{2}:\d{2})\s+'
    r'(\S+)\s+'
    r'([^\[:]+)(?:\[(\d+)\])?:\s+'
    r'(.*)$'
)

repaired = 0
partial = 0
failed = 0

with open(sys.argv[1], 'r', errors='ignore') as infile:
    with open(sys.argv[2], 'w') as outfile:
        for line_num, line in enumerate(infile, 1):
            line = line.strip()
            
            if not line:
                continue
            
            match = syslog_pattern.match(line)
            
            if match:
                # Valid syslog entry
                outfile.write(line + '\n')
            else:
                # Attempt repair
                
                # Check if it looks like a continuation
                if line.startswith(' ') or line.startswith('\t'):
                    # Likely a multiline entry continuation - keep as-is
                    outfile.write(line + '\n')
                    partial += 1
                    continue
                
                # Try to extract timestamp
                time_match = re.search(r'([A-Z][a-z]{2}\s+\d{1,2}\s+\d{2}:\d{2}:\d{2})', line)
                
                if time_match:
                    # Has timestamp, try to reconstruct
                    timestamp = time_match.group(1)
                    remaining = line[time_match.end():].strip()
                    
                    # Add placeholder hostname and process
                    repaired_line = f"{timestamp} unknown unknown: {remaining}"
                    outfile.write(repaired_line + '\n')
                    repaired += 1
                    print(f"[+] Line {line_num}: Reconstructed with placeholder", file=sys.stderr)
                else:
                    # Cannot determine structure
                    failed += 1
                    print(f"[!] Line {line_num}: Cannot repair - no timestamp", file=sys.stderr)

print(f"\n[*] Repaired: {repaired}, Partial: {partial}, Failed: {failed}", file=sys.stderr)
EOF python3 script.py "$input" "$output"
    
    echo "[+] Repaired log saved to: $output"
}
```

**Detecting anti-forensic tampering:**

```bash
# Check for signs of intentional corruption/tampering
detect_tampering() {
    local logfile="$1"
    
    echo "[*] Analyzing $logfile for tampering indicators..."
    
    # Check for suspicious gaps in sequence numbers
    if grep -q "kernel:.*seq" "$logfile"; then
        echo "[+] Checking kernel sequence numbers..."
        grep "kernel:" "$logfile" | \
            awk '{match($0, /seq:[0-9]+/, arr); print arr[0]}' | \
            sort -n | \
            awk '{if (NR>1 && $1-prev>100) print "Gap detected: "prev" -> "$1; prev=$1}'
    fi
    
    # Check for timestamp manipulation
    echo "[+] Checking for timestamp anomalies..."
    awk '{
        curr_time = $1" "$2" "$3
        if (NR > 1 && curr_time < prev_time) {
            print "[!] Timestamp regression at line "NR": "prev_time" -> "curr_time
        }
        prev_time = curr_time
    }' "$logfile" | head -10
    
    # Check for selective deletion patterns
    echo "[+] Checking for selective deletion patterns..."
    
    # Count entries per hour
    awk '{print $1" "$2" "$3}' "$logfile" | \
        awk '{hour=substr($3,1,2); print $1" "$2" "hour":00"}' | \
        sort | uniq -c | \
        awk '{
            if (NR > 1 && $1 < prev_count * 0.5) {
                print "[!] Suspicious drop at "$2" "$3" "$4": "prev_count" -> "$1" entries"
            }
            prev_count = $1
        }'
}

# Usage
detect_tampering /var/log/syslog
```

**File carving from corrupted logs:**

```bash
# Carve specific log entries from corrupted data
carve_log_entries() {
    local corrupted_file="$1"
    local pattern="$2"  # e.g., "Failed password"
    local output="carved_entries.log"
    
    echo "[*] Carving entries matching '$pattern'..."
    
    # Use dd to read file in blocks, handling errors
    dd if="$corrupted_file" of="/tmp/raw_data.bin" conv=noerror,sync 2>/dev/null
    
    # Extract matching patterns with context
    strings -n 50 "/tmp/raw_data.bin" | \
        grep -B 2 -A 2 "$pattern" > "$output"
    
    echo "[+] Carved $(wc -l < $output) lines to $output"
    
    # Clean up
    rm -f "/tmp/raw_data.bin"
}

# Usage
carve_log_entries /dev/sda1 "authentication failure"
```

## Missing Timestamps

Missing or malformed timestamps prevent chronological analysis and event correlation. Recovery techniques depend on log format and available context.

**Detecting missing timestamps:**

```bash
# Identify entries without timestamps
detect_missing_timestamps() {
    local logfile="$1"
    
    echo "[*] Scanning for missing timestamps in $logfile..."
    
    # Common timestamp patterns
    # Syslog: Mon DD HH:MM:SS
    # ISO: YYYY-MM-DDTHH:MM:SS
    # Apache: [DD/Mon/YYYY:HH:MM:SS +ZONE]
    
    awk '
        # Syslog format
        $1 ~ /^(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)$/ && 
        $2 ~ /^[0-9]{1,2}$/ && 
        $3 ~ /^[0-9]{2}:[0-9]{2}:[0-9]{2}$/ {
            has_timestamp=1
        }
        
        # ISO 8601
        $0 ~ /[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}/ {
            has_timestamp=1
        }
        
        # Apache timestamp
        $0 ~ /\[[0-9]{2}\/[A-Z][a-z]{2}\/[0-9]{4}:[0-9]{2}:[0-9]{2}:[0-9]{2}/ {
            has_timestamp=1
        }
        
        {
            if (!has_timestamp && NF > 0) {
                print NR": "$0
            }
            has_timestamp=0
        }
    ' "$logfile" | head -20
}

# Usage
detect_missing_timestamps /var/log/app.log
```

**Reconstructing timestamps from file metadata:**

```bash
# Use file metadata to estimate missing timestamps
reconstruct_from_metadata() {
    local logfile="$1"
    local output="${logfile%.log}_timestamped.log"
    
    echo "[*] Reconstructing timestamps using file metadata..."
    
    # Get file modification time
    file_mtime=$(stat -c %Y "$logfile")
    file_time=$(date -d @$file_mtime '+%b %d %H:%M:%S')
    
    echo "[+] File last modified: $file_time"
    
    # Count total lines
    total_lines=$(wc -l < "$logfile")
    
    # Calculate time interval between entries
    # Assume log spans 1 hour (adjustable)
    time_span=3600  # seconds
    interval=$((time_span / total_lines))
    
    echo "[+] Estimated interval: $interval seconds between entries"
    
    # Add estimated timestamps
    awk -v start_time="$file_mtime" -v total="$total_lines" -v span="$time_span" '
        {
            # Calculate timestamp for this line
            line_offset = (NR - 1) / total
            timestamp = start_time - (span * (1 - line_offset))
            
            # Format timestamp
            cmd = "date -d @"timestamp" \"+%b %d %H:%M:%S\""
            cmd | getline time_str
            close(cmd)
            
            print "[ESTIMATED:"time_str"] "$0
        }
    ' "$logfile" > "$output"
    
    echo "[+] Timestamped log saved to: $output"
    echo "[Inference] Timestamps are estimates based on file metadata"
}

# Usage
reconstruct_from_metadata /var/log/no_timestamps.log
```

**Inferring timestamps from context:**

```bash
# Use surrounding entries to estimate missing timestamps
infer_timestamps_from_context() {
    local logfile="$1"
    local output="${logfile%.log}_inferred.log"
    
    python3 << 'EOF'
import re
import sys
from datetime import datetime, timedelta

# Parse syslog timestamp
def parse_syslog_time(time_str):
    """Parse 'Mon DD HH:MM:SS' format"""
    try:
        # Add current year
        current_year = datetime.now().year
        full_str = f"{time_str} {current_year}"
        return datetime.strptime(full_str, "%b %d %H:%M:%S %Y")
    except:
        return None

# Syslog timestamp pattern
timestamp_pattern = re.compile(r'^([A-Z][a-z]{2}\s+\d{1,2}\s+\d{2}:\d{2}:\d{2})')

last_valid_time = None
entries_since_last = 0

with open(sys.argv[1], 'r', errors='ignore') as infile:
    with open(sys.argv[2], 'w') as outfile:
        for line in infile:
            line = line.strip()
            if not line:
                continue
            
            match = timestamp_pattern.match(line)
            
            if match:
                # Has timestamp
                last_valid_time = parse_syslog_time(match.group(1))
                entries_since_last = 0
                outfile.write(line + '\n')
            else:
                # Missing timestamp - infer
                if last_valid_time:
                    entries_since_last += 1
                    # Assume 1 second per entry
                    inferred_time = last_valid_time + timedelta(seconds=entries_since_last)
                    inferred_str = inferred_time.strftime("%b %d %H:%M:%S")
                    outfile.write(f"[INFERRED:{inferred_str}] {line}\n")
                else:
                    # No reference timestamp yet
                    outfile.write(f"[NO_TIMESTAMP] {line}\n")

print(f"[+] Inferred timestamps saved to: {sys.argv[2]}", file=sys.stderr)
EOF python3 script.py "$logfile" "$output"
}

# Usage
infer_timestamps_from_context /var/log/partial_timestamps.log
```

**Converting non-standard timestamp formats:**

```bash
# Normalize various timestamp formats to standard syslog format
normalize_timestamps() {
    local input="$1"
    local output="${input%.log}_normalized.log"
    
    python3 << 'EOF'
import re
import sys
from datetime import datetime


def normalize_timestamp(line):
    """Convert various timestamp formats to syslog format"""
    
    # ISO 8601: 2025-10-29T10:15:30
    iso_pattern = r'(\d{4})-(\d{2})-(\d{2})T(\d{2}):(\d{2}):(\d{2})'
    match = re.search(iso_pattern, line)
    if match:
        dt = datetime(*map(int, match.groups()))
        syslog_time = dt.strftime("%b %d %H:%M:%S")
        line = re.sub(iso_pattern, syslog_time, line)
    
    # Unix epoch: 1730196930
    epoch_pattern = r'\b(1[67]\d{8})\b'
    match = re.search(epoch_pattern, line)
    if match:
        dt = datetime.fromtimestamp(int(match.group(1)))
        syslog_time = dt.strftime("%b %d %H:%M:%S")
        line = re.sub(epoch_pattern, syslog_time, line)
    
    # Apache: [29/Oct/2025:10:15:30 +0000]
    apache_pattern = r'\[(\d{2})/(\w{3})/(\d{4}):(\d{2}):(\d{2}):(\d{2})\s+[^\]]+\]'
    match = re.search(apache_pattern, line)
    if match:
        day, month, year, hour, minute, second = match.groups()
        dt = datetime.strptime(f"{day} {month} {year} {hour}:{minute}:{second}", 
                              "%d %b %Y %H:%M:%S")
        syslog_time = dt.strftime("%b %d %H:%M:%S")
        line = re.sub(apache_pattern, syslog_time, line)
    
    # Windows Event Log: MM/DD/YYYY HH:MM:SS AM/PM
    windows_pattern = r'(\d{1,2})/(\d{1,2})/(\d{4})\s+(\d{1,2}):(\d{2}):(\d{2})\s+(AM|PM)'
    match = re.search(windows_pattern, line)
    if match:
        month, day, year, hour, minute, second, ampm = match.groups()
        dt = datetime.strptime(f"{month}/{day}/{year} {hour}:{minute}:{second} {ampm}", 
                              "%m/%d/%Y %I:%M:%S %p")
        syslog_time = dt.strftime("%b %d %H:%M:%S")
        line = re.sub(windows_pattern, syslog_time, line)
    
    return line


with open(sys.argv[1], 'r', errors='ignore') as infile:
    with open(sys.argv[2], 'w') as outfile:
        for line in infile:
            normalized = normalize_timestamp(line.strip())
            outfile.write(normalized + '\n')

print(f"[+] Normalized timestamps saved to: {sys.argv[2]}", file=sys.stderr)
EOF
    
    python3 -c "$(cat)" "$input" "$output"
}

# Usage
normalize_timestamps /var/log/mixed_timestamps.log
````

**Handling timezone issues:**

```bash
# Detect and correct timezone inconsistencies
fix_timezone_issues() {
    local logfile="$1"
    local target_tz="${2:-UTC}"
    local output="${logfile%.log}_${target_tz}.log"
    
    echo "[*] Converting all timestamps to $target_tz..."
    
    python3 << EOF
import re
import sys
from datetime import datetime
from dateutil import parser, tz

def convert_to_utc(timestamp_str):
    """Parse timestamp and convert to UTC"""
    try:
        # Try to parse various formats
        dt = parser.parse(timestamp_str)
        
        # If naive (no timezone), assume local
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=tz.tzlocal())
        
        # Convert to target timezone
        target = tz.gettz("$target_tz")
        dt_target = dt.astimezone(target)
        
        # Return in syslog format
        return dt_target.strftime("%b %d %H:%M:%S")
    except:
        return timestamp_str

# Common timestamp patterns with timezone info
patterns = [
    r'(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}[+-]\d{2}:?\d{2})',  # ISO with TZ
    r'\[(\d{2}/\w{3}/\d{4}:\d{2}:\d{2}:\d{2}\s+[+-]\d{4})\]',   # Apache with TZ
    r'(\w{3}\s+\d{1,2}\s+\d{2}:\d{2}:\d{2})',                   # Syslog (no TZ)
]

with open("$logfile", 'r', errors='ignore') as infile:
    with open("$output", 'w') as outfile:
        for line in infile:
            modified = line
            
            for pattern in patterns:
                match = re.search(pattern, line)
                if match:
                    original_ts = match.group(1)
                    converted_ts = convert_to_utc(original_ts)
                    modified = line.replace(original_ts, converted_ts)
                    break
            
            outfile.write(modified)

print(f"[+] Timezone-corrected log saved to: $output", file=sys.stderr)
EOF
}

# Usage
fix_timezone_issues /var/log/app.log UTC
````

**Recovering timestamps from packet captures:**

```bash
# Extract timestamps from network captures for comparison
extract_pcap_timestamps() {
    local pcap_file="$1"
    local output="pcap_timeline.txt"
    
    echo "[*] Extracting timestamps from $pcap_file..."
    
    # Requires tshark
    if ! command -v tshark &> /dev/null; then
        echo "[!] tshark not installed"
        return 1
    fi
    
    # Extract timestamp and basic info for correlation
    tshark -r "$pcap_file" -T fields \
        -e frame.time \
        -e ip.src \
        -e ip.dst \
        -e tcp.port \
        -e http.request.uri \
        -E header=y \
        -E separator=\| > "$output"
    
    echo "[+] PCAP timeline saved to: $output"
    echo "[+] Use for cross-reference with incomplete logs"
}

# Cross-reference log entries with PCAP timestamps
correlate_with_pcap() {
    local logfile="$1"
    local pcap_timeline="$2"
    
    echo "[*] Correlating log with PCAP timestamps..."
    
    # Extract IPs from log
    grep -oP '\b\d{1,3}(\.\d{1,3}){3}\b' "$logfile" | sort -u > log_ips.txt
    
    # Find matching entries in PCAP
    grep -f log_ips.txt "$pcap_timeline" | \
        awk -F'|' '{print $1" | "$2" -> "$3}' | \
        sort > correlated_timestamps.txt
    
    echo "[+] Correlated entries: $(wc -l < correlated_timestamps.txt)"
    echo "[+] Saved to: correlated_timestamps.txt"
}
```

**Handling multiline log entries without timestamps:**

```bash
# Process multiline entries where only first line has timestamp
process_multiline_logs() {
    local logfile="$1"
    local output="${logfile%.log}_processed.log"
    
    echo "[*] Processing multiline log entries..."
    
    awk '
    BEGIN {
        current_timestamp = ""
    }
    
    # Line with timestamp (syslog format)
    /^[A-Z][a-z]{2} [ 0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}/ {
        current_timestamp = $1" "$2" "$3
        print $0
        next
    }
    
    # Continuation line (starts with whitespace)
    /^[[:space:]]/ {
        if (current_timestamp != "") {
            print "[CONT:"current_timestamp"]"$0
        } else {
            print "[NO_TIMESTAMP]"$0
        }
        next
    }
    
    # Other lines without timestamp
    {
        if (current_timestamp != "") {
            print "[INFERRED:"current_timestamp"]"$0
        } else {
            print "[NO_TIMESTAMP]"$0
        }
    }
    ' "$logfile" > "$output"
    
    echo "[+] Processed log saved to: $output"
}

# Usage
process_multiline_logs /var/log/app.log
```

**Creating synthetic timestamps for analysis:**

```bash
# Generate synthetic timestamps based on log ordering
generate_synthetic_timestamps() {
    local logfile="$1"
    local start_time="${2:-$(date -d '1 hour ago' '+%s')}"
    local output="${logfile%.log}_synthetic.log"
    
    echo "[*] Generating synthetic timestamps..."
    echo "[*] Start time: $(date -d @$start_time)"
    
    awk -v start="$start_time" '
    BEGIN {
        interval = 1  # 1 second between entries
    }
    
    {
        if (NF > 0) {
            timestamp = start + (NR * interval)
            
            # Format as syslog timestamp
            cmd = "date -d @"timestamp" \"+%b %d %H:%M:%S\""
            cmd | getline time_str
            close(cmd)
            
            print "[SYNTHETIC:"time_str"] "$0
        }
    }
    ' "$logfile" > "$output"
    
    echo "[+] Synthetic timestamps added: $output"
    echo "[Inference] These are artificial timestamps for analysis ordering only"
}

# Usage
generate_synthetic_timestamps /var/log/no_time.log
```

**Comprehensive troubleshooting script:**

```bash
#!/bin/bash
# troubleshoot_logs.sh - Comprehensive log diagnostics

LOG_FILE="$1"
OUTPUT_DIR="diagnostics_$(date +%Y%m%d_%H%M%S)"

if [ ! -f "$LOG_FILE" ]; then
    echo "[!] Usage: $0 <logfile>"
    exit 1
fi

mkdir -p "$OUTPUT_DIR"

echo "============================================"
echo "   Log File Diagnostic Report"
echo "============================================"
echo "File: $LOG_FILE"
echo "Date: $(date)"
echo "============================================"
echo ""

# 1. Basic file information
echo "[*] Phase 1: Basic File Information"
{
    echo "=== File Statistics ==="
    ls -lh "$LOG_FILE"
    echo ""
    echo "Size: $(stat -c %s $LOG_FILE) bytes"
    echo "Lines: $(wc -l < $LOG_FILE)"
    echo "Inode: $(stat -c %i $LOG_FILE)"
    echo "Last modified: $(stat -c %y $LOG_FILE)"
    echo ""
    
    echo "=== File Type ==="
    file "$LOG_FILE"
    echo ""
    
} | tee "$OUTPUT_DIR/01_file_info.txt"

# 2. Corruption detection
echo "[*] Phase 2: Corruption Detection"
{
    echo "=== Null Byte Check ==="
    if grep -q $'\x00' "$LOG_FILE" 2>/dev/null; then
        echo "[!] WARNING: Null bytes detected"
        null_count=$(tr -cd '\000' < "$LOG_FILE" | wc -c)
        echo "    Count: $null_count"
    else
        echo "[+] No null bytes found"
    fi
    echo ""
    
    echo "=== Non-printable Characters ==="
    non_printable=$(cat "$LOG_FILE" | tr -d '[:print:]\n\r\t' | wc -c)
    total_chars=$(wc -c < "$LOG_FILE")
    if [ $total_chars -gt 0 ]; then
        percent=$((non_printable * 100 / total_chars))
        echo "Non-printable: $non_printable / $total_chars ($percent%)"
        if [ $percent -gt 5 ]; then
            echo "[!] WARNING: High non-printable content"
        fi
    fi
    echo ""
    
    echo "=== Line Integrity ==="
    # Check for very short or very long lines
    awk '{len=length($0); if(len<5) short++; if(len>1000) long++} 
         END {print "Short lines (<5 chars): "short"\nLong lines (>1000 chars): "long}' "$LOG_FILE"
    echo ""
    
} | tee "$OUTPUT_DIR/02_corruption.txt"

# 3. Timestamp analysis
echo "[*] Phase 3: Timestamp Analysis"
{
    echo "=== Timestamp Format Detection ==="
    
    # Syslog format
    syslog_count=$(grep -cE '^[A-Z][a-z]{2} [ 0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}' "$LOG_FILE" 2>/dev/null)
    echo "Syslog format (Mon DD HH:MM:SS): $syslog_count"
    
    # ISO 8601
    iso_count=$(grep -cE '[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}' "$LOG_FILE" 2>/dev/null)
    echo "ISO 8601 format: $iso_count"
    
    # Apache format
    apache_count=$(grep -cE '\[[0-9]{2}/[A-Z][a-z]{2}/[0-9]{4}:[0-9]{2}:[0-9]{2}:[0-9]{2}' "$LOG_FILE" 2>/dev/null)
    echo "Apache format: $apache_count"
    
    # Unix epoch
    epoch_count=$(grep -oE '\b1[67][0-9]{8}\b' "$LOG_FILE" 2>/dev/null | wc -l)
    echo "Unix epoch timestamps: $epoch_count"
    
    total_lines=$(wc -l < "$LOG_FILE")
    missing=$((total_lines - syslog_count - iso_count - apache_count))
    
    echo ""
    echo "Total lines: $total_lines"
    echo "Lines with timestamps: $((syslog_count + iso_count + apache_count))"
    echo "Lines without timestamps: $missing"
    
    if [ $missing -gt $((total_lines / 2)) ]; then
        echo "[!] WARNING: More than 50% of lines missing timestamps"
    fi
    echo ""
    
    echo "=== Timestamp Sequence Check ==="
    # Check for chronological ordering
    awk '/^[A-Z][a-z]{2} [ 0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}/ {
        curr = $1" "$2" "$3
        if (NR > 1 && prev > curr) {
            print "Line "NR": Timestamp regression - "prev" -> "curr
            count++
        }
        prev = curr
    } 
    END {if (count > 0) print "\nTotal regressions: "count; else print "[+] Timestamps in order"}' \
    "$LOG_FILE" | head -20
    echo ""
    
} | tee "$OUTPUT_DIR/03_timestamps.txt"

# 4. Completeness check
echo "[*] Phase 4: Completeness Analysis"
{
    echo "=== Log Rotation Detection ==="
    rotated_files=$(ls -1 ${LOG_FILE}* 2>/dev/null | wc -l)
    echo "Related files found: $rotated_files"
    ls -lh ${LOG_FILE}* 2>/dev/null
    echo ""
    
    echo "=== Temporal Coverage ==="
    first_timestamp=$(grep -m1 -oE '^[A-Z][a-z]{2} [ 0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}' "$LOG_FILE" 2>/dev/null)
    last_timestamp=$(grep -oE '^[A-Z][a-z]{2} [ 0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}' "$LOG_FILE" 2>/dev/null | tail -1)
    
    echo "First timestamp: ${first_timestamp:-NOT FOUND}"
    echo "Last timestamp: ${last_timestamp:-NOT FOUND}"
    echo ""
    
    echo "=== Content Sampling ==="
    echo "First 5 lines:"
    head -5 "$LOG_FILE"
    echo ""
    echo "Last 5 lines:"
    tail -5 "$LOG_FILE"
    echo ""
    
} | tee "$OUTPUT_DIR/04_completeness.txt"

# 5. Data quality metrics
echo "[*] Phase 5: Data Quality Metrics"
{
    echo "=== Field Consistency ==="
    # Sample 100 lines and check field count consistency
    head -100 "$LOG_FILE" | awk '{print NF}' | sort | uniq -c | sort -rn
    echo ""
    
    echo "=== Encoding Check ==="
    encoding=$(file -b --mime-encoding "$LOG_FILE")
    echo "Detected encoding: $encoding"
    
    if [ "$encoding" != "us-ascii" ] && [ "$encoding" != "utf-8" ]; then
        echo "[!] WARNING: Unusual encoding detected"
    fi
    echo ""
    
    echo "=== Duplicate Detection ==="
    duplicates=$(sort "$LOG_FILE" | uniq -d | wc -l)
    echo "Duplicate lines: $duplicates"
    
    if [ $duplicates -gt 0 ]; then
        echo "Sample duplicates:"
        sort "$LOG_FILE" | uniq -d | head -5
    fi
    echo ""
    
} | tee "$OUTPUT_DIR/05_quality.txt"

# 6. Generate recommendations
echo "[*] Phase 6: Generating Recommendations"
{
    echo "=== Recommended Actions ==="
    
    # Check for corruption
    if grep -q $'\x00' "$LOG_FILE" 2>/dev/null; then
        echo "- [HIGH] File contains null bytes - run corruption recovery"
        echo "  Command: strings -n 20 $LOG_FILE > ${LOG_FILE}_recovered.log"
    fi
    
    # Check for missing timestamps
    missing=$(($(wc -l < "$LOG_FILE") - $(grep -cE '^[A-Z][a-z]{2} [ 0-9]{2}' "$LOG_FILE" 2>/dev/null)))
    if [ $missing -gt 100 ]; then
        echo "- [MEDIUM] Significant missing timestamps detected"
        echo "  Command: ./infer_timestamps_from_context.sh $LOG_FILE"
    fi
    
    # Check for rotated logs
    if [ $rotated_files -gt 1 ]; then
        echo "- [INFO] Multiple log files detected - consider merging"
        echo "  Command: ./merge_rotated_logs.sh $LOG_FILE"
    fi
    
    # Check for mixed formats
    formats=$(($(grep -cE '^[A-Z][a-z]{2}' "$LOG_FILE" 2>/dev/null) > 0 ? 1 : 0))
    formats=$((formats + $(grep -cE '^[0-9]{4}-[0-9]{2}-[0-9]{2}' "$LOG_FILE" 2>/dev/null) > 0 ? 1 : 0))
    
    if [ $formats -gt 1 ]; then
        echo "- [LOW] Mixed timestamp formats detected"
        echo "  Command: ./normalize_timestamps.sh $LOG_FILE"
    fi
    
    echo ""
    
} | tee "$OUTPUT_DIR/06_recommendations.txt"

# Generate summary report
echo ""
echo "============================================"
echo "   Diagnostic Complete"
echo "============================================"
echo "Results saved to: $OUTPUT_DIR/"
echo ""
echo "Summary:"
cat "$OUTPUT_DIR"/*.txt | grep -E '\[!]|\[HIGH]|\[MEDIUM]' | sort -u
echo ""
echo "Full report: $OUTPUT_DIR/FULL_REPORT.txt"

# Combine all outputs
cat "$OUTPUT_DIR"/*.txt > "$OUTPUT_DIR/FULL_REPORT.txt"
```

**Recovery priority matrix:**

```bash
# Prioritize recovery efforts based on log condition
assess_recovery_priority() {
    local logfile="$1"
    
    echo "[*] Assessing recovery priority for $logfile"
    echo ""
    
    local score=0
    local issues=()
    
    # Check for corruption
    if grep -q $'\x00' "$logfile" 2>/dev/null; then
        score=$((score + 10))
        issues+=("CRITICAL: Null bytes detected (data corruption)")
    fi
    
    # Check for missing timestamps
    total=$(wc -l < "$logfile")
    timestamped=$(grep -cE '^[A-Z][a-z]{2} [ 0-9]{2}' "$logfile" 2>/dev/null)
    missing_pct=$(( (total - timestamped) * 100 / total ))
    
    if [ $missing_pct -gt 75 ]; then
        score=$((score + 8))
        issues+=("HIGH: $missing_pct% missing timestamps")
    elif [ $missing_pct -gt 25 ]; then
        score=$((score + 5))
        issues+=("MEDIUM: $missing_pct% missing timestamps")
    fi
    
    # Check readability
    non_printable=$(cat "$logfile" | tr -d '[:print:]\n\r\t' | wc -c)
    total_chars=$(wc -c < "$logfile")
    if [ $total_chars -gt 0 ]; then
        non_print_pct=$((non_printable * 100 / total_chars))
        if [ $non_print_pct -gt 10 ]; then
            score=$((score + 7))
            issues+=("HIGH: $non_print_pct% non-printable characters")
        fi
    fi
    
    # Determine priority
    if [ $score -ge 15 ]; then
        priority="CRITICAL"
    elif [ $score -ge 8 ]; then
        priority="HIGH"
    elif [ $score -gt 0 ]; then
        priority="MEDIUM"
    else
        priority="LOW"
        issues+=("File appears healthy")
    fi
    
    echo "Priority: $priority (Score: $score)"
    echo ""
    echo "Issues detected:"
    for issue in "${issues[@]}"; do
        echo "  - $issue"
    done
    echo ""
    
    # Recommended recovery order
    if [ $score -gt 0 ]; then
        echo "Recommended recovery order:"
        echo "  1. Extract valid data (strings extraction)"
        echo "  2. Repair corruption (remove null bytes)"
        echo "  3. Reconstruct timestamps"
        echo "  4. Normalize format"
        echo "  5. Validate recovered data"
    fi
}

# Usage
assess_recovery_priority /var/log/suspicious.log
```

**Important considerations:**

1. **Data preservation**: Always work on copies - never modify original evidence
2. **Documentation**: Record all recovery attempts and their success rates
3. **Validation**: Verify recovered data against known-good sources when possible
4. **Limitations**: Accept that some data may be unrecoverable
5. **Chain of custody**: Document all modifications made during recovery

**[Inference]** Recovery success depends on corruption severity and available context. Metadata-based reconstruction provides approximate timestamps but cannot guarantee accuracy.

**CTF-specific tips:**

- Check for intentional corruption as an anti-forensics technique
- Look for patterns in "corrupted" data - may contain encoded flags
- Examine hex dumps of corrupted sections for hidden data
- Consider that missing timestamps might be clues to specific event windows
- Validate recovered data against challenge descriptions for consistency
- Use multiple recovery techniques and compare results

---

# Troubleshooting & Debugging

Troubleshooting log analysis issues is critical in CTF scenarios where corrupted, malformed, or deliberately obfuscated logs can hide critical information. Understanding how to diagnose and fix encoding problems, recover truncated data, and extract information from circular buffers is essential for complete incident reconstruction.

## Encoding Issues

Encoding problems occur when logs contain characters that cannot be properly interpreted due to charset mismatches, multi-byte character corruption, or binary data in text logs.

### Detecting Encoding Issues

**Common symptoms**:

- Mojibake (garbled characters): `` instead of ``
- Replacement characters: `` (U+FFFD)
- Null bytes or control characters in text
- Unexpected line breaks or truncation
- Tools reporting "binary file" warnings

**Identify file encoding**:

```bash
# Using file command
file -i logfile.log
# Output: logfile.log: text/plain; charset=utf-8

# Using chardet (Python library)
chardetect logfile.log
# Output: logfile.log: utf-8 with confidence 0.99

# Install: pip3 install chardet

# Check for BOM (Byte Order Mark)
hexdump -C logfile.log | head -n1
# UTF-8 BOM: ef bb bf
# UTF-16 LE BOM: ff fe
# UTF-16 BE BOM: fe ff

# Using uchardet (universal charset detector)
sudo apt install uchardet
uchardet logfile.log
```

**Detect mixed encodings in single file**:

```python
#!/usr/bin/env python3
import sys
from collections import defaultdict

def detect_encoding_per_line(filename):
    """Detect encoding issues line by line"""
    import chardet
    
    encodings = defaultdict(list)
    
    with open(filename, 'rb') as f:
        for line_num, line in enumerate(f, 1):
            try:
                # Try UTF-8 first
                line.decode('utf-8')
                encodings['utf-8'].append(line_num)
            except UnicodeDecodeError:
                # Detect encoding for problematic lines
                result = chardet.detect(line)
                enc = result['encoding']
                confidence = result['confidence']
                
                if confidence > 0.7:
                    encodings[enc].append(line_num)
                else:
                    encodings['unknown'].append(line_num)
    
    # Report findings
    print("ENCODING ANALYSIS")
    print("=" * 60)
    for encoding, lines in sorted(encodings.items()):
        print(f"{encoding}: {len(lines)} lines")
        if len(lines) <= 10:
            print(f"  Line numbers: {lines}")
        else:
            print(f"  Line numbers: {lines[:5]} ... {lines[-5:]}")
    
    return encodings

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python3 detect_encoding.py <logfile>")
        sys.exit(1)
    
    detect_encoding_per_line(sys.argv[1])
```

Usage:

```bash
python3 detect_encoding.py mixed_encoding.log
```

### Common Encoding Conversions

**UTF-8 to Latin1 (ISO-8859-1)**:

```bash
# Convert file encoding
iconv -f UTF-8 -t ISO-8859-1 input.log > output.log

# With error handling (skip invalid chars)
iconv -f UTF-8 -t ISO-8859-1//IGNORE input.log > output.log

# Transliterate unsupported characters
iconv -f UTF-8 -t ASCII//TRANSLIT input.log > output.log

# Auto-detect source encoding
iconv -f $(uchardet input.log) -t UTF-8 input.log > output_utf8.log
```

**Common conversion scenarios**:

```bash
# Windows-1252 to UTF-8 (Windows logs)
iconv -f WINDOWS-1252 -t UTF-8 windows.log > utf8.log

# ISO-8859-1 to UTF-8 (older Linux systems)
iconv -f ISO-8859-1 -t UTF-8 old.log > new.log

# UTF-16 to UTF-8 (some Windows event logs)
iconv -f UTF-16LE -t UTF-8 utf16.log > utf8.log

# MacRoman to UTF-8 (old macOS)
iconv -f MACROMAN -t UTF-8 mac.log > utf8.log

# Fix mojibake (double-encoded UTF-8)
# Text was UTF-8 but interpreted as Latin1 and re-encoded as UTF-8
iconv -f UTF-8 -t LATIN1 | iconv -f UTF-8 -t UTF-8 mojibake.log > fixed.log
```

**Batch conversion**:

```bash
# Convert all log files in directory
for file in /var/log/*.log; do
    encoding=$(uchardet "$file")
    if [ "$encoding" != "UTF-8" ]; then
        echo "Converting $file from $encoding to UTF-8"
        iconv -f "$encoding" -t UTF-8 "$file" > "${file}.utf8"
        mv "${file}.utf8" "$file"
    fi
done
```

### Handling Binary Data in Text Logs

**Symptoms**: Binary data embedded in text logs (e.g., malicious payloads, encoded exploits)

**Extract and analyze binary sections**:

```bash
# Find lines with null bytes
grep -a $'\x00' logfile.log

# Find non-printable characters
grep -P '[\x00-\x08\x0B-\x0C\x0E-\x1F\x7F-\xFF]' logfile.log

# Extract printable strings from binary sections
strings -n 8 logfile.log > extracted_strings.txt

# Show hex dump of problematic sections
xxd logfile.log | grep -A5 -B5 "0000"

# Extract binary payloads
awk '/START_BINARY/,/END_BINARY/' logfile.log | \
    grep -v "START_BINARY\|END_BINARY" | \
    xxd -r -p > extracted_payload.bin
```

**Clean binary data from logs**:

```bash
# Remove null bytes
tr -d '\000' < input.log > output.log

# Replace all non-printable characters with space
tr -c '[:print:]\n\t' ' ' < input.log > output.log

# Keep only ASCII printable + newlines
LC_ALL=C grep -a '[[:print:]]' input.log > output.log

# Remove ANSI color codes
sed 's/\x1B\[[0-9;]*[JKmsu]//g' input.log > output.log
```

**Python-based binary cleaning**:

```python
#!/usr/bin/env python3
import sys
import re

def clean_binary_data(input_file, output_file):
    """Remove or replace binary data in log files"""
    
    with open(input_file, 'rb') as inf, open(output_file, 'wb') as outf:
        for line_num, line in enumerate(inf, 1):
            try:
                # Try to decode as UTF-8
                decoded = line.decode('utf-8')
                outf.write(line)
            except UnicodeDecodeError:
                # Handle binary data
                # Option 1: Skip line entirely
                # continue
                
                # Option 2: Replace binary with placeholder
                # outf.write(f"[BINARY DATA ON LINE {line_num}]\n".encode('utf-8'))
                
                # Option 3: Keep printable ASCII only
                cleaned = bytes(b if 32 <= b < 127 or b in (9, 10, 13) else 32 
                               for b in line)
                outf.write(cleaned)
                
                # Option 4: Base64 encode binary sections
                # import base64
                # encoded = base64.b64encode(line).decode('ascii')
                # outf.write(f"[BASE64: {encoded}]\n".encode('utf-8'))

if __name__ == "__main__":
    if len(sys.argv) < 3:
        print("Usage: python3 clean_binary.py <input> <output>")
        sys.exit(1)
    
    clean_binary_data(sys.argv[1], sys.argv[2])
    print(f"Cleaned log written to {sys.argv[2]}")
```

Usage:

```bash
python3 clean_binary.py dirty.log clean.log
```

### URL Encoding Issues

**Decode URL-encoded log entries**:

```bash
# Single line decoding
echo "%2F%65%74%63%2F%70%61%73%73%77%64" | python3 -c "import sys, urllib.parse; print(urllib.parse.unquote(sys.stdin.read()))"
# Output: /etc/passwd

# Decode entire file
python3 -c "
import sys, urllib.parse
for line in sys.stdin:
    print(urllib.parse.unquote(line.strip()))
" < encoded.log > decoded.log

# Decode only URL-encoded parts (preserve rest)
sed 's/%\([0-9A-F][0-9A-F]\)/\\x\1/g' encoded.log | xargs -0 echo -e
```

**Handle double or triple encoding**:

```python
#!/usr/bin/env python3
import urllib.parse
import sys

def decode_recursive(text, max_depth=5):
    """Recursively decode URL encoding"""
    previous = text
    
    for depth in range(max_depth):
        try:
            decoded = urllib.parse.unquote(previous)
            if decoded == previous:
                # No more decoding possible
                return decoded, depth
            previous = decoded
        except Exception as e:
            return previous, depth
    
    return previous, max_depth

# Process stdin
for line in sys.stdin:
    decoded, depth = decode_recursive(line.strip())
    if depth > 0:
        print(f"[Decoded {depth}x] {decoded}")
    else:
        print(decoded)
```

Usage:

```bash
cat encoded.log | python3 decode_recursive.py
```

### Character Encoding Detection and Repair

**Fix UTF-8 corruption**:

```python
#!/usr/bin/env python3
import sys

def fix_utf8(input_file, output_file):
    """Attempt to fix corrupted UTF-8 encoding"""
    
    with open(input_file, 'rb') as inf, open(output_file, 'w', encoding='utf-8', errors='replace') as outf:
        raw_data = inf.read()
        
        # Try to decode with error handling
        try:
            # Attempt UTF-8 with replacement
            decoded = raw_data.decode('utf-8', errors='replace')
        except Exception:
            # Fallback: try latin1 then encode to UTF-8
            try:
                decoded = raw_data.decode('latin1')
            except Exception:
                # Last resort: replace all non-decodable bytes
                decoded = raw_data.decode('ascii', errors='replace')
        
        outf.write(decoded)

if __name__ == "__main__":
    if len(sys.argv) < 3:
        print("Usage: python3 fix_utf8.py <input> <output>")
        sys.exit(1)
    
    fix_utf8(sys.argv[1], sys.argv[2])
```

**Detect and fix mojibake programmatically**:

```python
#!/usr/bin/env python3
# Requires: pip3 install ftfy

import ftfy
import sys

def fix_mojibake(input_file, output_file):
    """Fix mojibake (text corrupted by encoding errors)"""
    
    with open(input_file, 'r', encoding='utf-8', errors='replace') as inf:
        content = inf.read()
    
    # Fix common encoding mistakes
    fixed = ftfy.fix_text(content)
    
    with open(output_file, 'w', encoding='utf-8') as outf:
        outf.write(fixed)
    
    # Report what was fixed
    if fixed != content:
        print(f"Fixed encoding issues in {input_file}")
        print(f"Original length: {len(content)} chars")
        print(f"Fixed length: {len(fixed)} chars")
    else:
        print("No encoding issues detected")

if __name__ == "__main__":
    if len(sys.argv) < 3:
        print("Usage: python3 fix_mojibake.py <input> <output>")
        sys.exit(1)
    
    fix_mojibake(sys.argv[1], sys.argv[2])
```

### Handling Multi-byte Character Issues

**Common problems with multi-byte characters** (UTF-8, UTF-16, UTF-32):

- Partial character sequences
- Byte order issues
- Surrogate pairs corruption (UTF-16)

**Validate UTF-8 byte sequences**:

```bash
# Check for invalid UTF-8 sequences
iconv -f UTF-8 -t UTF-8 input.log > /dev/null 2>&1
if [ $? -ne 0 ]; then
    echo "Invalid UTF-8 sequences detected"
fi

# Find specific invalid sequences
grep -axv '.*' input.log | cat -A
```

**Repair UTF-8 sequences**:

```python
#!/usr/bin/env python3
import sys

def repair_utf8_sequences(input_file, output_file):
    """Repair partial or corrupted UTF-8 sequences"""
    
    with open(input_file, 'rb') as inf:
        raw_bytes = inf.read()
    
    repaired = bytearray()
    i = 0
    
    while i < len(raw_bytes):
        byte = raw_bytes[i]
        
        # ASCII (1 byte)
        if byte < 0x80:
            repaired.append(byte)
            i += 1
        # 2-byte sequence
        elif 0xC0 <= byte < 0xE0:
            if i + 1 < len(raw_bytes) and 0x80 <= raw_bytes[i+1] < 0xC0:
                repaired.extend(raw_bytes[i:i+2])
                i += 2
            else:
                # Corrupted sequence - replace with ?
                repaired.append(ord('?'))
                i += 1
        # 3-byte sequence
        elif 0xE0 <= byte < 0xF0:
            if (i + 2 < len(raw_bytes) and 
                0x80 <= raw_bytes[i+1] < 0xC0 and 
                0x80 <= raw_bytes[i+2] < 0xC0):
                repaired.extend(raw_bytes[i:i+3])
                i += 3
            else:
                repaired.append(ord('?'))
                i += 1
        # 4-byte sequence
        elif 0xF0 <= byte < 0xF8:
            if (i + 3 < len(raw_bytes) and 
                0x80 <= raw_bytes[i+1] < 0xC0 and 
                0x80 <= raw_bytes[i+2] < 0xC0 and 
                0x80 <= raw_bytes[i+3] < 0xC0):
                repaired.extend(raw_bytes[i:i+4])
                i += 4
            else:
                repaired.append(ord('?'))
                i += 1
        else:
            # Invalid start byte
            repaired.append(ord('?'))
            i += 1
    
    with open(output_file, 'wb') as outf:
        outf.write(bytes(repaired))

if __name__ == "__main__":
    if len(sys.argv) < 3:
        print("Usage: python3 repair_utf8.py <input> <output>")
        sys.exit(1)
    
    repair_utf8_sequences(sys.argv[1], sys.argv[2])
    print(f"Repaired UTF-8 sequences written to {sys.argv[2]}")
```

### CTF-Specific Encoding Challenges

**Challenge: Multiple layers of encoding**:

```python
#!/usr/bin/env python3
import base64
import urllib.parse
import binascii

def decode_multilayer(encoded_text):
    """Try common CTF encoding combinations"""
    
    attempts = []
    
    # Try URL decode
    try:
        decoded = urllib.parse.unquote(encoded_text)
        if decoded != encoded_text:
            attempts.append(("URL decode", decoded))
            # Try further decoding
            attempts.extend(decode_multilayer(decoded))
    except:
        pass
    
    # Try Base64
    try:
        decoded = base64.b64decode(encoded_text).decode('utf-8')
        attempts.append(("Base64 decode", decoded))
        attempts.extend(decode_multilayer(decoded))
    except:
        pass
    
    # Try Hex
    try:
        # Remove spaces, 0x prefixes
        hex_clean = encoded_text.replace(' ', '').replace('0x', '').replace('\\x', '')
        decoded = binascii.unhexlify(hex_clean).decode('utf-8')
        attempts.append(("Hex decode", decoded))
        attempts.extend(decode_multilayer(decoded))
    except:
        pass
    
    return attempts

# Example usage
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python3 multilayer_decode.py <encoded_string>")
        sys.exit(1)
    
    encoded = sys.argv[1]
    results = decode_multilayer(encoded)
    
    print("DECODING ATTEMPTS:")
    print("=" * 60)
    for method, result in results:
        print(f"\n{method}:")
        print(result[:200])  # Show first 200 chars
```

Usage:

```bash
python3 multilayer_decode.py "JTY2JTZDJTYxJTY3JTdCJTY4JTY1JTc4JTVGJTY1JTZFJTYzJTZGJTY0JTY1JTY0JTdE"
```

## Truncated Entries

Truncated log entries occur when log rotation, buffer limits, or storage constraints cut off log messages before completion. Recovering or reconstructing truncated data is essential for complete incident analysis.

### Detecting Truncated Entries

**Common indicators**:

- Sudden line endings mid-sentence
- Missing closing brackets/quotes
- Incomplete timestamps
- Fixed-length cutoffs (e.g., exactly 1024, 4096, 8192 bytes)
- Increasing truncation toward end of file

**Identify truncated lines**:

```bash
# Find lines shorter than expected (e.g., typical line > 50 chars)
awk 'length($0) < 50 {print NR": "$0}' logfile.log

# Find lines with incomplete JSON
grep -n '{' logfile.log | while read line; do
    line_num=$(echo "$line" | cut -d: -f1)
    content=$(echo "$line" | cut -d: -f2-)
    
    # Count opening and closing braces
    open=$(echo "$content" | tr -cd '{' | wc -c)
    close=$(echo "$content" | tr -cd '}' | wc -c)
    
    if [ $open -ne $close ]; then
        echo "Line $line_num: Incomplete JSON (open:$open close:$close)"
    fi
done

# Find lines with incomplete quotes
awk '{
    gsub(/\\"/, "", $0);  # Remove escaped quotes
    quotes = gsub(/"/, "&");
    if (quotes % 2 != 0) {
        print NR": Unmatched quotes - "$0
    }
}' logfile.log

# Check for lines exactly at buffer boundaries
awk 'length($0) == 1024 || length($0) == 2048 || length($0) == 4096 || length($0) == 8192 {
    print NR": Possible truncation at buffer boundary ("length($0)" bytes)"
}' logfile.log
```

**Automated truncation detection**:

```python
#!/usr/bin/env python3
import sys
import re
from collections import defaultdict

def detect_truncation(filename):
    """Detect various forms of log truncation"""
    
    issues = defaultdict(list)
    line_lengths = []
    
    with open(filename, 'r', encoding='utf-8', errors='replace') as f:
        for line_num, line in enumerate(f, 1):
            line = line.rstrip('\n')
            length = len(line)
            line_lengths.append(length)
            
            # Check for buffer boundary truncation
            if length in [1024, 2048, 4096, 8192, 16384]:
                issues['buffer_boundary'].append((line_num, length))
            
            # Check for incomplete JSON
            if '{' in line or '[' in line:
                open_braces = line.count('{') + line.count('[')
                close_braces = line.count('}') + line.count(']')
                if open_braces != close_braces:
                    issues['incomplete_json'].append(line_num)
            
            # Check for incomplete XML
            if '<' in line:
                open_tags = len(re.findall(r'<[^/][^>]*>', line))
                close_tags = len(re.findall(r'</[^>]+>', line))
                if open_tags != close_tags:
                    issues['incomplete_xml'].append(line_num)
            
            # Check for unmatched quotes
            # Remove escaped quotes first
            clean_line = re.sub(r'\\"', '', line)
            if clean_line.count('"') % 2 != 0:
                issues['unmatched_quotes'].append(line_num)
            
            # Check for incomplete timestamps
            if re.search(r'\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}(?!:)', line):
                issues['incomplete_timestamp'].append(line_num)
    
    # Generate report
    print("TRUNCATION DETECTION REPORT")
    print("=" * 60)
    print(f"Total lines: {len(line_lengths)}")
    print(f"Average line length: {sum(line_lengths)/len(line_lengths):.1f} chars")
    print(f"Max line length: {max(line_lengths)} chars")
    print(f"Min line length: {min(line_lengths)} chars")
    
    for issue_type, occurrences in issues.items():
        print(f"\n{issue_type.replace('_', ' ').title()}: {len(occurrences)} occurrences")
        if len(occurrences) <= 10:
            print(f"  Lines: {occurrences}")
        else:
            print(f"  Lines: {occurrences[:5]} ... {occurrences[-5:]}")
    
    return issues

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python3 detect_truncation.py <logfile>")
        sys.exit(1)
    
    detect_truncation(sys.argv[1])
```

### Reconstructing Truncated Data

**Method 1: Combine with continuation lines**:

```bash
# Some log systems split long lines across multiple entries
# Look for continuation indicators: leading spaces, specific markers

# Rejoin lines that start with whitespace (common in stack traces)
awk '
    /^[^ \t]/ {
        if (buffer) print buffer;
        buffer = $0;
    }
    /^[ \t]/ {
        buffer = buffer " " $0;
    }
    END {
        if (buffer) print buffer;
    }
' truncated.log > reconstructed.log

# Rejoin based on incomplete JSON
python3 << 'EOF'
import sys
import json

buffer = ""
for line in sys.stdin:
    buffer += line.strip()
    try:
        # Try to parse as JSON
        obj = json.loads(buffer)
        # Success - print and reset buffer
        print(json.dumps(obj))
        buffer = ""
    except json.JSONDecodeError:
        # Incomplete JSON - continue buffering
        continue

# Print any remaining buffer (couldn't be parsed)
if buffer:
    print(f"[INCOMPLETE]: {buffer}", file=sys.stderr)
EOF < truncated.log > reconstructed.log
```

**Method 2: Recover from multiple sources**:

```bash
# If logs are sent to multiple destinations, compare them

# Source 1: Local file (truncated)
# Source 2: Remote syslog server (may have complete entries)
# Source 3: Application memory/buffer

# Compare line counts
wc -l /var/log/local.log
wc -l /remote/syslog/server.log

# Find lines present in remote but missing in local
comm -13 <(cut -c1-50 /var/log/local.log | sort) \
         <(cut -c1-50 /remote/syslog/server.log | sort)
```

**Method 3: Extract from core dumps or memory**:

```bash
# If application crashed, core dump may contain complete log buffers

# Extract strings from core dump
strings core.12345 | grep -E '^\[.*\]' > recovered_logs.txt

# Search for specific log patterns
strings core.12345 | grep -A5 -B5 "ERROR"

# For running processes, extract from /proc
sudo cat /proc/<PID>/maps  # Find memory regions
sudo dd if=/proc/<PID>/mem bs=1 skip=<start_address> count=<length> 2>/dev/null | \
    strings | grep -E '\d{4}-\d{2}-\d{2}'
```

**Method 4: Reconstruct from forensic disk image**:

```bash
# Recover deleted or overwritten log files

# Mount disk image read-only
mount -o ro,loop disk.img /mnt/evidence

# Search for log fragments in unallocated space
foremost -t all -i disk.img -o carved_output/

# Use grep on raw disk to find log patterns
grep -a -b -o -E '\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}.*' disk.img > extracted.log

# Use photorec for file carving
photorec /d recovery_output/ /cmd disk.img search
```

### Handling Fixed-Length Truncation

**When logs are cut at exact byte boundaries**:

```python
#!/usr/bin/env python3
import sys

def merge_truncated_segments(input_file, output_file, max_length=4096):
    """
    Merge log entries that were truncated at fixed boundaries
    Assumes truncation happens mid-entry without clear delimiters
    """
    
    with open(input_file, 'r', encoding='utf-8', errors='replace') as inf, \
         open(output_file, 'w', encoding='utf-8') as outf:
        
        buffer = ""
        
        for line in inf:
            line = line.rstrip('\n')
            
            # Check if this line was truncated
            if len(line) == max_length:
                # Likely truncated - buffer it
                buffer += line
            else:
                # Complete line or continuation
                if buffer:
                    # Flush buffer with this line
                    outf.write(buffer + line + '\n')
                    buffer = ""
                else:
                    # Regular complete line
                    outf.write(line + '\n')
        
        # Flush any remaining buffer
        if buffer:
            outf.write(buffer + '\n')

if __name__ == "__main__":
    if len(sys.argv) < 3:
        print("Usage: python3 merge_truncated.py <input> <output> [max_length]")
        sys.exit(1)
    
    input_file = sys.argv[1]
    output_file = sys.argv[2]
    max_length = int(sys.argv[3]) if len(sys.argv) > 3 else 4096
    
    merge_truncated_segments(input_file, output_file, max_length)
    print(f"Merged segments written to {output_file}")
```

### Recovering Truncated JSON Logs

**Repair truncated JSON entries**:

```python
#!/usr/bin/env python3
import json
import sys
import re

def repair_json_log(input_file, output_file):
    """Attempt to repair truncated JSON log entries"""
    
    with open(input_file, 'r', encoding='utf-8', errors='replace') as inf, \
         open(output_file, 'w', encoding='utf-8') as outf:
        
        buffer = ""
        line_num = 0
        
        for line in inf:
            line_num += 1
            buffer += line.strip()
            
            try:
                # Try to parse accumulated buffer
                obj = json.loads(buffer)
                # Success - write and reset
                outf.write(json.dumps(obj) + '\n')
                buffer = ""
            except json.JSONDecodeError as e:
                # Check if we can repair
                if "Expecting" in str(e) or "Unterminated" in str(e):
                    # Try adding closing brackets
                    open_braces = buffer.count('{')
                    close_braces = buffer.count('}')
                    open_brackets = buffer.count('[')
                    close_brackets = buffer.count(']')
                    
                    # Calculate what's missing
                    missing_braces = open_braces - close_braces
                    missing_brackets = open_brackets - close_brackets
                    
                    # Attempt repair
                    repaired = buffer + (']' * missing_brackets) + ('}' * missing_braces)
                    
                    try:
                        obj = json.loads(repaired)
                        # Repair successful
                        outf.write(json.dumps(obj) + ' [REPAIRED]\n')
                        buffer = ""
                    except:
                        # Repair failed - continue buffering
                        continue
                else:
                    # Unrecoverable error - skip this entry
                    print(f"Line {line_num}: Unrecoverable JSON error", file=sys.stderr)
                    buffer = ""

if __name__ == "__main__":
    if len(sys.argv) < 3:
        print("Usage: python3 repair_json.py <input> <output>")
        sys.exit(1)
    
    repair_json_log(sys.argv[1], sys.argv[2])
```

### Statistical Reconstruction

**When complete recovery isn't possible, use statistical methods**:

```python
#!/usr/bin/env python3
import sys
import re
from collections import Counter

def analyze_truncation_patterns(filename):
    """Analyze truncation patterns to infer missing data"""
    
    line_endings = []
    truncated_fields = Counter()
    
    with open(filename, 'r', encoding='utf-8', errors='replace') as f:
        for line in f:
            # Check what field the line ends with
            # Common log format: timestamp level source message
            parts = line.strip().split()
            
            if len(parts) < 4:
                # Likely truncated
                truncated_fields[len(parts)] += 1
                line_endings.append(parts[-1] if parts else "")
    
    print("TRUNCATION PATTERN ANALYSIS")
    print("=" * 60)
    print(f"\nField count distribution in truncated lines:")
    for field_count, occurrences in sorted(truncated_fields.items()):
        print(f"  {field_count} fields: {occurrences} lines")
    
    print(f"\nMost common line endings (last token):")
    for ending, count in Counter(line_endings).most_common
```